{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. No RevIN](#1-no-revin-instanse-normalization)\n",
    "- [2. No channel-independence (Channel-Mixing)](#2-no-channel-independence-channel-mixing)\n",
    "- [3. No Patching](#3-no-patching)\n",
    "- [4. Time series decomposition](#4-ts-decomposition)\n",
    "\n",
    "Ablation study on PatchTST components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. No RevIN (Instanse Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "seq_lens = [512, 512, 336, 168, 168]\n",
    "\n",
    "model = \"PatchTST\"\n",
    "loss = \"MSE\"\n",
    "itr=1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_revin.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_DE_336_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1222990\n",
      "\tspeed: 0.0613s/iter; left time: 1365.9819s\n",
      "\titers: 200, epoch: 1 | loss: 0.1122023\n",
      "\tspeed: 0.0312s/iter; left time: 693.7261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.58s\n",
      "Steps: 224 | Train Loss: 0.1267766 Vali Loss: 0.0935439 Test Loss: 0.0950520\n",
      "Validation loss decreased (inf --> 0.093544).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0419376\n",
      "\tspeed: 0.0607s/iter; left time: 1339.2066s\n",
      "\titers: 200, epoch: 2 | loss: 0.0284766\n",
      "\tspeed: 0.0316s/iter; left time: 694.4481s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.37s\n",
      "Steps: 224 | Train Loss: 0.0480114 Vali Loss: 0.0276108 Test Loss: 0.0294567\n",
      "Validation loss decreased (0.093544 --> 0.027611).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0220861\n",
      "\tspeed: 0.0613s/iter; left time: 1338.8868s\n",
      "\titers: 200, epoch: 3 | loss: 0.0211231\n",
      "\tspeed: 0.0317s/iter; left time: 690.3014s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.37s\n",
      "Steps: 224 | Train Loss: 0.0230827 Vali Loss: 0.0250716 Test Loss: 0.0266407\n",
      "Validation loss decreased (0.027611 --> 0.025072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0220476\n",
      "\tspeed: 0.0602s/iter; left time: 1301.6703s\n",
      "\titers: 200, epoch: 4 | loss: 0.0232012\n",
      "\tspeed: 0.0317s/iter; left time: 682.4173s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 224 | Train Loss: 0.0197940 Vali Loss: 0.0232434 Test Loss: 0.0251538\n",
      "Validation loss decreased (0.025072 --> 0.023243).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0189569\n",
      "\tspeed: 0.0614s/iter; left time: 1314.2460s\n",
      "\titers: 200, epoch: 5 | loss: 0.0175613\n",
      "\tspeed: 0.0324s/iter; left time: 689.2760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.47s\n",
      "Steps: 224 | Train Loss: 0.0180648 Vali Loss: 0.0219851 Test Loss: 0.0238013\n",
      "Validation loss decreased (0.023243 --> 0.021985).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0212432\n",
      "\tspeed: 0.0611s/iter; left time: 1294.1797s\n",
      "\titers: 200, epoch: 6 | loss: 0.0167787\n",
      "\tspeed: 0.0318s/iter; left time: 670.6090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 224 | Train Loss: 0.0171204 Vali Loss: 0.0217660 Test Loss: 0.0235090\n",
      "Validation loss decreased (0.021985 --> 0.021766).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0163779\n",
      "\tspeed: 0.0619s/iter; left time: 1296.7421s\n",
      "\titers: 200, epoch: 7 | loss: 0.0178758\n",
      "\tspeed: 0.0320s/iter; left time: 667.8398s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 224 | Train Loss: 0.0164427 Vali Loss: 0.0213234 Test Loss: 0.0231106\n",
      "Validation loss decreased (0.021766 --> 0.021323).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0153116\n",
      "\tspeed: 0.0618s/iter; left time: 1281.3828s\n",
      "\titers: 200, epoch: 8 | loss: 0.0156371\n",
      "\tspeed: 0.0318s/iter; left time: 657.0754s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.43s\n",
      "Steps: 224 | Train Loss: 0.0160626 Vali Loss: 0.0215641 Test Loss: 0.0235629\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0175868\n",
      "\tspeed: 0.0613s/iter; left time: 1257.8290s\n",
      "\titers: 200, epoch: 9 | loss: 0.0172233\n",
      "\tspeed: 0.0321s/iter; left time: 655.4217s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 224 | Train Loss: 0.0157584 Vali Loss: 0.0211341 Test Loss: 0.0230975\n",
      "Validation loss decreased (0.021323 --> 0.021134).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0164381\n",
      "\tspeed: 0.0615s/iter; left time: 1247.0120s\n",
      "\titers: 200, epoch: 10 | loss: 0.0152910\n",
      "\tspeed: 0.0320s/iter; left time: 645.5465s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 224 | Train Loss: 0.0154029 Vali Loss: 0.0212651 Test Loss: 0.0235113\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0129175\n",
      "\tspeed: 0.0606s/iter; left time: 1215.8429s\n",
      "\titers: 200, epoch: 11 | loss: 0.0147491\n",
      "\tspeed: 0.0321s/iter; left time: 640.5145s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.43s\n",
      "Steps: 224 | Train Loss: 0.0152441 Vali Loss: 0.0207131 Test Loss: 0.0226023\n",
      "Validation loss decreased (0.021134 --> 0.020713).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0151582\n",
      "\tspeed: 0.0617s/iter; left time: 1223.1939s\n",
      "\titers: 200, epoch: 12 | loss: 0.0152677\n",
      "\tspeed: 0.0321s/iter; left time: 633.5226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.45s\n",
      "Steps: 224 | Train Loss: 0.0150385 Vali Loss: 0.0210057 Test Loss: 0.0228386\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0136673\n",
      "\tspeed: 0.0604s/iter; left time: 1185.0591s\n",
      "\titers: 200, epoch: 13 | loss: 0.0141618\n",
      "\tspeed: 0.0330s/iter; left time: 643.2613s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.50s\n",
      "Steps: 224 | Train Loss: 0.0149006 Vali Loss: 0.0205923 Test Loss: 0.0225011\n",
      "Validation loss decreased (0.020713 --> 0.020592).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0141812\n",
      "\tspeed: 0.0610s/iter; left time: 1182.7181s\n",
      "\titers: 200, epoch: 14 | loss: 0.0148937\n",
      "\tspeed: 0.0321s/iter; left time: 618.7816s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.45s\n",
      "Steps: 224 | Train Loss: 0.0147474 Vali Loss: 0.0204325 Test Loss: 0.0223470\n",
      "Validation loss decreased (0.020592 --> 0.020433).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0142198\n",
      "\tspeed: 0.0626s/iter; left time: 1199.0431s\n",
      "\titers: 200, epoch: 15 | loss: 0.0126263\n",
      "\tspeed: 0.0321s/iter; left time: 612.3347s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.47s\n",
      "Steps: 224 | Train Loss: 0.0146567 Vali Loss: 0.0203714 Test Loss: 0.0223274\n",
      "Validation loss decreased (0.020433 --> 0.020371).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0147880\n",
      "\tspeed: 0.0618s/iter; left time: 1170.3107s\n",
      "\titers: 200, epoch: 16 | loss: 0.0144410\n",
      "\tspeed: 0.0323s/iter; left time: 608.6422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.51s\n",
      "Steps: 224 | Train Loss: 0.0146212 Vali Loss: 0.0207287 Test Loss: 0.0225603\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0148663\n",
      "\tspeed: 0.0618s/iter; left time: 1157.2427s\n",
      "\titers: 200, epoch: 17 | loss: 0.0141908\n",
      "\tspeed: 0.0322s/iter; left time: 599.9513s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.58s\n",
      "Steps: 224 | Train Loss: 0.0144554 Vali Loss: 0.0202148 Test Loss: 0.0221464\n",
      "Validation loss decreased (0.020371 --> 0.020215).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0148504\n",
      "\tspeed: 0.0634s/iter; left time: 1173.1980s\n",
      "\titers: 200, epoch: 18 | loss: 0.0172588\n",
      "\tspeed: 0.0330s/iter; left time: 607.7259s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.58s\n",
      "Steps: 224 | Train Loss: 0.0144195 Vali Loss: 0.0200641 Test Loss: 0.0218430\n",
      "Validation loss decreased (0.020215 --> 0.020064).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0126696\n",
      "\tspeed: 0.0628s/iter; left time: 1146.6626s\n",
      "\titers: 200, epoch: 19 | loss: 0.0133310\n",
      "\tspeed: 0.0325s/iter; left time: 591.0060s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 224 | Train Loss: 0.0143151 Vali Loss: 0.0205285 Test Loss: 0.0225459\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0141876\n",
      "\tspeed: 0.0620s/iter; left time: 1118.9593s\n",
      "\titers: 200, epoch: 20 | loss: 0.0154408\n",
      "\tspeed: 0.0325s/iter; left time: 582.6493s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.53s\n",
      "Steps: 224 | Train Loss: 0.0142808 Vali Loss: 0.0203298 Test Loss: 0.0220595\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0120954\n",
      "\tspeed: 0.0624s/iter; left time: 1112.0831s\n",
      "\titers: 200, epoch: 21 | loss: 0.0147030\n",
      "\tspeed: 0.0325s/iter; left time: 575.6891s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 224 | Train Loss: 0.0142541 Vali Loss: 0.0202322 Test Loss: 0.0224036\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0121426\n",
      "\tspeed: 0.0614s/iter; left time: 1079.7388s\n",
      "\titers: 200, epoch: 22 | loss: 0.0135340\n",
      "\tspeed: 0.0337s/iter; left time: 589.0041s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.65s\n",
      "Steps: 224 | Train Loss: 0.0141983 Vali Loss: 0.0201447 Test Loss: 0.0221023\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0145164\n",
      "\tspeed: 0.0613s/iter; left time: 1065.5831s\n",
      "\titers: 200, epoch: 23 | loss: 0.0128908\n",
      "\tspeed: 0.0324s/iter; left time: 559.3210s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:07.52s\n",
      "Steps: 224 | Train Loss: 0.0141312 Vali Loss: 0.0203349 Test Loss: 0.0227068\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0137177\n",
      "\tspeed: 0.0605s/iter; left time: 1036.7531s\n",
      "\titers: 200, epoch: 24 | loss: 0.0136329\n",
      "\tspeed: 0.0320s/iter; left time: 545.3447s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 224 | Train Loss: 0.0140988 Vali Loss: 0.0202100 Test Loss: 0.0221388\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0139595\n",
      "\tspeed: 0.0582s/iter; left time: 985.3597s\n",
      "\titers: 200, epoch: 25 | loss: 0.0172756\n",
      "\tspeed: 0.0322s/iter; left time: 541.0742s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 224 | Train Loss: 0.0140711 Vali Loss: 0.0200398 Test Loss: 0.0219033\n",
      "Validation loss decreased (0.020064 --> 0.020040).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0137785\n",
      "\tspeed: 0.0594s/iter; left time: 991.2793s\n",
      "\titers: 200, epoch: 26 | loss: 0.0135769\n",
      "\tspeed: 0.0319s/iter; left time: 529.0913s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 224 | Train Loss: 0.0140351 Vali Loss: 0.0200323 Test Loss: 0.0218325\n",
      "Validation loss decreased (0.020040 --> 0.020032).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0150175\n",
      "\tspeed: 0.0582s/iter; left time: 958.7707s\n",
      "\titers: 200, epoch: 27 | loss: 0.0154345\n",
      "\tspeed: 0.0323s/iter; left time: 529.1345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:07.40s\n",
      "Steps: 224 | Train Loss: 0.0139935 Vali Loss: 0.0199558 Test Loss: 0.0218374\n",
      "Validation loss decreased (0.020032 --> 0.019956).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0122481\n",
      "\tspeed: 0.0620s/iter; left time: 1008.3606s\n",
      "\titers: 200, epoch: 28 | loss: 0.0150768\n",
      "\tspeed: 0.0320s/iter; left time: 517.3078s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:07.44s\n",
      "Steps: 224 | Train Loss: 0.0139888 Vali Loss: 0.0201497 Test Loss: 0.0222160\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0144535\n",
      "\tspeed: 0.0578s/iter; left time: 926.5491s\n",
      "\titers: 200, epoch: 29 | loss: 0.0119596\n",
      "\tspeed: 0.0318s/iter; left time: 506.6883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.31s\n",
      "Steps: 224 | Train Loss: 0.0139765 Vali Loss: 0.0200327 Test Loss: 0.0220907\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0161402\n",
      "\tspeed: 0.0594s/iter; left time: 938.5773s\n",
      "\titers: 200, epoch: 30 | loss: 0.0147173\n",
      "\tspeed: 0.0325s/iter; left time: 511.1246s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.52s\n",
      "Steps: 224 | Train Loss: 0.0139347 Vali Loss: 0.0199370 Test Loss: 0.0221591\n",
      "Validation loss decreased (0.019956 --> 0.019937).  Saving model ...\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0135855\n",
      "\tspeed: 0.0615s/iter; left time: 957.6345s\n",
      "\titers: 200, epoch: 31 | loss: 0.0129211\n",
      "\tspeed: 0.0329s/iter; left time: 508.6179s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:07.55s\n",
      "Steps: 224 | Train Loss: 0.0139453 Vali Loss: 0.0200089 Test Loss: 0.0217233\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0130386\n",
      "\tspeed: 0.0644s/iter; left time: 988.5046s\n",
      "\titers: 200, epoch: 32 | loss: 0.0151058\n",
      "\tspeed: 0.0357s/iter; left time: 544.7457s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:08.07s\n",
      "Steps: 224 | Train Loss: 0.0139052 Vali Loss: 0.0198388 Test Loss: 0.0217656\n",
      "Validation loss decreased (0.019937 --> 0.019839).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0151810\n",
      "\tspeed: 0.0651s/iter; left time: 985.8966s\n",
      "\titers: 200, epoch: 33 | loss: 0.0121507\n",
      "\tspeed: 0.0357s/iter; left time: 535.9345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:08.08s\n",
      "Steps: 224 | Train Loss: 0.0138984 Vali Loss: 0.0199295 Test Loss: 0.0219177\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0138505\n",
      "\tspeed: 0.0660s/iter; left time: 983.3781s\n",
      "\titers: 200, epoch: 34 | loss: 0.0144497\n",
      "\tspeed: 0.0342s/iter; left time: 505.8203s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.99s\n",
      "Steps: 224 | Train Loss: 0.0138961 Vali Loss: 0.0199464 Test Loss: 0.0218640\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0145499\n",
      "\tspeed: 0.0688s/iter; left time: 1010.8486s\n",
      "\titers: 200, epoch: 35 | loss: 0.0122951\n",
      "\tspeed: 0.0339s/iter; left time: 494.1309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:08.05s\n",
      "Steps: 224 | Train Loss: 0.0138513 Vali Loss: 0.0198913 Test Loss: 0.0217917\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0140391\n",
      "\tspeed: 0.0666s/iter; left time: 962.9506s\n",
      "\titers: 200, epoch: 36 | loss: 0.0156457\n",
      "\tspeed: 0.0347s/iter; left time: 497.6383s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 224 | Train Loss: 0.0138767 Vali Loss: 0.0201202 Test Loss: 0.0219720\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0174159\n",
      "\tspeed: 0.0648s/iter; left time: 923.0512s\n",
      "\titers: 200, epoch: 37 | loss: 0.0135666\n",
      "\tspeed: 0.0340s/iter; left time: 481.0490s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 224 | Train Loss: 0.0138484 Vali Loss: 0.0199732 Test Loss: 0.0218658\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0138687\n",
      "\tspeed: 0.0650s/iter; left time: 910.7446s\n",
      "\titers: 200, epoch: 38 | loss: 0.0129105\n",
      "\tspeed: 0.0345s/iter; left time: 479.5221s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:08.02s\n",
      "Steps: 224 | Train Loss: 0.0138696 Vali Loss: 0.0199889 Test Loss: 0.0218638\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0153570\n",
      "\tspeed: 0.0644s/iter; left time: 887.8654s\n",
      "\titers: 200, epoch: 39 | loss: 0.0127968\n",
      "\tspeed: 0.0344s/iter; left time: 470.9072s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 224 | Train Loss: 0.0138804 Vali Loss: 0.0198075 Test Loss: 0.0218017\n",
      "Validation loss decreased (0.019839 --> 0.019807).  Saving model ...\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0122012\n",
      "\tspeed: 0.0659s/iter; left time: 893.5174s\n",
      "\titers: 200, epoch: 40 | loss: 0.0156413\n",
      "\tspeed: 0.0365s/iter; left time: 490.9232s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:08.17s\n",
      "Steps: 224 | Train Loss: 0.0138368 Vali Loss: 0.0199014 Test Loss: 0.0218669\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0124090\n",
      "\tspeed: 0.0655s/iter; left time: 873.6033s\n",
      "\titers: 200, epoch: 41 | loss: 0.0136367\n",
      "\tspeed: 0.0337s/iter; left time: 446.8212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 224 | Train Loss: 0.0138275 Vali Loss: 0.0198964 Test Loss: 0.0220496\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0127942\n",
      "\tspeed: 0.0647s/iter; left time: 848.3028s\n",
      "\titers: 200, epoch: 42 | loss: 0.0153207\n",
      "\tspeed: 0.0347s/iter; left time: 452.1709s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:07.90s\n",
      "Steps: 224 | Train Loss: 0.0138310 Vali Loss: 0.0199408 Test Loss: 0.0219102\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0150594\n",
      "\tspeed: 0.0666s/iter; left time: 858.7574s\n",
      "\titers: 200, epoch: 43 | loss: 0.0142191\n",
      "\tspeed: 0.0352s/iter; left time: 450.0219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:08.08s\n",
      "Steps: 224 | Train Loss: 0.0138246 Vali Loss: 0.0199161 Test Loss: 0.0220573\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0119022\n",
      "\tspeed: 0.0658s/iter; left time: 833.1813s\n",
      "\titers: 200, epoch: 44 | loss: 0.0138348\n",
      "\tspeed: 0.0340s/iter; left time: 427.6046s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:07.86s\n",
      "Steps: 224 | Train Loss: 0.0137990 Vali Loss: 0.0199644 Test Loss: 0.0219828\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0142369\n",
      "\tspeed: 0.0663s/iter; left time: 825.3889s\n",
      "\titers: 200, epoch: 45 | loss: 0.0147083\n",
      "\tspeed: 0.0339s/iter; left time: 418.2626s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 224 | Train Loss: 0.0138106 Vali Loss: 0.0199118 Test Loss: 0.0219210\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0138345\n",
      "\tspeed: 0.0656s/iter; left time: 802.0637s\n",
      "\titers: 200, epoch: 46 | loss: 0.0150223\n",
      "\tspeed: 0.0350s/iter; left time: 424.0148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:08.06s\n",
      "Steps: 224 | Train Loss: 0.0137858 Vali Loss: 0.0199091 Test Loss: 0.0218749\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0139790\n",
      "\tspeed: 0.0641s/iter; left time: 768.4079s\n",
      "\titers: 200, epoch: 47 | loss: 0.0130415\n",
      "\tspeed: 0.0349s/iter; left time: 415.6390s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:07.96s\n",
      "Steps: 224 | Train Loss: 0.0137935 Vali Loss: 0.0199717 Test Loss: 0.0220268\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0148001\n",
      "\tspeed: 0.0634s/iter; left time: 746.6352s\n",
      "\titers: 200, epoch: 48 | loss: 0.0131510\n",
      "\tspeed: 0.0338s/iter; left time: 395.0902s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:07.81s\n",
      "Steps: 224 | Train Loss: 0.0137964 Vali Loss: 0.0198949 Test Loss: 0.0219121\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0140426\n",
      "\tspeed: 0.0647s/iter; left time: 747.0507s\n",
      "\titers: 200, epoch: 49 | loss: 0.0147091\n",
      "\tspeed: 0.0333s/iter; left time: 381.6140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 224 | Train Loss: 0.0137915 Vali Loss: 0.0198982 Test Loss: 0.0218868\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02180168777704239, rmse:0.1476539522409439, mae:0.09520922601222992, rse:0.5210912227630615\n",
      "Intermediate time for DE and pred_len 24: 00h:08m:00.96s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_DE_512_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1211567\n",
      "\tspeed: 0.0795s/iter; left time: 1757.5864s\n",
      "\titers: 200, epoch: 1 | loss: 0.1046408\n",
      "\tspeed: 0.0497s/iter; left time: 1094.2503s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 222 | Train Loss: 0.1242929 Vali Loss: 0.0921334 Test Loss: 0.0956747\n",
      "Validation loss decreased (inf --> 0.092133).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0427316\n",
      "\tspeed: 0.0925s/iter; left time: 2024.8100s\n",
      "\titers: 200, epoch: 2 | loss: 0.0360543\n",
      "\tspeed: 0.0503s/iter; left time: 1095.4724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 222 | Train Loss: 0.0495366 Vali Loss: 0.0375714 Test Loss: 0.0424508\n",
      "Validation loss decreased (0.092133 --> 0.037571).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0282392\n",
      "\tspeed: 0.0945s/iter; left time: 2047.2490s\n",
      "\titers: 200, epoch: 3 | loss: 0.0292974\n",
      "\tspeed: 0.0500s/iter; left time: 1076.9347s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 222 | Train Loss: 0.0303366 Vali Loss: 0.0345866 Test Loss: 0.0395371\n",
      "Validation loss decreased (0.037571 --> 0.034587).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0250123\n",
      "\tspeed: 0.0925s/iter; left time: 1982.9671s\n",
      "\titers: 200, epoch: 4 | loss: 0.0266757\n",
      "\tspeed: 0.0508s/iter; left time: 1083.4839s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.52s\n",
      "Steps: 222 | Train Loss: 0.0267789 Vali Loss: 0.0351737 Test Loss: 0.0422639\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0277342\n",
      "\tspeed: 0.0912s/iter; left time: 1934.9650s\n",
      "\titers: 200, epoch: 5 | loss: 0.0253331\n",
      "\tspeed: 0.0500s/iter; left time: 1055.3085s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 222 | Train Loss: 0.0252380 Vali Loss: 0.0339698 Test Loss: 0.0410371\n",
      "Validation loss decreased (0.034587 --> 0.033970).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0240003\n",
      "\tspeed: 0.0936s/iter; left time: 1964.8351s\n",
      "\titers: 200, epoch: 6 | loss: 0.0239523\n",
      "\tspeed: 0.0490s/iter; left time: 1023.7036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 222 | Train Loss: 0.0244408 Vali Loss: 0.0329343 Test Loss: 0.0393448\n",
      "Validation loss decreased (0.033970 --> 0.032934).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0246434\n",
      "\tspeed: 0.0935s/iter; left time: 1941.9605s\n",
      "\titers: 200, epoch: 7 | loss: 0.0262259\n",
      "\tspeed: 0.0501s/iter; left time: 1034.9272s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.39s\n",
      "Steps: 222 | Train Loss: 0.0240215 Vali Loss: 0.0328176 Test Loss: 0.0386598\n",
      "Validation loss decreased (0.032934 --> 0.032818).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0231876\n",
      "\tspeed: 0.0922s/iter; left time: 1894.9070s\n",
      "\titers: 200, epoch: 8 | loss: 0.0238847\n",
      "\tspeed: 0.0501s/iter; left time: 1024.7123s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.35s\n",
      "Steps: 222 | Train Loss: 0.0236295 Vali Loss: 0.0320381 Test Loss: 0.0376855\n",
      "Validation loss decreased (0.032818 --> 0.032038).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0231493\n",
      "\tspeed: 0.0995s/iter; left time: 2022.9142s\n",
      "\titers: 200, epoch: 9 | loss: 0.0213140\n",
      "\tspeed: 0.0512s/iter; left time: 1036.3602s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.70s\n",
      "Steps: 222 | Train Loss: 0.0233542 Vali Loss: 0.0310939 Test Loss: 0.0364179\n",
      "Validation loss decreased (0.032038 --> 0.031094).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0240150\n",
      "\tspeed: 0.0930s/iter; left time: 1869.0709s\n",
      "\titers: 200, epoch: 10 | loss: 0.0250064\n",
      "\tspeed: 0.0492s/iter; left time: 985.1227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.34s\n",
      "Steps: 222 | Train Loss: 0.0230431 Vali Loss: 0.0318775 Test Loss: 0.0378130\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0221963\n",
      "\tspeed: 0.0931s/iter; left time: 1851.7991s\n",
      "\titers: 200, epoch: 11 | loss: 0.0231455\n",
      "\tspeed: 0.0516s/iter; left time: 1020.6763s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 222 | Train Loss: 0.0229509 Vali Loss: 0.0310655 Test Loss: 0.0365913\n",
      "Validation loss decreased (0.031094 --> 0.031065).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0235978\n",
      "\tspeed: 0.0920s/iter; left time: 1808.2538s\n",
      "\titers: 200, epoch: 12 | loss: 0.0198639\n",
      "\tspeed: 0.0506s/iter; left time: 988.9222s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 222 | Train Loss: 0.0227393 Vali Loss: 0.0312596 Test Loss: 0.0370144\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0211119\n",
      "\tspeed: 0.0918s/iter; left time: 1783.4331s\n",
      "\titers: 200, epoch: 13 | loss: 0.0218012\n",
      "\tspeed: 0.0493s/iter; left time: 952.4029s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 222 | Train Loss: 0.0226356 Vali Loss: 0.0310768 Test Loss: 0.0369859\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0216071\n",
      "\tspeed: 0.0907s/iter; left time: 1743.7480s\n",
      "\titers: 200, epoch: 14 | loss: 0.0224764\n",
      "\tspeed: 0.0493s/iter; left time: 942.5669s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0224288 Vali Loss: 0.0311917 Test Loss: 0.0372712\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0242154\n",
      "\tspeed: 0.0918s/iter; left time: 1742.9386s\n",
      "\titers: 200, epoch: 15 | loss: 0.0240646\n",
      "\tspeed: 0.0489s/iter; left time: 924.6183s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0224298 Vali Loss: 0.0313455 Test Loss: 0.0377980\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0246799\n",
      "\tspeed: 0.0916s/iter; left time: 1719.4874s\n",
      "\titers: 200, epoch: 16 | loss: 0.0186449\n",
      "\tspeed: 0.0502s/iter; left time: 936.7278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 222 | Train Loss: 0.0222451 Vali Loss: 0.0310605 Test Loss: 0.0372748\n",
      "Validation loss decreased (0.031065 --> 0.031061).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0223297\n",
      "\tspeed: 0.0910s/iter; left time: 1687.8289s\n",
      "\titers: 200, epoch: 17 | loss: 0.0224854\n",
      "\tspeed: 0.0510s/iter; left time: 941.3186s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0221587 Vali Loss: 0.0311874 Test Loss: 0.0372214\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0234007\n",
      "\tspeed: 0.0929s/iter; left time: 1703.3257s\n",
      "\titers: 200, epoch: 18 | loss: 0.0206719\n",
      "\tspeed: 0.0505s/iter; left time: 920.2323s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0221142 Vali Loss: 0.0305699 Test Loss: 0.0367941\n",
      "Validation loss decreased (0.031061 --> 0.030570).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0215925\n",
      "\tspeed: 0.0941s/iter; left time: 1703.7624s\n",
      "\titers: 200, epoch: 19 | loss: 0.0215316\n",
      "\tspeed: 0.0504s/iter; left time: 907.3937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 222 | Train Loss: 0.0219753 Vali Loss: 0.0307330 Test Loss: 0.0371570\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0211730\n",
      "\tspeed: 0.0966s/iter; left time: 1727.7496s\n",
      "\titers: 200, epoch: 20 | loss: 0.0200390\n",
      "\tspeed: 0.0491s/iter; left time: 872.9667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.38s\n",
      "Steps: 222 | Train Loss: 0.0218705 Vali Loss: 0.0308751 Test Loss: 0.0372903\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0211775\n",
      "\tspeed: 0.0909s/iter; left time: 1604.8162s\n",
      "\titers: 200, epoch: 21 | loss: 0.0211960\n",
      "\tspeed: 0.0496s/iter; left time: 871.0781s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:11.31s\n",
      "Steps: 222 | Train Loss: 0.0218680 Vali Loss: 0.0309477 Test Loss: 0.0376379\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0190556\n",
      "\tspeed: 0.0917s/iter; left time: 1598.8915s\n",
      "\titers: 200, epoch: 22 | loss: 0.0211424\n",
      "\tspeed: 0.0499s/iter; left time: 865.7267s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0218106 Vali Loss: 0.0301766 Test Loss: 0.0363203\n",
      "Validation loss decreased (0.030570 --> 0.030177).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0218515\n",
      "\tspeed: 0.0932s/iter; left time: 1605.3351s\n",
      "\titers: 200, epoch: 23 | loss: 0.0205890\n",
      "\tspeed: 0.0493s/iter; left time: 843.1417s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:11.39s\n",
      "Steps: 222 | Train Loss: 0.0217329 Vali Loss: 0.0304868 Test Loss: 0.0365636\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0217730\n",
      "\tspeed: 0.0908s/iter; left time: 1542.4728s\n",
      "\titers: 200, epoch: 24 | loss: 0.0215464\n",
      "\tspeed: 0.0504s/iter; left time: 851.6977s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.38s\n",
      "Steps: 222 | Train Loss: 0.0216485 Vali Loss: 0.0305330 Test Loss: 0.0369576\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0208029\n",
      "\tspeed: 0.0904s/iter; left time: 1516.2784s\n",
      "\titers: 200, epoch: 25 | loss: 0.0230684\n",
      "\tspeed: 0.0489s/iter; left time: 815.2146s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0216396 Vali Loss: 0.0307227 Test Loss: 0.0374036\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0216803\n",
      "\tspeed: 0.0894s/iter; left time: 1479.2788s\n",
      "\titers: 200, epoch: 26 | loss: 0.0221263\n",
      "\tspeed: 0.0510s/iter; left time: 838.8345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 222 | Train Loss: 0.0216196 Vali Loss: 0.0304883 Test Loss: 0.0369750\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0219150\n",
      "\tspeed: 0.0909s/iter; left time: 1484.9353s\n",
      "\titers: 200, epoch: 27 | loss: 0.0205100\n",
      "\tspeed: 0.0509s/iter; left time: 826.7998s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 222 | Train Loss: 0.0215644 Vali Loss: 0.0305989 Test Loss: 0.0369882\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0214528\n",
      "\tspeed: 0.0921s/iter; left time: 1483.0199s\n",
      "\titers: 200, epoch: 28 | loss: 0.0225728\n",
      "\tspeed: 0.0505s/iter; left time: 808.7824s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 222 | Train Loss: 0.0215246 Vali Loss: 0.0305688 Test Loss: 0.0372255\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0228054\n",
      "\tspeed: 0.0979s/iter; left time: 1555.4951s\n",
      "\titers: 200, epoch: 29 | loss: 0.0198161\n",
      "\tspeed: 0.0509s/iter; left time: 803.5571s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:11.65s\n",
      "Steps: 222 | Train Loss: 0.0214989 Vali Loss: 0.0308266 Test Loss: 0.0376735\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0225791\n",
      "\tspeed: 0.0898s/iter; left time: 1406.4823s\n",
      "\titers: 200, epoch: 30 | loss: 0.0206602\n",
      "\tspeed: 0.0501s/iter; left time: 779.8894s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0214766 Vali Loss: 0.0305522 Test Loss: 0.0371724\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0220604\n",
      "\tspeed: 0.0937s/iter; left time: 1446.7781s\n",
      "\titers: 200, epoch: 31 | loss: 0.0228703\n",
      "\tspeed: 0.0486s/iter; left time: 745.0544s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:11.25s\n",
      "Steps: 222 | Train Loss: 0.0214585 Vali Loss: 0.0305483 Test Loss: 0.0373443\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0206055\n",
      "\tspeed: 0.0893s/iter; left time: 1359.1688s\n",
      "\titers: 200, epoch: 32 | loss: 0.0194962\n",
      "\tspeed: 0.0502s/iter; left time: 758.6881s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:11.27s\n",
      "Steps: 222 | Train Loss: 0.0214005 Vali Loss: 0.0306938 Test Loss: 0.0374660\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.0363202802836895, rmse:0.19057880342006683, mae:0.1305510699748993, rse:0.6748781204223633\n",
      "Intermediate time for DE and pred_len 96: 00h:07m:48.74s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_DE_512_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1201970\n",
      "\tspeed: 0.0648s/iter; left time: 1431.4377s\n",
      "\titers: 200, epoch: 1 | loss: 0.1105937\n",
      "\tspeed: 0.0497s/iter; left time: 1092.4091s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.73s\n",
      "Steps: 222 | Train Loss: 0.1248253 Vali Loss: 0.0926596 Test Loss: 0.0964285\n",
      "Validation loss decreased (inf --> 0.092660).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0401608\n",
      "\tspeed: 0.1001s/iter; left time: 2190.7127s\n",
      "\titers: 200, epoch: 2 | loss: 0.0348200\n",
      "\tspeed: 0.0507s/iter; left time: 1104.4066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.61s\n",
      "Steps: 222 | Train Loss: 0.0505578 Vali Loss: 0.0382297 Test Loss: 0.0440547\n",
      "Validation loss decreased (0.092660 --> 0.038230).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0341885\n",
      "\tspeed: 0.0972s/iter; left time: 2105.0437s\n",
      "\titers: 200, epoch: 3 | loss: 0.0301632\n",
      "\tspeed: 0.0497s/iter; left time: 1072.4435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 222 | Train Loss: 0.0315709 Vali Loss: 0.0371159 Test Loss: 0.0438923\n",
      "Validation loss decreased (0.038230 --> 0.037116).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0303969\n",
      "\tspeed: 0.1027s/iter; left time: 2201.9701s\n",
      "\titers: 200, epoch: 4 | loss: 0.0268241\n",
      "\tspeed: 0.0523s/iter; left time: 1115.1075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.72s\n",
      "Steps: 222 | Train Loss: 0.0282432 Vali Loss: 0.0384147 Test Loss: 0.0474597\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0252197\n",
      "\tspeed: 0.0941s/iter; left time: 1995.4230s\n",
      "\titers: 200, epoch: 5 | loss: 0.0290112\n",
      "\tspeed: 0.0495s/iter; left time: 1044.3064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.48s\n",
      "Steps: 222 | Train Loss: 0.0270696 Vali Loss: 0.0356162 Test Loss: 0.0434251\n",
      "Validation loss decreased (0.037116 --> 0.035616).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0258138\n",
      "\tspeed: 0.0957s/iter; left time: 2008.1879s\n",
      "\titers: 200, epoch: 6 | loss: 0.0258394\n",
      "\tspeed: 0.0503s/iter; left time: 1051.2614s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 222 | Train Loss: 0.0261948 Vali Loss: 0.0369859 Test Loss: 0.0446513\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0253084\n",
      "\tspeed: 0.0927s/iter; left time: 1924.9626s\n",
      "\titers: 200, epoch: 7 | loss: 0.0254496\n",
      "\tspeed: 0.0508s/iter; left time: 1050.0359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.48s\n",
      "Steps: 222 | Train Loss: 0.0256539 Vali Loss: 0.0347947 Test Loss: 0.0414930\n",
      "Validation loss decreased (0.035616 --> 0.034795).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0247787\n",
      "\tspeed: 0.0940s/iter; left time: 1931.9962s\n",
      "\titers: 200, epoch: 8 | loss: 0.0252390\n",
      "\tspeed: 0.0504s/iter; left time: 1029.7618s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.46s\n",
      "Steps: 222 | Train Loss: 0.0252846 Vali Loss: 0.0341283 Test Loss: 0.0409065\n",
      "Validation loss decreased (0.034795 --> 0.034128).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0245752\n",
      "\tspeed: 0.0928s/iter; left time: 1886.6479s\n",
      "\titers: 200, epoch: 9 | loss: 0.0251687\n",
      "\tspeed: 0.0501s/iter; left time: 1013.2576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.35s\n",
      "Steps: 222 | Train Loss: 0.0250233 Vali Loss: 0.0326594 Test Loss: 0.0391561\n",
      "Validation loss decreased (0.034128 --> 0.032659).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0240354\n",
      "\tspeed: 0.0925s/iter; left time: 1860.0336s\n",
      "\titers: 200, epoch: 10 | loss: 0.0253937\n",
      "\tspeed: 0.0506s/iter; left time: 1012.6363s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0247593 Vali Loss: 0.0332664 Test Loss: 0.0404661\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0261423\n",
      "\tspeed: 0.0922s/iter; left time: 1833.5847s\n",
      "\titers: 200, epoch: 11 | loss: 0.0256753\n",
      "\tspeed: 0.0497s/iter; left time: 983.4680s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.36s\n",
      "Steps: 222 | Train Loss: 0.0245814 Vali Loss: 0.0325092 Test Loss: 0.0393523\n",
      "Validation loss decreased (0.032659 --> 0.032509).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0264353\n",
      "\tspeed: 0.0940s/iter; left time: 1848.8127s\n",
      "\titers: 200, epoch: 12 | loss: 0.0250716\n",
      "\tspeed: 0.0505s/iter; left time: 988.4411s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 222 | Train Loss: 0.0244160 Vali Loss: 0.0331964 Test Loss: 0.0405376\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0224045\n",
      "\tspeed: 0.0980s/iter; left time: 1905.0400s\n",
      "\titers: 200, epoch: 13 | loss: 0.0240942\n",
      "\tspeed: 0.0500s/iter; left time: 966.6507s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.38s\n",
      "Steps: 222 | Train Loss: 0.0241919 Vali Loss: 0.0327617 Test Loss: 0.0397237\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0224042\n",
      "\tspeed: 0.0930s/iter; left time: 1787.1382s\n",
      "\titers: 200, epoch: 14 | loss: 0.0228675\n",
      "\tspeed: 0.0511s/iter; left time: 977.4657s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 222 | Train Loss: 0.0240532 Vali Loss: 0.0325049 Test Loss: 0.0400462\n",
      "Validation loss decreased (0.032509 --> 0.032505).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0224264\n",
      "\tspeed: 0.0981s/iter; left time: 1864.0594s\n",
      "\titers: 200, epoch: 15 | loss: 0.0239447\n",
      "\tspeed: 0.0517s/iter; left time: 975.9017s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.59s\n",
      "Steps: 222 | Train Loss: 0.0238901 Vali Loss: 0.0326816 Test Loss: 0.0407697\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0228343\n",
      "\tspeed: 0.0941s/iter; left time: 1766.4159s\n",
      "\titers: 200, epoch: 16 | loss: 0.0244847\n",
      "\tspeed: 0.0502s/iter; left time: 936.7524s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 222 | Train Loss: 0.0238586 Vali Loss: 0.0328233 Test Loss: 0.0409972\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0249028\n",
      "\tspeed: 0.0937s/iter; left time: 1737.2823s\n",
      "\titers: 200, epoch: 17 | loss: 0.0245231\n",
      "\tspeed: 0.0498s/iter; left time: 918.8094s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 222 | Train Loss: 0.0236727 Vali Loss: 0.0318149 Test Loss: 0.0389934\n",
      "Validation loss decreased (0.032505 --> 0.031815).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0259864\n",
      "\tspeed: 0.0936s/iter; left time: 1714.8901s\n",
      "\titers: 200, epoch: 18 | loss: 0.0249082\n",
      "\tspeed: 0.0509s/iter; left time: 928.2427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.45s\n",
      "Steps: 222 | Train Loss: 0.0236133 Vali Loss: 0.0326205 Test Loss: 0.0401949\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0230845\n",
      "\tspeed: 0.0912s/iter; left time: 1651.2096s\n",
      "\titers: 200, epoch: 19 | loss: 0.0244532\n",
      "\tspeed: 0.0502s/iter; left time: 904.5764s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 222 | Train Loss: 0.0234732 Vali Loss: 0.0324181 Test Loss: 0.0406701\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0229646\n",
      "\tspeed: 0.0930s/iter; left time: 1663.2793s\n",
      "\titers: 200, epoch: 20 | loss: 0.0225817\n",
      "\tspeed: 0.0500s/iter; left time: 888.4947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0233922 Vali Loss: 0.0328225 Test Loss: 0.0413763\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0235294\n",
      "\tspeed: 0.0924s/iter; left time: 1631.6374s\n",
      "\titers: 200, epoch: 21 | loss: 0.0233960\n",
      "\tspeed: 0.0508s/iter; left time: 891.5378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 222 | Train Loss: 0.0233425 Vali Loss: 0.0323448 Test Loss: 0.0405806\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0227897\n",
      "\tspeed: 0.0918s/iter; left time: 1600.9618s\n",
      "\titers: 200, epoch: 22 | loss: 0.0233587\n",
      "\tspeed: 0.0511s/iter; left time: 885.7920s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:11.46s\n",
      "Steps: 222 | Train Loss: 0.0232400 Vali Loss: 0.0327108 Test Loss: 0.0415128\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0230084\n",
      "\tspeed: 0.0923s/iter; left time: 1589.5557s\n",
      "\titers: 200, epoch: 23 | loss: 0.0227019\n",
      "\tspeed: 0.0503s/iter; left time: 861.2833s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 222 | Train Loss: 0.0231599 Vali Loss: 0.0328264 Test Loss: 0.0417051\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0220565\n",
      "\tspeed: 0.0949s/iter; left time: 1612.9663s\n",
      "\titers: 200, epoch: 24 | loss: 0.0225118\n",
      "\tspeed: 0.0491s/iter; left time: 829.3603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0231256 Vali Loss: 0.0326759 Test Loss: 0.0414577\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0253219\n",
      "\tspeed: 0.0909s/iter; left time: 1525.4396s\n",
      "\titers: 200, epoch: 25 | loss: 0.0229667\n",
      "\tspeed: 0.0506s/iter; left time: 843.3038s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:11.45s\n",
      "Steps: 222 | Train Loss: 0.0230608 Vali Loss: 0.0324623 Test Loss: 0.0410861\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0217467\n",
      "\tspeed: 0.0909s/iter; left time: 1504.6154s\n",
      "\titers: 200, epoch: 26 | loss: 0.0222396\n",
      "\tspeed: 0.0505s/iter; left time: 830.3565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:11.34s\n",
      "Steps: 222 | Train Loss: 0.0230109 Vali Loss: 0.0326253 Test Loss: 0.0417797\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0220535\n",
      "\tspeed: 0.0938s/iter; left time: 1530.8813s\n",
      "\titers: 200, epoch: 27 | loss: 0.0242244\n",
      "\tspeed: 0.0510s/iter; left time: 827.1126s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 222 | Train Loss: 0.0229573 Vali Loss: 0.0326033 Test Loss: 0.0413879\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.0389934666454792, rmse:0.197467640042305, mae:0.1372816115617752, rse:0.6994462013244629\n",
      "Intermediate time for DE and pred_len 168: 00h:06m:38.81s\n",
      "Intermediate time for DE: 00h:22m:28.51s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1296732\n",
      "\tspeed: 0.0642s/iter; left time: 1424.7886s\n",
      "\titers: 200, epoch: 1 | loss: 0.1225555\n",
      "\tspeed: 0.0491s/iter; left time: 1085.7828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.54s\n",
      "Steps: 223 | Train Loss: 0.1316660 Vali Loss: 0.0944922 Test Loss: 0.1096129\n",
      "Validation loss decreased (inf --> 0.094492).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0377319\n",
      "\tspeed: 0.0930s/iter; left time: 2043.8811s\n",
      "\titers: 200, epoch: 2 | loss: 0.0261822\n",
      "\tspeed: 0.0498s/iter; left time: 1090.4178s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 223 | Train Loss: 0.0454864 Vali Loss: 0.0248178 Test Loss: 0.0346409\n",
      "Validation loss decreased (0.094492 --> 0.024818).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0243949\n",
      "\tspeed: 0.0902s/iter; left time: 1962.3430s\n",
      "\titers: 200, epoch: 3 | loss: 0.0203790\n",
      "\tspeed: 0.0503s/iter; left time: 1088.3158s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 223 | Train Loss: 0.0222196 Vali Loss: 0.0212921 Test Loss: 0.0278196\n",
      "Validation loss decreased (0.024818 --> 0.021292).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0207562\n",
      "\tspeed: 0.0921s/iter; left time: 1982.1556s\n",
      "\titers: 200, epoch: 4 | loss: 0.0200337\n",
      "\tspeed: 0.0494s/iter; left time: 1058.8152s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.29s\n",
      "Steps: 223 | Train Loss: 0.0183250 Vali Loss: 0.0207108 Test Loss: 0.0272615\n",
      "Validation loss decreased (0.021292 --> 0.020711).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0151180\n",
      "\tspeed: 0.0898s/iter; left time: 1914.2871s\n",
      "\titers: 200, epoch: 5 | loss: 0.0161550\n",
      "\tspeed: 0.0496s/iter; left time: 1052.0268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 223 | Train Loss: 0.0166715 Vali Loss: 0.0204913 Test Loss: 0.0278639\n",
      "Validation loss decreased (0.020711 --> 0.020491).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0140818\n",
      "\tspeed: 0.0897s/iter; left time: 1890.7865s\n",
      "\titers: 200, epoch: 6 | loss: 0.0173617\n",
      "\tspeed: 0.0496s/iter; left time: 1041.6864s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 223 | Train Loss: 0.0157911 Vali Loss: 0.0205434 Test Loss: 0.0280114\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0153887\n",
      "\tspeed: 0.0896s/iter; left time: 1869.9129s\n",
      "\titers: 200, epoch: 7 | loss: 0.0141236\n",
      "\tspeed: 0.0496s/iter; left time: 1030.0978s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.34s\n",
      "Steps: 223 | Train Loss: 0.0154214 Vali Loss: 0.0204262 Test Loss: 0.0280592\n",
      "Validation loss decreased (0.020491 --> 0.020426).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0162005\n",
      "\tspeed: 0.0900s/iter; left time: 1857.6455s\n",
      "\titers: 200, epoch: 8 | loss: 0.0151308\n",
      "\tspeed: 0.0483s/iter; left time: 992.8827s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 223 | Train Loss: 0.0151685 Vali Loss: 0.0205546 Test Loss: 0.0274767\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0159596\n",
      "\tspeed: 0.0912s/iter; left time: 1862.4431s\n",
      "\titers: 200, epoch: 9 | loss: 0.0138793\n",
      "\tspeed: 0.0501s/iter; left time: 1017.7224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 223 | Train Loss: 0.0149537 Vali Loss: 0.0201726 Test Loss: 0.0266926\n",
      "Validation loss decreased (0.020426 --> 0.020173).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0142701\n",
      "\tspeed: 0.0892s/iter; left time: 1800.5891s\n",
      "\titers: 200, epoch: 10 | loss: 0.0156937\n",
      "\tspeed: 0.0494s/iter; left time: 993.1446s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.29s\n",
      "Steps: 223 | Train Loss: 0.0147354 Vali Loss: 0.0200238 Test Loss: 0.0267262\n",
      "Validation loss decreased (0.020173 --> 0.020024).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0162956\n",
      "\tspeed: 0.0890s/iter; left time: 1776.5867s\n",
      "\titers: 200, epoch: 11 | loss: 0.0157453\n",
      "\tspeed: 0.0491s/iter; left time: 976.4388s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.22s\n",
      "Steps: 223 | Train Loss: 0.0147208 Vali Loss: 0.0202973 Test Loss: 0.0269148\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0120478\n",
      "\tspeed: 0.0902s/iter; left time: 1781.3780s\n",
      "\titers: 200, epoch: 12 | loss: 0.0157311\n",
      "\tspeed: 0.0494s/iter; left time: 969.8544s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.35s\n",
      "Steps: 223 | Train Loss: 0.0144875 Vali Loss: 0.0199512 Test Loss: 0.0266800\n",
      "Validation loss decreased (0.020024 --> 0.019951).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0137041\n",
      "\tspeed: 0.0914s/iter; left time: 1783.6364s\n",
      "\titers: 200, epoch: 13 | loss: 0.0147808\n",
      "\tspeed: 0.0507s/iter; left time: 984.8644s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 223 | Train Loss: 0.0143374 Vali Loss: 0.0200135 Test Loss: 0.0267759\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0139965\n",
      "\tspeed: 0.0905s/iter; left time: 1746.9507s\n",
      "\titers: 200, epoch: 14 | loss: 0.0139776\n",
      "\tspeed: 0.0487s/iter; left time: 934.5460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.21s\n",
      "Steps: 223 | Train Loss: 0.0142924 Vali Loss: 0.0201385 Test Loss: 0.0269570\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0141281\n",
      "\tspeed: 0.0904s/iter; left time: 1724.9560s\n",
      "\titers: 200, epoch: 15 | loss: 0.0135372\n",
      "\tspeed: 0.0496s/iter; left time: 940.9557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.25s\n",
      "Steps: 223 | Train Loss: 0.0141842 Vali Loss: 0.0197869 Test Loss: 0.0266150\n",
      "Validation loss decreased (0.019951 --> 0.019787).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0136077\n",
      "\tspeed: 0.0910s/iter; left time: 1715.0691s\n",
      "\titers: 200, epoch: 16 | loss: 0.0126454\n",
      "\tspeed: 0.0494s/iter; left time: 925.9890s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 223 | Train Loss: 0.0141280 Vali Loss: 0.0200392 Test Loss: 0.0273709\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0148440\n",
      "\tspeed: 0.0888s/iter; left time: 1653.9312s\n",
      "\titers: 200, epoch: 17 | loss: 0.0133331\n",
      "\tspeed: 0.0505s/iter; left time: 936.0830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.42s\n",
      "Steps: 223 | Train Loss: 0.0140650 Vali Loss: 0.0198028 Test Loss: 0.0266699\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0139154\n",
      "\tspeed: 0.0897s/iter; left time: 1650.5469s\n",
      "\titers: 200, epoch: 18 | loss: 0.0142051\n",
      "\tspeed: 0.0499s/iter; left time: 913.0987s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.39s\n",
      "Steps: 223 | Train Loss: 0.0139809 Vali Loss: 0.0200569 Test Loss: 0.0271863\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0132001\n",
      "\tspeed: 0.0898s/iter; left time: 1632.2960s\n",
      "\titers: 200, epoch: 19 | loss: 0.0152973\n",
      "\tspeed: 0.0497s/iter; left time: 899.7510s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.32s\n",
      "Steps: 223 | Train Loss: 0.0138989 Vali Loss: 0.0201339 Test Loss: 0.0271165\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0138798\n",
      "\tspeed: 0.0902s/iter; left time: 1620.6189s\n",
      "\titers: 200, epoch: 20 | loss: 0.0122192\n",
      "\tspeed: 0.0498s/iter; left time: 889.2672s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 223 | Train Loss: 0.0138612 Vali Loss: 0.0199807 Test Loss: 0.0271351\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0130000\n",
      "\tspeed: 0.0895s/iter; left time: 1588.4832s\n",
      "\titers: 200, epoch: 21 | loss: 0.0156088\n",
      "\tspeed: 0.0493s/iter; left time: 869.0453s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 223 | Train Loss: 0.0138289 Vali Loss: 0.0198444 Test Loss: 0.0270159\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0137042\n",
      "\tspeed: 0.0902s/iter; left time: 1580.2412s\n",
      "\titers: 200, epoch: 22 | loss: 0.0144793\n",
      "\tspeed: 0.0480s/iter; left time: 835.8598s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:11.02s\n",
      "Steps: 223 | Train Loss: 0.0137971 Vali Loss: 0.0200208 Test Loss: 0.0274525\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0149143\n",
      "\tspeed: 0.0905s/iter; left time: 1565.7847s\n",
      "\titers: 200, epoch: 23 | loss: 0.0120591\n",
      "\tspeed: 0.0504s/iter; left time: 866.6580s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 223 | Train Loss: 0.0137535 Vali Loss: 0.0198494 Test Loss: 0.0272250\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0139830\n",
      "\tspeed: 0.0920s/iter; left time: 1569.8186s\n",
      "\titers: 200, epoch: 24 | loss: 0.0142577\n",
      "\tspeed: 0.0505s/iter; left time: 856.8954s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 223 | Train Loss: 0.0137470 Vali Loss: 0.0199485 Test Loss: 0.0269865\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0144327\n",
      "\tspeed: 0.0920s/iter; left time: 1549.5189s\n",
      "\titers: 200, epoch: 25 | loss: 0.0133431\n",
      "\tspeed: 0.0492s/iter; left time: 824.1753s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 223 | Train Loss: 0.0136602 Vali Loss: 0.0199846 Test Loss: 0.0274140\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.026614956557750702, rmse:0.1631409078836441, mae:0.10954777151346207, rse:0.5627899169921875\n",
      "Intermediate time for GB and pred_len 24: 00h:05m:57.47s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1323240\n",
      "\tspeed: 0.0647s/iter; left time: 1430.2825s\n",
      "\titers: 200, epoch: 1 | loss: 0.1149986\n",
      "\tspeed: 0.0489s/iter; left time: 1076.0708s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.54s\n",
      "Steps: 222 | Train Loss: 0.1320870 Vali Loss: 0.0994640 Test Loss: 0.1168359\n",
      "Validation loss decreased (inf --> 0.099464).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0393860\n",
      "\tspeed: 0.0945s/iter; left time: 2067.8632s\n",
      "\titers: 200, epoch: 2 | loss: 0.0319159\n",
      "\tspeed: 0.0487s/iter; left time: 1059.7558s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.22s\n",
      "Steps: 222 | Train Loss: 0.0481805 Vali Loss: 0.0330101 Test Loss: 0.0472668\n",
      "Validation loss decreased (0.099464 --> 0.033010).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0261547\n",
      "\tspeed: 0.0906s/iter; left time: 1963.0412s\n",
      "\titers: 200, epoch: 3 | loss: 0.0259022\n",
      "\tspeed: 0.0494s/iter; left time: 1065.5595s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.32s\n",
      "Steps: 222 | Train Loss: 0.0273623 Vali Loss: 0.0302135 Test Loss: 0.0438073\n",
      "Validation loss decreased (0.033010 --> 0.030213).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0226433\n",
      "\tspeed: 0.0986s/iter; left time: 2113.7828s\n",
      "\titers: 200, epoch: 4 | loss: 0.0245221\n",
      "\tspeed: 0.0495s/iter; left time: 1055.2120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0241833 Vali Loss: 0.0304314 Test Loss: 0.0490374\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0251091\n",
      "\tspeed: 0.0916s/iter; left time: 1942.7026s\n",
      "\titers: 200, epoch: 5 | loss: 0.0233933\n",
      "\tspeed: 0.0503s/iter; left time: 1061.3379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 222 | Train Loss: 0.0231457 Vali Loss: 0.0307405 Test Loss: 0.0483087\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0226541\n",
      "\tspeed: 0.0910s/iter; left time: 1910.3219s\n",
      "\titers: 200, epoch: 6 | loss: 0.0229256\n",
      "\tspeed: 0.0499s/iter; left time: 1041.9831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 222 | Train Loss: 0.0226541 Vali Loss: 0.0304898 Test Loss: 0.0448133\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0223542\n",
      "\tspeed: 0.0918s/iter; left time: 1906.7280s\n",
      "\titers: 200, epoch: 7 | loss: 0.0214087\n",
      "\tspeed: 0.0498s/iter; left time: 1030.3454s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 222 | Train Loss: 0.0222953 Vali Loss: 0.0299704 Test Loss: 0.0423198\n",
      "Validation loss decreased (0.030213 --> 0.029970).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0221992\n",
      "\tspeed: 0.0919s/iter; left time: 1888.2188s\n",
      "\titers: 200, epoch: 8 | loss: 0.0222362\n",
      "\tspeed: 0.0497s/iter; left time: 1016.9253s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.34s\n",
      "Steps: 222 | Train Loss: 0.0220033 Vali Loss: 0.0299967 Test Loss: 0.0426890\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0217263\n",
      "\tspeed: 0.0906s/iter; left time: 1842.3125s\n",
      "\titers: 200, epoch: 9 | loss: 0.0209402\n",
      "\tspeed: 0.0505s/iter; left time: 1020.3849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 222 | Train Loss: 0.0217836 Vali Loss: 0.0296291 Test Loss: 0.0417050\n",
      "Validation loss decreased (0.029970 --> 0.029629).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0209449\n",
      "\tspeed: 0.0932s/iter; left time: 1873.5750s\n",
      "\titers: 200, epoch: 10 | loss: 0.0209933\n",
      "\tspeed: 0.0493s/iter; left time: 986.1424s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0215704 Vali Loss: 0.0298925 Test Loss: 0.0420922\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0209658\n",
      "\tspeed: 0.0925s/iter; left time: 1839.4380s\n",
      "\titers: 200, epoch: 11 | loss: 0.0199631\n",
      "\tspeed: 0.0498s/iter; left time: 985.4300s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0214490 Vali Loss: 0.0293610 Test Loss: 0.0405640\n",
      "Validation loss decreased (0.029629 --> 0.029361).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0216354\n",
      "\tspeed: 0.0979s/iter; left time: 1923.9231s\n",
      "\titers: 200, epoch: 12 | loss: 0.0204532\n",
      "\tspeed: 0.0493s/iter; left time: 963.9546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.31s\n",
      "Steps: 222 | Train Loss: 0.0212971 Vali Loss: 0.0297991 Test Loss: 0.0413098\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0192091\n",
      "\tspeed: 0.0949s/iter; left time: 1844.5210s\n",
      "\titers: 200, epoch: 13 | loss: 0.0213485\n",
      "\tspeed: 0.0501s/iter; left time: 968.8332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.36s\n",
      "Steps: 222 | Train Loss: 0.0212116 Vali Loss: 0.0300655 Test Loss: 0.0417966\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0204063\n",
      "\tspeed: 0.0917s/iter; left time: 1762.2465s\n",
      "\titers: 200, epoch: 14 | loss: 0.0211266\n",
      "\tspeed: 0.0493s/iter; left time: 941.9815s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.40s\n",
      "Steps: 222 | Train Loss: 0.0211267 Vali Loss: 0.0298509 Test Loss: 0.0411874\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0234450\n",
      "\tspeed: 0.0951s/iter; left time: 1805.6532s\n",
      "\titers: 200, epoch: 15 | loss: 0.0221137\n",
      "\tspeed: 0.0495s/iter; left time: 935.5911s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.24s\n",
      "Steps: 222 | Train Loss: 0.0210179 Vali Loss: 0.0296575 Test Loss: 0.0406219\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0207905\n",
      "\tspeed: 0.0913s/iter; left time: 1713.8075s\n",
      "\titers: 200, epoch: 16 | loss: 0.0185365\n",
      "\tspeed: 0.0497s/iter; left time: 928.3932s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 222 | Train Loss: 0.0209206 Vali Loss: 0.0299610 Test Loss: 0.0415653\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0202313\n",
      "\tspeed: 0.0898s/iter; left time: 1664.9106s\n",
      "\titers: 200, epoch: 17 | loss: 0.0208116\n",
      "\tspeed: 0.0511s/iter; left time: 942.5208s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.46s\n",
      "Steps: 222 | Train Loss: 0.0208105 Vali Loss: 0.0303820 Test Loss: 0.0420282\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0221342\n",
      "\tspeed: 0.0924s/iter; left time: 1694.0103s\n",
      "\titers: 200, epoch: 18 | loss: 0.0200218\n",
      "\tspeed: 0.0494s/iter; left time: 901.0935s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0207714 Vali Loss: 0.0299171 Test Loss: 0.0420240\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0193397\n",
      "\tspeed: 0.0903s/iter; left time: 1634.6046s\n",
      "\titers: 200, epoch: 19 | loss: 0.0221446\n",
      "\tspeed: 0.0499s/iter; left time: 898.2636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:11.37s\n",
      "Steps: 222 | Train Loss: 0.0206576 Vali Loss: 0.0299749 Test Loss: 0.0419878\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0193885\n",
      "\tspeed: 0.0910s/iter; left time: 1626.8947s\n",
      "\titers: 200, epoch: 20 | loss: 0.0205301\n",
      "\tspeed: 0.0502s/iter; left time: 893.2843s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:11.36s\n",
      "Steps: 222 | Train Loss: 0.0206312 Vali Loss: 0.0298362 Test Loss: 0.0417427\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0197740\n",
      "\tspeed: 0.0917s/iter; left time: 1619.6744s\n",
      "\titers: 200, epoch: 21 | loss: 0.0227434\n",
      "\tspeed: 0.0507s/iter; left time: 890.4546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 222 | Train Loss: 0.0206033 Vali Loss: 0.0300504 Test Loss: 0.0424300\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.040564022958278656, rmse:0.2014051228761673, mae:0.1430739164352417, rse:0.6964869499206543\n",
      "Intermediate time for GB and pred_len 96: 00h:05m:08.04s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1281811\n",
      "\tspeed: 0.0763s/iter; left time: 1686.8462s\n",
      "\titers: 200, epoch: 1 | loss: 0.1206248\n",
      "\tspeed: 0.0498s/iter; left time: 1096.5213s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 222 | Train Loss: 0.1323972 Vali Loss: 0.1000619 Test Loss: 0.1167016\n",
      "Validation loss decreased (inf --> 0.100062).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0420797\n",
      "\tspeed: 0.0914s/iter; left time: 1998.8544s\n",
      "\titers: 200, epoch: 2 | loss: 0.0311064\n",
      "\tspeed: 0.0495s/iter; left time: 1077.9152s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.0486551 Vali Loss: 0.0341354 Test Loss: 0.0488688\n",
      "Validation loss decreased (0.100062 --> 0.034135).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0288977\n",
      "\tspeed: 0.0959s/iter; left time: 2075.8970s\n",
      "\titers: 200, epoch: 3 | loss: 0.0273455\n",
      "\tspeed: 0.0494s/iter; left time: 1065.5569s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.25s\n",
      "Steps: 222 | Train Loss: 0.0279936 Vali Loss: 0.0316944 Test Loss: 0.0481999\n",
      "Validation loss decreased (0.034135 --> 0.031694).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0267540\n",
      "\tspeed: 0.0914s/iter; left time: 1958.9806s\n",
      "\titers: 200, epoch: 4 | loss: 0.0237245\n",
      "\tspeed: 0.0497s/iter; left time: 1059.7725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0252568 Vali Loss: 0.0321467 Test Loss: 0.0511265\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0232651\n",
      "\tspeed: 0.0917s/iter; left time: 1944.7708s\n",
      "\titers: 200, epoch: 5 | loss: 0.0249807\n",
      "\tspeed: 0.0501s/iter; left time: 1056.7997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.26s\n",
      "Steps: 222 | Train Loss: 0.0244643 Vali Loss: 0.0326758 Test Loss: 0.0522774\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0223662\n",
      "\tspeed: 0.0910s/iter; left time: 1909.6339s\n",
      "\titers: 200, epoch: 6 | loss: 0.0243166\n",
      "\tspeed: 0.0494s/iter; left time: 1032.7791s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 222 | Train Loss: 0.0239696 Vali Loss: 0.0324708 Test Loss: 0.0497946\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0234233\n",
      "\tspeed: 0.0883s/iter; left time: 1833.4758s\n",
      "\titers: 200, epoch: 7 | loss: 0.0236040\n",
      "\tspeed: 0.0498s/iter; left time: 1030.0115s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 222 | Train Loss: 0.0235794 Vali Loss: 0.0324006 Test Loss: 0.0479286\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0232106\n",
      "\tspeed: 0.0965s/iter; left time: 1983.6651s\n",
      "\titers: 200, epoch: 8 | loss: 0.0215790\n",
      "\tspeed: 0.0506s/iter; left time: 1034.8418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.44s\n",
      "Steps: 222 | Train Loss: 0.0232308 Vali Loss: 0.0325789 Test Loss: 0.0474472\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0229674\n",
      "\tspeed: 0.0908s/iter; left time: 1845.9255s\n",
      "\titers: 200, epoch: 9 | loss: 0.0228480\n",
      "\tspeed: 0.0516s/iter; left time: 1043.5017s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 222 | Train Loss: 0.0230120 Vali Loss: 0.0318836 Test Loss: 0.0453121\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0225253\n",
      "\tspeed: 0.0954s/iter; left time: 1917.6663s\n",
      "\titers: 200, epoch: 10 | loss: 0.0229859\n",
      "\tspeed: 0.0499s/iter; left time: 997.7649s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 222 | Train Loss: 0.0228531 Vali Loss: 0.0317617 Test Loss: 0.0442896\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0240105\n",
      "\tspeed: 0.0924s/iter; left time: 1837.1662s\n",
      "\titers: 200, epoch: 11 | loss: 0.0231950\n",
      "\tspeed: 0.0492s/iter; left time: 972.8934s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 222 | Train Loss: 0.0226830 Vali Loss: 0.0319622 Test Loss: 0.0452901\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0240522\n",
      "\tspeed: 0.0930s/iter; left time: 1828.0749s\n",
      "\titers: 200, epoch: 12 | loss: 0.0224349\n",
      "\tspeed: 0.0509s/iter; left time: 996.3293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.66s\n",
      "Steps: 222 | Train Loss: 0.0225790 Vali Loss: 0.0319162 Test Loss: 0.0447830\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0210353\n",
      "\tspeed: 0.0938s/iter; left time: 1822.4410s\n",
      "\titers: 200, epoch: 13 | loss: 0.0226072\n",
      "\tspeed: 0.0499s/iter; left time: 965.1226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 222 | Train Loss: 0.0224231 Vali Loss: 0.0321067 Test Loss: 0.0451294\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.048199884593486786, rmse:0.21954472362995148, mae:0.15564729273319244, rse:0.7611930966377258\n",
      "Intermediate time for GB and pred_len 168: 00h:03m:16.33s\n",
      "Intermediate time for GB: 00h:14m:21.84s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1235685\n",
      "\tspeed: 0.0390s/iter; left time: 868.9787s\n",
      "\titers: 200, epoch: 1 | loss: 0.1166312\n",
      "\tspeed: 0.0210s/iter; left time: 467.2693s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.60s\n",
      "Steps: 224 | Train Loss: 0.1306442 Vali Loss: 0.0743393 Test Loss: 0.0910288\n",
      "Validation loss decreased (inf --> 0.074339).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0414139\n",
      "\tspeed: 0.0446s/iter; left time: 984.8645s\n",
      "\titers: 200, epoch: 2 | loss: 0.0266411\n",
      "\tspeed: 0.0212s/iter; left time: 466.4558s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.99s\n",
      "Steps: 224 | Train Loss: 0.0490632 Vali Loss: 0.0170418 Test Loss: 0.0199101\n",
      "Validation loss decreased (0.074339 --> 0.017042).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0221347\n",
      "\tspeed: 0.0448s/iter; left time: 978.7034s\n",
      "\titers: 200, epoch: 3 | loss: 0.0209304\n",
      "\tspeed: 0.0207s/iter; left time: 451.1952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.01s\n",
      "Steps: 224 | Train Loss: 0.0225532 Vali Loss: 0.0141340 Test Loss: 0.0165208\n",
      "Validation loss decreased (0.017042 --> 0.014134).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0175069\n",
      "\tspeed: 0.0459s/iter; left time: 992.7903s\n",
      "\titers: 200, epoch: 4 | loss: 0.0180933\n",
      "\tspeed: 0.0208s/iter; left time: 448.1361s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.09s\n",
      "Steps: 224 | Train Loss: 0.0189549 Vali Loss: 0.0126682 Test Loss: 0.0149519\n",
      "Validation loss decreased (0.014134 --> 0.012668).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0158055\n",
      "\tspeed: 0.0438s/iter; left time: 937.6032s\n",
      "\titers: 200, epoch: 5 | loss: 0.0164286\n",
      "\tspeed: 0.0230s/iter; left time: 489.4497s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.19s\n",
      "Steps: 224 | Train Loss: 0.0169486 Vali Loss: 0.0124042 Test Loss: 0.0145784\n",
      "Validation loss decreased (0.012668 --> 0.012404).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0160598\n",
      "\tspeed: 0.0498s/iter; left time: 1054.0352s\n",
      "\titers: 200, epoch: 6 | loss: 0.0156461\n",
      "\tspeed: 0.0211s/iter; left time: 445.7079s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.09s\n",
      "Steps: 224 | Train Loss: 0.0154126 Vali Loss: 0.0123237 Test Loss: 0.0142485\n",
      "Validation loss decreased (0.012404 --> 0.012324).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0163689\n",
      "\tspeed: 0.0448s/iter; left time: 939.4813s\n",
      "\titers: 200, epoch: 7 | loss: 0.0149154\n",
      "\tspeed: 0.0205s/iter; left time: 427.0370s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.90s\n",
      "Steps: 224 | Train Loss: 0.0142636 Vali Loss: 0.0114231 Test Loss: 0.0135450\n",
      "Validation loss decreased (0.012324 --> 0.011423).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0132940\n",
      "\tspeed: 0.0455s/iter; left time: 943.5529s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116993\n",
      "\tspeed: 0.0212s/iter; left time: 437.8071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.05s\n",
      "Steps: 224 | Train Loss: 0.0130814 Vali Loss: 0.0109222 Test Loss: 0.0140578\n",
      "Validation loss decreased (0.011423 --> 0.010922).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0113352\n",
      "\tspeed: 0.0448s/iter; left time: 919.0434s\n",
      "\titers: 200, epoch: 9 | loss: 0.0105395\n",
      "\tspeed: 0.0209s/iter; left time: 425.7818s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 224 | Train Loss: 0.0120762 Vali Loss: 0.0108338 Test Loss: 0.0172134\n",
      "Validation loss decreased (0.010922 --> 0.010834).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0116649\n",
      "\tspeed: 0.0455s/iter; left time: 922.6808s\n",
      "\titers: 200, epoch: 10 | loss: 0.0099158\n",
      "\tspeed: 0.0241s/iter; left time: 486.5839s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.45s\n",
      "Steps: 224 | Train Loss: 0.0114562 Vali Loss: 0.0104998 Test Loss: 0.0165373\n",
      "Validation loss decreased (0.010834 --> 0.010500).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0113771\n",
      "\tspeed: 0.0491s/iter; left time: 985.4491s\n",
      "\titers: 200, epoch: 11 | loss: 0.0114219\n",
      "\tspeed: 0.0204s/iter; left time: 406.2655s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.99s\n",
      "Steps: 224 | Train Loss: 0.0110821 Vali Loss: 0.0103820 Test Loss: 0.0219415\n",
      "Validation loss decreased (0.010500 --> 0.010382).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0108638\n",
      "\tspeed: 0.0437s/iter; left time: 866.4537s\n",
      "\titers: 200, epoch: 12 | loss: 0.0102516\n",
      "\tspeed: 0.0207s/iter; left time: 408.2405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 224 | Train Loss: 0.0106935 Vali Loss: 0.0100026 Test Loss: 0.0214718\n",
      "Validation loss decreased (0.010382 --> 0.010003).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0098957\n",
      "\tspeed: 0.0440s/iter; left time: 863.6807s\n",
      "\titers: 200, epoch: 13 | loss: 0.0092505\n",
      "\tspeed: 0.0211s/iter; left time: 412.6698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 224 | Train Loss: 0.0104717 Vali Loss: 0.0099261 Test Loss: 0.0182114\n",
      "Validation loss decreased (0.010003 --> 0.009926).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0108826\n",
      "\tspeed: 0.0465s/iter; left time: 900.9547s\n",
      "\titers: 200, epoch: 14 | loss: 0.0088690\n",
      "\tspeed: 0.0213s/iter; left time: 410.9291s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 224 | Train Loss: 0.0102703 Vali Loss: 0.0098426 Test Loss: 0.0177606\n",
      "Validation loss decreased (0.009926 --> 0.009843).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0105166\n",
      "\tspeed: 0.0485s/iter; left time: 929.9886s\n",
      "\titers: 200, epoch: 15 | loss: 0.0097342\n",
      "\tspeed: 0.0236s/iter; left time: 450.5054s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.63s\n",
      "Steps: 224 | Train Loss: 0.0101483 Vali Loss: 0.0098085 Test Loss: 0.0205569\n",
      "Validation loss decreased (0.009843 --> 0.009809).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0087296\n",
      "\tspeed: 0.0486s/iter; left time: 920.4659s\n",
      "\titers: 200, epoch: 16 | loss: 0.0096730\n",
      "\tspeed: 0.0205s/iter; left time: 386.9364s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 224 | Train Loss: 0.0101333 Vali Loss: 0.0097114 Test Loss: 0.0196735\n",
      "Validation loss decreased (0.009809 --> 0.009711).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0108441\n",
      "\tspeed: 0.0452s/iter; left time: 845.9819s\n",
      "\titers: 200, epoch: 17 | loss: 0.0097255\n",
      "\tspeed: 0.0209s/iter; left time: 388.8264s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 224 | Train Loss: 0.0099599 Vali Loss: 0.0095411 Test Loss: 0.0179444\n",
      "Validation loss decreased (0.009711 --> 0.009541).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0088029\n",
      "\tspeed: 0.0469s/iter; left time: 867.0468s\n",
      "\titers: 200, epoch: 18 | loss: 0.0095050\n",
      "\tspeed: 0.0204s/iter; left time: 376.0770s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.10s\n",
      "Steps: 224 | Train Loss: 0.0098107 Vali Loss: 0.0095047 Test Loss: 0.0188733\n",
      "Validation loss decreased (0.009541 --> 0.009505).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0096069\n",
      "\tspeed: 0.0449s/iter; left time: 820.5002s\n",
      "\titers: 200, epoch: 19 | loss: 0.0100707\n",
      "\tspeed: 0.0206s/iter; left time: 374.7365s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:04.93s\n",
      "Steps: 224 | Train Loss: 0.0097356 Vali Loss: 0.0095299 Test Loss: 0.0189880\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0096012\n",
      "\tspeed: 0.0479s/iter; left time: 864.6471s\n",
      "\titers: 200, epoch: 20 | loss: 0.0103081\n",
      "\tspeed: 0.0241s/iter; left time: 431.9204s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.54s\n",
      "Steps: 224 | Train Loss: 0.0096700 Vali Loss: 0.0094713 Test Loss: 0.0193751\n",
      "Validation loss decreased (0.009505 --> 0.009471).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0099798\n",
      "\tspeed: 0.0468s/iter; left time: 834.6476s\n",
      "\titers: 200, epoch: 21 | loss: 0.0106904\n",
      "\tspeed: 0.0208s/iter; left time: 368.2852s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.08s\n",
      "Steps: 224 | Train Loss: 0.0096355 Vali Loss: 0.0094038 Test Loss: 0.0190703\n",
      "Validation loss decreased (0.009471 --> 0.009404).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0098239\n",
      "\tspeed: 0.0452s/iter; left time: 795.9743s\n",
      "\titers: 200, epoch: 22 | loss: 0.0093015\n",
      "\tspeed: 0.0207s/iter; left time: 362.5382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 224 | Train Loss: 0.0095963 Vali Loss: 0.0094222 Test Loss: 0.0179196\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0091338\n",
      "\tspeed: 0.0464s/iter; left time: 805.9490s\n",
      "\titers: 200, epoch: 23 | loss: 0.0098065\n",
      "\tspeed: 0.0209s/iter; left time: 360.8202s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.12s\n",
      "Steps: 224 | Train Loss: 0.0095129 Vali Loss: 0.0092669 Test Loss: 0.0183864\n",
      "Validation loss decreased (0.009404 --> 0.009267).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0093905\n",
      "\tspeed: 0.0456s/iter; left time: 782.6386s\n",
      "\titers: 200, epoch: 24 | loss: 0.0102782\n",
      "\tspeed: 0.0209s/iter; left time: 355.8069s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 224 | Train Loss: 0.0095316 Vali Loss: 0.0093195 Test Loss: 0.0209839\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0104343\n",
      "\tspeed: 0.0585s/iter; left time: 989.4332s\n",
      "\titers: 200, epoch: 25 | loss: 0.0088957\n",
      "\tspeed: 0.0229s/iter; left time: 385.1195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:05.56s\n",
      "Steps: 224 | Train Loss: 0.0094462 Vali Loss: 0.0092798 Test Loss: 0.0181019\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0092811\n",
      "\tspeed: 0.0439s/iter; left time: 732.5904s\n",
      "\titers: 200, epoch: 26 | loss: 0.0094501\n",
      "\tspeed: 0.0208s/iter; left time: 345.7400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 224 | Train Loss: 0.0096182 Vali Loss: 0.0094344 Test Loss: 0.0172716\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0091324\n",
      "\tspeed: 0.0433s/iter; left time: 714.1331s\n",
      "\titers: 200, epoch: 27 | loss: 0.0106261\n",
      "\tspeed: 0.0213s/iter; left time: 349.0559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:04.93s\n",
      "Steps: 224 | Train Loss: 0.0094139 Vali Loss: 0.0092859 Test Loss: 0.0192018\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0077775\n",
      "\tspeed: 0.0488s/iter; left time: 792.9774s\n",
      "\titers: 200, epoch: 28 | loss: 0.0089043\n",
      "\tspeed: 0.0215s/iter; left time: 346.5681s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:05.20s\n",
      "Steps: 224 | Train Loss: 0.0093605 Vali Loss: 0.0092553 Test Loss: 0.0188021\n",
      "Validation loss decreased (0.009267 --> 0.009255).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0087630\n",
      "\tspeed: 0.0510s/iter; left time: 816.9954s\n",
      "\titers: 200, epoch: 29 | loss: 0.0096931\n",
      "\tspeed: 0.0219s/iter; left time: 348.2522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.23s\n",
      "Steps: 224 | Train Loss: 0.0093333 Vali Loss: 0.0091649 Test Loss: 0.0186937\n",
      "Validation loss decreased (0.009255 --> 0.009165).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0092893\n",
      "\tspeed: 0.0516s/iter; left time: 814.9851s\n",
      "\titers: 200, epoch: 30 | loss: 0.0085216\n",
      "\tspeed: 0.0221s/iter; left time: 347.7434s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.25s\n",
      "Steps: 224 | Train Loss: 0.0094238 Vali Loss: 0.0093011 Test Loss: 0.0215952\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0095737\n",
      "\tspeed: 0.0485s/iter; left time: 755.2774s\n",
      "\titers: 200, epoch: 31 | loss: 0.0114639\n",
      "\tspeed: 0.0237s/iter; left time: 366.7788s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:05.41s\n",
      "Steps: 224 | Train Loss: 0.0093481 Vali Loss: 0.0091937 Test Loss: 0.0199297\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0099808\n",
      "\tspeed: 0.0479s/iter; left time: 735.4189s\n",
      "\titers: 200, epoch: 32 | loss: 0.0079586\n",
      "\tspeed: 0.0247s/iter; left time: 376.7539s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:05.59s\n",
      "Steps: 224 | Train Loss: 0.0092504 Vali Loss: 0.0091131 Test Loss: 0.0179925\n",
      "Validation loss decreased (0.009165 --> 0.009113).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0097558\n",
      "\tspeed: 0.0504s/iter; left time: 763.0779s\n",
      "\titers: 200, epoch: 33 | loss: 0.0094637\n",
      "\tspeed: 0.0218s/iter; left time: 327.5777s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.12s\n",
      "Steps: 224 | Train Loss: 0.0092967 Vali Loss: 0.0091046 Test Loss: 0.0196414\n",
      "Validation loss decreased (0.009113 --> 0.009105).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0093406\n",
      "\tspeed: 0.0515s/iter; left time: 767.4945s\n",
      "\titers: 200, epoch: 34 | loss: 0.0088018\n",
      "\tspeed: 0.0218s/iter; left time: 322.7226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:05.19s\n",
      "Steps: 224 | Train Loss: 0.0092845 Vali Loss: 0.0091666 Test Loss: 0.0193905\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0092743\n",
      "\tspeed: 0.0504s/iter; left time: 739.9328s\n",
      "\titers: 200, epoch: 35 | loss: 0.0083632\n",
      "\tspeed: 0.0214s/iter; left time: 312.4219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:05.33s\n",
      "Steps: 224 | Train Loss: 0.0092069 Vali Loss: 0.0090927 Test Loss: 0.0179031\n",
      "Validation loss decreased (0.009105 --> 0.009093).  Saving model ...\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0098525\n",
      "\tspeed: 0.0501s/iter; left time: 724.4961s\n",
      "\titers: 200, epoch: 36 | loss: 0.0083036\n",
      "\tspeed: 0.0245s/iter; left time: 351.4289s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 224 | Train Loss: 0.0092492 Vali Loss: 0.0090937 Test Loss: 0.0208244\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0083163\n",
      "\tspeed: 0.0496s/iter; left time: 706.3787s\n",
      "\titers: 200, epoch: 37 | loss: 0.0091936\n",
      "\tspeed: 0.0246s/iter; left time: 347.6624s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 224 | Train Loss: 0.0092016 Vali Loss: 0.0090485 Test Loss: 0.0183503\n",
      "Validation loss decreased (0.009093 --> 0.009049).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0080988\n",
      "\tspeed: 0.0480s/iter; left time: 672.8010s\n",
      "\titers: 200, epoch: 38 | loss: 0.0095375\n",
      "\tspeed: 0.0217s/iter; left time: 301.6484s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:05.15s\n",
      "Steps: 224 | Train Loss: 0.0092376 Vali Loss: 0.0091220 Test Loss: 0.0189542\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0091136\n",
      "\tspeed: 0.0507s/iter; left time: 699.1222s\n",
      "\titers: 200, epoch: 39 | loss: 0.0088850\n",
      "\tspeed: 0.0231s/iter; left time: 316.0700s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:05.31s\n",
      "Steps: 224 | Train Loss: 0.0092021 Vali Loss: 0.0090822 Test Loss: 0.0201605\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0085782\n",
      "\tspeed: 0.0523s/iter; left time: 710.1104s\n",
      "\titers: 200, epoch: 40 | loss: 0.0086339\n",
      "\tspeed: 0.0218s/iter; left time: 293.8584s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:05.45s\n",
      "Steps: 224 | Train Loss: 0.0091749 Vali Loss: 0.0090968 Test Loss: 0.0189611\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0097929\n",
      "\tspeed: 0.0514s/iter; left time: 685.7722s\n",
      "\titers: 200, epoch: 41 | loss: 0.0088069\n",
      "\tspeed: 0.0251s/iter; left time: 332.6106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 224 | Train Loss: 0.0091887 Vali Loss: 0.0091413 Test Loss: 0.0185922\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0104537\n",
      "\tspeed: 0.0476s/iter; left time: 624.0175s\n",
      "\titers: 200, epoch: 42 | loss: 0.0085751\n",
      "\tspeed: 0.0237s/iter; left time: 308.5987s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:05.44s\n",
      "Steps: 224 | Train Loss: 0.0092292 Vali Loss: 0.0091321 Test Loss: 0.0179263\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0089744\n",
      "\tspeed: 0.0499s/iter; left time: 642.7301s\n",
      "\titers: 200, epoch: 43 | loss: 0.0091920\n",
      "\tspeed: 0.0238s/iter; left time: 303.9073s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:05.40s\n",
      "Steps: 224 | Train Loss: 0.0092599 Vali Loss: 0.0091090 Test Loss: 0.0174770\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0096402\n",
      "\tspeed: 0.0519s/iter; left time: 657.3166s\n",
      "\titers: 200, epoch: 44 | loss: 0.0095632\n",
      "\tspeed: 0.0208s/iter; left time: 260.8448s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 224 | Train Loss: 0.0092053 Vali Loss: 0.0091317 Test Loss: 0.0196726\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0099961\n",
      "\tspeed: 0.0459s/iter; left time: 570.8017s\n",
      "\titers: 200, epoch: 45 | loss: 0.0104771\n",
      "\tspeed: 0.0202s/iter; left time: 249.3631s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 224 | Train Loss: 0.0092551 Vali Loss: 0.0091076 Test Loss: 0.0179130\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0084438\n",
      "\tspeed: 0.0508s/iter; left time: 620.7413s\n",
      "\titers: 200, epoch: 46 | loss: 0.0094436\n",
      "\tspeed: 0.0215s/iter; left time: 260.1835s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:05.14s\n",
      "Steps: 224 | Train Loss: 0.0091619 Vali Loss: 0.0090871 Test Loss: 0.0172821\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0097950\n",
      "\tspeed: 0.0594s/iter; left time: 712.9968s\n",
      "\titers: 200, epoch: 47 | loss: 0.0084180\n",
      "\tspeed: 0.0206s/iter; left time: 245.3597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 224 | Train Loss: 0.0091385 Vali Loss: 0.0091280 Test Loss: 0.0175781\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01835033856332302, rmse:0.1354634165763855, mae:0.08412059396505356, rse:0.39865246415138245\n",
      "Intermediate time for ES and pred_len 24: 00h:05m:37.05s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1243099\n",
      "\tspeed: 0.0408s/iter; left time: 909.8857s\n",
      "\titers: 200, epoch: 1 | loss: 0.1112777\n",
      "\tspeed: 0.0269s/iter; left time: 596.4748s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 224 | Train Loss: 0.1304680 Vali Loss: 0.0749759 Test Loss: 0.0934199\n",
      "Validation loss decreased (inf --> 0.074976).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0422750\n",
      "\tspeed: 0.0489s/iter; left time: 1080.3395s\n",
      "\titers: 200, epoch: 2 | loss: 0.0295403\n",
      "\tspeed: 0.0228s/iter; left time: 500.0161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.75s\n",
      "Steps: 224 | Train Loss: 0.0484345 Vali Loss: 0.0234643 Test Loss: 0.0291021\n",
      "Validation loss decreased (0.074976 --> 0.023464).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0261098\n",
      "\tspeed: 0.0590s/iter; left time: 1290.1647s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251757\n",
      "\tspeed: 0.0210s/iter; left time: 456.4315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 224 | Train Loss: 0.0267445 Vali Loss: 0.0204402 Test Loss: 0.0251841\n",
      "Validation loss decreased (0.023464 --> 0.020440).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0220553\n",
      "\tspeed: 0.0576s/iter; left time: 1246.2445s\n",
      "\titers: 200, epoch: 4 | loss: 0.0216708\n",
      "\tspeed: 0.0215s/iter; left time: 462.3749s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.03s\n",
      "Steps: 224 | Train Loss: 0.0230021 Vali Loss: 0.0190462 Test Loss: 0.0245052\n",
      "Validation loss decreased (0.020440 --> 0.019046).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0207762\n",
      "\tspeed: 0.0516s/iter; left time: 1103.9211s\n",
      "\titers: 200, epoch: 5 | loss: 0.0181857\n",
      "\tspeed: 0.0282s/iter; left time: 601.4712s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.04s\n",
      "Steps: 224 | Train Loss: 0.0199932 Vali Loss: 0.0178145 Test Loss: 0.0349214\n",
      "Validation loss decreased (0.019046 --> 0.017814).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0178383\n",
      "\tspeed: 0.0516s/iter; left time: 1092.2344s\n",
      "\titers: 200, epoch: 6 | loss: 0.0184396\n",
      "\tspeed: 0.0259s/iter; left time: 546.8263s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 224 | Train Loss: 0.0179342 Vali Loss: 0.0170949 Test Loss: 0.0270423\n",
      "Validation loss decreased (0.017814 --> 0.017095).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0178106\n",
      "\tspeed: 0.0560s/iter; left time: 1172.8880s\n",
      "\titers: 200, epoch: 7 | loss: 0.0158232\n",
      "\tspeed: 0.0219s/iter; left time: 456.0232s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 224 | Train Loss: 0.0169621 Vali Loss: 0.0164534 Test Loss: 0.0254716\n",
      "Validation loss decreased (0.017095 --> 0.016453).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0172422\n",
      "\tspeed: 0.0583s/iter; left time: 1208.9053s\n",
      "\titers: 200, epoch: 8 | loss: 0.0163265\n",
      "\tspeed: 0.0217s/iter; left time: 447.1165s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 224 | Train Loss: 0.0164334 Vali Loss: 0.0159479 Test Loss: 0.0251510\n",
      "Validation loss decreased (0.016453 --> 0.015948).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0159623\n",
      "\tspeed: 0.0501s/iter; left time: 1026.6598s\n",
      "\titers: 200, epoch: 9 | loss: 0.0149368\n",
      "\tspeed: 0.0284s/iter; left time: 579.1308s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.91s\n",
      "Steps: 224 | Train Loss: 0.0161261 Vali Loss: 0.0157977 Test Loss: 0.0278607\n",
      "Validation loss decreased (0.015948 --> 0.015798).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0162833\n",
      "\tspeed: 0.0500s/iter; left time: 1014.1826s\n",
      "\titers: 200, epoch: 10 | loss: 0.0167888\n",
      "\tspeed: 0.0237s/iter; left time: 479.2145s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 224 | Train Loss: 0.0157125 Vali Loss: 0.0157265 Test Loss: 0.0291120\n",
      "Validation loss decreased (0.015798 --> 0.015727).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0157385\n",
      "\tspeed: 0.0579s/iter; left time: 1161.2911s\n",
      "\titers: 200, epoch: 11 | loss: 0.0161697\n",
      "\tspeed: 0.0217s/iter; left time: 432.5084s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.20s\n",
      "Steps: 224 | Train Loss: 0.0155772 Vali Loss: 0.0154291 Test Loss: 0.0283270\n",
      "Validation loss decreased (0.015727 --> 0.015429).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0157377\n",
      "\tspeed: 0.0561s/iter; left time: 1113.0318s\n",
      "\titers: 200, epoch: 12 | loss: 0.0161220\n",
      "\tspeed: 0.0223s/iter; left time: 439.3078s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.88s\n",
      "Steps: 224 | Train Loss: 0.0153947 Vali Loss: 0.0155128 Test Loss: 0.0267294\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0145680\n",
      "\tspeed: 0.0482s/iter; left time: 945.9392s\n",
      "\titers: 200, epoch: 13 | loss: 0.0151161\n",
      "\tspeed: 0.0303s/iter; left time: 591.0740s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 224 | Train Loss: 0.0152482 Vali Loss: 0.0152456 Test Loss: 0.0313342\n",
      "Validation loss decreased (0.015429 --> 0.015246).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0144295\n",
      "\tspeed: 0.0499s/iter; left time: 966.7032s\n",
      "\titers: 200, epoch: 14 | loss: 0.0149595\n",
      "\tspeed: 0.0218s/iter; left time: 420.8543s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.35s\n",
      "Steps: 224 | Train Loss: 0.0156610 Vali Loss: 0.0153661 Test Loss: 0.0282838\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0138063\n",
      "\tspeed: 0.0495s/iter; left time: 949.3575s\n",
      "\titers: 200, epoch: 15 | loss: 0.0160840\n",
      "\tspeed: 0.0220s/iter; left time: 418.8333s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.21s\n",
      "Steps: 224 | Train Loss: 0.0153227 Vali Loss: 0.0163661 Test Loss: 0.0256972\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0147027\n",
      "\tspeed: 0.0573s/iter; left time: 1084.7788s\n",
      "\titers: 200, epoch: 16 | loss: 0.0159377\n",
      "\tspeed: 0.0230s/iter; left time: 432.9811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 224 | Train Loss: 0.0152654 Vali Loss: 0.0152266 Test Loss: 0.0313957\n",
      "Validation loss decreased (0.015246 --> 0.015227).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0147857\n",
      "\tspeed: 0.0495s/iter; left time: 926.3405s\n",
      "\titers: 200, epoch: 17 | loss: 0.0158567\n",
      "\tspeed: 0.0276s/iter; left time: 513.0748s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:05.96s\n",
      "Steps: 224 | Train Loss: 0.0149680 Vali Loss: 0.0152546 Test Loss: 0.0288141\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0150313\n",
      "\tspeed: 0.0534s/iter; left time: 987.7152s\n",
      "\titers: 200, epoch: 18 | loss: 0.0153670\n",
      "\tspeed: 0.0217s/iter; left time: 398.4083s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.27s\n",
      "Steps: 224 | Train Loss: 0.0148922 Vali Loss: 0.0151359 Test Loss: 0.0352062\n",
      "Validation loss decreased (0.015227 --> 0.015136).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0152953\n",
      "\tspeed: 0.0577s/iter; left time: 1054.6472s\n",
      "\titers: 200, epoch: 19 | loss: 0.0164243\n",
      "\tspeed: 0.0208s/iter; left time: 378.7296s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.68s\n",
      "Steps: 224 | Train Loss: 0.0154202 Vali Loss: 0.0154077 Test Loss: 0.0260406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0139980\n",
      "\tspeed: 0.0535s/iter; left time: 965.1725s\n",
      "\titers: 200, epoch: 20 | loss: 0.0153673\n",
      "\tspeed: 0.0294s/iter; left time: 527.0029s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.41s\n",
      "Steps: 224 | Train Loss: 0.0148775 Vali Loss: 0.0150819 Test Loss: 0.0291436\n",
      "Validation loss decreased (0.015136 --> 0.015082).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0152630\n",
      "\tspeed: 0.0539s/iter; left time: 960.7991s\n",
      "\titers: 200, epoch: 21 | loss: 0.0139147\n",
      "\tspeed: 0.0249s/iter; left time: 441.7064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 224 | Train Loss: 0.0147524 Vali Loss: 0.0150175 Test Loss: 0.0302568\n",
      "Validation loss decreased (0.015082 --> 0.015018).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0152948\n",
      "\tspeed: 0.0604s/iter; left time: 1063.5700s\n",
      "\titers: 200, epoch: 22 | loss: 0.0151246\n",
      "\tspeed: 0.0221s/iter; left time: 386.3076s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:05.33s\n",
      "Steps: 224 | Train Loss: 0.0147291 Vali Loss: 0.0150637 Test Loss: 0.0294112\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0138211\n",
      "\tspeed: 0.0629s/iter; left time: 1092.2086s\n",
      "\titers: 200, epoch: 23 | loss: 0.0160007\n",
      "\tspeed: 0.0218s/iter; left time: 377.1996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.51s\n",
      "Steps: 224 | Train Loss: 0.0146824 Vali Loss: 0.0149438 Test Loss: 0.0299825\n",
      "Validation loss decreased (0.015018 --> 0.014944).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0137403\n",
      "\tspeed: 0.0496s/iter; left time: 851.4460s\n",
      "\titers: 200, epoch: 24 | loss: 0.0137934\n",
      "\tspeed: 0.0311s/iter; left time: 529.4937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 224 | Train Loss: 0.0146960 Vali Loss: 0.0149939 Test Loss: 0.0284647\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0147044\n",
      "\tspeed: 0.0503s/iter; left time: 851.2193s\n",
      "\titers: 200, epoch: 25 | loss: 0.0162568\n",
      "\tspeed: 0.0231s/iter; left time: 388.5475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:05.45s\n",
      "Steps: 224 | Train Loss: 0.0146439 Vali Loss: 0.0149701 Test Loss: 0.0297256\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0135267\n",
      "\tspeed: 0.0602s/iter; left time: 1005.8288s\n",
      "\titers: 200, epoch: 26 | loss: 0.0140600\n",
      "\tspeed: 0.0210s/iter; left time: 348.9544s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 224 | Train Loss: 0.0146344 Vali Loss: 0.0149647 Test Loss: 0.0292373\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0148893\n",
      "\tspeed: 0.0540s/iter; left time: 890.1667s\n",
      "\titers: 200, epoch: 27 | loss: 0.0141754\n",
      "\tspeed: 0.0248s/iter; left time: 406.3985s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 224 | Train Loss: 0.0145488 Vali Loss: 0.0149350 Test Loss: 0.0312378\n",
      "Validation loss decreased (0.014944 --> 0.014935).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0155144\n",
      "\tspeed: 0.0496s/iter; left time: 805.6401s\n",
      "\titers: 200, epoch: 28 | loss: 0.0170834\n",
      "\tspeed: 0.0262s/iter; left time: 423.5397s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:05.81s\n",
      "Steps: 224 | Train Loss: 0.0145691 Vali Loss: 0.0149834 Test Loss: 0.0311552\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0156587\n",
      "\tspeed: 0.0500s/iter; left time: 801.7907s\n",
      "\titers: 200, epoch: 29 | loss: 0.0152508\n",
      "\tspeed: 0.0218s/iter; left time: 346.8408s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 224 | Train Loss: 0.0145267 Vali Loss: 0.0149104 Test Loss: 0.0324814\n",
      "Validation loss decreased (0.014935 --> 0.014910).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0140890\n",
      "\tspeed: 0.0583s/iter; left time: 921.2751s\n",
      "\titers: 200, epoch: 30 | loss: 0.0135267\n",
      "\tspeed: 0.0219s/iter; left time: 344.6854s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 224 | Train Loss: 0.0145276 Vali Loss: 0.0149391 Test Loss: 0.0319644\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0140089\n",
      "\tspeed: 0.0494s/iter; left time: 769.8832s\n",
      "\titers: 200, epoch: 31 | loss: 0.0148899\n",
      "\tspeed: 0.0288s/iter; left time: 446.1483s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:06.13s\n",
      "Steps: 224 | Train Loss: 0.0144888 Vali Loss: 0.0148844 Test Loss: 0.0310164\n",
      "Validation loss decreased (0.014910 --> 0.014884).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0149111\n",
      "\tspeed: 0.0499s/iter; left time: 766.1411s\n",
      "\titers: 200, epoch: 32 | loss: 0.0149692\n",
      "\tspeed: 0.0212s/iter; left time: 323.2594s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 224 | Train Loss: 0.0145123 Vali Loss: 0.0149362 Test Loss: 0.0324887\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0149417\n",
      "\tspeed: 0.0498s/iter; left time: 753.7339s\n",
      "\titers: 200, epoch: 33 | loss: 0.0151715\n",
      "\tspeed: 0.0259s/iter; left time: 389.3683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 224 | Train Loss: 0.0145124 Vali Loss: 0.0149080 Test Loss: 0.0316317\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0130034\n",
      "\tspeed: 0.0498s/iter; left time: 743.0850s\n",
      "\titers: 200, epoch: 34 | loss: 0.0153687\n",
      "\tspeed: 0.0242s/iter; left time: 358.3881s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:05.66s\n",
      "Steps: 224 | Train Loss: 0.0144306 Vali Loss: 0.0149091 Test Loss: 0.0325113\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0147780\n",
      "\tspeed: 0.0527s/iter; left time: 773.5512s\n",
      "\titers: 200, epoch: 35 | loss: 0.0155870\n",
      "\tspeed: 0.0241s/iter; left time: 350.8509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 224 | Train Loss: 0.0145847 Vali Loss: 0.0149064 Test Loss: 0.0315548\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0142555\n",
      "\tspeed: 0.0543s/iter; left time: 784.7606s\n",
      "\titers: 200, epoch: 36 | loss: 0.0138206\n",
      "\tspeed: 0.0223s/iter; left time: 320.6794s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 224 | Train Loss: 0.0145906 Vali Loss: 0.0149479 Test Loss: 0.0295329\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0151577\n",
      "\tspeed: 0.0573s/iter; left time: 815.2827s\n",
      "\titers: 200, epoch: 37 | loss: 0.0137488\n",
      "\tspeed: 0.0237s/iter; left time: 334.9976s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 224 | Train Loss: 0.0144642 Vali Loss: 0.0148912 Test Loss: 0.0300454\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0134963\n",
      "\tspeed: 0.0567s/iter; left time: 794.7362s\n",
      "\titers: 200, epoch: 38 | loss: 0.0137275\n",
      "\tspeed: 0.0229s/iter; left time: 318.1253s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:05.98s\n",
      "Steps: 224 | Train Loss: 0.0147023 Vali Loss: 0.0149361 Test Loss: 0.0297693\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0135200\n",
      "\tspeed: 0.0570s/iter; left time: 786.6198s\n",
      "\titers: 200, epoch: 39 | loss: 0.0149992\n",
      "\tspeed: 0.0232s/iter; left time: 317.8585s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:06.13s\n",
      "Steps: 224 | Train Loss: 0.0145202 Vali Loss: 0.0149646 Test Loss: 0.0300370\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0150121\n",
      "\tspeed: 0.0539s/iter; left time: 730.6786s\n",
      "\titers: 200, epoch: 40 | loss: 0.0140267\n",
      "\tspeed: 0.0219s/iter; left time: 295.2207s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:05.64s\n",
      "Steps: 224 | Train Loss: 0.0145302 Vali Loss: 0.0149335 Test Loss: 0.0295037\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0145172\n",
      "\tspeed: 0.0535s/iter; left time: 714.4068s\n",
      "\titers: 200, epoch: 41 | loss: 0.0145294\n",
      "\tspeed: 0.0255s/iter; left time: 337.3747s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:05.74s\n",
      "Steps: 224 | Train Loss: 0.0144130 Vali Loss: 0.0149308 Test Loss: 0.0298137\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.031016401946544647, rmse:0.1761147379875183, mae:0.11428863555192947, rse:0.5173721313476562\n",
      "Intermediate time for ES and pred_len 96: 00h:05m:26.67s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1284169\n",
      "\tspeed: 0.0524s/iter; left time: 1163.3076s\n",
      "\titers: 200, epoch: 1 | loss: 0.1172287\n",
      "\tspeed: 0.0246s/iter; left time: 544.2204s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.96s\n",
      "Steps: 223 | Train Loss: 0.1315195 Vali Loss: 0.0755308 Test Loss: 0.0935711\n",
      "Validation loss decreased (inf --> 0.075531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0394520\n",
      "\tspeed: 0.0530s/iter; left time: 1164.6696s\n",
      "\titers: 200, epoch: 2 | loss: 0.0300034\n",
      "\tspeed: 0.0277s/iter; left time: 606.4663s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 223 | Train Loss: 0.0483511 Vali Loss: 0.0254592 Test Loss: 0.0312638\n",
      "Validation loss decreased (0.075531 --> 0.025459).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0298315\n",
      "\tspeed: 0.0561s/iter; left time: 1221.3866s\n",
      "\titers: 200, epoch: 3 | loss: 0.0257679\n",
      "\tspeed: 0.0241s/iter; left time: 522.1400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.03s\n",
      "Steps: 223 | Train Loss: 0.0275876 Vali Loss: 0.0222081 Test Loss: 0.0273541\n",
      "Validation loss decreased (0.025459 --> 0.022208).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0234629\n",
      "\tspeed: 0.0582s/iter; left time: 1252.6069s\n",
      "\titers: 200, epoch: 4 | loss: 0.0214712\n",
      "\tspeed: 0.0207s/iter; left time: 443.4441s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 223 | Train Loss: 0.0232781 Vali Loss: 0.0205097 Test Loss: 0.0334549\n",
      "Validation loss decreased (0.022208 --> 0.020510).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0214271\n",
      "\tspeed: 0.0517s/iter; left time: 1102.6461s\n",
      "\titers: 200, epoch: 5 | loss: 0.0183603\n",
      "\tspeed: 0.0248s/iter; left time: 526.3614s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 223 | Train Loss: 0.0199682 Vali Loss: 0.0189914 Test Loss: 0.0321025\n",
      "Validation loss decreased (0.020510 --> 0.018991).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0175757\n",
      "\tspeed: 0.0596s/iter; left time: 1256.9485s\n",
      "\titers: 200, epoch: 6 | loss: 0.0184193\n",
      "\tspeed: 0.0235s/iter; left time: 492.5546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.02s\n",
      "Steps: 223 | Train Loss: 0.0184786 Vali Loss: 0.0183154 Test Loss: 0.0325361\n",
      "Validation loss decreased (0.018991 --> 0.018315).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0176433\n",
      "\tspeed: 0.0559s/iter; left time: 1166.0189s\n",
      "\titers: 200, epoch: 7 | loss: 0.0172115\n",
      "\tspeed: 0.0265s/iter; left time: 549.2930s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 223 | Train Loss: 0.0178102 Vali Loss: 0.0178938 Test Loss: 0.0310357\n",
      "Validation loss decreased (0.018315 --> 0.017894).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0161289\n",
      "\tspeed: 0.0564s/iter; left time: 1165.1145s\n",
      "\titers: 200, epoch: 8 | loss: 0.0173672\n",
      "\tspeed: 0.0304s/iter; left time: 624.1347s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 223 | Train Loss: 0.0174180 Vali Loss: 0.0177719 Test Loss: 0.0310839\n",
      "Validation loss decreased (0.017894 --> 0.017772).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0172031\n",
      "\tspeed: 0.0568s/iter; left time: 1159.1500s\n",
      "\titers: 200, epoch: 9 | loss: 0.0161021\n",
      "\tspeed: 0.0246s/iter; left time: 500.3097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 223 | Train Loss: 0.0170769 Vali Loss: 0.0179210 Test Loss: 0.0304198\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0170793\n",
      "\tspeed: 0.0587s/iter; left time: 1186.3701s\n",
      "\titers: 200, epoch: 10 | loss: 0.0183556\n",
      "\tspeed: 0.0241s/iter; left time: 484.8811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 223 | Train Loss: 0.0170377 Vali Loss: 0.0174098 Test Loss: 0.0347562\n",
      "Validation loss decreased (0.017772 --> 0.017410).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0170728\n",
      "\tspeed: 0.0588s/iter; left time: 1173.3801s\n",
      "\titers: 200, epoch: 11 | loss: 0.0171281\n",
      "\tspeed: 0.0233s/iter; left time: 462.4850s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.24s\n",
      "Steps: 223 | Train Loss: 0.0183533 Vali Loss: 0.0177499 Test Loss: 0.0281153\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0142452\n",
      "\tspeed: 0.0619s/iter; left time: 1222.7719s\n",
      "\titers: 200, epoch: 12 | loss: 0.0168665\n",
      "\tspeed: 0.0284s/iter; left time: 558.0180s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 223 | Train Loss: 0.0166846 Vali Loss: 0.0173330 Test Loss: 0.0298354\n",
      "Validation loss decreased (0.017410 --> 0.017333).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0161570\n",
      "\tspeed: 0.0564s/iter; left time: 1101.0648s\n",
      "\titers: 200, epoch: 13 | loss: 0.0165948\n",
      "\tspeed: 0.0299s/iter; left time: 581.4689s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 223 | Train Loss: 0.0166812 Vali Loss: 0.0173812 Test Loss: 0.0305343\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0160646\n",
      "\tspeed: 0.0580s/iter; left time: 1120.3349s\n",
      "\titers: 200, epoch: 14 | loss: 0.0174675\n",
      "\tspeed: 0.0300s/iter; left time: 575.3751s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.64s\n",
      "Steps: 223 | Train Loss: 0.0164507 Vali Loss: 0.0174721 Test Loss: 0.0310623\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0159258\n",
      "\tspeed: 0.0585s/iter; left time: 1115.2378s\n",
      "\titers: 200, epoch: 15 | loss: 0.0180293\n",
      "\tspeed: 0.0224s/iter; left time: 424.5677s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.10s\n",
      "Steps: 223 | Train Loss: 0.0163814 Vali Loss: 0.0171970 Test Loss: 0.0328074\n",
      "Validation loss decreased (0.017333 --> 0.017197).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0171973\n",
      "\tspeed: 0.0501s/iter; left time: 944.4440s\n",
      "\titers: 200, epoch: 16 | loss: 0.0169949\n",
      "\tspeed: 0.0218s/iter; left time: 408.5448s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 223 | Train Loss: 0.0166250 Vali Loss: 0.0173631 Test Loss: 0.0302975\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0154431\n",
      "\tspeed: 0.0539s/iter; left time: 1005.0651s\n",
      "\titers: 200, epoch: 17 | loss: 0.0158963\n",
      "\tspeed: 0.0213s/iter; left time: 394.8036s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:05.66s\n",
      "Steps: 223 | Train Loss: 0.0163361 Vali Loss: 0.0173100 Test Loss: 0.0320395\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0158747\n",
      "\tspeed: 0.0562s/iter; left time: 1034.8290s\n",
      "\titers: 200, epoch: 18 | loss: 0.0172246\n",
      "\tspeed: 0.0217s/iter; left time: 396.5335s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 223 | Train Loss: 0.0162627 Vali Loss: 0.0172289 Test Loss: 0.0336265\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0153920\n",
      "\tspeed: 0.0572s/iter; left time: 1039.5450s\n",
      "\titers: 200, epoch: 19 | loss: 0.0159192\n",
      "\tspeed: 0.0217s/iter; left time: 393.0789s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 223 | Train Loss: 0.0161746 Vali Loss: 0.0171079 Test Loss: 0.0315786\n",
      "Validation loss decreased (0.017197 --> 0.017108).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0172178\n",
      "\tspeed: 0.0557s/iter; left time: 1001.4507s\n",
      "\titers: 200, epoch: 20 | loss: 0.0154738\n",
      "\tspeed: 0.0216s/iter; left time: 385.1928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.81s\n",
      "Steps: 223 | Train Loss: 0.0162445 Vali Loss: 0.0171308 Test Loss: 0.0327551\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0156633\n",
      "\tspeed: 0.0563s/iter; left time: 998.0374s\n",
      "\titers: 200, epoch: 21 | loss: 0.0158444\n",
      "\tspeed: 0.0223s/iter; left time: 392.8931s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 223 | Train Loss: 0.0161826 Vali Loss: 0.0171602 Test Loss: 0.0327335\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0160388\n",
      "\tspeed: 0.0589s/iter; left time: 1030.9458s\n",
      "\titers: 200, epoch: 22 | loss: 0.0170065\n",
      "\tspeed: 0.0209s/iter; left time: 364.5106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 223 | Train Loss: 0.0160776 Vali Loss: 0.0170937 Test Loss: 0.0331659\n",
      "Validation loss decreased (0.017108 --> 0.017094).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0147220\n",
      "\tspeed: 0.0566s/iter; left time: 979.3674s\n",
      "\titers: 200, epoch: 23 | loss: 0.0174204\n",
      "\tspeed: 0.0218s/iter; left time: 374.5212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.18s\n",
      "Steps: 223 | Train Loss: 0.0160284 Vali Loss: 0.0170745 Test Loss: 0.0332947\n",
      "Validation loss decreased (0.017094 --> 0.017074).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0157670\n",
      "\tspeed: 0.0562s/iter; left time: 958.7489s\n",
      "\titers: 200, epoch: 24 | loss: 0.0147003\n",
      "\tspeed: 0.0212s/iter; left time: 359.3181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:05.21s\n",
      "Steps: 223 | Train Loss: 0.0159721 Vali Loss: 0.0169896 Test Loss: 0.0325431\n",
      "Validation loss decreased (0.017074 --> 0.016990).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0167779\n",
      "\tspeed: 0.0568s/iter; left time: 957.3935s\n",
      "\titers: 200, epoch: 25 | loss: 0.0158870\n",
      "\tspeed: 0.0217s/iter; left time: 363.2943s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 223 | Train Loss: 0.0160009 Vali Loss: 0.0169696 Test Loss: 0.0342427\n",
      "Validation loss decreased (0.016990 --> 0.016970).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0182935\n",
      "\tspeed: 0.0596s/iter; left time: 990.8976s\n",
      "\titers: 200, epoch: 26 | loss: 0.0152293\n",
      "\tspeed: 0.0222s/iter; left time: 366.3654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 223 | Train Loss: 0.0159626 Vali Loss: 0.0170733 Test Loss: 0.0365577\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0150043\n",
      "\tspeed: 0.0633s/iter; left time: 1037.6241s\n",
      "\titers: 200, epoch: 27 | loss: 0.0167594\n",
      "\tspeed: 0.0218s/iter; left time: 355.8895s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 223 | Train Loss: 0.0158955 Vali Loss: 0.0169665 Test Loss: 0.0339735\n",
      "Validation loss decreased (0.016970 --> 0.016967).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0150428\n",
      "\tspeed: 0.0610s/iter; left time: 987.0886s\n",
      "\titers: 200, epoch: 28 | loss: 0.0171211\n",
      "\tspeed: 0.0215s/iter; left time: 346.4888s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:05.28s\n",
      "Steps: 223 | Train Loss: 0.0160956 Vali Loss: 0.0170350 Test Loss: 0.0318645\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0164731\n",
      "\tspeed: 0.0489s/iter; left time: 780.6280s\n",
      "\titers: 200, epoch: 29 | loss: 0.0175466\n",
      "\tspeed: 0.0242s/iter; left time: 383.6763s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.57s\n",
      "Steps: 223 | Train Loss: 0.0159007 Vali Loss: 0.0170863 Test Loss: 0.0330290\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0160695\n",
      "\tspeed: 0.0539s/iter; left time: 847.6881s\n",
      "\titers: 200, epoch: 30 | loss: 0.0164711\n",
      "\tspeed: 0.0228s/iter; left time: 356.8785s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.73s\n",
      "Steps: 223 | Train Loss: 0.0158933 Vali Loss: 0.0171437 Test Loss: 0.0331343\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0156804\n",
      "\tspeed: 0.0567s/iter; left time: 878.9248s\n",
      "\titers: 200, epoch: 31 | loss: 0.0155068\n",
      "\tspeed: 0.0244s/iter; left time: 376.6577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 223 | Train Loss: 0.0158197 Vali Loss: 0.0170106 Test Loss: 0.0339011\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0151057\n",
      "\tspeed: 0.0530s/iter; left time: 810.8884s\n",
      "\titers: 200, epoch: 32 | loss: 0.0165237\n",
      "\tspeed: 0.0235s/iter; left time: 356.7748s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:05.60s\n",
      "Steps: 223 | Train Loss: 0.0159418 Vali Loss: 0.0170865 Test Loss: 0.0326944\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0154904\n",
      "\tspeed: 0.0533s/iter; left time: 803.4754s\n",
      "\titers: 200, epoch: 33 | loss: 0.0160965\n",
      "\tspeed: 0.0235s/iter; left time: 351.4242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.63s\n",
      "Steps: 223 | Train Loss: 0.0159461 Vali Loss: 0.0170641 Test Loss: 0.0318416\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0153600\n",
      "\tspeed: 0.0536s/iter; left time: 796.0348s\n",
      "\titers: 200, epoch: 34 | loss: 0.0170636\n",
      "\tspeed: 0.0245s/iter; left time: 361.6019s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:05.63s\n",
      "Steps: 223 | Train Loss: 0.0158354 Vali Loss: 0.0170254 Test Loss: 0.0326165\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0164788\n",
      "\tspeed: 0.0520s/iter; left time: 760.6979s\n",
      "\titers: 200, epoch: 35 | loss: 0.0158054\n",
      "\tspeed: 0.0263s/iter; left time: 381.6591s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:05.82s\n",
      "Steps: 223 | Train Loss: 0.0158333 Vali Loss: 0.0169850 Test Loss: 0.0334912\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0153885\n",
      "\tspeed: 0.0530s/iter; left time: 762.3483s\n",
      "\titers: 200, epoch: 36 | loss: 0.0159685\n",
      "\tspeed: 0.0277s/iter; left time: 396.3024s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.04s\n",
      "Steps: 223 | Train Loss: 0.0157919 Vali Loss: 0.0170162 Test Loss: 0.0343361\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0170595\n",
      "\tspeed: 0.0532s/iter; left time: 753.4112s\n",
      "\titers: 200, epoch: 37 | loss: 0.0156103\n",
      "\tspeed: 0.0273s/iter; left time: 384.6836s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:06.23s\n",
      "Steps: 223 | Train Loss: 0.0157774 Vali Loss: 0.0169519 Test Loss: 0.0330856\n",
      "Validation loss decreased (0.016967 --> 0.016952).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0175597\n",
      "\tspeed: 0.0543s/iter; left time: 757.3635s\n",
      "\titers: 200, epoch: 38 | loss: 0.0155797\n",
      "\tspeed: 0.0234s/iter; left time: 324.2870s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:05.82s\n",
      "Steps: 223 | Train Loss: 0.0158271 Vali Loss: 0.0170436 Test Loss: 0.0333813\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0153492\n",
      "\tspeed: 0.0539s/iter; left time: 739.9745s\n",
      "\titers: 200, epoch: 39 | loss: 0.0160599\n",
      "\tspeed: 0.0229s/iter; left time: 311.8109s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:05.82s\n",
      "Steps: 223 | Train Loss: 0.0157613 Vali Loss: 0.0169965 Test Loss: 0.0336042\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0156489\n",
      "\tspeed: 0.0547s/iter; left time: 739.1188s\n",
      "\titers: 200, epoch: 40 | loss: 0.0156041\n",
      "\tspeed: 0.0253s/iter; left time: 339.7186s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 223 | Train Loss: 0.0159860 Vali Loss: 0.0170430 Test Loss: 0.0325757\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0168099\n",
      "\tspeed: 0.0561s/iter; left time: 745.1231s\n",
      "\titers: 200, epoch: 41 | loss: 0.0156008\n",
      "\tspeed: 0.0256s/iter; left time: 337.2177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:05.93s\n",
      "Steps: 223 | Train Loss: 0.0157786 Vali Loss: 0.0170154 Test Loss: 0.0328521\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0149853\n",
      "\tspeed: 0.0556s/iter; left time: 725.7657s\n",
      "\titers: 200, epoch: 42 | loss: 0.0156122\n",
      "\tspeed: 0.0287s/iter; left time: 372.3287s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 223 | Train Loss: 0.0157521 Vali Loss: 0.0169720 Test Loss: 0.0328154\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0155255\n",
      "\tspeed: 0.0493s/iter; left time: 632.3585s\n",
      "\titers: 200, epoch: 43 | loss: 0.0152470\n",
      "\tspeed: 0.0222s/iter; left time: 282.1869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:05.14s\n",
      "Steps: 223 | Train Loss: 0.0157695 Vali Loss: 0.0169560 Test Loss: 0.0332695\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0151010\n",
      "\tspeed: 0.0468s/iter; left time: 589.7841s\n",
      "\titers: 200, epoch: 44 | loss: 0.0157190\n",
      "\tspeed: 0.0252s/iter; left time: 315.2813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:05.53s\n",
      "Steps: 223 | Train Loss: 0.0158228 Vali Loss: 0.0170257 Test Loss: 0.0328763\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0156760\n",
      "\tspeed: 0.0569s/iter; left time: 704.9917s\n",
      "\titers: 200, epoch: 45 | loss: 0.0156764\n",
      "\tspeed: 0.0241s/iter; left time: 296.7446s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 223 | Train Loss: 0.0157282 Vali Loss: 0.0169368 Test Loss: 0.0327699\n",
      "Validation loss decreased (0.016952 --> 0.016937).  Saving model ...\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0161030\n",
      "\tspeed: 0.0577s/iter; left time: 701.9405s\n",
      "\titers: 200, epoch: 46 | loss: 0.0172997\n",
      "\tspeed: 0.0243s/iter; left time: 293.1598s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 223 | Train Loss: 0.0157710 Vali Loss: 0.0169600 Test Loss: 0.0329193\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0147430\n",
      "\tspeed: 0.0590s/iter; left time: 704.2325s\n",
      "\titers: 200, epoch: 47 | loss: 0.0161302\n",
      "\tspeed: 0.0267s/iter; left time: 316.0707s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 223 | Train Loss: 0.0157679 Vali Loss: 0.0169480 Test Loss: 0.0331634\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0164486\n",
      "\tspeed: 0.0602s/iter; left time: 705.8398s\n",
      "\titers: 200, epoch: 48 | loss: 0.0148175\n",
      "\tspeed: 0.0279s/iter; left time: 324.3523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 223 | Train Loss: 0.0157819 Vali Loss: 0.0169770 Test Loss: 0.0333828\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0163971\n",
      "\tspeed: 0.0550s/iter; left time: 632.5106s\n",
      "\titers: 200, epoch: 49 | loss: 0.0148533\n",
      "\tspeed: 0.0324s/iter; left time: 368.8891s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 223 | Train Loss: 0.0157606 Vali Loss: 0.0168984 Test Loss: 0.0325417\n",
      "Validation loss decreased (0.016937 --> 0.016898).  Saving model ...\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0165030\n",
      "\tspeed: 0.0520s/iter; left time: 585.6893s\n",
      "\titers: 200, epoch: 50 | loss: 0.0154998\n",
      "\tspeed: 0.0303s/iter; left time: 338.1065s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 223 | Train Loss: 0.0157381 Vali Loss: 0.0169541 Test Loss: 0.0337758\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0161881\n",
      "\tspeed: 0.0580s/iter; left time: 640.8526s\n",
      "\titers: 200, epoch: 51 | loss: 0.0148654\n",
      "\tspeed: 0.0242s/iter; left time: 265.1323s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 223 | Train Loss: 0.0157762 Vali Loss: 0.0169161 Test Loss: 0.0328058\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0165587\n",
      "\tspeed: 0.0585s/iter; left time: 633.8590s\n",
      "\titers: 200, epoch: 52 | loss: 0.0159679\n",
      "\tspeed: 0.0238s/iter; left time: 254.8991s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 223 | Train Loss: 0.0157141 Vali Loss: 0.0169452 Test Loss: 0.0326098\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0154703\n",
      "\tspeed: 0.0578s/iter; left time: 613.2819s\n",
      "\titers: 200, epoch: 53 | loss: 0.0147402\n",
      "\tspeed: 0.0239s/iter; left time: 251.3977s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 223 | Train Loss: 0.0157296 Vali Loss: 0.0169105 Test Loss: 0.0328408\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0152968\n",
      "\tspeed: 0.0619s/iter; left time: 642.4387s\n",
      "\titers: 200, epoch: 54 | loss: 0.0161895\n",
      "\tspeed: 0.0228s/iter; left time: 234.4221s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:06.06s\n",
      "Steps: 223 | Train Loss: 0.0157269 Vali Loss: 0.0169528 Test Loss: 0.0334324\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0161664\n",
      "\tspeed: 0.0598s/iter; left time: 607.6549s\n",
      "\titers: 200, epoch: 55 | loss: 0.0158240\n",
      "\tspeed: 0.0304s/iter; left time: 305.7205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 223 | Train Loss: 0.0157380 Vali Loss: 0.0169289 Test Loss: 0.0334821\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0162431\n",
      "\tspeed: 0.0574s/iter; left time: 569.9137s\n",
      "\titers: 200, epoch: 56 | loss: 0.0165172\n",
      "\tspeed: 0.0216s/iter; left time: 212.6983s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:05.60s\n",
      "Steps: 223 | Train Loss: 0.0157867 Vali Loss: 0.0168710 Test Loss: 0.0329189\n",
      "Validation loss decreased (0.016898 --> 0.016871).  Saving model ...\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0147179\n",
      "\tspeed: 0.0473s/iter; left time: 459.4196s\n",
      "\titers: 200, epoch: 57 | loss: 0.0167741\n",
      "\tspeed: 0.0245s/iter; left time: 235.1707s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:05.39s\n",
      "Steps: 223 | Train Loss: 0.0158918 Vali Loss: 0.0170040 Test Loss: 0.0338830\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.0151420\n",
      "\tspeed: 0.0565s/iter; left time: 536.0585s\n",
      "\titers: 200, epoch: 58 | loss: 0.0155338\n",
      "\tspeed: 0.0262s/iter; left time: 246.0373s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 58\n",
      "Cost time: 00h:00m:06.44s\n",
      "Steps: 223 | Train Loss: 0.0157274 Vali Loss: 0.0169733 Test Loss: 0.0335529\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.0157903\n",
      "\tspeed: 0.0601s/iter; left time: 556.7467s\n",
      "\titers: 200, epoch: 59 | loss: 0.0146947\n",
      "\tspeed: 0.0230s/iter; left time: 210.5659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 59\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 223 | Train Loss: 0.0157228 Vali Loss: 0.0170033 Test Loss: 0.0348816\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.0163458\n",
      "\tspeed: 0.0610s/iter; left time: 551.2444s\n",
      "\titers: 200, epoch: 60 | loss: 0.0159373\n",
      "\tspeed: 0.0242s/iter; left time: 216.2076s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 60\n",
      "Cost time: 00h:00m:06.02s\n",
      "Steps: 223 | Train Loss: 0.0157675 Vali Loss: 0.0168795 Test Loss: 0.0327201\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.0158163\n",
      "\tspeed: 0.0611s/iter; left time: 539.3154s\n",
      "\titers: 200, epoch: 61 | loss: 0.0149162\n",
      "\tspeed: 0.0266s/iter; left time: 232.3363s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 61\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 223 | Train Loss: 0.0157603 Vali Loss: 0.0169108 Test Loss: 0.0329584\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.0162279\n",
      "\tspeed: 0.0564s/iter; left time: 484.8630s\n",
      "\titers: 200, epoch: 62 | loss: 0.0151691\n",
      "\tspeed: 0.0295s/iter; left time: 251.1132s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 62\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 223 | Train Loss: 0.0157876 Vali Loss: 0.0169134 Test Loss: 0.0335492\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.0156255\n",
      "\tspeed: 0.0505s/iter; left time: 422.5204s\n",
      "\titers: 200, epoch: 63 | loss: 0.0155422\n",
      "\tspeed: 0.0305s/iter; left time: 252.4653s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 63\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 223 | Train Loss: 0.0157253 Vali Loss: 0.0169123 Test Loss: 0.0329814\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.0164308\n",
      "\tspeed: 0.0583s/iter; left time: 475.5381s\n",
      "\titers: 200, epoch: 64 | loss: 0.0141026\n",
      "\tspeed: 0.0235s/iter; left time: 189.6175s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 64\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 223 | Train Loss: 0.0157296 Vali Loss: 0.0169038 Test Loss: 0.0327831\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.0158460\n",
      "\tspeed: 0.0586s/iter; left time: 464.7772s\n",
      "\titers: 200, epoch: 65 | loss: 0.0152336\n",
      "\tspeed: 0.0255s/iter; left time: 199.7689s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 65\n",
      "Cost time: 00h:00m:06.43s\n",
      "Steps: 223 | Train Loss: 0.0157248 Vali Loss: 0.0169858 Test Loss: 0.0333585\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.0160218\n",
      "\tspeed: 0.0621s/iter; left time: 478.1986s\n",
      "\titers: 200, epoch: 66 | loss: 0.0162508\n",
      "\tspeed: 0.0244s/iter; left time: 185.9102s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 66\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 223 | Train Loss: 0.0157725 Vali Loss: 0.0169603 Test Loss: 0.0329206\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.03291890770196915, rmse:0.181435689330101, mae:0.12120398133993149, rse:0.5330417156219482\n",
      "Intermediate time for ES and pred_len 168: 00h:09m:06.54s\n",
      "Intermediate time for ES: 00h:20m:10.26s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1128716\n",
      "\tspeed: 0.0367s/iter; left time: 826.3518s\n",
      "\titers: 200, epoch: 1 | loss: 0.0919740\n",
      "\tspeed: 0.0243s/iter; left time: 544.4542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 226 | Train Loss: 0.1099309 Vali Loss: 0.0601149 Test Loss: 0.0664455\n",
      "Validation loss decreased (inf --> 0.060115).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0454220\n",
      "\tspeed: 0.0475s/iter; left time: 1057.0121s\n",
      "\titers: 200, epoch: 2 | loss: 0.0240695\n",
      "\tspeed: 0.0253s/iter; left time: 562.1266s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.08s\n",
      "Steps: 226 | Train Loss: 0.0443392 Vali Loss: 0.0177004 Test Loss: 0.0197107\n",
      "Validation loss decreased (0.060115 --> 0.017700).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0174665\n",
      "\tspeed: 0.0471s/iter; left time: 1037.8882s\n",
      "\titers: 200, epoch: 3 | loss: 0.0156029\n",
      "\tspeed: 0.0259s/iter; left time: 569.0298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 226 | Train Loss: 0.0176472 Vali Loss: 0.0159071 Test Loss: 0.0172814\n",
      "Validation loss decreased (0.017700 --> 0.015907).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0167951\n",
      "\tspeed: 0.0426s/iter; left time: 929.6801s\n",
      "\titers: 200, epoch: 4 | loss: 0.0133514\n",
      "\tspeed: 0.0227s/iter; left time: 493.0963s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.24s\n",
      "Steps: 226 | Train Loss: 0.0145339 Vali Loss: 0.0150010 Test Loss: 0.0160672\n",
      "Validation loss decreased (0.015907 --> 0.015001).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0125075\n",
      "\tspeed: 0.0489s/iter; left time: 1056.3346s\n",
      "\titers: 200, epoch: 5 | loss: 0.0118828\n",
      "\tspeed: 0.0257s/iter; left time: 553.4823s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.43s\n",
      "Steps: 226 | Train Loss: 0.0128717 Vali Loss: 0.0137019 Test Loss: 0.0140345\n",
      "Validation loss decreased (0.015001 --> 0.013702).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0114066\n",
      "\tspeed: 0.0476s/iter; left time: 1017.8518s\n",
      "\titers: 200, epoch: 6 | loss: 0.0113185\n",
      "\tspeed: 0.0245s/iter; left time: 521.6975s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.75s\n",
      "Steps: 226 | Train Loss: 0.0118683 Vali Loss: 0.0131602 Test Loss: 0.0132944\n",
      "Validation loss decreased (0.013702 --> 0.013160).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0114604\n",
      "\tspeed: 0.0449s/iter; left time: 950.4081s\n",
      "\titers: 200, epoch: 7 | loss: 0.0117390\n",
      "\tspeed: 0.0224s/iter; left time: 472.3927s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.51s\n",
      "Steps: 226 | Train Loss: 0.0110327 Vali Loss: 0.0118688 Test Loss: 0.0124213\n",
      "Validation loss decreased (0.013160 --> 0.011869).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0095407\n",
      "\tspeed: 0.0464s/iter; left time: 971.3975s\n",
      "\titers: 200, epoch: 8 | loss: 0.0096248\n",
      "\tspeed: 0.0242s/iter; left time: 504.8198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 226 | Train Loss: 0.0102839 Vali Loss: 0.0116305 Test Loss: 0.0123497\n",
      "Validation loss decreased (0.011869 --> 0.011631).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0097565\n",
      "\tspeed: 0.0488s/iter; left time: 1010.0380s\n",
      "\titers: 200, epoch: 9 | loss: 0.0093351\n",
      "\tspeed: 0.0252s/iter; left time: 519.7482s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.19s\n",
      "Steps: 226 | Train Loss: 0.0097569 Vali Loss: 0.0111807 Test Loss: 0.0120926\n",
      "Validation loss decreased (0.011631 --> 0.011181).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0086483\n",
      "\tspeed: 0.0487s/iter; left time: 996.2656s\n",
      "\titers: 200, epoch: 10 | loss: 0.0097031\n",
      "\tspeed: 0.0255s/iter; left time: 519.5460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0093534 Vali Loss: 0.0108828 Test Loss: 0.0119646\n",
      "Validation loss decreased (0.011181 --> 0.010883).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0088099\n",
      "\tspeed: 0.0471s/iter; left time: 952.5624s\n",
      "\titers: 200, epoch: 11 | loss: 0.0084883\n",
      "\tspeed: 0.0242s/iter; left time: 486.7701s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.67s\n",
      "Steps: 226 | Train Loss: 0.0090376 Vali Loss: 0.0107497 Test Loss: 0.0119239\n",
      "Validation loss decreased (0.010883 --> 0.010750).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0105577\n",
      "\tspeed: 0.0452s/iter; left time: 904.0544s\n",
      "\titers: 200, epoch: 12 | loss: 0.0092621\n",
      "\tspeed: 0.0248s/iter; left time: 493.2401s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 226 | Train Loss: 0.0087718 Vali Loss: 0.0106976 Test Loss: 0.0119551\n",
      "Validation loss decreased (0.010750 --> 0.010698).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0079246\n",
      "\tspeed: 0.0404s/iter; left time: 799.8532s\n",
      "\titers: 200, epoch: 13 | loss: 0.0085691\n",
      "\tspeed: 0.0254s/iter; left time: 499.7591s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.50s\n",
      "Steps: 226 | Train Loss: 0.0085790 Vali Loss: 0.0104539 Test Loss: 0.0117490\n",
      "Validation loss decreased (0.010698 --> 0.010454).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0095796\n",
      "\tspeed: 0.0490s/iter; left time: 958.9949s\n",
      "\titers: 200, epoch: 14 | loss: 0.0087225\n",
      "\tspeed: 0.0257s/iter; left time: 500.5856s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 226 | Train Loss: 0.0084359 Vali Loss: 0.0104884 Test Loss: 0.0118595\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0077258\n",
      "\tspeed: 0.0465s/iter; left time: 899.1372s\n",
      "\titers: 200, epoch: 15 | loss: 0.0071234\n",
      "\tspeed: 0.0264s/iter; left time: 507.8580s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 226 | Train Loss: 0.0082627 Vali Loss: 0.0103015 Test Loss: 0.0115612\n",
      "Validation loss decreased (0.010454 --> 0.010302).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0081572\n",
      "\tspeed: 0.0468s/iter; left time: 894.6238s\n",
      "\titers: 200, epoch: 16 | loss: 0.0073905\n",
      "\tspeed: 0.0280s/iter; left time: 532.9382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 226 | Train Loss: 0.0081118 Vali Loss: 0.0102584 Test Loss: 0.0115973\n",
      "Validation loss decreased (0.010302 --> 0.010258).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0077903\n",
      "\tspeed: 0.0450s/iter; left time: 850.3065s\n",
      "\titers: 200, epoch: 17 | loss: 0.0085271\n",
      "\tspeed: 0.0272s/iter; left time: 510.4301s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.01s\n",
      "Steps: 226 | Train Loss: 0.0080381 Vali Loss: 0.0102395 Test Loss: 0.0115598\n",
      "Validation loss decreased (0.010258 --> 0.010239).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0074358\n",
      "\tspeed: 0.0483s/iter; left time: 901.3562s\n",
      "\titers: 200, epoch: 18 | loss: 0.0079556\n",
      "\tspeed: 0.0273s/iter; left time: 507.0421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.38s\n",
      "Steps: 226 | Train Loss: 0.0080005 Vali Loss: 0.0101840 Test Loss: 0.0115057\n",
      "Validation loss decreased (0.010239 --> 0.010184).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0083683\n",
      "\tspeed: 0.0481s/iter; left time: 887.3875s\n",
      "\titers: 200, epoch: 19 | loss: 0.0084214\n",
      "\tspeed: 0.0279s/iter; left time: 510.9362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0078926 Vali Loss: 0.0100710 Test Loss: 0.0114132\n",
      "Validation loss decreased (0.010184 --> 0.010071).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0084008\n",
      "\tspeed: 0.0459s/iter; left time: 835.2010s\n",
      "\titers: 200, epoch: 20 | loss: 0.0095158\n",
      "\tspeed: 0.0275s/iter; left time: 497.1251s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0078249 Vali Loss: 0.0101010 Test Loss: 0.0114476\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0076617\n",
      "\tspeed: 0.0457s/iter; left time: 821.0012s\n",
      "\titers: 200, epoch: 21 | loss: 0.0072457\n",
      "\tspeed: 0.0245s/iter; left time: 438.2443s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.74s\n",
      "Steps: 226 | Train Loss: 0.0077804 Vali Loss: 0.0100627 Test Loss: 0.0113515\n",
      "Validation loss decreased (0.010071 --> 0.010063).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0068569\n",
      "\tspeed: 0.0459s/iter; left time: 814.8669s\n",
      "\titers: 200, epoch: 22 | loss: 0.0080052\n",
      "\tspeed: 0.0259s/iter; left time: 457.8088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0077491 Vali Loss: 0.0100271 Test Loss: 0.0113290\n",
      "Validation loss decreased (0.010063 --> 0.010027).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0075235\n",
      "\tspeed: 0.0487s/iter; left time: 853.9193s\n",
      "\titers: 200, epoch: 23 | loss: 0.0069177\n",
      "\tspeed: 0.0224s/iter; left time: 389.7305s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 226 | Train Loss: 0.0076993 Vali Loss: 0.0099996 Test Loss: 0.0113290\n",
      "Validation loss decreased (0.010027 --> 0.010000).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0078292\n",
      "\tspeed: 0.0469s/iter; left time: 810.7168s\n",
      "\titers: 200, epoch: 24 | loss: 0.0076700\n",
      "\tspeed: 0.0269s/iter; left time: 463.3922s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0076460 Vali Loss: 0.0099254 Test Loss: 0.0112511\n",
      "Validation loss decreased (0.010000 --> 0.009925).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0077398\n",
      "\tspeed: 0.0470s/iter; left time: 802.2339s\n",
      "\titers: 200, epoch: 25 | loss: 0.0080779\n",
      "\tspeed: 0.0264s/iter; left time: 447.3652s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.08s\n",
      "Steps: 226 | Train Loss: 0.0076634 Vali Loss: 0.0099582 Test Loss: 0.0112776\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0074780\n",
      "\tspeed: 0.0466s/iter; left time: 785.9828s\n",
      "\titers: 200, epoch: 26 | loss: 0.0075827\n",
      "\tspeed: 0.0261s/iter; left time: 436.3797s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 226 | Train Loss: 0.0076121 Vali Loss: 0.0099094 Test Loss: 0.0112128\n",
      "Validation loss decreased (0.009925 --> 0.009909).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0079850\n",
      "\tspeed: 0.0500s/iter; left time: 831.7564s\n",
      "\titers: 200, epoch: 27 | loss: 0.0078030\n",
      "\tspeed: 0.0250s/iter; left time: 412.5499s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0075464 Vali Loss: 0.0099611 Test Loss: 0.0113010\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0069769\n",
      "\tspeed: 0.0478s/iter; left time: 783.5626s\n",
      "\titers: 200, epoch: 28 | loss: 0.0071861\n",
      "\tspeed: 0.0264s/iter; left time: 431.0163s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 226 | Train Loss: 0.0075400 Vali Loss: 0.0099150 Test Loss: 0.0112339\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0080889\n",
      "\tspeed: 0.0491s/iter; left time: 793.3192s\n",
      "\titers: 200, epoch: 29 | loss: 0.0076567\n",
      "\tspeed: 0.0248s/iter; left time: 399.3400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 226 | Train Loss: 0.0075251 Vali Loss: 0.0099069 Test Loss: 0.0112995\n",
      "Validation loss decreased (0.009909 --> 0.009907).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0065259\n",
      "\tspeed: 0.0439s/iter; left time: 700.6394s\n",
      "\titers: 200, epoch: 30 | loss: 0.0075698\n",
      "\tspeed: 0.0271s/iter; left time: 429.9469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 226 | Train Loss: 0.0074991 Vali Loss: 0.0098355 Test Loss: 0.0111918\n",
      "Validation loss decreased (0.009907 --> 0.009836).  Saving model ...\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0075317\n",
      "\tspeed: 0.0473s/iter; left time: 743.7332s\n",
      "\titers: 200, epoch: 31 | loss: 0.0086341\n",
      "\tspeed: 0.0250s/iter; left time: 390.9650s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 226 | Train Loss: 0.0074795 Vali Loss: 0.0098114 Test Loss: 0.0111375\n",
      "Validation loss decreased (0.009836 --> 0.009811).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0076852\n",
      "\tspeed: 0.0475s/iter; left time: 736.3181s\n",
      "\titers: 200, epoch: 32 | loss: 0.0069425\n",
      "\tspeed: 0.0267s/iter; left time: 411.8058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 226 | Train Loss: 0.0074564 Vali Loss: 0.0098412 Test Loss: 0.0112313\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0085172\n",
      "\tspeed: 0.0451s/iter; left time: 688.2950s\n",
      "\titers: 200, epoch: 33 | loss: 0.0081953\n",
      "\tspeed: 0.0265s/iter; left time: 402.2795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 226 | Train Loss: 0.0074508 Vali Loss: 0.0098692 Test Loss: 0.0112476\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0063183\n",
      "\tspeed: 0.0441s/iter; left time: 662.9747s\n",
      "\titers: 200, epoch: 34 | loss: 0.0077615\n",
      "\tspeed: 0.0264s/iter; left time: 395.2159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:06.01s\n",
      "Steps: 226 | Train Loss: 0.0074334 Vali Loss: 0.0098005 Test Loss: 0.0111092\n",
      "Validation loss decreased (0.009811 --> 0.009801).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0068418\n",
      "\tspeed: 0.0480s/iter; left time: 710.5507s\n",
      "\titers: 200, epoch: 35 | loss: 0.0075842\n",
      "\tspeed: 0.0262s/iter; left time: 385.0667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.23s\n",
      "Steps: 226 | Train Loss: 0.0074288 Vali Loss: 0.0098764 Test Loss: 0.0112255\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0085077\n",
      "\tspeed: 0.0449s/iter; left time: 655.8095s\n",
      "\titers: 200, epoch: 36 | loss: 0.0076539\n",
      "\tspeed: 0.0263s/iter; left time: 381.6022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.01s\n",
      "Steps: 226 | Train Loss: 0.0074260 Vali Loss: 0.0097790 Test Loss: 0.0110372\n",
      "Validation loss decreased (0.009801 --> 0.009779).  Saving model ...\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0069371\n",
      "\tspeed: 0.0482s/iter; left time: 692.4355s\n",
      "\titers: 200, epoch: 37 | loss: 0.0080106\n",
      "\tspeed: 0.0239s/iter; left time: 341.3171s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:05.96s\n",
      "Steps: 226 | Train Loss: 0.0074054 Vali Loss: 0.0098604 Test Loss: 0.0112190\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0070336\n",
      "\tspeed: 0.0484s/iter; left time: 684.4896s\n",
      "\titers: 200, epoch: 38 | loss: 0.0076715\n",
      "\tspeed: 0.0273s/iter; left time: 382.5688s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:06.21s\n",
      "Steps: 226 | Train Loss: 0.0073978 Vali Loss: 0.0098322 Test Loss: 0.0112414\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0071844\n",
      "\tspeed: 0.0469s/iter; left time: 652.0552s\n",
      "\titers: 200, epoch: 39 | loss: 0.0066780\n",
      "\tspeed: 0.0268s/iter; left time: 370.6447s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 226 | Train Loss: 0.0074299 Vali Loss: 0.0097835 Test Loss: 0.0111078\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0065640\n",
      "\tspeed: 0.0477s/iter; left time: 653.5499s\n",
      "\titers: 200, epoch: 40 | loss: 0.0078668\n",
      "\tspeed: 0.0264s/iter; left time: 358.1417s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 226 | Train Loss: 0.0073860 Vali Loss: 0.0097779 Test Loss: 0.0111073\n",
      "Validation loss decreased (0.009779 --> 0.009778).  Saving model ...\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0077651\n",
      "\tspeed: 0.0498s/iter; left time: 670.7447s\n",
      "\titers: 200, epoch: 41 | loss: 0.0065422\n",
      "\tspeed: 0.0269s/iter; left time: 359.9463s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:06.48s\n",
      "Steps: 226 | Train Loss: 0.0073971 Vali Loss: 0.0098634 Test Loss: 0.0112728\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0071227\n",
      "\tspeed: 0.0487s/iter; left time: 644.4144s\n",
      "\titers: 200, epoch: 42 | loss: 0.0074113\n",
      "\tspeed: 0.0265s/iter; left time: 348.5710s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.23s\n",
      "Steps: 226 | Train Loss: 0.0073542 Vali Loss: 0.0097589 Test Loss: 0.0111162\n",
      "Validation loss decreased (0.009778 --> 0.009759).  Saving model ...\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0076495\n",
      "\tspeed: 0.0474s/iter; left time: 616.0548s\n",
      "\titers: 200, epoch: 43 | loss: 0.0071825\n",
      "\tspeed: 0.0265s/iter; left time: 341.6872s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:06.24s\n",
      "Steps: 226 | Train Loss: 0.0073650 Vali Loss: 0.0097709 Test Loss: 0.0110386\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0085925\n",
      "\tspeed: 0.0438s/iter; left time: 559.5586s\n",
      "\titers: 200, epoch: 44 | loss: 0.0074999\n",
      "\tspeed: 0.0262s/iter; left time: 331.8429s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:06.12s\n",
      "Steps: 226 | Train Loss: 0.0073613 Vali Loss: 0.0097453 Test Loss: 0.0110496\n",
      "Validation loss decreased (0.009759 --> 0.009745).  Saving model ...\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0071460\n",
      "\tspeed: 0.0486s/iter; left time: 610.2360s\n",
      "\titers: 200, epoch: 45 | loss: 0.0082683\n",
      "\tspeed: 0.0272s/iter; left time: 338.3396s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:06.10s\n",
      "Steps: 226 | Train Loss: 0.0073519 Vali Loss: 0.0098793 Test Loss: 0.0111911\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0077008\n",
      "\tspeed: 0.0442s/iter; left time: 544.8672s\n",
      "\titers: 200, epoch: 46 | loss: 0.0069996\n",
      "\tspeed: 0.0269s/iter; left time: 329.0903s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:06.12s\n",
      "Steps: 226 | Train Loss: 0.0073610 Vali Loss: 0.0098040 Test Loss: 0.0111710\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0078941\n",
      "\tspeed: 0.0477s/iter; left time: 577.4845s\n",
      "\titers: 200, epoch: 47 | loss: 0.0078461\n",
      "\tspeed: 0.0262s/iter; left time: 314.8649s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:06.14s\n",
      "Steps: 226 | Train Loss: 0.0073608 Vali Loss: 0.0099016 Test Loss: 0.0113324\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0077719\n",
      "\tspeed: 0.0480s/iter; left time: 570.4268s\n",
      "\titers: 200, epoch: 48 | loss: 0.0059703\n",
      "\tspeed: 0.0283s/iter; left time: 333.7328s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:06.48s\n",
      "Steps: 226 | Train Loss: 0.0073397 Vali Loss: 0.0097900 Test Loss: 0.0111150\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0076034\n",
      "\tspeed: 0.0472s/iter; left time: 550.4222s\n",
      "\titers: 200, epoch: 49 | loss: 0.0076480\n",
      "\tspeed: 0.0255s/iter; left time: 294.5639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 226 | Train Loss: 0.0073238 Vali Loss: 0.0097820 Test Loss: 0.0111313\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0077784\n",
      "\tspeed: 0.0458s/iter; left time: 523.2287s\n",
      "\titers: 200, epoch: 50 | loss: 0.0086826\n",
      "\tspeed: 0.0249s/iter; left time: 281.7022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:05.87s\n",
      "Steps: 226 | Train Loss: 0.0073742 Vali Loss: 0.0097795 Test Loss: 0.0111120\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0070409\n",
      "\tspeed: 0.0390s/iter; left time: 436.8578s\n",
      "\titers: 200, epoch: 51 | loss: 0.0080854\n",
      "\tspeed: 0.0229s/iter; left time: 254.4288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:05.29s\n",
      "Steps: 226 | Train Loss: 0.0073432 Vali Loss: 0.0097531 Test Loss: 0.0110949\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0080639\n",
      "\tspeed: 0.0417s/iter; left time: 457.5949s\n",
      "\titers: 200, epoch: 52 | loss: 0.0076307\n",
      "\tspeed: 0.0251s/iter; left time: 272.8667s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 226 | Train Loss: 0.0073577 Vali Loss: 0.0097429 Test Loss: 0.0110933\n",
      "Validation loss decreased (0.009745 --> 0.009743).  Saving model ...\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0067979\n",
      "\tspeed: 0.0474s/iter; left time: 509.8565s\n",
      "\titers: 200, epoch: 53 | loss: 0.0070733\n",
      "\tspeed: 0.0275s/iter; left time: 292.4261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:06.38s\n",
      "Steps: 226 | Train Loss: 0.0073570 Vali Loss: 0.0097354 Test Loss: 0.0110799\n",
      "Validation loss decreased (0.009743 --> 0.009735).  Saving model ...\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0074099\n",
      "\tspeed: 0.0454s/iter; left time: 477.4238s\n",
      "\titers: 200, epoch: 54 | loss: 0.0065128\n",
      "\tspeed: 0.0231s/iter; left time: 241.0124s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:05.61s\n",
      "Steps: 226 | Train Loss: 0.0073377 Vali Loss: 0.0098519 Test Loss: 0.0112452\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0073642\n",
      "\tspeed: 0.0460s/iter; left time: 473.5861s\n",
      "\titers: 200, epoch: 55 | loss: 0.0070603\n",
      "\tspeed: 0.0264s/iter; left time: 268.8456s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 226 | Train Loss: 0.0073712 Vali Loss: 0.0097421 Test Loss: 0.0110505\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0075464\n",
      "\tspeed: 0.0466s/iter; left time: 469.0349s\n",
      "\titers: 200, epoch: 56 | loss: 0.0079625\n",
      "\tspeed: 0.0254s/iter; left time: 253.3787s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0073074 Vali Loss: 0.0098135 Test Loss: 0.0111922\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0062994\n",
      "\tspeed: 0.0486s/iter; left time: 478.6940s\n",
      "\titers: 200, epoch: 57 | loss: 0.0074914\n",
      "\tspeed: 0.0267s/iter; left time: 260.6590s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 226 | Train Loss: 0.0073146 Vali Loss: 0.0097597 Test Loss: 0.0111272\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.0070301\n",
      "\tspeed: 0.0492s/iter; left time: 472.9524s\n",
      "\titers: 200, epoch: 58 | loss: 0.0080159\n",
      "\tspeed: 0.0253s/iter; left time: 240.5907s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 58\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 226 | Train Loss: 0.0073411 Vali Loss: 0.0098542 Test Loss: 0.0112294\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.0077752\n",
      "\tspeed: 0.0432s/iter; left time: 406.1894s\n",
      "\titers: 200, epoch: 59 | loss: 0.0071535\n",
      "\tspeed: 0.0261s/iter; left time: 242.6972s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 59\n",
      "Cost time: 00h:00m:05.80s\n",
      "Steps: 226 | Train Loss: 0.0073310 Vali Loss: 0.0098229 Test Loss: 0.0111914\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.0076468\n",
      "\tspeed: 0.0496s/iter; left time: 454.7313s\n",
      "\titers: 200, epoch: 60 | loss: 0.0069263\n",
      "\tspeed: 0.0272s/iter; left time: 246.3952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 60\n",
      "Cost time: 00h:00m:06.44s\n",
      "Steps: 226 | Train Loss: 0.0073323 Vali Loss: 0.0097625 Test Loss: 0.0110864\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.0074560\n",
      "\tspeed: 0.0471s/iter; left time: 421.5195s\n",
      "\titers: 200, epoch: 61 | loss: 0.0083241\n",
      "\tspeed: 0.0235s/iter; left time: 207.5506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 61\n",
      "Cost time: 00h:00m:05.86s\n",
      "Steps: 226 | Train Loss: 0.0073178 Vali Loss: 0.0097292 Test Loss: 0.0110488\n",
      "Validation loss decreased (0.009735 --> 0.009729).  Saving model ...\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.0072145\n",
      "\tspeed: 0.0462s/iter; left time: 402.2560s\n",
      "\titers: 200, epoch: 62 | loss: 0.0068843\n",
      "\tspeed: 0.0226s/iter; left time: 195.0023s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 62\n",
      "Cost time: 00h:00m:05.60s\n",
      "Steps: 226 | Train Loss: 0.0073189 Vali Loss: 0.0097757 Test Loss: 0.0110886\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.0078795\n",
      "\tspeed: 0.0470s/iter; left time: 398.6356s\n",
      "\titers: 200, epoch: 63 | loss: 0.0076407\n",
      "\tspeed: 0.0255s/iter; left time: 214.0120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 63\n",
      "Cost time: 00h:00m:05.98s\n",
      "Steps: 226 | Train Loss: 0.0073113 Vali Loss: 0.0098350 Test Loss: 0.0112361\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.0063528\n",
      "\tspeed: 0.0450s/iter; left time: 371.9840s\n",
      "\titers: 200, epoch: 64 | loss: 0.0075218\n",
      "\tspeed: 0.0249s/iter; left time: 203.4058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 64\n",
      "Cost time: 00h:00m:05.90s\n",
      "Steps: 226 | Train Loss: 0.0073152 Vali Loss: 0.0098300 Test Loss: 0.0111841\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.0070828\n",
      "\tspeed: 0.0433s/iter; left time: 347.6982s\n",
      "\titers: 200, epoch: 65 | loss: 0.0068030\n",
      "\tspeed: 0.0254s/iter; left time: 201.9666s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 65\n",
      "Cost time: 00h:00m:05.61s\n",
      "Steps: 226 | Train Loss: 0.0073437 Vali Loss: 0.0097178 Test Loss: 0.0109760\n",
      "Validation loss decreased (0.009729 --> 0.009718).  Saving model ...\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.0071458\n",
      "\tspeed: 0.0479s/iter; left time: 373.8890s\n",
      "\titers: 200, epoch: 66 | loss: 0.0072068\n",
      "\tspeed: 0.0272s/iter; left time: 209.4415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 66\n",
      "Cost time: 00h:00m:06.26s\n",
      "Steps: 226 | Train Loss: 0.0073280 Vali Loss: 0.0097456 Test Loss: 0.0110517\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3100205086376224e-07\n",
      "\titers: 100, epoch: 67 | loss: 0.0083490\n",
      "\tspeed: 0.0485s/iter; left time: 368.0396s\n",
      "\titers: 200, epoch: 67 | loss: 0.0076469\n",
      "\tspeed: 0.0261s/iter; left time: 195.6571s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 67\n",
      "Cost time: 00h:00m:06.25s\n",
      "Steps: 226 | Train Loss: 0.0072792 Vali Loss: 0.0097689 Test Loss: 0.0110874\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.1790184577738603e-07\n",
      "\titers: 100, epoch: 68 | loss: 0.0069373\n",
      "\tspeed: 0.0475s/iter; left time: 349.4323s\n",
      "\titers: 200, epoch: 68 | loss: 0.0075724\n",
      "\tspeed: 0.0266s/iter; left time: 193.3757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 68\n",
      "Cost time: 00h:00m:06.29s\n",
      "Steps: 226 | Train Loss: 0.0073193 Vali Loss: 0.0098146 Test Loss: 0.0111480\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.0611166119964742e-07\n",
      "\titers: 100, epoch: 69 | loss: 0.0078234\n",
      "\tspeed: 0.0478s/iter; left time: 340.9984s\n",
      "\titers: 200, epoch: 69 | loss: 0.0066599\n",
      "\tspeed: 0.0265s/iter; left time: 186.0542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 69\n",
      "Cost time: 00h:00m:06.25s\n",
      "Steps: 226 | Train Loss: 0.0073367 Vali Loss: 0.0097884 Test Loss: 0.0111599\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.550049507968268e-08\n",
      "\titers: 100, epoch: 70 | loss: 0.0059663\n",
      "\tspeed: 0.0494s/iter; left time: 341.0797s\n",
      "\titers: 200, epoch: 70 | loss: 0.0079825\n",
      "\tspeed: 0.0260s/iter; left time: 176.6865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 70\n",
      "Cost time: 00h:00m:06.29s\n",
      "Steps: 226 | Train Loss: 0.0073102 Vali Loss: 0.0097673 Test Loss: 0.0111429\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 8.595044557171442e-08\n",
      "\titers: 100, epoch: 71 | loss: 0.0081901\n",
      "\tspeed: 0.0460s/iter; left time: 307.0410s\n",
      "\titers: 200, epoch: 71 | loss: 0.0072638\n",
      "\tspeed: 0.0260s/iter; left time: 171.0411s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 71\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 226 | Train Loss: 0.0073409 Vali Loss: 0.0097836 Test Loss: 0.0111003\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.735540101454298e-08\n",
      "\titers: 100, epoch: 72 | loss: 0.0067434\n",
      "\tspeed: 0.0493s/iter; left time: 318.0908s\n",
      "\titers: 200, epoch: 72 | loss: 0.0084814\n",
      "\tspeed: 0.0267s/iter; left time: 169.5453s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 72\n",
      "Cost time: 00h:00m:06.46s\n",
      "Steps: 226 | Train Loss: 0.0073292 Vali Loss: 0.0097669 Test Loss: 0.0111092\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 6.961986091308869e-08\n",
      "\titers: 100, epoch: 73 | loss: 0.0083585\n",
      "\tspeed: 0.0458s/iter; left time: 285.5602s\n",
      "\titers: 200, epoch: 73 | loss: 0.0078706\n",
      "\tspeed: 0.0282s/iter; left time: 173.0990s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 73\n",
      "Cost time: 00h:00m:06.22s\n",
      "Steps: 226 | Train Loss: 0.0073063 Vali Loss: 0.0097382 Test Loss: 0.0110653\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.265787482177981e-08\n",
      "\titers: 100, epoch: 74 | loss: 0.0060933\n",
      "\tspeed: 0.0476s/iter; left time: 285.4438s\n",
      "\titers: 200, epoch: 74 | loss: 0.0085692\n",
      "\tspeed: 0.0273s/iter; left time: 161.0397s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 74\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0073037 Vali Loss: 0.0097348 Test Loss: 0.0110978\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.639208733960184e-08\n",
      "\titers: 100, epoch: 75 | loss: 0.0066312\n",
      "\tspeed: 0.0472s/iter; left time: 272.8819s\n",
      "\titers: 200, epoch: 75 | loss: 0.0079251\n",
      "\tspeed: 0.0263s/iter; left time: 149.5174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 75\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 226 | Train Loss: 0.0073133 Vali Loss: 0.0097491 Test Loss: 0.0110843\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01097604539245367, rmse:0.10476662218570709, mae:0.06404493004083633, rse:0.4041867256164551\n",
      "Intermediate time for FR and pred_len 24: 00h:09m:10.94s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1113841\n",
      "\tspeed: 0.0408s/iter; left time: 914.6024s\n",
      "\titers: 200, epoch: 1 | loss: 0.0960350\n",
      "\tspeed: 0.0205s/iter; left time: 457.8952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.56s\n",
      "Steps: 225 | Train Loss: 0.1140842 Vali Loss: 0.0640091 Test Loss: 0.0724323\n",
      "Validation loss decreased (inf --> 0.064009).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0396767\n",
      "\tspeed: 0.0352s/iter; left time: 781.1620s\n",
      "\titers: 200, epoch: 2 | loss: 0.0237787\n",
      "\tspeed: 0.0205s/iter; left time: 453.5848s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 225 | Train Loss: 0.0416248 Vali Loss: 0.0233879 Test Loss: 0.0283799\n",
      "Validation loss decreased (0.064009 --> 0.023388).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0195205\n",
      "\tspeed: 0.0432s/iter; left time: 949.2620s\n",
      "\titers: 200, epoch: 3 | loss: 0.0207301\n",
      "\tspeed: 0.0214s/iter; left time: 467.6039s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0201231 Vali Loss: 0.0215313 Test Loss: 0.0250091\n",
      "Validation loss decreased (0.023388 --> 0.021531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0159275\n",
      "\tspeed: 0.0474s/iter; left time: 1029.7220s\n",
      "\titers: 200, epoch: 4 | loss: 0.0168217\n",
      "\tspeed: 0.0232s/iter; left time: 501.0622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0174908 Vali Loss: 0.0196409 Test Loss: 0.0223041\n",
      "Validation loss decreased (0.021531 --> 0.019641).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0158458\n",
      "\tspeed: 0.0460s/iter; left time: 988.0546s\n",
      "\titers: 200, epoch: 5 | loss: 0.0151958\n",
      "\tspeed: 0.0206s/iter; left time: 441.2402s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.42s\n",
      "Steps: 225 | Train Loss: 0.0158961 Vali Loss: 0.0179276 Test Loss: 0.0212669\n",
      "Validation loss decreased (0.019641 --> 0.017928).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0146401\n",
      "\tspeed: 0.0456s/iter; left time: 969.9722s\n",
      "\titers: 200, epoch: 6 | loss: 0.0138012\n",
      "\tspeed: 0.0248s/iter; left time: 524.5493s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.67s\n",
      "Steps: 225 | Train Loss: 0.0148701 Vali Loss: 0.0172438 Test Loss: 0.0206284\n",
      "Validation loss decreased (0.017928 --> 0.017244).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0131631\n",
      "\tspeed: 0.0473s/iter; left time: 995.8564s\n",
      "\titers: 200, epoch: 7 | loss: 0.0135497\n",
      "\tspeed: 0.0271s/iter; left time: 568.0906s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 225 | Train Loss: 0.0141523 Vali Loss: 0.0167315 Test Loss: 0.0206269\n",
      "Validation loss decreased (0.017244 --> 0.016731).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0155353\n",
      "\tspeed: 0.0477s/iter; left time: 993.6517s\n",
      "\titers: 200, epoch: 8 | loss: 0.0145908\n",
      "\tspeed: 0.0259s/iter; left time: 536.5331s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 225 | Train Loss: 0.0136613 Vali Loss: 0.0164862 Test Loss: 0.0204916\n",
      "Validation loss decreased (0.016731 --> 0.016486).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0124979\n",
      "\tspeed: 0.0487s/iter; left time: 1002.8748s\n",
      "\titers: 200, epoch: 9 | loss: 0.0141903\n",
      "\tspeed: 0.0259s/iter; left time: 530.1232s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.01s\n",
      "Steps: 225 | Train Loss: 0.0132657 Vali Loss: 0.0163018 Test Loss: 0.0203900\n",
      "Validation loss decreased (0.016486 --> 0.016302).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0130249\n",
      "\tspeed: 0.0478s/iter; left time: 973.9434s\n",
      "\titers: 200, epoch: 10 | loss: 0.0145992\n",
      "\tspeed: 0.0191s/iter; left time: 387.0053s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.01s\n",
      "Steps: 225 | Train Loss: 0.0130031 Vali Loss: 0.0162973 Test Loss: 0.0206071\n",
      "Validation loss decreased (0.016302 --> 0.016297).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0117351\n",
      "\tspeed: 0.0388s/iter; left time: 782.7197s\n",
      "\titers: 200, epoch: 11 | loss: 0.0126542\n",
      "\tspeed: 0.0160s/iter; left time: 321.0418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:03.83s\n",
      "Steps: 225 | Train Loss: 0.0128600 Vali Loss: 0.0161040 Test Loss: 0.0201517\n",
      "Validation loss decreased (0.016297 --> 0.016104).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0118891\n",
      "\tspeed: 0.0406s/iter; left time: 809.8770s\n",
      "\titers: 200, epoch: 12 | loss: 0.0141304\n",
      "\tspeed: 0.0177s/iter; left time: 350.5664s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0126416 Vali Loss: 0.0161925 Test Loss: 0.0203752\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0115469\n",
      "\tspeed: 0.0373s/iter; left time: 734.5803s\n",
      "\titers: 200, epoch: 13 | loss: 0.0135689\n",
      "\tspeed: 0.0162s/iter; left time: 318.5020s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:03.78s\n",
      "Steps: 225 | Train Loss: 0.0124961 Vali Loss: 0.0161324 Test Loss: 0.0203338\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0133804\n",
      "\tspeed: 0.0362s/iter; left time: 705.3949s\n",
      "\titers: 200, epoch: 14 | loss: 0.0134761\n",
      "\tspeed: 0.0153s/iter; left time: 296.2817s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:03.72s\n",
      "Steps: 225 | Train Loss: 0.0123601 Vali Loss: 0.0159018 Test Loss: 0.0199754\n",
      "Validation loss decreased (0.016104 --> 0.015902).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0123548\n",
      "\tspeed: 0.0398s/iter; left time: 766.5722s\n",
      "\titers: 200, epoch: 15 | loss: 0.0117204\n",
      "\tspeed: 0.0196s/iter; left time: 374.9376s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:04.35s\n",
      "Steps: 225 | Train Loss: 0.0122618 Vali Loss: 0.0158032 Test Loss: 0.0197771\n",
      "Validation loss decreased (0.015902 --> 0.015803).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0119051\n",
      "\tspeed: 0.0367s/iter; left time: 698.0305s\n",
      "\titers: 200, epoch: 16 | loss: 0.0120862\n",
      "\tspeed: 0.0155s/iter; left time: 293.9645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:03.83s\n",
      "Steps: 225 | Train Loss: 0.0121902 Vali Loss: 0.0158244 Test Loss: 0.0198974\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0119850\n",
      "\tspeed: 0.0363s/iter; left time: 682.0079s\n",
      "\titers: 200, epoch: 17 | loss: 0.0132078\n",
      "\tspeed: 0.0161s/iter; left time: 300.5830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:03.95s\n",
      "Steps: 225 | Train Loss: 0.0120864 Vali Loss: 0.0159179 Test Loss: 0.0201066\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0134467\n",
      "\tspeed: 0.0397s/iter; left time: 736.6075s\n",
      "\titers: 200, epoch: 18 | loss: 0.0124690\n",
      "\tspeed: 0.0152s/iter; left time: 280.7597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:03.94s\n",
      "Steps: 225 | Train Loss: 0.0120324 Vali Loss: 0.0157427 Test Loss: 0.0197965\n",
      "Validation loss decreased (0.015803 --> 0.015743).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0115921\n",
      "\tspeed: 0.0366s/iter; left time: 671.8514s\n",
      "\titers: 200, epoch: 19 | loss: 0.0118622\n",
      "\tspeed: 0.0157s/iter; left time: 286.4176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:03.81s\n",
      "Steps: 225 | Train Loss: 0.0119771 Vali Loss: 0.0157152 Test Loss: 0.0197621\n",
      "Validation loss decreased (0.015743 --> 0.015715).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0113165\n",
      "\tspeed: 0.0359s/iter; left time: 650.2540s\n",
      "\titers: 200, epoch: 20 | loss: 0.0128192\n",
      "\tspeed: 0.0177s/iter; left time: 318.9280s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:04.14s\n",
      "Steps: 225 | Train Loss: 0.0119356 Vali Loss: 0.0156835 Test Loss: 0.0197760\n",
      "Validation loss decreased (0.015715 --> 0.015684).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0105805\n",
      "\tspeed: 0.0393s/iter; left time: 703.6825s\n",
      "\titers: 200, epoch: 21 | loss: 0.0113129\n",
      "\tspeed: 0.0155s/iter; left time: 275.2871s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:03.86s\n",
      "Steps: 225 | Train Loss: 0.0118538 Vali Loss: 0.0157063 Test Loss: 0.0198067\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0119138\n",
      "\tspeed: 0.0349s/iter; left time: 617.7095s\n",
      "\titers: 200, epoch: 22 | loss: 0.0130232\n",
      "\tspeed: 0.0166s/iter; left time: 292.6130s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:03.88s\n",
      "Steps: 225 | Train Loss: 0.0118490 Vali Loss: 0.0157687 Test Loss: 0.0199778\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0140942\n",
      "\tspeed: 0.0424s/iter; left time: 740.6491s\n",
      "\titers: 200, epoch: 23 | loss: 0.0104837\n",
      "\tspeed: 0.0153s/iter; left time: 266.2464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:04.08s\n",
      "Steps: 225 | Train Loss: 0.0117976 Vali Loss: 0.0155976 Test Loss: 0.0196915\n",
      "Validation loss decreased (0.015684 --> 0.015598).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0119596\n",
      "\tspeed: 0.0377s/iter; left time: 648.9023s\n",
      "\titers: 200, epoch: 24 | loss: 0.0122438\n",
      "\tspeed: 0.0155s/iter; left time: 265.8476s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:03.92s\n",
      "Steps: 225 | Train Loss: 0.0117347 Vali Loss: 0.0155869 Test Loss: 0.0196945\n",
      "Validation loss decreased (0.015598 --> 0.015587).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0110265\n",
      "\tspeed: 0.0375s/iter; left time: 638.2705s\n",
      "\titers: 200, epoch: 25 | loss: 0.0106533\n",
      "\tspeed: 0.0181s/iter; left time: 305.6683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:04.07s\n",
      "Steps: 225 | Train Loss: 0.0117112 Vali Loss: 0.0156426 Test Loss: 0.0198788\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0123516\n",
      "\tspeed: 0.0395s/iter; left time: 662.9959s\n",
      "\titers: 200, epoch: 26 | loss: 0.0110251\n",
      "\tspeed: 0.0169s/iter; left time: 282.2685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:04.26s\n",
      "Steps: 225 | Train Loss: 0.0117144 Vali Loss: 0.0156266 Test Loss: 0.0196654\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0118690\n",
      "\tspeed: 0.0396s/iter; left time: 654.8026s\n",
      "\titers: 200, epoch: 27 | loss: 0.0126628\n",
      "\tspeed: 0.0160s/iter; left time: 263.0844s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:03.85s\n",
      "Steps: 225 | Train Loss: 0.0116711 Vali Loss: 0.0155167 Test Loss: 0.0196460\n",
      "Validation loss decreased (0.015587 --> 0.015517).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0111083\n",
      "\tspeed: 0.0411s/iter; left time: 670.6492s\n",
      "\titers: 200, epoch: 28 | loss: 0.0124047\n",
      "\tspeed: 0.0184s/iter; left time: 298.7979s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 225 | Train Loss: 0.0116553 Vali Loss: 0.0155300 Test Loss: 0.0196517\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0111487\n",
      "\tspeed: 0.0366s/iter; left time: 589.7061s\n",
      "\titers: 200, epoch: 29 | loss: 0.0116333\n",
      "\tspeed: 0.0164s/iter; left time: 262.9206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0116603 Vali Loss: 0.0154778 Test Loss: 0.0194986\n",
      "Validation loss decreased (0.015517 --> 0.015478).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0111717\n",
      "\tspeed: 0.0387s/iter; left time: 613.9011s\n",
      "\titers: 200, epoch: 30 | loss: 0.0109237\n",
      "\tspeed: 0.0169s/iter; left time: 266.6478s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:04.10s\n",
      "Steps: 225 | Train Loss: 0.0116186 Vali Loss: 0.0155104 Test Loss: 0.0196734\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0108955\n",
      "\tspeed: 0.0385s/iter; left time: 602.0151s\n",
      "\titers: 200, epoch: 31 | loss: 0.0120876\n",
      "\tspeed: 0.0154s/iter; left time: 239.2646s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:03.84s\n",
      "Steps: 225 | Train Loss: 0.0115731 Vali Loss: 0.0155629 Test Loss: 0.0196749\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0112643\n",
      "\tspeed: 0.0374s/iter; left time: 576.8115s\n",
      "\titers: 200, epoch: 32 | loss: 0.0106460\n",
      "\tspeed: 0.0159s/iter; left time: 243.8734s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:03.87s\n",
      "Steps: 225 | Train Loss: 0.0116208 Vali Loss: 0.0154745 Test Loss: 0.0194838\n",
      "Validation loss decreased (0.015478 --> 0.015474).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0120849\n",
      "\tspeed: 0.0411s/iter; left time: 624.1683s\n",
      "\titers: 200, epoch: 33 | loss: 0.0106973\n",
      "\tspeed: 0.0213s/iter; left time: 321.8344s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:04.76s\n",
      "Steps: 225 | Train Loss: 0.0115906 Vali Loss: 0.0154839 Test Loss: 0.0196082\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0114681\n",
      "\tspeed: 0.0376s/iter; left time: 563.8245s\n",
      "\titers: 200, epoch: 34 | loss: 0.0103871\n",
      "\tspeed: 0.0179s/iter; left time: 266.1766s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:04.21s\n",
      "Steps: 225 | Train Loss: 0.0115730 Vali Loss: 0.0153933 Test Loss: 0.0194390\n",
      "Validation loss decreased (0.015474 --> 0.015393).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0116464\n",
      "\tspeed: 0.0395s/iter; left time: 582.9110s\n",
      "\titers: 200, epoch: 35 | loss: 0.0106844\n",
      "\tspeed: 0.0174s/iter; left time: 255.3271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:04.18s\n",
      "Steps: 225 | Train Loss: 0.0115658 Vali Loss: 0.0154608 Test Loss: 0.0195885\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0130586\n",
      "\tspeed: 0.0395s/iter; left time: 574.2656s\n",
      "\titers: 200, epoch: 36 | loss: 0.0131336\n",
      "\tspeed: 0.0172s/iter; left time: 248.5702s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:04.21s\n",
      "Steps: 225 | Train Loss: 0.0115078 Vali Loss: 0.0154698 Test Loss: 0.0195335\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0120452\n",
      "\tspeed: 0.0369s/iter; left time: 527.4090s\n",
      "\titers: 200, epoch: 37 | loss: 0.0121271\n",
      "\tspeed: 0.0174s/iter; left time: 246.8181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:03.99s\n",
      "Steps: 225 | Train Loss: 0.0115680 Vali Loss: 0.0154238 Test Loss: 0.0195199\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0119433\n",
      "\tspeed: 0.0402s/iter; left time: 565.6545s\n",
      "\titers: 200, epoch: 38 | loss: 0.0110818\n",
      "\tspeed: 0.0199s/iter; left time: 278.1313s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 225 | Train Loss: 0.0115156 Vali Loss: 0.0153970 Test Loss: 0.0194227\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0100204\n",
      "\tspeed: 0.0388s/iter; left time: 537.5923s\n",
      "\titers: 200, epoch: 39 | loss: 0.0113502\n",
      "\tspeed: 0.0160s/iter; left time: 220.2156s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:03.92s\n",
      "Steps: 225 | Train Loss: 0.0115199 Vali Loss: 0.0154849 Test Loss: 0.0196535\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0115077\n",
      "\tspeed: 0.0388s/iter; left time: 528.1749s\n",
      "\titers: 200, epoch: 40 | loss: 0.0103217\n",
      "\tspeed: 0.0152s/iter; left time: 204.9452s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:04.03s\n",
      "Steps: 225 | Train Loss: 0.0115064 Vali Loss: 0.0154055 Test Loss: 0.0194664\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0103715\n",
      "\tspeed: 0.0406s/iter; left time: 544.6653s\n",
      "\titers: 200, epoch: 41 | loss: 0.0105630\n",
      "\tspeed: 0.0177s/iter; left time: 235.3172s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:04.44s\n",
      "Steps: 225 | Train Loss: 0.0115057 Vali Loss: 0.0154118 Test Loss: 0.0195385\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0116243\n",
      "\tspeed: 0.0414s/iter; left time: 546.0989s\n",
      "\titers: 200, epoch: 42 | loss: 0.0111042\n",
      "\tspeed: 0.0176s/iter; left time: 230.1162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:04.20s\n",
      "Steps: 225 | Train Loss: 0.0114945 Vali Loss: 0.0154482 Test Loss: 0.0195912\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0120247\n",
      "\tspeed: 0.0407s/iter; left time: 527.4954s\n",
      "\titers: 200, epoch: 43 | loss: 0.0122419\n",
      "\tspeed: 0.0194s/iter; left time: 248.7993s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:04.64s\n",
      "Steps: 225 | Train Loss: 0.0115088 Vali Loss: 0.0153739 Test Loss: 0.0193811\n",
      "Validation loss decreased (0.015393 --> 0.015374).  Saving model ...\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0123291\n",
      "\tspeed: 0.0399s/iter; left time: 507.4273s\n",
      "\titers: 200, epoch: 44 | loss: 0.0108651\n",
      "\tspeed: 0.0178s/iter; left time: 224.7418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:04.16s\n",
      "Steps: 225 | Train Loss: 0.0115130 Vali Loss: 0.0154280 Test Loss: 0.0196132\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0116522\n",
      "\tspeed: 0.0400s/iter; left time: 500.3381s\n",
      "\titers: 200, epoch: 45 | loss: 0.0100763\n",
      "\tspeed: 0.0194s/iter; left time: 240.3175s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:04.50s\n",
      "Steps: 225 | Train Loss: 0.0115149 Vali Loss: 0.0154189 Test Loss: 0.0195373\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0119316\n",
      "\tspeed: 0.0431s/iter; left time: 529.3590s\n",
      "\titers: 200, epoch: 46 | loss: 0.0121877\n",
      "\tspeed: 0.0176s/iter; left time: 214.4317s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:04.38s\n",
      "Steps: 225 | Train Loss: 0.0114768 Vali Loss: 0.0154554 Test Loss: 0.0196083\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0123775\n",
      "\tspeed: 0.0387s/iter; left time: 466.5767s\n",
      "\titers: 200, epoch: 47 | loss: 0.0115821\n",
      "\tspeed: 0.0168s/iter; left time: 201.2726s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:03.95s\n",
      "Steps: 225 | Train Loss: 0.0114788 Vali Loss: 0.0153788 Test Loss: 0.0194893\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0123712\n",
      "\tspeed: 0.0459s/iter; left time: 542.5049s\n",
      "\titers: 200, epoch: 48 | loss: 0.0106740\n",
      "\tspeed: 0.0213s/iter; left time: 249.7321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:05.12s\n",
      "Steps: 225 | Train Loss: 0.0115063 Vali Loss: 0.0153871 Test Loss: 0.0194643\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0134893\n",
      "\tspeed: 0.0404s/iter; left time: 469.1885s\n",
      "\titers: 200, epoch: 49 | loss: 0.0116201\n",
      "\tspeed: 0.0169s/iter; left time: 194.5966s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:04.11s\n",
      "Steps: 225 | Train Loss: 0.0114783 Vali Loss: 0.0154132 Test Loss: 0.0195795\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0116107\n",
      "\tspeed: 0.0401s/iter; left time: 455.7130s\n",
      "\titers: 200, epoch: 50 | loss: 0.0103245\n",
      "\tspeed: 0.0196s/iter; left time: 221.3832s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 225 | Train Loss: 0.0114765 Vali Loss: 0.0153764 Test Loss: 0.0194849\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0117851\n",
      "\tspeed: 0.0446s/iter; left time: 496.9526s\n",
      "\titers: 200, epoch: 51 | loss: 0.0107125\n",
      "\tspeed: 0.0162s/iter; left time: 178.7868s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0114667 Vali Loss: 0.0154133 Test Loss: 0.0194795\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0106644\n",
      "\tspeed: 0.0373s/iter; left time: 407.3283s\n",
      "\titers: 200, epoch: 52 | loss: 0.0115350\n",
      "\tspeed: 0.0155s/iter; left time: 167.6295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:03.84s\n",
      "Steps: 225 | Train Loss: 0.0114815 Vali Loss: 0.0153970 Test Loss: 0.0195376\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0115413\n",
      "\tspeed: 0.0445s/iter; left time: 475.7675s\n",
      "\titers: 200, epoch: 53 | loss: 0.0111369\n",
      "\tspeed: 0.0204s/iter; left time: 215.7892s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:04.94s\n",
      "Steps: 225 | Train Loss: 0.0114722 Vali Loss: 0.0154105 Test Loss: 0.0195179\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019381113350391388, rmse:0.13921606540679932, mae:0.087315134704113, rse:0.5385247468948364\n",
      "Intermediate time for FR and pred_len 96: 00h:05m:21.19s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1163086\n",
      "\tspeed: 0.0407s/iter; left time: 911.0389s\n",
      "\titers: 200, epoch: 1 | loss: 0.0959582\n",
      "\tspeed: 0.0156s/iter; left time: 348.3436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.55s\n",
      "Steps: 225 | Train Loss: 0.1164011 Vali Loss: 0.0671606 Test Loss: 0.0756801\n",
      "Validation loss decreased (inf --> 0.067161).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0380078\n",
      "\tspeed: 0.0387s/iter; left time: 857.1911s\n",
      "\titers: 200, epoch: 2 | loss: 0.0249385\n",
      "\tspeed: 0.0164s/iter; left time: 362.2370s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0402786 Vali Loss: 0.0245649 Test Loss: 0.0294748\n",
      "Validation loss decreased (0.067161 --> 0.024565).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0198591\n",
      "\tspeed: 0.0410s/iter; left time: 899.4307s\n",
      "\titers: 200, epoch: 3 | loss: 0.0181067\n",
      "\tspeed: 0.0217s/iter; left time: 474.2215s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0208037 Vali Loss: 0.0224972 Test Loss: 0.0261129\n",
      "Validation loss decreased (0.024565 --> 0.022497).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0205366\n",
      "\tspeed: 0.0397s/iter; left time: 863.3462s\n",
      "\titers: 200, epoch: 4 | loss: 0.0180572\n",
      "\tspeed: 0.0159s/iter; left time: 342.8854s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:03.87s\n",
      "Steps: 225 | Train Loss: 0.0183149 Vali Loss: 0.0198551 Test Loss: 0.0232241\n",
      "Validation loss decreased (0.022497 --> 0.019855).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0173023\n",
      "\tspeed: 0.0417s/iter; left time: 896.1665s\n",
      "\titers: 200, epoch: 5 | loss: 0.0153106\n",
      "\tspeed: 0.0165s/iter; left time: 353.5063s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:03.99s\n",
      "Steps: 225 | Train Loss: 0.0167519 Vali Loss: 0.0186968 Test Loss: 0.0227034\n",
      "Validation loss decreased (0.019855 --> 0.018697).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0151763\n",
      "\tspeed: 0.0472s/iter; left time: 1005.0808s\n",
      "\titers: 200, epoch: 6 | loss: 0.0169578\n",
      "\tspeed: 0.0166s/iter; left time: 350.7544s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.39s\n",
      "Steps: 225 | Train Loss: 0.0158253 Vali Loss: 0.0187366 Test Loss: 0.0234431\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0141559\n",
      "\tspeed: 0.0396s/iter; left time: 833.3195s\n",
      "\titers: 200, epoch: 7 | loss: 0.0142905\n",
      "\tspeed: 0.0173s/iter; left time: 361.9078s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.13s\n",
      "Steps: 225 | Train Loss: 0.0150663 Vali Loss: 0.0178995 Test Loss: 0.0225593\n",
      "Validation loss decreased (0.018697 --> 0.017900).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0129712\n",
      "\tspeed: 0.0383s/iter; left time: 797.0842s\n",
      "\titers: 200, epoch: 8 | loss: 0.0136827\n",
      "\tspeed: 0.0230s/iter; left time: 476.7195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.70s\n",
      "Steps: 225 | Train Loss: 0.0145871 Vali Loss: 0.0177982 Test Loss: 0.0223871\n",
      "Validation loss decreased (0.017900 --> 0.017798).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0150651\n",
      "\tspeed: 0.0398s/iter; left time: 820.3513s\n",
      "\titers: 200, epoch: 9 | loss: 0.0142423\n",
      "\tspeed: 0.0163s/iter; left time: 333.2855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:03.88s\n",
      "Steps: 225 | Train Loss: 0.0142830 Vali Loss: 0.0177567 Test Loss: 0.0227289\n",
      "Validation loss decreased (0.017798 --> 0.017757).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0143350\n",
      "\tspeed: 0.0377s/iter; left time: 767.3888s\n",
      "\titers: 200, epoch: 10 | loss: 0.0136892\n",
      "\tspeed: 0.0157s/iter; left time: 318.5943s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:03.81s\n",
      "Steps: 225 | Train Loss: 0.0140856 Vali Loss: 0.0176054 Test Loss: 0.0224962\n",
      "Validation loss decreased (0.017757 --> 0.017605).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0144481\n",
      "\tspeed: 0.0448s/iter; left time: 903.6252s\n",
      "\titers: 200, epoch: 11 | loss: 0.0122086\n",
      "\tspeed: 0.0167s/iter; left time: 334.6707s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 225 | Train Loss: 0.0139002 Vali Loss: 0.0175063 Test Loss: 0.0222891\n",
      "Validation loss decreased (0.017605 --> 0.017506).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0130901\n",
      "\tspeed: 0.0390s/iter; left time: 777.0067s\n",
      "\titers: 200, epoch: 12 | loss: 0.0136973\n",
      "\tspeed: 0.0160s/iter; left time: 317.9730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:03.86s\n",
      "Steps: 225 | Train Loss: 0.0136572 Vali Loss: 0.0174897 Test Loss: 0.0223212\n",
      "Validation loss decreased (0.017506 --> 0.017490).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0129098\n",
      "\tspeed: 0.0392s/iter; left time: 772.1506s\n",
      "\titers: 200, epoch: 13 | loss: 0.0127208\n",
      "\tspeed: 0.0214s/iter; left time: 420.2287s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.71s\n",
      "Steps: 225 | Train Loss: 0.0136023 Vali Loss: 0.0174103 Test Loss: 0.0221600\n",
      "Validation loss decreased (0.017490 --> 0.017410).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0145014\n",
      "\tspeed: 0.0417s/iter; left time: 812.7398s\n",
      "\titers: 200, epoch: 14 | loss: 0.0142842\n",
      "\tspeed: 0.0160s/iter; left time: 309.7685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:03.96s\n",
      "Steps: 225 | Train Loss: 0.0134571 Vali Loss: 0.0173082 Test Loss: 0.0219032\n",
      "Validation loss decreased (0.017410 --> 0.017308).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0122999\n",
      "\tspeed: 0.0383s/iter; left time: 737.0365s\n",
      "\titers: 200, epoch: 15 | loss: 0.0124080\n",
      "\tspeed: 0.0159s/iter; left time: 303.9583s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:03.92s\n",
      "Steps: 225 | Train Loss: 0.0133875 Vali Loss: 0.0173671 Test Loss: 0.0223021\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0129976\n",
      "\tspeed: 0.0471s/iter; left time: 896.2514s\n",
      "\titers: 200, epoch: 16 | loss: 0.0143465\n",
      "\tspeed: 0.0162s/iter; left time: 305.6643s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0132982 Vali Loss: 0.0173716 Test Loss: 0.0222940\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0128105\n",
      "\tspeed: 0.0384s/iter; left time: 721.0799s\n",
      "\titers: 200, epoch: 17 | loss: 0.0133504\n",
      "\tspeed: 0.0164s/iter; left time: 307.6095s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:03.92s\n",
      "Steps: 225 | Train Loss: 0.0131884 Vali Loss: 0.0171983 Test Loss: 0.0221098\n",
      "Validation loss decreased (0.017308 --> 0.017198).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0127477\n",
      "\tspeed: 0.0390s/iter; left time: 725.3888s\n",
      "\titers: 200, epoch: 18 | loss: 0.0133616\n",
      "\tspeed: 0.0215s/iter; left time: 396.6378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0131289 Vali Loss: 0.0172486 Test Loss: 0.0223695\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0132135\n",
      "\tspeed: 0.0466s/iter; left time: 854.6046s\n",
      "\titers: 200, epoch: 19 | loss: 0.0139983\n",
      "\tspeed: 0.0231s/iter; left time: 420.8309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 225 | Train Loss: 0.0130505 Vali Loss: 0.0171564 Test Loss: 0.0222004\n",
      "Validation loss decreased (0.017198 --> 0.017156).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0127651\n",
      "\tspeed: 0.0384s/iter; left time: 696.5206s\n",
      "\titers: 200, epoch: 20 | loss: 0.0131906\n",
      "\tspeed: 0.0156s/iter; left time: 280.5278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:03.89s\n",
      "Steps: 225 | Train Loss: 0.0130117 Vali Loss: 0.0170236 Test Loss: 0.0218656\n",
      "Validation loss decreased (0.017156 --> 0.017024).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0132290\n",
      "\tspeed: 0.0383s/iter; left time: 685.8056s\n",
      "\titers: 200, epoch: 21 | loss: 0.0138149\n",
      "\tspeed: 0.0153s/iter; left time: 273.2173s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:03.71s\n",
      "Steps: 225 | Train Loss: 0.0129826 Vali Loss: 0.0170017 Test Loss: 0.0218221\n",
      "Validation loss decreased (0.017024 --> 0.017002).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0138705\n",
      "\tspeed: 0.0396s/iter; left time: 700.5474s\n",
      "\titers: 200, epoch: 22 | loss: 0.0132168\n",
      "\tspeed: 0.0226s/iter; left time: 397.4560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:04.70s\n",
      "Steps: 225 | Train Loss: 0.0129330 Vali Loss: 0.0171318 Test Loss: 0.0222023\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0117958\n",
      "\tspeed: 0.0405s/iter; left time: 707.4557s\n",
      "\titers: 200, epoch: 23 | loss: 0.0121447\n",
      "\tspeed: 0.0156s/iter; left time: 270.4232s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:04.01s\n",
      "Steps: 225 | Train Loss: 0.0128791 Vali Loss: 0.0169629 Test Loss: 0.0218639\n",
      "Validation loss decreased (0.017002 --> 0.016963).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0117950\n",
      "\tspeed: 0.0393s/iter; left time: 677.3486s\n",
      "\titers: 200, epoch: 24 | loss: 0.0116022\n",
      "\tspeed: 0.0155s/iter; left time: 265.2126s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0128464 Vali Loss: 0.0169410 Test Loss: 0.0217760\n",
      "Validation loss decreased (0.016963 --> 0.016941).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0132211\n",
      "\tspeed: 0.0447s/iter; left time: 760.1049s\n",
      "\titers: 200, epoch: 25 | loss: 0.0133817\n",
      "\tspeed: 0.0179s/iter; left time: 302.0139s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0128302 Vali Loss: 0.0170455 Test Loss: 0.0220150\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0118063\n",
      "\tspeed: 0.0380s/iter; left time: 637.8996s\n",
      "\titers: 200, epoch: 26 | loss: 0.0115659\n",
      "\tspeed: 0.0158s/iter; left time: 263.3682s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:03.87s\n",
      "Steps: 225 | Train Loss: 0.0127902 Vali Loss: 0.0169368 Test Loss: 0.0216894\n",
      "Validation loss decreased (0.016941 --> 0.016937).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0110586\n",
      "\tspeed: 0.0389s/iter; left time: 644.3470s\n",
      "\titers: 200, epoch: 27 | loss: 0.0133453\n",
      "\tspeed: 0.0208s/iter; left time: 341.6108s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 225 | Train Loss: 0.0127443 Vali Loss: 0.0169492 Test Loss: 0.0217148\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0135904\n",
      "\tspeed: 0.0429s/iter; left time: 700.4644s\n",
      "\titers: 200, epoch: 28 | loss: 0.0124276\n",
      "\tspeed: 0.0169s/iter; left time: 274.1935s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:03.97s\n",
      "Steps: 225 | Train Loss: 0.0127048 Vali Loss: 0.0169174 Test Loss: 0.0216821\n",
      "Validation loss decreased (0.016937 --> 0.016917).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0128987\n",
      "\tspeed: 0.0372s/iter; left time: 598.7599s\n",
      "\titers: 200, epoch: 29 | loss: 0.0128150\n",
      "\tspeed: 0.0155s/iter; left time: 248.1545s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:03.82s\n",
      "Steps: 225 | Train Loss: 0.0127015 Vali Loss: 0.0168976 Test Loss: 0.0216197\n",
      "Validation loss decreased (0.016917 --> 0.016898).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0123768\n",
      "\tspeed: 0.0433s/iter; left time: 687.1347s\n",
      "\titers: 200, epoch: 30 | loss: 0.0126488\n",
      "\tspeed: 0.0238s/iter; left time: 375.8884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.21s\n",
      "Steps: 225 | Train Loss: 0.0126791 Vali Loss: 0.0169204 Test Loss: 0.0216944\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0132034\n",
      "\tspeed: 0.0378s/iter; left time: 591.7431s\n",
      "\titers: 200, epoch: 31 | loss: 0.0124356\n",
      "\tspeed: 0.0154s/iter; left time: 239.7172s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:03.76s\n",
      "Steps: 225 | Train Loss: 0.0126608 Vali Loss: 0.0169443 Test Loss: 0.0217171\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0126917\n",
      "\tspeed: 0.0397s/iter; left time: 612.2016s\n",
      "\titers: 200, epoch: 32 | loss: 0.0139379\n",
      "\tspeed: 0.0150s/iter; left time: 230.5296s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0126862 Vali Loss: 0.0168743 Test Loss: 0.0216627\n",
      "Validation loss decreased (0.016898 --> 0.016874).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0121382\n",
      "\tspeed: 0.0487s/iter; left time: 740.9962s\n",
      "\titers: 200, epoch: 33 | loss: 0.0134788\n",
      "\tspeed: 0.0159s/iter; left time: 240.5201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0126394 Vali Loss: 0.0168715 Test Loss: 0.0216864\n",
      "Validation loss decreased (0.016874 --> 0.016871).  Saving model ...\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0133380\n",
      "\tspeed: 0.0386s/iter; left time: 577.5417s\n",
      "\titers: 200, epoch: 34 | loss: 0.0120537\n",
      "\tspeed: 0.0158s/iter; left time: 235.2861s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0126101 Vali Loss: 0.0168341 Test Loss: 0.0216229\n",
      "Validation loss decreased (0.016871 --> 0.016834).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0127342\n",
      "\tspeed: 0.0386s/iter; left time: 569.5485s\n",
      "\titers: 200, epoch: 35 | loss: 0.0137883\n",
      "\tspeed: 0.0204s/iter; left time: 299.0700s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0126581 Vali Loss: 0.0168800 Test Loss: 0.0216368\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0129292\n",
      "\tspeed: 0.0430s/iter; left time: 625.1072s\n",
      "\titers: 200, epoch: 36 | loss: 0.0142377\n",
      "\tspeed: 0.0158s/iter; left time: 227.8650s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0126065 Vali Loss: 0.0168263 Test Loss: 0.0216261\n",
      "Validation loss decreased (0.016834 --> 0.016826).  Saving model ...\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0144185\n",
      "\tspeed: 0.0388s/iter; left time: 554.9932s\n",
      "\titers: 200, epoch: 37 | loss: 0.0119868\n",
      "\tspeed: 0.0151s/iter; left time: 215.1416s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:03.78s\n",
      "Steps: 225 | Train Loss: 0.0125955 Vali Loss: 0.0169518 Test Loss: 0.0218011\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0122964\n",
      "\tspeed: 0.0443s/iter; left time: 623.1646s\n",
      "\titers: 200, epoch: 38 | loss: 0.0118971\n",
      "\tspeed: 0.0211s/iter; left time: 295.5919s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:05.05s\n",
      "Steps: 225 | Train Loss: 0.0125857 Vali Loss: 0.0168467 Test Loss: 0.0216006\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0128205\n",
      "\tspeed: 0.0403s/iter; left time: 558.1238s\n",
      "\titers: 200, epoch: 39 | loss: 0.0116396\n",
      "\tspeed: 0.0167s/iter; left time: 229.2487s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:04.08s\n",
      "Steps: 225 | Train Loss: 0.0125828 Vali Loss: 0.0168230 Test Loss: 0.0214886\n",
      "Validation loss decreased (0.016826 --> 0.016823).  Saving model ...\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0135852\n",
      "\tspeed: 0.0387s/iter; left time: 527.0576s\n",
      "\titers: 200, epoch: 40 | loss: 0.0120434\n",
      "\tspeed: 0.0207s/iter; left time: 279.9543s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 225 | Train Loss: 0.0125880 Vali Loss: 0.0168890 Test Loss: 0.0216618\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0116446\n",
      "\tspeed: 0.0483s/iter; left time: 646.7323s\n",
      "\titers: 200, epoch: 41 | loss: 0.0124125\n",
      "\tspeed: 0.0173s/iter; left time: 229.7905s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:04.41s\n",
      "Steps: 225 | Train Loss: 0.0125534 Vali Loss: 0.0168535 Test Loss: 0.0215649\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0124781\n",
      "\tspeed: 0.0422s/iter; left time: 555.5906s\n",
      "\titers: 200, epoch: 42 | loss: 0.0123067\n",
      "\tspeed: 0.0213s/iter; left time: 278.2179s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:04.85s\n",
      "Steps: 225 | Train Loss: 0.0125586 Vali Loss: 0.0168594 Test Loss: 0.0216211\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0127494\n",
      "\tspeed: 0.0471s/iter; left time: 610.3690s\n",
      "\titers: 200, epoch: 43 | loss: 0.0140274\n",
      "\tspeed: 0.0223s/iter; left time: 286.8690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 225 | Train Loss: 0.0125723 Vali Loss: 0.0168140 Test Loss: 0.0215049\n",
      "Validation loss decreased (0.016823 --> 0.016814).  Saving model ...\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0122186\n",
      "\tspeed: 0.0391s/iter; left time: 497.0265s\n",
      "\titers: 200, epoch: 44 | loss: 0.0118770\n",
      "\tspeed: 0.0164s/iter; left time: 206.4759s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0125745 Vali Loss: 0.0167932 Test Loss: 0.0215052\n",
      "Validation loss decreased (0.016814 --> 0.016793).  Saving model ...\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0120009\n",
      "\tspeed: 0.0387s/iter; left time: 483.6525s\n",
      "\titers: 200, epoch: 45 | loss: 0.0124589\n",
      "\tspeed: 0.0197s/iter; left time: 243.7141s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:04.55s\n",
      "Steps: 225 | Train Loss: 0.0125587 Vali Loss: 0.0168187 Test Loss: 0.0215558\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0133484\n",
      "\tspeed: 0.0488s/iter; left time: 598.8816s\n",
      "\titers: 200, epoch: 46 | loss: 0.0134764\n",
      "\tspeed: 0.0162s/iter; left time: 197.7391s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:04.14s\n",
      "Steps: 225 | Train Loss: 0.0125362 Vali Loss: 0.0168852 Test Loss: 0.0217203\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0124050\n",
      "\tspeed: 0.0387s/iter; left time: 466.7395s\n",
      "\titers: 200, epoch: 47 | loss: 0.0133607\n",
      "\tspeed: 0.0158s/iter; left time: 189.2435s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:03.94s\n",
      "Steps: 225 | Train Loss: 0.0125397 Vali Loss: 0.0167599 Test Loss: 0.0214477\n",
      "Validation loss decreased (0.016793 --> 0.016760).  Saving model ...\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0133222\n",
      "\tspeed: 0.0450s/iter; left time: 532.3111s\n",
      "\titers: 200, epoch: 48 | loss: 0.0111298\n",
      "\tspeed: 0.0214s/iter; left time: 251.0576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 225 | Train Loss: 0.0125691 Vali Loss: 0.0169273 Test Loss: 0.0217705\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0110154\n",
      "\tspeed: 0.0456s/iter; left time: 528.4407s\n",
      "\titers: 200, epoch: 49 | loss: 0.0124768\n",
      "\tspeed: 0.0225s/iter; left time: 259.1402s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:05.40s\n",
      "Steps: 225 | Train Loss: 0.0125564 Vali Loss: 0.0168432 Test Loss: 0.0215787\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0128464\n",
      "\tspeed: 0.0424s/iter; left time: 482.3756s\n",
      "\titers: 200, epoch: 50 | loss: 0.0131624\n",
      "\tspeed: 0.0222s/iter; left time: 250.6670s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 225 | Train Loss: 0.0125534 Vali Loss: 0.0168483 Test Loss: 0.0216448\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0111215\n",
      "\tspeed: 0.0430s/iter; left time: 479.0803s\n",
      "\titers: 200, epoch: 51 | loss: 0.0122721\n",
      "\tspeed: 0.0172s/iter; left time: 190.1557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:04.08s\n",
      "Steps: 225 | Train Loss: 0.0125398 Vali Loss: 0.0168382 Test Loss: 0.0215763\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0140247\n",
      "\tspeed: 0.0411s/iter; left time: 449.1896s\n",
      "\titers: 200, epoch: 52 | loss: 0.0110990\n",
      "\tspeed: 0.0162s/iter; left time: 174.8441s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:03.93s\n",
      "Steps: 225 | Train Loss: 0.0125283 Vali Loss: 0.0168521 Test Loss: 0.0215805\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0119962\n",
      "\tspeed: 0.0444s/iter; left time: 475.1415s\n",
      "\titers: 200, epoch: 53 | loss: 0.0125828\n",
      "\tspeed: 0.0201s/iter; left time: 212.8281s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0125533 Vali Loss: 0.0168662 Test Loss: 0.0217082\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0110893\n",
      "\tspeed: 0.0394s/iter; left time: 413.0468s\n",
      "\titers: 200, epoch: 54 | loss: 0.0135871\n",
      "\tspeed: 0.0163s/iter; left time: 169.4941s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0125420 Vali Loss: 0.0168111 Test Loss: 0.0215567\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0126214\n",
      "\tspeed: 0.0407s/iter; left time: 417.3350s\n",
      "\titers: 200, epoch: 55 | loss: 0.0112188\n",
      "\tspeed: 0.0192s/iter; left time: 194.5268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:04.32s\n",
      "Steps: 225 | Train Loss: 0.0125635 Vali Loss: 0.0168251 Test Loss: 0.0216542\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0120411\n",
      "\tspeed: 0.0409s/iter; left time: 409.6515s\n",
      "\titers: 200, epoch: 56 | loss: 0.0110059\n",
      "\tspeed: 0.0184s/iter; left time: 182.2896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:04.26s\n",
      "Steps: 225 | Train Loss: 0.0125221 Vali Loss: 0.0169018 Test Loss: 0.0217581\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0125544\n",
      "\tspeed: 0.0399s/iter; left time: 391.1170s\n",
      "\titers: 200, epoch: 57 | loss: 0.0121166\n",
      "\tspeed: 0.0164s/iter; left time: 158.8493s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:04.00s\n",
      "Steps: 225 | Train Loss: 0.0125559 Vali Loss: 0.0168544 Test Loss: 0.0216700\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.021447671577334404, rmse:0.1464502364397049, mae:0.09291932731866837, rse:0.567215621471405\n",
      "Intermediate time for FR and pred_len 168: 00h:05m:47.83s\n",
      "Intermediate time for FR: 00h:20m:19.96s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1377798\n",
      "\tspeed: 0.0314s/iter; left time: 707.1201s\n",
      "\titers: 200, epoch: 1 | loss: 0.1134172\n",
      "\tspeed: 0.0169s/iter; left time: 378.1894s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.37s\n",
      "Steps: 226 | Train Loss: 0.1356952 Vali Loss: 0.0691775 Test Loss: 0.0705373\n",
      "Validation loss decreased (inf --> 0.069177).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0454729\n",
      "\tspeed: 0.0403s/iter; left time: 897.9605s\n",
      "\titers: 200, epoch: 2 | loss: 0.0281168\n",
      "\tspeed: 0.0166s/iter; left time: 367.4979s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.15s\n",
      "Steps: 226 | Train Loss: 0.0508938 Vali Loss: 0.0187556 Test Loss: 0.0189566\n",
      "Validation loss decreased (0.069177 --> 0.018756).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0249880\n",
      "\tspeed: 0.0380s/iter; left time: 837.9958s\n",
      "\titers: 200, epoch: 3 | loss: 0.0227560\n",
      "\tspeed: 0.0156s/iter; left time: 343.0872s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:03.89s\n",
      "Steps: 226 | Train Loss: 0.0242646 Vali Loss: 0.0175866 Test Loss: 0.0178655\n",
      "Validation loss decreased (0.018756 --> 0.017587).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0204239\n",
      "\tspeed: 0.0374s/iter; left time: 816.9283s\n",
      "\titers: 200, epoch: 4 | loss: 0.0167463\n",
      "\tspeed: 0.0181s/iter; left time: 392.6846s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.10s\n",
      "Steps: 226 | Train Loss: 0.0206555 Vali Loss: 0.0154143 Test Loss: 0.0155426\n",
      "Validation loss decreased (0.017587 --> 0.015414).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0191221\n",
      "\tspeed: 0.0401s/iter; left time: 865.8100s\n",
      "\titers: 200, epoch: 5 | loss: 0.0179364\n",
      "\tspeed: 0.0166s/iter; left time: 356.6793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 226 | Train Loss: 0.0187411 Vali Loss: 0.0150593 Test Loss: 0.0152536\n",
      "Validation loss decreased (0.015414 --> 0.015059).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0172716\n",
      "\tspeed: 0.0366s/iter; left time: 781.9464s\n",
      "\titers: 200, epoch: 6 | loss: 0.0160908\n",
      "\tspeed: 0.0158s/iter; left time: 335.3546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:03.82s\n",
      "Steps: 226 | Train Loss: 0.0173488 Vali Loss: 0.0142850 Test Loss: 0.0140409\n",
      "Validation loss decreased (0.015059 --> 0.014285).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0192723\n",
      "\tspeed: 0.0388s/iter; left time: 821.4624s\n",
      "\titers: 200, epoch: 7 | loss: 0.0155777\n",
      "\tspeed: 0.0155s/iter; left time: 325.4104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:03.84s\n",
      "Steps: 226 | Train Loss: 0.0162256 Vali Loss: 0.0136374 Test Loss: 0.0131467\n",
      "Validation loss decreased (0.014285 --> 0.013637).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0142089\n",
      "\tspeed: 0.0372s/iter; left time: 778.5808s\n",
      "\titers: 200, epoch: 8 | loss: 0.0156101\n",
      "\tspeed: 0.0154s/iter; left time: 321.6397s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:03.75s\n",
      "Steps: 226 | Train Loss: 0.0153529 Vali Loss: 0.0132084 Test Loss: 0.0128746\n",
      "Validation loss decreased (0.013637 --> 0.013208).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0145481\n",
      "\tspeed: 0.0356s/iter; left time: 737.3247s\n",
      "\titers: 200, epoch: 9 | loss: 0.0146893\n",
      "\tspeed: 0.0176s/iter; left time: 363.1967s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.11s\n",
      "Steps: 226 | Train Loss: 0.0145578 Vali Loss: 0.0127671 Test Loss: 0.0125622\n",
      "Validation loss decreased (0.013208 --> 0.012767).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0143092\n",
      "\tspeed: 0.0400s/iter; left time: 817.7824s\n",
      "\titers: 200, epoch: 10 | loss: 0.0153943\n",
      "\tspeed: 0.0153s/iter; left time: 311.9292s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 226 | Train Loss: 0.0139438 Vali Loss: 0.0124113 Test Loss: 0.0123004\n",
      "Validation loss decreased (0.012767 --> 0.012411).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0136494\n",
      "\tspeed: 0.0383s/iter; left time: 774.2644s\n",
      "\titers: 200, epoch: 11 | loss: 0.0137830\n",
      "\tspeed: 0.0154s/iter; left time: 311.1302s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:03.92s\n",
      "Steps: 226 | Train Loss: 0.0135146 Vali Loss: 0.0120716 Test Loss: 0.0119796\n",
      "Validation loss decreased (0.012411 --> 0.012072).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0143659\n",
      "\tspeed: 0.0392s/iter; left time: 783.6507s\n",
      "\titers: 200, epoch: 12 | loss: 0.0141641\n",
      "\tspeed: 0.0160s/iter; left time: 319.1634s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.10s\n",
      "Steps: 226 | Train Loss: 0.0132291 Vali Loss: 0.0119187 Test Loss: 0.0118970\n",
      "Validation loss decreased (0.012072 --> 0.011919).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0125218\n",
      "\tspeed: 0.0374s/iter; left time: 739.4350s\n",
      "\titers: 200, epoch: 13 | loss: 0.0105008\n",
      "\tspeed: 0.0155s/iter; left time: 306.1304s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:03.72s\n",
      "Steps: 226 | Train Loss: 0.0128624 Vali Loss: 0.0116479 Test Loss: 0.0117148\n",
      "Validation loss decreased (0.011919 --> 0.011648).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0129370\n",
      "\tspeed: 0.0386s/iter; left time: 755.8248s\n",
      "\titers: 200, epoch: 14 | loss: 0.0128722\n",
      "\tspeed: 0.0188s/iter; left time: 365.2389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.20s\n",
      "Steps: 226 | Train Loss: 0.0126495 Vali Loss: 0.0116615 Test Loss: 0.0116488\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0116378\n",
      "\tspeed: 0.0387s/iter; left time: 748.0828s\n",
      "\titers: 200, epoch: 15 | loss: 0.0139435\n",
      "\tspeed: 0.0166s/iter; left time: 318.6525s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:03.97s\n",
      "Steps: 226 | Train Loss: 0.0124032 Vali Loss: 0.0113834 Test Loss: 0.0114933\n",
      "Validation loss decreased (0.011648 --> 0.011383).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0125016\n",
      "\tspeed: 0.0367s/iter; left time: 701.9640s\n",
      "\titers: 200, epoch: 16 | loss: 0.0106876\n",
      "\tspeed: 0.0158s/iter; left time: 299.9581s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:03.76s\n",
      "Steps: 226 | Train Loss: 0.0122276 Vali Loss: 0.0113101 Test Loss: 0.0114584\n",
      "Validation loss decreased (0.011383 --> 0.011310).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0115151\n",
      "\tspeed: 0.0387s/iter; left time: 731.5339s\n",
      "\titers: 200, epoch: 17 | loss: 0.0120240\n",
      "\tspeed: 0.0162s/iter; left time: 305.2480s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.11s\n",
      "Steps: 226 | Train Loss: 0.0120420 Vali Loss: 0.0112633 Test Loss: 0.0113524\n",
      "Validation loss decreased (0.011310 --> 0.011263).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0127090\n",
      "\tspeed: 0.0381s/iter; left time: 710.4053s\n",
      "\titers: 200, epoch: 18 | loss: 0.0114372\n",
      "\tspeed: 0.0158s/iter; left time: 292.6143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:03.80s\n",
      "Steps: 226 | Train Loss: 0.0119783 Vali Loss: 0.0111370 Test Loss: 0.0113526\n",
      "Validation loss decreased (0.011263 --> 0.011137).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0128387\n",
      "\tspeed: 0.0366s/iter; left time: 674.8819s\n",
      "\titers: 200, epoch: 19 | loss: 0.0113665\n",
      "\tspeed: 0.0160s/iter; left time: 292.4530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:03.97s\n",
      "Steps: 226 | Train Loss: 0.0118393 Vali Loss: 0.0110697 Test Loss: 0.0112947\n",
      "Validation loss decreased (0.011137 --> 0.011070).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0107364\n",
      "\tspeed: 0.0411s/iter; left time: 748.1290s\n",
      "\titers: 200, epoch: 20 | loss: 0.0134741\n",
      "\tspeed: 0.0162s/iter; left time: 292.8654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 226 | Train Loss: 0.0116963 Vali Loss: 0.0109283 Test Loss: 0.0112035\n",
      "Validation loss decreased (0.011070 --> 0.010928).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0119992\n",
      "\tspeed: 0.0371s/iter; left time: 667.2474s\n",
      "\titers: 200, epoch: 21 | loss: 0.0122915\n",
      "\tspeed: 0.0162s/iter; left time: 289.3438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:03.85s\n",
      "Steps: 226 | Train Loss: 0.0116031 Vali Loss: 0.0109897 Test Loss: 0.0112439\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0104237\n",
      "\tspeed: 0.0400s/iter; left time: 709.7304s\n",
      "\titers: 200, epoch: 22 | loss: 0.0119740\n",
      "\tspeed: 0.0196s/iter; left time: 345.3108s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:04.49s\n",
      "Steps: 226 | Train Loss: 0.0115132 Vali Loss: 0.0108854 Test Loss: 0.0111452\n",
      "Validation loss decreased (0.010928 --> 0.010885).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0096794\n",
      "\tspeed: 0.0427s/iter; left time: 747.6887s\n",
      "\titers: 200, epoch: 23 | loss: 0.0107608\n",
      "\tspeed: 0.0175s/iter; left time: 304.7474s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:04.49s\n",
      "Steps: 226 | Train Loss: 0.0114976 Vali Loss: 0.0107883 Test Loss: 0.0111224\n",
      "Validation loss decreased (0.010885 --> 0.010788).  Saving model ...\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0098029\n",
      "\tspeed: 0.0372s/iter; left time: 643.5511s\n",
      "\titers: 200, epoch: 24 | loss: 0.0121425\n",
      "\tspeed: 0.0159s/iter; left time: 274.0367s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:03.95s\n",
      "Steps: 226 | Train Loss: 0.0114222 Vali Loss: 0.0107622 Test Loss: 0.0111308\n",
      "Validation loss decreased (0.010788 --> 0.010762).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0106548\n",
      "\tspeed: 0.0396s/iter; left time: 676.6428s\n",
      "\titers: 200, epoch: 25 | loss: 0.0118627\n",
      "\tspeed: 0.0156s/iter; left time: 264.3130s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:03.95s\n",
      "Steps: 226 | Train Loss: 0.0113376 Vali Loss: 0.0107183 Test Loss: 0.0110923\n",
      "Validation loss decreased (0.010762 --> 0.010718).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0095081\n",
      "\tspeed: 0.0400s/iter; left time: 674.5490s\n",
      "\titers: 200, epoch: 26 | loss: 0.0126063\n",
      "\tspeed: 0.0240s/iter; left time: 402.0363s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:05.23s\n",
      "Steps: 226 | Train Loss: 0.0113432 Vali Loss: 0.0106847 Test Loss: 0.0110543\n",
      "Validation loss decreased (0.010718 --> 0.010685).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0102837\n",
      "\tspeed: 0.0411s/iter; left time: 682.6351s\n",
      "\titers: 200, epoch: 27 | loss: 0.0128509\n",
      "\tspeed: 0.0154s/iter; left time: 253.7999s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:03.83s\n",
      "Steps: 226 | Train Loss: 0.0112524 Vali Loss: 0.0106316 Test Loss: 0.0110191\n",
      "Validation loss decreased (0.010685 --> 0.010632).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0112080\n",
      "\tspeed: 0.0379s/iter; left time: 621.3778s\n",
      "\titers: 200, epoch: 28 | loss: 0.0097912\n",
      "\tspeed: 0.0160s/iter; left time: 261.3668s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:03.95s\n",
      "Steps: 226 | Train Loss: 0.0112507 Vali Loss: 0.0106654 Test Loss: 0.0110507\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0117082\n",
      "\tspeed: 0.0456s/iter; left time: 737.3555s\n",
      "\titers: 200, epoch: 29 | loss: 0.0121781\n",
      "\tspeed: 0.0150s/iter; left time: 240.7264s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:04.22s\n",
      "Steps: 226 | Train Loss: 0.0112207 Vali Loss: 0.0106669 Test Loss: 0.0110702\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0096842\n",
      "\tspeed: 0.0361s/iter; left time: 575.5568s\n",
      "\titers: 200, epoch: 30 | loss: 0.0130751\n",
      "\tspeed: 0.0153s/iter; left time: 242.5021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:03.74s\n",
      "Steps: 226 | Train Loss: 0.0111561 Vali Loss: 0.0106135 Test Loss: 0.0109990\n",
      "Validation loss decreased (0.010632 --> 0.010614).  Saving model ...\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0104511\n",
      "\tspeed: 0.0394s/iter; left time: 618.7753s\n",
      "\titers: 200, epoch: 31 | loss: 0.0122376\n",
      "\tspeed: 0.0193s/iter; left time: 301.8339s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 226 | Train Loss: 0.0111730 Vali Loss: 0.0105777 Test Loss: 0.0109957\n",
      "Validation loss decreased (0.010614 --> 0.010578).  Saving model ...\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0099746\n",
      "\tspeed: 0.0429s/iter; left time: 665.1567s\n",
      "\titers: 200, epoch: 32 | loss: 0.0106001\n",
      "\tspeed: 0.0168s/iter; left time: 259.0305s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:04.01s\n",
      "Steps: 226 | Train Loss: 0.0111217 Vali Loss: 0.0105124 Test Loss: 0.0109593\n",
      "Validation loss decreased (0.010578 --> 0.010512).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0115203\n",
      "\tspeed: 0.0366s/iter; left time: 559.4682s\n",
      "\titers: 200, epoch: 33 | loss: 0.0115479\n",
      "\tspeed: 0.0152s/iter; left time: 231.3184s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:03.75s\n",
      "Steps: 226 | Train Loss: 0.0111433 Vali Loss: 0.0105266 Test Loss: 0.0109338\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0098360\n",
      "\tspeed: 0.0469s/iter; left time: 706.2570s\n",
      "\titers: 200, epoch: 34 | loss: 0.0118058\n",
      "\tspeed: 0.0164s/iter; left time: 244.6445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0110997 Vali Loss: 0.0105341 Test Loss: 0.0109536\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0106727\n",
      "\tspeed: 0.0352s/iter; left time: 521.5914s\n",
      "\titers: 200, epoch: 35 | loss: 0.0121903\n",
      "\tspeed: 0.0168s/iter; left time: 246.5298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:03.88s\n",
      "Steps: 226 | Train Loss: 0.0110825 Vali Loss: 0.0105078 Test Loss: 0.0109191\n",
      "Validation loss decreased (0.010512 --> 0.010508).  Saving model ...\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0109003\n",
      "\tspeed: 0.0425s/iter; left time: 620.2390s\n",
      "\titers: 200, epoch: 36 | loss: 0.0107535\n",
      "\tspeed: 0.0212s/iter; left time: 307.8069s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:04.92s\n",
      "Steps: 226 | Train Loss: 0.0110776 Vali Loss: 0.0104943 Test Loss: 0.0109287\n",
      "Validation loss decreased (0.010508 --> 0.010494).  Saving model ...\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0111721\n",
      "\tspeed: 0.0437s/iter; left time: 627.5663s\n",
      "\titers: 200, epoch: 37 | loss: 0.0111821\n",
      "\tspeed: 0.0165s/iter; left time: 235.6323s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 226 | Train Loss: 0.0110402 Vali Loss: 0.0104634 Test Loss: 0.0109022\n",
      "Validation loss decreased (0.010494 --> 0.010463).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0107937\n",
      "\tspeed: 0.0412s/iter; left time: 583.2147s\n",
      "\titers: 200, epoch: 38 | loss: 0.0121695\n",
      "\tspeed: 0.0154s/iter; left time: 216.5530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:04.11s\n",
      "Steps: 226 | Train Loss: 0.0110640 Vali Loss: 0.0104891 Test Loss: 0.0109258\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0102771\n",
      "\tspeed: 0.0486s/iter; left time: 675.8196s\n",
      "\titers: 200, epoch: 39 | loss: 0.0103673\n",
      "\tspeed: 0.0199s/iter; left time: 274.8001s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:05.07s\n",
      "Steps: 226 | Train Loss: 0.0110183 Vali Loss: 0.0104331 Test Loss: 0.0108840\n",
      "Validation loss decreased (0.010463 --> 0.010433).  Saving model ...\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0122261\n",
      "\tspeed: 0.0368s/iter; left time: 504.2283s\n",
      "\titers: 200, epoch: 40 | loss: 0.0112992\n",
      "\tspeed: 0.0182s/iter; left time: 247.8934s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:04.10s\n",
      "Steps: 226 | Train Loss: 0.0110450 Vali Loss: 0.0104266 Test Loss: 0.0109060\n",
      "Validation loss decreased (0.010433 --> 0.010427).  Saving model ...\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0105818\n",
      "\tspeed: 0.0376s/iter; left time: 506.4033s\n",
      "\titers: 200, epoch: 41 | loss: 0.0132477\n",
      "\tspeed: 0.0219s/iter; left time: 292.4338s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:04.76s\n",
      "Steps: 226 | Train Loss: 0.0110133 Vali Loss: 0.0104213 Test Loss: 0.0108878\n",
      "Validation loss decreased (0.010427 --> 0.010421).  Saving model ...\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0100628\n",
      "\tspeed: 0.0434s/iter; left time: 574.7919s\n",
      "\titers: 200, epoch: 42 | loss: 0.0111368\n",
      "\tspeed: 0.0186s/iter; left time: 243.9086s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:04.50s\n",
      "Steps: 226 | Train Loss: 0.0109489 Vali Loss: 0.0104123 Test Loss: 0.0108733\n",
      "Validation loss decreased (0.010421 --> 0.010412).  Saving model ...\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0115083\n",
      "\tspeed: 0.0402s/iter; left time: 522.8612s\n",
      "\titers: 200, epoch: 43 | loss: 0.0104760\n",
      "\tspeed: 0.0185s/iter; left time: 239.0482s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:04.23s\n",
      "Steps: 226 | Train Loss: 0.0109990 Vali Loss: 0.0103668 Test Loss: 0.0108622\n",
      "Validation loss decreased (0.010412 --> 0.010367).  Saving model ...\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0111897\n",
      "\tspeed: 0.0484s/iter; left time: 618.6437s\n",
      "\titers: 200, epoch: 44 | loss: 0.0107037\n",
      "\tspeed: 0.0172s/iter; left time: 218.4136s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:04.32s\n",
      "Steps: 226 | Train Loss: 0.0110186 Vali Loss: 0.0104406 Test Loss: 0.0108855\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0110654\n",
      "\tspeed: 0.0381s/iter; left time: 478.3879s\n",
      "\titers: 200, epoch: 45 | loss: 0.0112306\n",
      "\tspeed: 0.0153s/iter; left time: 190.6268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:03.94s\n",
      "Steps: 226 | Train Loss: 0.0110232 Vali Loss: 0.0103820 Test Loss: 0.0108644\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0123103\n",
      "\tspeed: 0.0368s/iter; left time: 454.1900s\n",
      "\titers: 200, epoch: 46 | loss: 0.0101690\n",
      "\tspeed: 0.0212s/iter; left time: 259.1501s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0109794 Vali Loss: 0.0104060 Test Loss: 0.0108795\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0109692\n",
      "\tspeed: 0.0389s/iter; left time: 470.5442s\n",
      "\titers: 200, epoch: 47 | loss: 0.0125076\n",
      "\tspeed: 0.0162s/iter; left time: 194.4782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 226 | Train Loss: 0.0109345 Vali Loss: 0.0103910 Test Loss: 0.0108578\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0114056\n",
      "\tspeed: 0.0374s/iter; left time: 444.4565s\n",
      "\titers: 200, epoch: 48 | loss: 0.0106638\n",
      "\tspeed: 0.0172s/iter; left time: 202.0412s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:04.04s\n",
      "Steps: 226 | Train Loss: 0.0109523 Vali Loss: 0.0104009 Test Loss: 0.0108567\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0106638\n",
      "\tspeed: 0.0453s/iter; left time: 528.1298s\n",
      "\titers: 200, epoch: 49 | loss: 0.0125533\n",
      "\tspeed: 0.0154s/iter; left time: 177.8244s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:04.31s\n",
      "Steps: 226 | Train Loss: 0.0109971 Vali Loss: 0.0104058 Test Loss: 0.0108558\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0118140\n",
      "\tspeed: 0.0383s/iter; left time: 438.2017s\n",
      "\titers: 200, epoch: 50 | loss: 0.0120683\n",
      "\tspeed: 0.0156s/iter; left time: 176.2724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:04.08s\n",
      "Steps: 226 | Train Loss: 0.0109812 Vali Loss: 0.0104026 Test Loss: 0.0108490\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0122474\n",
      "\tspeed: 0.0415s/iter; left time: 464.7840s\n",
      "\titers: 200, epoch: 51 | loss: 0.0104948\n",
      "\tspeed: 0.0204s/iter; left time: 226.8309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 226 | Train Loss: 0.0109754 Vali Loss: 0.0103916 Test Loss: 0.0108564\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0111523\n",
      "\tspeed: 0.0487s/iter; left time: 534.3213s\n",
      "\titers: 200, epoch: 52 | loss: 0.0121978\n",
      "\tspeed: 0.0230s/iter; left time: 250.0751s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:05.31s\n",
      "Steps: 226 | Train Loss: 0.0109734 Vali Loss: 0.0103702 Test Loss: 0.0108537\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0112344\n",
      "\tspeed: 0.0447s/iter; left time: 480.0841s\n",
      "\titers: 200, epoch: 53 | loss: 0.0115661\n",
      "\tspeed: 0.0268s/iter; left time: 285.0449s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:05.91s\n",
      "Steps: 226 | Train Loss: 0.0109682 Vali Loss: 0.0103765 Test Loss: 0.0108480\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01086221169680357, rmse:0.10422193259000778, mae:0.06578190624713898, rse:0.3938033878803253\n",
      "Intermediate time for IT and pred_len 24: 00h:05m:10.46s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1392661\n",
      "\tspeed: 0.0307s/iter; left time: 688.2546s\n",
      "\titers: 200, epoch: 1 | loss: 0.1204742\n",
      "\tspeed: 0.0182s/iter; left time: 406.0846s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.58s\n",
      "Steps: 225 | Train Loss: 0.1416145 Vali Loss: 0.0775000 Test Loss: 0.0788219\n",
      "Validation loss decreased (inf --> 0.077500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0494399\n",
      "\tspeed: 0.0501s/iter; left time: 1111.1051s\n",
      "\titers: 200, epoch: 2 | loss: 0.0351452\n",
      "\tspeed: 0.0159s/iter; left time: 351.1722s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0541683 Vali Loss: 0.0280526 Test Loss: 0.0307065\n",
      "Validation loss decreased (0.077500 --> 0.028053).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0307060\n",
      "\tspeed: 0.0398s/iter; left time: 873.9285s\n",
      "\titers: 200, epoch: 3 | loss: 0.0298869\n",
      "\tspeed: 0.0187s/iter; left time: 408.9647s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.45s\n",
      "Steps: 225 | Train Loss: 0.0315663 Vali Loss: 0.0246129 Test Loss: 0.0260684\n",
      "Validation loss decreased (0.028053 --> 0.024613).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0269790\n",
      "\tspeed: 0.0395s/iter; left time: 857.2184s\n",
      "\titers: 200, epoch: 4 | loss: 0.0263730\n",
      "\tspeed: 0.0246s/iter; left time: 532.0685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.97s\n",
      "Steps: 225 | Train Loss: 0.0273871 Vali Loss: 0.0219977 Test Loss: 0.0225720\n",
      "Validation loss decreased (0.024613 --> 0.021998).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0226175\n",
      "\tspeed: 0.0406s/iter; left time: 872.2530s\n",
      "\titers: 200, epoch: 5 | loss: 0.0209078\n",
      "\tspeed: 0.0154s/iter; left time: 328.9814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:03.91s\n",
      "Steps: 225 | Train Loss: 0.0243868 Vali Loss: 0.0199966 Test Loss: 0.0200491\n",
      "Validation loss decreased (0.021998 --> 0.019997).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0221272\n",
      "\tspeed: 0.0414s/iter; left time: 880.3996s\n",
      "\titers: 200, epoch: 6 | loss: 0.0230589\n",
      "\tspeed: 0.0164s/iter; left time: 346.6339s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.04s\n",
      "Steps: 225 | Train Loss: 0.0224359 Vali Loss: 0.0190496 Test Loss: 0.0193386\n",
      "Validation loss decreased (0.019997 --> 0.019050).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0198746\n",
      "\tspeed: 0.0531s/iter; left time: 1118.7714s\n",
      "\titers: 200, epoch: 7 | loss: 0.0217593\n",
      "\tspeed: 0.0159s/iter; left time: 333.1639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.59s\n",
      "Steps: 225 | Train Loss: 0.0213778 Vali Loss: 0.0183945 Test Loss: 0.0188222\n",
      "Validation loss decreased (0.019050 --> 0.018394).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0216212\n",
      "\tspeed: 0.0397s/iter; left time: 826.0295s\n",
      "\titers: 200, epoch: 8 | loss: 0.0201936\n",
      "\tspeed: 0.0155s/iter; left time: 320.3548s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:03.89s\n",
      "Steps: 225 | Train Loss: 0.0206121 Vali Loss: 0.0181737 Test Loss: 0.0186618\n",
      "Validation loss decreased (0.018394 --> 0.018174).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0186395\n",
      "\tspeed: 0.0385s/iter; left time: 793.5908s\n",
      "\titers: 200, epoch: 9 | loss: 0.0181642\n",
      "\tspeed: 0.0234s/iter; left time: 479.1724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0200061 Vali Loss: 0.0179292 Test Loss: 0.0186835\n",
      "Validation loss decreased (0.018174 --> 0.017929).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0188193\n",
      "\tspeed: 0.0399s/iter; left time: 812.9797s\n",
      "\titers: 200, epoch: 10 | loss: 0.0192629\n",
      "\tspeed: 0.0192s/iter; left time: 389.1762s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.30s\n",
      "Steps: 225 | Train Loss: 0.0196039 Vali Loss: 0.0174620 Test Loss: 0.0184903\n",
      "Validation loss decreased (0.017929 --> 0.017462).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0191812\n",
      "\tspeed: 0.0393s/iter; left time: 791.6841s\n",
      "\titers: 200, epoch: 11 | loss: 0.0187980\n",
      "\tspeed: 0.0165s/iter; left time: 331.2367s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0191906 Vali Loss: 0.0173062 Test Loss: 0.0184059\n",
      "Validation loss decreased (0.017462 --> 0.017306).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0182141\n",
      "\tspeed: 0.0501s/iter; left time: 997.6922s\n",
      "\titers: 200, epoch: 12 | loss: 0.0185133\n",
      "\tspeed: 0.0160s/iter; left time: 316.3352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 225 | Train Loss: 0.0188441 Vali Loss: 0.0172411 Test Loss: 0.0183969\n",
      "Validation loss decreased (0.017306 --> 0.017241).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0194464\n",
      "\tspeed: 0.0430s/iter; left time: 847.1020s\n",
      "\titers: 200, epoch: 13 | loss: 0.0186859\n",
      "\tspeed: 0.0170s/iter; left time: 333.6627s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.39s\n",
      "Steps: 225 | Train Loss: 0.0186049 Vali Loss: 0.0170427 Test Loss: 0.0182708\n",
      "Validation loss decreased (0.017241 --> 0.017043).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0203162\n",
      "\tspeed: 0.0383s/iter; left time: 745.2474s\n",
      "\titers: 200, epoch: 14 | loss: 0.0189898\n",
      "\tspeed: 0.0213s/iter; left time: 412.9784s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0185252 Vali Loss: 0.0170671 Test Loss: 0.0182412\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0181951\n",
      "\tspeed: 0.0438s/iter; left time: 843.1686s\n",
      "\titers: 200, epoch: 15 | loss: 0.0186964\n",
      "\tspeed: 0.0154s/iter; left time: 294.6511s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:03.90s\n",
      "Steps: 225 | Train Loss: 0.0185144 Vali Loss: 0.0168636 Test Loss: 0.0181672\n",
      "Validation loss decreased (0.017043 --> 0.016864).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0191887\n",
      "\tspeed: 0.0386s/iter; left time: 733.9795s\n",
      "\titers: 200, epoch: 16 | loss: 0.0185525\n",
      "\tspeed: 0.0161s/iter; left time: 304.2775s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:03.89s\n",
      "Steps: 225 | Train Loss: 0.0181708 Vali Loss: 0.0168464 Test Loss: 0.0181170\n",
      "Validation loss decreased (0.016864 --> 0.016846).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0163151\n",
      "\tspeed: 0.0435s/iter; left time: 817.9885s\n",
      "\titers: 200, epoch: 17 | loss: 0.0181100\n",
      "\tspeed: 0.0170s/iter; left time: 318.0484s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.60s\n",
      "Steps: 225 | Train Loss: 0.0180505 Vali Loss: 0.0168423 Test Loss: 0.0181461\n",
      "Validation loss decreased (0.016846 --> 0.016842).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0194149\n",
      "\tspeed: 0.0385s/iter; left time: 714.3310s\n",
      "\titers: 200, epoch: 18 | loss: 0.0171451\n",
      "\tspeed: 0.0156s/iter; left time: 289.1414s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:03.93s\n",
      "Steps: 225 | Train Loss: 0.0181414 Vali Loss: 0.0169269 Test Loss: 0.0180586\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0182438\n",
      "\tspeed: 0.0373s/iter; left time: 685.1473s\n",
      "\titers: 200, epoch: 19 | loss: 0.0176386\n",
      "\tspeed: 0.0194s/iter; left time: 354.4164s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:04.45s\n",
      "Steps: 225 | Train Loss: 0.0179795 Vali Loss: 0.0167794 Test Loss: 0.0180080\n",
      "Validation loss decreased (0.016842 --> 0.016779).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0172766\n",
      "\tspeed: 0.0477s/iter; left time: 864.4272s\n",
      "\titers: 200, epoch: 20 | loss: 0.0182251\n",
      "\tspeed: 0.0157s/iter; left time: 283.8901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:03.96s\n",
      "Steps: 225 | Train Loss: 0.0178784 Vali Loss: 0.0166909 Test Loss: 0.0179906\n",
      "Validation loss decreased (0.016779 --> 0.016691).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0180581\n",
      "\tspeed: 0.0361s/iter; left time: 646.7739s\n",
      "\titers: 200, epoch: 21 | loss: 0.0151387\n",
      "\tspeed: 0.0185s/iter; left time: 329.9807s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:04.10s\n",
      "Steps: 225 | Train Loss: 0.0179008 Vali Loss: 0.0167441 Test Loss: 0.0179501\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0192179\n",
      "\tspeed: 0.0394s/iter; left time: 697.1273s\n",
      "\titers: 200, epoch: 22 | loss: 0.0170067\n",
      "\tspeed: 0.0236s/iter; left time: 414.3055s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 225 | Train Loss: 0.0178270 Vali Loss: 0.0167418 Test Loss: 0.0179967\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0188884\n",
      "\tspeed: 0.0380s/iter; left time: 663.6254s\n",
      "\titers: 200, epoch: 23 | loss: 0.0179435\n",
      "\tspeed: 0.0155s/iter; left time: 269.5471s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:03.82s\n",
      "Steps: 225 | Train Loss: 0.0178563 Vali Loss: 0.0167881 Test Loss: 0.0179131\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0166721\n",
      "\tspeed: 0.0409s/iter; left time: 704.5206s\n",
      "\titers: 200, epoch: 24 | loss: 0.0167049\n",
      "\tspeed: 0.0182s/iter; left time: 311.3412s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 225 | Train Loss: 0.0177363 Vali Loss: 0.0166589 Test Loss: 0.0179240\n",
      "Validation loss decreased (0.016691 --> 0.016659).  Saving model ...\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0176328\n",
      "\tspeed: 0.0437s/iter; left time: 743.7240s\n",
      "\titers: 200, epoch: 25 | loss: 0.0188900\n",
      "\tspeed: 0.0169s/iter; left time: 285.7572s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:04.42s\n",
      "Steps: 225 | Train Loss: 0.0177095 Vali Loss: 0.0166848 Test Loss: 0.0179018\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0192010\n",
      "\tspeed: 0.0395s/iter; left time: 662.8999s\n",
      "\titers: 200, epoch: 26 | loss: 0.0169397\n",
      "\tspeed: 0.0162s/iter; left time: 270.7029s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:04.21s\n",
      "Steps: 225 | Train Loss: 0.0176629 Vali Loss: 0.0166513 Test Loss: 0.0178868\n",
      "Validation loss decreased (0.016659 --> 0.016651).  Saving model ...\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0183525\n",
      "\tspeed: 0.0360s/iter; left time: 596.6274s\n",
      "\titers: 200, epoch: 27 | loss: 0.0177022\n",
      "\tspeed: 0.0161s/iter; left time: 265.6034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:03.98s\n",
      "Steps: 225 | Train Loss: 0.0176258 Vali Loss: 0.0166351 Test Loss: 0.0179146\n",
      "Validation loss decreased (0.016651 --> 0.016635).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0178615\n",
      "\tspeed: 0.0470s/iter; left time: 767.7431s\n",
      "\titers: 200, epoch: 28 | loss: 0.0168865\n",
      "\tspeed: 0.0189s/iter; left time: 306.3214s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:04.71s\n",
      "Steps: 225 | Train Loss: 0.0176033 Vali Loss: 0.0166053 Test Loss: 0.0178937\n",
      "Validation loss decreased (0.016635 --> 0.016605).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0178555\n",
      "\tspeed: 0.0448s/iter; left time: 720.5228s\n",
      "\titers: 200, epoch: 29 | loss: 0.0161150\n",
      "\tspeed: 0.0198s/iter; left time: 316.2341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 225 | Train Loss: 0.0177332 Vali Loss: 0.0166127 Test Loss: 0.0179433\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0179884\n",
      "\tspeed: 0.0407s/iter; left time: 646.0243s\n",
      "\titers: 200, epoch: 30 | loss: 0.0178666\n",
      "\tspeed: 0.0174s/iter; left time: 274.2310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:04.29s\n",
      "Steps: 225 | Train Loss: 0.0176146 Vali Loss: 0.0166176 Test Loss: 0.0178403\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0168105\n",
      "\tspeed: 0.0417s/iter; left time: 653.0652s\n",
      "\titers: 200, epoch: 31 | loss: 0.0173617\n",
      "\tspeed: 0.0166s/iter; left time: 258.5197s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:04.29s\n",
      "Steps: 225 | Train Loss: 0.0176003 Vali Loss: 0.0166262 Test Loss: 0.0179176\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0176612\n",
      "\tspeed: 0.0419s/iter; left time: 646.9951s\n",
      "\titers: 200, epoch: 32 | loss: 0.0180373\n",
      "\tspeed: 0.0186s/iter; left time: 285.6559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 225 | Train Loss: 0.0175764 Vali Loss: 0.0165676 Test Loss: 0.0178750\n",
      "Validation loss decreased (0.016605 --> 0.016568).  Saving model ...\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0189767\n",
      "\tspeed: 0.0430s/iter; left time: 654.1170s\n",
      "\titers: 200, epoch: 33 | loss: 0.0189711\n",
      "\tspeed: 0.0186s/iter; left time: 280.9845s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:04.50s\n",
      "Steps: 225 | Train Loss: 0.0175471 Vali Loss: 0.0165941 Test Loss: 0.0178602\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0194196\n",
      "\tspeed: 0.0400s/iter; left time: 598.6058s\n",
      "\titers: 200, epoch: 34 | loss: 0.0171694\n",
      "\tspeed: 0.0188s/iter; left time: 279.3051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:04.50s\n",
      "Steps: 225 | Train Loss: 0.0177479 Vali Loss: 0.0165743 Test Loss: 0.0179026\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0174454\n",
      "\tspeed: 0.0399s/iter; left time: 588.7410s\n",
      "\titers: 200, epoch: 35 | loss: 0.0170476\n",
      "\tspeed: 0.0169s/iter; left time: 248.0224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:04.13s\n",
      "Steps: 225 | Train Loss: 0.0175278 Vali Loss: 0.0165909 Test Loss: 0.0178695\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0187867\n",
      "\tspeed: 0.0420s/iter; left time: 609.7748s\n",
      "\titers: 200, epoch: 36 | loss: 0.0183198\n",
      "\tspeed: 0.0179s/iter; left time: 257.9643s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:04.21s\n",
      "Steps: 225 | Train Loss: 0.0174957 Vali Loss: 0.0165789 Test Loss: 0.0178887\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0183138\n",
      "\tspeed: 0.0410s/iter; left time: 587.0511s\n",
      "\titers: 200, epoch: 37 | loss: 0.0150607\n",
      "\tspeed: 0.0162s/iter; left time: 229.8148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:04.06s\n",
      "Steps: 225 | Train Loss: 0.0174517 Vali Loss: 0.0165912 Test Loss: 0.0178495\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0182852\n",
      "\tspeed: 0.0420s/iter; left time: 590.7422s\n",
      "\titers: 200, epoch: 38 | loss: 0.0169175\n",
      "\tspeed: 0.0171s/iter; left time: 238.8473s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:04.24s\n",
      "Steps: 225 | Train Loss: 0.0175080 Vali Loss: 0.0166019 Test Loss: 0.0178501\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0156278\n",
      "\tspeed: 0.0421s/iter; left time: 582.4355s\n",
      "\titers: 200, epoch: 39 | loss: 0.0177135\n",
      "\tspeed: 0.0164s/iter; left time: 224.9442s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:04.16s\n",
      "Steps: 225 | Train Loss: 0.0174615 Vali Loss: 0.0165793 Test Loss: 0.0178496\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0160366\n",
      "\tspeed: 0.0396s/iter; left time: 539.8779s\n",
      "\titers: 200, epoch: 40 | loss: 0.0179165\n",
      "\tspeed: 0.0169s/iter; left time: 228.9658s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:04.15s\n",
      "Steps: 225 | Train Loss: 0.0175011 Vali Loss: 0.0165816 Test Loss: 0.0178705\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0154619\n",
      "\tspeed: 0.0416s/iter; left time: 557.3494s\n",
      "\titers: 200, epoch: 41 | loss: 0.0161167\n",
      "\tspeed: 0.0164s/iter; left time: 217.5904s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:04.25s\n",
      "Steps: 225 | Train Loss: 0.0175633 Vali Loss: 0.0166112 Test Loss: 0.0177791\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0202491\n",
      "\tspeed: 0.0427s/iter; left time: 562.0311s\n",
      "\titers: 200, epoch: 42 | loss: 0.0169839\n",
      "\tspeed: 0.0180s/iter; left time: 235.1703s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:04.56s\n",
      "Steps: 225 | Train Loss: 0.0174733 Vali Loss: 0.0166012 Test Loss: 0.0178329\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01787497289478779, rmse:0.13369731605052948, mae:0.08597136288881302, rse:0.5055239796638489\n",
      "Intermediate time for IT and pred_len 96: 00h:04m:16.29s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_revin_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_revin_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1453078\n",
      "\tspeed: 0.0322s/iter; left time: 721.7015s\n",
      "\titers: 200, epoch: 1 | loss: 0.1207458\n",
      "\tspeed: 0.0168s/iter; left time: 374.5495s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 225 | Train Loss: 0.1447215 Vali Loss: 0.0810303 Test Loss: 0.0813294\n",
      "Validation loss decreased (inf --> 0.081030).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0436250\n",
      "\tspeed: 0.0401s/iter; left time: 888.7858s\n",
      "\titers: 200, epoch: 2 | loss: 0.0376781\n",
      "\tspeed: 0.0192s/iter; left time: 424.3142s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0535831 Vali Loss: 0.0297240 Test Loss: 0.0314284\n",
      "Validation loss decreased (0.081030 --> 0.029724).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0338296\n",
      "\tspeed: 0.0488s/iter; left time: 1072.2832s\n",
      "\titers: 200, epoch: 3 | loss: 0.0325195\n",
      "\tspeed: 0.0248s/iter; left time: 542.5256s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.69s\n",
      "Steps: 225 | Train Loss: 0.0330422 Vali Loss: 0.0259590 Test Loss: 0.0272268\n",
      "Validation loss decreased (0.029724 --> 0.025959).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0318606\n",
      "\tspeed: 0.0404s/iter; left time: 878.8154s\n",
      "\titers: 200, epoch: 4 | loss: 0.0266739\n",
      "\tspeed: 0.0237s/iter; left time: 512.9830s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.99s\n",
      "Steps: 225 | Train Loss: 0.0283657 Vali Loss: 0.0227529 Test Loss: 0.0220421\n",
      "Validation loss decreased (0.025959 --> 0.022753).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0238208\n",
      "\tspeed: 0.0437s/iter; left time: 939.5025s\n",
      "\titers: 200, epoch: 5 | loss: 0.0228798\n",
      "\tspeed: 0.0200s/iter; left time: 427.9773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.78s\n",
      "Steps: 225 | Train Loss: 0.0252112 Vali Loss: 0.0213654 Test Loss: 0.0207886\n",
      "Validation loss decreased (0.022753 --> 0.021365).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0247101\n",
      "\tspeed: 0.0426s/iter; left time: 906.8229s\n",
      "\titers: 200, epoch: 6 | loss: 0.0228858\n",
      "\tspeed: 0.0168s/iter; left time: 356.5408s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.42s\n",
      "Steps: 225 | Train Loss: 0.0235146 Vali Loss: 0.0199140 Test Loss: 0.0200730\n",
      "Validation loss decreased (0.021365 --> 0.019914).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0229948\n",
      "\tspeed: 0.0491s/iter; left time: 1032.5559s\n",
      "\titers: 200, epoch: 7 | loss: 0.0219986\n",
      "\tspeed: 0.0166s/iter; left time: 347.7827s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.43s\n",
      "Steps: 225 | Train Loss: 0.0223978 Vali Loss: 0.0194992 Test Loss: 0.0198671\n",
      "Validation loss decreased (0.019914 --> 0.019499).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0198778\n",
      "\tspeed: 0.0511s/iter; left time: 1063.3477s\n",
      "\titers: 200, epoch: 8 | loss: 0.0218490\n",
      "\tspeed: 0.0171s/iter; left time: 353.7793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.49s\n",
      "Steps: 225 | Train Loss: 0.0218119 Vali Loss: 0.0193452 Test Loss: 0.0198867\n",
      "Validation loss decreased (0.019499 --> 0.019345).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0218882\n",
      "\tspeed: 0.0493s/iter; left time: 1016.2603s\n",
      "\titers: 200, epoch: 9 | loss: 0.0207634\n",
      "\tspeed: 0.0166s/iter; left time: 340.7163s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.29s\n",
      "Steps: 225 | Train Loss: 0.0212938 Vali Loss: 0.0188900 Test Loss: 0.0195716\n",
      "Validation loss decreased (0.019345 --> 0.018890).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0212140\n",
      "\tspeed: 0.0479s/iter; left time: 976.0093s\n",
      "\titers: 200, epoch: 10 | loss: 0.0213589\n",
      "\tspeed: 0.0169s/iter; left time: 342.3834s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.34s\n",
      "Steps: 225 | Train Loss: 0.0206631 Vali Loss: 0.0189463 Test Loss: 0.0195167\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0210216\n",
      "\tspeed: 0.0483s/iter; left time: 974.0693s\n",
      "\titers: 200, epoch: 11 | loss: 0.0209635\n",
      "\tspeed: 0.0175s/iter; left time: 350.2636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.51s\n",
      "Steps: 225 | Train Loss: 0.0201509 Vali Loss: 0.0184880 Test Loss: 0.0193298\n",
      "Validation loss decreased (0.018890 --> 0.018488).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0192917\n",
      "\tspeed: 0.0490s/iter; left time: 975.5385s\n",
      "\titers: 200, epoch: 12 | loss: 0.0195717\n",
      "\tspeed: 0.0210s/iter; left time: 417.1946s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 225 | Train Loss: 0.0199455 Vali Loss: 0.0186785 Test Loss: 0.0193164\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0193675\n",
      "\tspeed: 0.0428s/iter; left time: 843.9610s\n",
      "\titers: 200, epoch: 13 | loss: 0.0198724\n",
      "\tspeed: 0.0205s/iter; left time: 401.7550s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.74s\n",
      "Steps: 225 | Train Loss: 0.0204984 Vali Loss: 0.0184913 Test Loss: 0.0194524\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0189032\n",
      "\tspeed: 0.0437s/iter; left time: 851.2255s\n",
      "\titers: 200, epoch: 14 | loss: 0.0198566\n",
      "\tspeed: 0.0202s/iter; left time: 391.6641s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0197242 Vali Loss: 0.0185386 Test Loss: 0.0192303\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0193591\n",
      "\tspeed: 0.0432s/iter; left time: 832.3689s\n",
      "\titers: 200, epoch: 15 | loss: 0.0191613\n",
      "\tspeed: 0.0223s/iter; left time: 426.4516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:04.93s\n",
      "Steps: 225 | Train Loss: 0.0195737 Vali Loss: 0.0185342 Test Loss: 0.0192261\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0198897\n",
      "\tspeed: 0.0440s/iter; left time: 836.3660s\n",
      "\titers: 200, epoch: 16 | loss: 0.0210519\n",
      "\tspeed: 0.0204s/iter; left time: 386.4928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:04.70s\n",
      "Steps: 225 | Train Loss: 0.0194667 Vali Loss: 0.0184915 Test Loss: 0.0191256\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0184417\n",
      "\tspeed: 0.0502s/iter; left time: 943.0232s\n",
      "\titers: 200, epoch: 17 | loss: 0.0198883\n",
      "\tspeed: 0.0249s/iter; left time: 465.5056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 225 | Train Loss: 0.0194164 Vali Loss: 0.0183380 Test Loss: 0.0191246\n",
      "Validation loss decreased (0.018488 --> 0.018338).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0193421\n",
      "\tspeed: 0.0507s/iter; left time: 940.9454s\n",
      "\titers: 200, epoch: 18 | loss: 0.0193089\n",
      "\tspeed: 0.0190s/iter; left time: 351.2651s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:05.31s\n",
      "Steps: 225 | Train Loss: 0.0193018 Vali Loss: 0.0183337 Test Loss: 0.0190956\n",
      "Validation loss decreased (0.018338 --> 0.018334).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0180100\n",
      "\tspeed: 0.0474s/iter; left time: 869.4505s\n",
      "\titers: 200, epoch: 19 | loss: 0.0208043\n",
      "\tspeed: 0.0218s/iter; left time: 397.9688s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.12s\n",
      "Steps: 225 | Train Loss: 0.0192760 Vali Loss: 0.0182659 Test Loss: 0.0190671\n",
      "Validation loss decreased (0.018334 --> 0.018266).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0189537\n",
      "\tspeed: 0.0390s/iter; left time: 707.0205s\n",
      "\titers: 200, epoch: 20 | loss: 0.0194589\n",
      "\tspeed: 0.0259s/iter; left time: 466.8717s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.04s\n",
      "Steps: 225 | Train Loss: 0.0192607 Vali Loss: 0.0183959 Test Loss: 0.0191063\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0189893\n",
      "\tspeed: 0.0404s/iter; left time: 723.1905s\n",
      "\titers: 200, epoch: 21 | loss: 0.0187753\n",
      "\tspeed: 0.0209s/iter; left time: 372.0814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:04.84s\n",
      "Steps: 225 | Train Loss: 0.0192075 Vali Loss: 0.0182782 Test Loss: 0.0190127\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0194451\n",
      "\tspeed: 0.0424s/iter; left time: 749.5785s\n",
      "\titers: 200, epoch: 22 | loss: 0.0195122\n",
      "\tspeed: 0.0205s/iter; left time: 360.9811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 225 | Train Loss: 0.0191497 Vali Loss: 0.0182339 Test Loss: 0.0190123\n",
      "Validation loss decreased (0.018266 --> 0.018234).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0185303\n",
      "\tspeed: 0.0445s/iter; left time: 776.8425s\n",
      "\titers: 200, epoch: 23 | loss: 0.0185139\n",
      "\tspeed: 0.0170s/iter; left time: 294.6427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 225 | Train Loss: 0.0191384 Vali Loss: 0.0182579 Test Loss: 0.0190225\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0191628\n",
      "\tspeed: 0.0482s/iter; left time: 830.7634s\n",
      "\titers: 200, epoch: 24 | loss: 0.0272066\n",
      "\tspeed: 0.0176s/iter; left time: 300.8628s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:04.47s\n",
      "Steps: 225 | Train Loss: 0.0190796 Vali Loss: 0.0184180 Test Loss: 0.0190936\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0196890\n",
      "\tspeed: 0.0517s/iter; left time: 878.8391s\n",
      "\titers: 200, epoch: 25 | loss: 0.0194826\n",
      "\tspeed: 0.0183s/iter; left time: 309.8283s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 225 | Train Loss: 0.0190040 Vali Loss: 0.0181813 Test Loss: 0.0189833\n",
      "Validation loss decreased (0.018234 --> 0.018181).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0181253\n",
      "\tspeed: 0.0502s/iter; left time: 842.8770s\n",
      "\titers: 200, epoch: 26 | loss: 0.0187839\n",
      "\tspeed: 0.0165s/iter; left time: 274.7301s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:04.59s\n",
      "Steps: 225 | Train Loss: 0.0189785 Vali Loss: 0.0181851 Test Loss: 0.0189863\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0186646\n",
      "\tspeed: 0.0527s/iter; left time: 872.7093s\n",
      "\titers: 200, epoch: 27 | loss: 0.0195630\n",
      "\tspeed: 0.0177s/iter; left time: 291.2806s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0190600 Vali Loss: 0.0182300 Test Loss: 0.0189839\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0203652\n",
      "\tspeed: 0.0537s/iter; left time: 876.7723s\n",
      "\titers: 200, epoch: 28 | loss: 0.0199740\n",
      "\tspeed: 0.0169s/iter; left time: 273.4299s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:04.34s\n",
      "Steps: 225 | Train Loss: 0.0189316 Vali Loss: 0.0181757 Test Loss: 0.0189563\n",
      "Validation loss decreased (0.018181 --> 0.018176).  Saving model ...\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0206350\n",
      "\tspeed: 0.0514s/iter; left time: 828.1764s\n",
      "\titers: 200, epoch: 29 | loss: 0.0181286\n",
      "\tspeed: 0.0175s/iter; left time: 280.2370s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:04.74s\n",
      "Steps: 225 | Train Loss: 0.0188996 Vali Loss: 0.0181486 Test Loss: 0.0190109\n",
      "Validation loss decreased (0.018176 --> 0.018149).  Saving model ...\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0196977\n",
      "\tspeed: 0.0497s/iter; left time: 789.4193s\n",
      "\titers: 200, epoch: 30 | loss: 0.0200620\n",
      "\tspeed: 0.0175s/iter; left time: 275.6010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 225 | Train Loss: 0.0189143 Vali Loss: 0.0181804 Test Loss: 0.0189311\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0180183\n",
      "\tspeed: 0.0516s/iter; left time: 807.7268s\n",
      "\titers: 200, epoch: 31 | loss: 0.0181029\n",
      "\tspeed: 0.0168s/iter; left time: 261.7345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:04.60s\n",
      "Steps: 225 | Train Loss: 0.0190194 Vali Loss: 0.0182616 Test Loss: 0.0189574\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0182489\n",
      "\tspeed: 0.0499s/iter; left time: 769.0311s\n",
      "\titers: 200, epoch: 32 | loss: 0.0195527\n",
      "\tspeed: 0.0181s/iter; left time: 276.8467s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:04.67s\n",
      "Steps: 225 | Train Loss: 0.0190022 Vali Loss: 0.0181670 Test Loss: 0.0189566\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0196477\n",
      "\tspeed: 0.0528s/iter; left time: 802.3022s\n",
      "\titers: 200, epoch: 33 | loss: 0.0195144\n",
      "\tspeed: 0.0257s/iter; left time: 388.3721s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.55s\n",
      "Steps: 225 | Train Loss: 0.0188667 Vali Loss: 0.0181575 Test Loss: 0.0189364\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0190932\n",
      "\tspeed: 0.0499s/iter; left time: 746.7050s\n",
      "\titers: 200, epoch: 34 | loss: 0.0182831\n",
      "\tspeed: 0.0291s/iter; left time: 433.1500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 225 | Train Loss: 0.0189012 Vali Loss: 0.0181455 Test Loss: 0.0189204\n",
      "Validation loss decreased (0.018149 --> 0.018145).  Saving model ...\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0182164\n",
      "\tspeed: 0.0487s/iter; left time: 719.0844s\n",
      "\titers: 200, epoch: 35 | loss: 0.0195959\n",
      "\tspeed: 0.0289s/iter; left time: 423.2001s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.21s\n",
      "Steps: 225 | Train Loss: 0.0188660 Vali Loss: 0.0181558 Test Loss: 0.0189215\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0192430\n",
      "\tspeed: 0.0424s/iter; left time: 615.8216s\n",
      "\titers: 200, epoch: 36 | loss: 0.0184059\n",
      "\tspeed: 0.0267s/iter; left time: 385.5996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:05.91s\n",
      "Steps: 225 | Train Loss: 0.0188386 Vali Loss: 0.0181532 Test Loss: 0.0189139\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0203774\n",
      "\tspeed: 0.0496s/iter; left time: 709.7479s\n",
      "\titers: 200, epoch: 37 | loss: 0.0187826\n",
      "\tspeed: 0.0264s/iter; left time: 375.4847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:06.34s\n",
      "Steps: 225 | Train Loss: 0.0188490 Vali Loss: 0.0180972 Test Loss: 0.0189571\n",
      "Validation loss decreased (0.018145 --> 0.018097).  Saving model ...\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.0191643\n",
      "\tspeed: 0.0483s/iter; left time: 680.0791s\n",
      "\titers: 200, epoch: 38 | loss: 0.0181443\n",
      "\tspeed: 0.0251s/iter; left time: 350.3224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 38\n",
      "Cost time: 00h:00m:05.86s\n",
      "Steps: 225 | Train Loss: 0.0188056 Vali Loss: 0.0181692 Test Loss: 0.0188996\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.0184905\n",
      "\tspeed: 0.0479s/iter; left time: 663.5101s\n",
      "\titers: 200, epoch: 39 | loss: 0.0178740\n",
      "\tspeed: 0.0257s/iter; left time: 354.0836s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 39\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 225 | Train Loss: 0.0188431 Vali Loss: 0.0181344 Test Loss: 0.0189271\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.0185107\n",
      "\tspeed: 0.0448s/iter; left time: 610.3057s\n",
      "\titers: 200, epoch: 40 | loss: 0.0184007\n",
      "\tspeed: 0.0244s/iter; left time: 330.5293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 40\n",
      "Cost time: 00h:00m:05.62s\n",
      "Steps: 225 | Train Loss: 0.0188096 Vali Loss: 0.0181285 Test Loss: 0.0189206\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.0174925\n",
      "\tspeed: 0.0467s/iter; left time: 625.9813s\n",
      "\titers: 200, epoch: 41 | loss: 0.0192582\n",
      "\tspeed: 0.0243s/iter; left time: 323.8159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 41\n",
      "Cost time: 00h:00m:05.60s\n",
      "Steps: 225 | Train Loss: 0.0187985 Vali Loss: 0.0181139 Test Loss: 0.0189121\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.0177357\n",
      "\tspeed: 0.0449s/iter; left time: 591.2866s\n",
      "\titers: 200, epoch: 42 | loss: 0.0188432\n",
      "\tspeed: 0.0266s/iter; left time: 347.3000s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 42\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 225 | Train Loss: 0.0188488 Vali Loss: 0.0180975 Test Loss: 0.0189411\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.0186121\n",
      "\tspeed: 0.0501s/iter; left time: 648.3163s\n",
      "\titers: 200, epoch: 43 | loss: 0.0201827\n",
      "\tspeed: 0.0275s/iter; left time: 353.3531s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 43\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 225 | Train Loss: 0.0188482 Vali Loss: 0.0181299 Test Loss: 0.0189098\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.0187149\n",
      "\tspeed: 0.0450s/iter; left time: 572.1567s\n",
      "\titers: 200, epoch: 44 | loss: 0.0181006\n",
      "\tspeed: 0.0281s/iter; left time: 354.7640s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 44\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 225 | Train Loss: 0.0187628 Vali Loss: 0.0180758 Test Loss: 0.0189177\n",
      "Validation loss decreased (0.018097 --> 0.018076).  Saving model ...\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.0181435\n",
      "\tspeed: 0.0521s/iter; left time: 651.6721s\n",
      "\titers: 200, epoch: 45 | loss: 0.0196680\n",
      "\tspeed: 0.0259s/iter; left time: 320.9418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 45\n",
      "Cost time: 00h:00m:06.29s\n",
      "Steps: 225 | Train Loss: 0.0187586 Vali Loss: 0.0181032 Test Loss: 0.0189198\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.0186186\n",
      "\tspeed: 0.0465s/iter; left time: 570.3167s\n",
      "\titers: 200, epoch: 46 | loss: 0.0185255\n",
      "\tspeed: 0.0270s/iter; left time: 328.8205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 46\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 225 | Train Loss: 0.0187782 Vali Loss: 0.0180834 Test Loss: 0.0189319\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.0172828\n",
      "\tspeed: 0.0475s/iter; left time: 572.9542s\n",
      "\titers: 200, epoch: 47 | loss: 0.0193958\n",
      "\tspeed: 0.0273s/iter; left time: 325.8086s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 47\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 225 | Train Loss: 0.0187856 Vali Loss: 0.0181030 Test Loss: 0.0188926\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.0187976\n",
      "\tspeed: 0.0485s/iter; left time: 573.2992s\n",
      "\titers: 200, epoch: 48 | loss: 0.0174188\n",
      "\tspeed: 0.0286s/iter; left time: 335.1820s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 48\n",
      "Cost time: 00h:00m:06.26s\n",
      "Steps: 225 | Train Loss: 0.0187455 Vali Loss: 0.0180691 Test Loss: 0.0189094\n",
      "Validation loss decreased (0.018076 --> 0.018069).  Saving model ...\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.0197352\n",
      "\tspeed: 0.0501s/iter; left time: 580.8285s\n",
      "\titers: 200, epoch: 49 | loss: 0.0174863\n",
      "\tspeed: 0.0289s/iter; left time: 332.1918s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 49\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 225 | Train Loss: 0.0188050 Vali Loss: 0.0181176 Test Loss: 0.0188957\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.0180126\n",
      "\tspeed: 0.0503s/iter; left time: 572.6959s\n",
      "\titers: 200, epoch: 50 | loss: 0.0206608\n",
      "\tspeed: 0.0273s/iter; left time: 308.2766s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 50\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 225 | Train Loss: 0.0188263 Vali Loss: 0.0180650 Test Loss: 0.0188972\n",
      "Validation loss decreased (0.018069 --> 0.018065).  Saving model ...\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.0170122\n",
      "\tspeed: 0.0449s/iter; left time: 500.5756s\n",
      "\titers: 200, epoch: 51 | loss: 0.0184574\n",
      "\tspeed: 0.0283s/iter; left time: 312.7791s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 51\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 225 | Train Loss: 0.0188780 Vali Loss: 0.0181203 Test Loss: 0.0189263\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.0204459\n",
      "\tspeed: 0.0498s/iter; left time: 543.9412s\n",
      "\titers: 200, epoch: 52 | loss: 0.0183172\n",
      "\tspeed: 0.0249s/iter; left time: 269.6140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 52\n",
      "Cost time: 00h:00m:06.06s\n",
      "Steps: 225 | Train Loss: 0.0187591 Vali Loss: 0.0180762 Test Loss: 0.0188945\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.0190909\n",
      "\tspeed: 0.0462s/iter; left time: 494.3914s\n",
      "\titers: 200, epoch: 53 | loss: 0.0182948\n",
      "\tspeed: 0.0249s/iter; left time: 264.4795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 53\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 225 | Train Loss: 0.0187505 Vali Loss: 0.0180655 Test Loss: 0.0189294\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.0201165\n",
      "\tspeed: 0.0467s/iter; left time: 488.9342s\n",
      "\titers: 200, epoch: 54 | loss: 0.0195936\n",
      "\tspeed: 0.0162s/iter; left time: 167.7459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 54\n",
      "Cost time: 00h:00m:04.59s\n",
      "Steps: 225 | Train Loss: 0.0187627 Vali Loss: 0.0180803 Test Loss: 0.0189013\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.0197573\n",
      "\tspeed: 0.0428s/iter; left time: 439.1817s\n",
      "\titers: 200, epoch: 55 | loss: 0.0185649\n",
      "\tspeed: 0.0158s/iter; left time: 160.4360s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 55\n",
      "Cost time: 00h:00m:04.19s\n",
      "Steps: 225 | Train Loss: 0.0187140 Vali Loss: 0.0180478 Test Loss: 0.0189054\n",
      "Validation loss decreased (0.018065 --> 0.018048).  Saving model ...\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.0179772\n",
      "\tspeed: 0.0431s/iter; left time: 432.5783s\n",
      "\titers: 200, epoch: 56 | loss: 0.0185630\n",
      "\tspeed: 0.0171s/iter; left time: 170.1690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 56\n",
      "Cost time: 00h:00m:04.55s\n",
      "Steps: 225 | Train Loss: 0.0189897 Vali Loss: 0.0180521 Test Loss: 0.0189287\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.0185743\n",
      "\tspeed: 0.0437s/iter; left time: 428.2494s\n",
      "\titers: 200, epoch: 57 | loss: 0.0180025\n",
      "\tspeed: 0.0169s/iter; left time: 163.5950s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 57\n",
      "Cost time: 00h:00m:04.47s\n",
      "Steps: 225 | Train Loss: 0.0188075 Vali Loss: 0.0180966 Test Loss: 0.0189194\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.0188034\n",
      "\tspeed: 0.0438s/iter; left time: 419.4573s\n",
      "\titers: 200, epoch: 58 | loss: 0.0187247\n",
      "\tspeed: 0.0174s/iter; left time: 165.0577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 58\n",
      "Cost time: 00h:00m:04.55s\n",
      "Steps: 225 | Train Loss: 0.0188204 Vali Loss: 0.0180457 Test Loss: 0.0189054\n",
      "Validation loss decreased (0.018048 --> 0.018046).  Saving model ...\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.0192051\n",
      "\tspeed: 0.0436s/iter; left time: 407.8222s\n",
      "\titers: 200, epoch: 59 | loss: 0.0188715\n",
      "\tspeed: 0.0152s/iter; left time: 140.5351s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 59\n",
      "Cost time: 00h:00m:04.33s\n",
      "Steps: 225 | Train Loss: 0.0187089 Vali Loss: 0.0180814 Test Loss: 0.0189237\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.0189546\n",
      "\tspeed: 0.0413s/iter; left time: 377.1464s\n",
      "\titers: 200, epoch: 60 | loss: 0.0185984\n",
      "\tspeed: 0.0180s/iter; left time: 162.6092s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 60\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 225 | Train Loss: 0.0187344 Vali Loss: 0.0181039 Test Loss: 0.0189054\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.0181787\n",
      "\tspeed: 0.0443s/iter; left time: 393.8750s\n",
      "\titers: 200, epoch: 61 | loss: 0.0182559\n",
      "\tspeed: 0.0164s/iter; left time: 144.5154s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 61\n",
      "Cost time: 00h:00m:04.60s\n",
      "Steps: 225 | Train Loss: 0.0187585 Vali Loss: 0.0180341 Test Loss: 0.0189171\n",
      "Validation loss decreased (0.018046 --> 0.018034).  Saving model ...\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.0185583\n",
      "\tspeed: 0.0428s/iter; left time: 371.4303s\n",
      "\titers: 200, epoch: 62 | loss: 0.0187741\n",
      "\tspeed: 0.0184s/iter; left time: 158.0706s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 62\n",
      "Cost time: 00h:00m:04.45s\n",
      "Steps: 225 | Train Loss: 0.0187532 Vali Loss: 0.0180761 Test Loss: 0.0189095\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.0177707\n",
      "\tspeed: 0.0457s/iter; left time: 386.6311s\n",
      "\titers: 200, epoch: 63 | loss: 0.0186384\n",
      "\tspeed: 0.0166s/iter; left time: 138.4133s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 63\n",
      "Cost time: 00h:00m:04.76s\n",
      "Steps: 225 | Train Loss: 0.0188340 Vali Loss: 0.0180893 Test Loss: 0.0189124\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.0192905\n",
      "\tspeed: 0.0433s/iter; left time: 356.1375s\n",
      "\titers: 200, epoch: 64 | loss: 0.0201995\n",
      "\tspeed: 0.0154s/iter; left time: 125.3958s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 64\n",
      "Cost time: 00h:00m:04.32s\n",
      "Steps: 225 | Train Loss: 0.0187623 Vali Loss: 0.0180487 Test Loss: 0.0189027\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.0164961\n",
      "\tspeed: 0.0429s/iter; left time: 343.1238s\n",
      "\titers: 200, epoch: 65 | loss: 0.0189645\n",
      "\tspeed: 0.0171s/iter; left time: 135.4261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 65\n",
      "Cost time: 00h:00m:04.47s\n",
      "Steps: 225 | Train Loss: 0.0187791 Vali Loss: 0.0180567 Test Loss: 0.0189105\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.0186134\n",
      "\tspeed: 0.0441s/iter; left time: 342.9573s\n",
      "\titers: 200, epoch: 66 | loss: 0.0193064\n",
      "\tspeed: 0.0168s/iter; left time: 129.1047s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 66\n",
      "Cost time: 00h:00m:04.50s\n",
      "Steps: 225 | Train Loss: 0.0187621 Vali Loss: 0.0180707 Test Loss: 0.0189063\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3100205086376224e-07\n",
      "\titers: 100, epoch: 67 | loss: 0.0169868\n",
      "\tspeed: 0.0444s/iter; left time: 335.2970s\n",
      "\titers: 200, epoch: 67 | loss: 0.0187755\n",
      "\tspeed: 0.0172s/iter; left time: 127.9428s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 67\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0187363 Vali Loss: 0.0180527 Test Loss: 0.0189101\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.1790184577738603e-07\n",
      "\titers: 100, epoch: 68 | loss: 0.0205655\n",
      "\tspeed: 0.0429s/iter; left time: 314.0892s\n",
      "\titers: 200, epoch: 68 | loss: 0.0192052\n",
      "\tspeed: 0.0156s/iter; left time: 113.0725s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 68\n",
      "Cost time: 00h:00m:04.36s\n",
      "Steps: 225 | Train Loss: 0.0187399 Vali Loss: 0.0180342 Test Loss: 0.0189029\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0611166119964742e-07\n",
      "\titers: 100, epoch: 69 | loss: 0.0181332\n",
      "\tspeed: 0.0467s/iter; left time: 331.9297s\n",
      "\titers: 200, epoch: 69 | loss: 0.0181357\n",
      "\tspeed: 0.0178s/iter; left time: 124.4025s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 69\n",
      "Cost time: 00h:00m:04.92s\n",
      "Steps: 225 | Train Loss: 0.0187259 Vali Loss: 0.0180604 Test Loss: 0.0189235\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.550049507968268e-08\n",
      "\titers: 100, epoch: 70 | loss: 0.0190984\n",
      "\tspeed: 0.0446s/iter; left time: 306.5891s\n",
      "\titers: 200, epoch: 70 | loss: 0.0191101\n",
      "\tspeed: 0.0157s/iter; left time: 106.6014s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 70\n",
      "Cost time: 00h:00m:04.44s\n",
      "Steps: 225 | Train Loss: 0.0187441 Vali Loss: 0.0180864 Test Loss: 0.0189139\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.595044557171442e-08\n",
      "\titers: 100, epoch: 71 | loss: 0.0190680\n",
      "\tspeed: 0.0418s/iter; left time: 277.8912s\n",
      "\titers: 200, epoch: 71 | loss: 0.0193501\n",
      "\tspeed: 0.0163s/iter; left time: 106.7338s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 71\n",
      "Cost time: 00h:00m:04.27s\n",
      "Steps: 225 | Train Loss: 0.0187691 Vali Loss: 0.0180732 Test Loss: 0.0188935\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_revin_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.018917052075266838, rmse:0.13753926753997803, mae:0.08967844396829605, rse:0.520534098148346\n",
      "Intermediate time for IT and pred_len 168: 00h:08m:05.48s\n",
      "Intermediate time for IT: 00h:17m:32.23s\n",
      "Total time: 01h:34m:52.81s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"no_revin_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --revin 0 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">-RevIN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0218</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.1906</td>\n",
       "      <td>0.1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.1814</td>\n",
       "      <td>0.1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.0873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.0929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.0658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.0897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model             -RevIN                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0218  0.1477  0.0952\n",
       "        96        0.0363  0.1906  0.1306\n",
       "        168       0.0390  0.1975  0.1373\n",
       "ES      24        0.0184  0.1355  0.0841\n",
       "        96        0.0310  0.1761  0.1143\n",
       "        168       0.0329  0.1814  0.1212\n",
       "FR      24        0.0110  0.1048  0.0640\n",
       "        96        0.0194  0.1392  0.0873\n",
       "        168       0.0214  0.1465  0.0929\n",
       "GB      24        0.0266  0.1631  0.1095\n",
       "        96        0.0406  0.2014  0.1431\n",
       "        168       0.0482  0.2195  0.1556\n",
       "IT      24        0.0109  0.1042  0.0658\n",
       "        96        0.0179  0.1337  0.0860\n",
       "        168       0.0189  0.1375  0.0897"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['-RevIN'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_no_revin.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. No channel independence (Channel-Mixing)\n",
    "\n",
    "It is a channel mixing model, and therefore it needs more dimension of embeddings to capture complex patterns between features. \n",
    "\n",
    "Therefore, it is not fair to keep same d_model and d_ff as in channel mixing. In this regard, we scale them based on number of input features.\n",
    "\n",
    "In other words, for DE data with 5 columns, d_model = 128 x 5, and d_ff = 256 x 5.\n",
    "\n",
    "For ES: d_model = 128 x 3 and d_ff = 256 x 3, etc. It is adjusted automatically in code.\n",
    "\n",
    "Since it converges fast, we reduced max number of epochs and patience.\n",
    "\n",
    "Complete results are in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_336_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0228087\n",
      "\tspeed: 0.1079s/iter; left time: 2405.8189s\n",
      "\titers: 200, epoch: 1 | loss: 0.0179323\n",
      "\tspeed: 0.0775s/iter; left time: 1719.9694s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:17.96s\n",
      "Steps: 224 | Train Loss: 0.0250762 Vali Loss: 0.0243956 Test Loss: 0.0266887\n",
      "Validation loss decreased (inf --> 0.024396).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0143175\n",
      "\tspeed: 0.1491s/iter; left time: 3291.7626s\n",
      "\titers: 200, epoch: 2 | loss: 0.0121034\n",
      "\tspeed: 0.0787s/iter; left time: 1729.8753s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:17.94s\n",
      "Steps: 224 | Train Loss: 0.0153061 Vali Loss: 0.0225978 Test Loss: 0.0254660\n",
      "Validation loss decreased (0.024396 --> 0.022598).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0110634\n",
      "\tspeed: 0.1487s/iter; left time: 3250.0557s\n",
      "\titers: 200, epoch: 3 | loss: 0.0108149\n",
      "\tspeed: 0.0796s/iter; left time: 1731.9148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:18.02s\n",
      "Steps: 224 | Train Loss: 0.0116806 Vali Loss: 0.0242847 Test Loss: 0.0281949\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0086492\n",
      "\tspeed: 0.1372s/iter; left time: 2966.4333s\n",
      "\titers: 200, epoch: 4 | loss: 0.0074749\n",
      "\tspeed: 0.0809s/iter; left time: 1740.8705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:18.26s\n",
      "Steps: 224 | Train Loss: 0.0083592 Vali Loss: 0.0257715 Test Loss: 0.0307205\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059109\n",
      "\tspeed: 0.1393s/iter; left time: 2982.1964s\n",
      "\titers: 200, epoch: 5 | loss: 0.0057949\n",
      "\tspeed: 0.0816s/iter; left time: 1737.7424s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:18.52s\n",
      "Steps: 224 | Train Loss: 0.0061490 Vali Loss: 0.0265567 Test Loss: 0.0322603\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0054164\n",
      "\tspeed: 0.1403s/iter; left time: 2972.1342s\n",
      "\titers: 200, epoch: 6 | loss: 0.0045454\n",
      "\tspeed: 0.0823s/iter; left time: 1734.4703s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:18.53s\n",
      "Steps: 224 | Train Loss: 0.0048884 Vali Loss: 0.0265694 Test Loss: 0.0316106\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0043331\n",
      "\tspeed: 0.1394s/iter; left time: 2921.8639s\n",
      "\titers: 200, epoch: 7 | loss: 0.0042686\n",
      "\tspeed: 0.0807s/iter; left time: 1682.9731s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:18.40s\n",
      "Steps: 224 | Train Loss: 0.0041620 Vali Loss: 0.0267623 Test Loss: 0.0317451\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0035710\n",
      "\tspeed: 0.1412s/iter; left time: 2926.6554s\n",
      "\titers: 200, epoch: 8 | loss: 0.0034310\n",
      "\tspeed: 0.0807s/iter; left time: 1665.8266s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:18.32s\n",
      "Steps: 224 | Train Loss: 0.0036270 Vali Loss: 0.0266479 Test Loss: 0.0319233\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0033539\n",
      "\tspeed: 0.1379s/iter; left time: 2828.0550s\n",
      "\titers: 200, epoch: 9 | loss: 0.0033634\n",
      "\tspeed: 0.0790s/iter; left time: 1612.9094s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:18.09s\n",
      "Steps: 224 | Train Loss: 0.0032561 Vali Loss: 0.0268548 Test Loss: 0.0317931\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0028689\n",
      "\tspeed: 0.1404s/iter; left time: 2848.5519s\n",
      "\titers: 200, epoch: 10 | loss: 0.0029293\n",
      "\tspeed: 0.0817s/iter; left time: 1649.8352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:18.65s\n",
      "Steps: 224 | Train Loss: 0.0029481 Vali Loss: 0.0266907 Test Loss: 0.0310390\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0026734\n",
      "\tspeed: 0.1423s/iter; left time: 2854.2505s\n",
      "\titers: 200, epoch: 11 | loss: 0.0027685\n",
      "\tspeed: 0.0831s/iter; left time: 1659.1757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:18.60s\n",
      "Steps: 224 | Train Loss: 0.0027446 Vali Loss: 0.0267117 Test Loss: 0.0316906\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0026027\n",
      "\tspeed: 0.1404s/iter; left time: 2785.3602s\n",
      "\titers: 200, epoch: 12 | loss: 0.0023858\n",
      "\tspeed: 0.0815s/iter; left time: 1607.7385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:18.52s\n",
      "Steps: 224 | Train Loss: 0.0025758 Vali Loss: 0.0268825 Test Loss: 0.0318096\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.025466015562415123, rmse:0.1595807522535324, mae:0.10604768246412277, rse:0.5631825923919678\n",
      "Intermediate time for DE and pred_len 24: 00h:04m:38.28s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_512_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0267612\n",
      "\tspeed: 0.1345s/iter; left time: 2973.3365s\n",
      "\titers: 200, epoch: 1 | loss: 0.0241627\n",
      "\tspeed: 0.1159s/iter; left time: 2549.6671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.17s\n",
      "Steps: 222 | Train Loss: 0.0298788 Vali Loss: 0.0330326 Test Loss: 0.0381434\n",
      "Validation loss decreased (inf --> 0.033033).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0213286\n",
      "\tspeed: 0.2111s/iter; left time: 4618.2832s\n",
      "\titers: 200, epoch: 2 | loss: 0.0150444\n",
      "\tspeed: 0.1157s/iter; left time: 2520.3333s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.02s\n",
      "Steps: 222 | Train Loss: 0.0205023 Vali Loss: 0.0394690 Test Loss: 0.0567440\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0106942\n",
      "\tspeed: 0.1993s/iter; left time: 4316.6006s\n",
      "\titers: 200, epoch: 3 | loss: 0.0080675\n",
      "\tspeed: 0.1165s/iter; left time: 2511.3193s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.06s\n",
      "Steps: 222 | Train Loss: 0.0105057 Vali Loss: 0.0410978 Test Loss: 0.0570585\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0070927\n",
      "\tspeed: 0.1976s/iter; left time: 4236.2235s\n",
      "\titers: 200, epoch: 4 | loss: 0.0059473\n",
      "\tspeed: 0.1179s/iter; left time: 2514.4882s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.36s\n",
      "Steps: 222 | Train Loss: 0.0069310 Vali Loss: 0.0419279 Test Loss: 0.0561629\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0055561\n",
      "\tspeed: 0.1992s/iter; left time: 4225.1680s\n",
      "\titers: 200, epoch: 5 | loss: 0.0049814\n",
      "\tspeed: 0.1168s/iter; left time: 2465.3382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.20s\n",
      "Steps: 222 | Train Loss: 0.0053010 Vali Loss: 0.0415496 Test Loss: 0.0554823\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0042187\n",
      "\tspeed: 0.1983s/iter; left time: 4163.4699s\n",
      "\titers: 200, epoch: 6 | loss: 0.0039673\n",
      "\tspeed: 0.1167s/iter; left time: 2438.6321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.09s\n",
      "Steps: 222 | Train Loss: 0.0043732 Vali Loss: 0.0405251 Test Loss: 0.0546589\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0037001\n",
      "\tspeed: 0.2002s/iter; left time: 4158.9581s\n",
      "\titers: 200, epoch: 7 | loss: 0.0036983\n",
      "\tspeed: 0.1165s/iter; left time: 2407.0645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.20s\n",
      "Steps: 222 | Train Loss: 0.0037926 Vali Loss: 0.0397968 Test Loss: 0.0534103\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0033180\n",
      "\tspeed: 0.1978s/iter; left time: 4064.8042s\n",
      "\titers: 200, epoch: 8 | loss: 0.0033665\n",
      "\tspeed: 0.1168s/iter; left time: 2388.4869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.22s\n",
      "Steps: 222 | Train Loss: 0.0033597 Vali Loss: 0.0391120 Test Loss: 0.0523807\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0031007\n",
      "\tspeed: 0.1994s/iter; left time: 4052.9757s\n",
      "\titers: 200, epoch: 9 | loss: 0.0029520\n",
      "\tspeed: 0.1165s/iter; left time: 2355.4508s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.23s\n",
      "Steps: 222 | Train Loss: 0.0030491 Vali Loss: 0.0393237 Test Loss: 0.0516142\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0027767\n",
      "\tspeed: 0.1985s/iter; left time: 3991.0370s\n",
      "\titers: 200, epoch: 10 | loss: 0.0026486\n",
      "\tspeed: 0.1173s/iter; left time: 2345.4190s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.19s\n",
      "Steps: 222 | Train Loss: 0.0027984 Vali Loss: 0.0385833 Test Loss: 0.0513738\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0025453\n",
      "\tspeed: 0.2015s/iter; left time: 4006.8533s\n",
      "\titers: 200, epoch: 11 | loss: 0.0026200\n",
      "\tspeed: 0.1161s/iter; left time: 2295.9504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.17s\n",
      "Steps: 222 | Train Loss: 0.0025790 Vali Loss: 0.0392718 Test Loss: 0.0512391\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.03814338520169258, rmse:0.1953033208847046, mae:0.13842742145061493, rse:0.6916085481643677\n",
      "Intermediate time for DE and pred_len 96: 00h:05m:59.31s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_512_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0303482\n",
      "\tspeed: 0.1472s/iter; left time: 3252.9327s\n",
      "\titers: 200, epoch: 1 | loss: 0.0259837\n",
      "\tspeed: 0.1169s/iter; left time: 2572.6714s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.66s\n",
      "Steps: 222 | Train Loss: 0.0312894 Vali Loss: 0.0338812 Test Loss: 0.0400263\n",
      "Validation loss decreased (inf --> 0.033881).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0223873\n",
      "\tspeed: 0.2150s/iter; left time: 4704.1597s\n",
      "\titers: 200, epoch: 2 | loss: 0.0144032\n",
      "\tspeed: 0.1179s/iter; left time: 2567.6302s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0203739 Vali Loss: 0.0428706 Test Loss: 0.0591011\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0095180\n",
      "\tspeed: 0.2053s/iter; left time: 4445.1696s\n",
      "\titers: 200, epoch: 3 | loss: 0.0086363\n",
      "\tspeed: 0.1180s/iter; left time: 2542.7405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.46s\n",
      "Steps: 222 | Train Loss: 0.0102204 Vali Loss: 0.0409299 Test Loss: 0.0589293\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0070051\n",
      "\tspeed: 0.2010s/iter; left time: 4308.7348s\n",
      "\titers: 200, epoch: 4 | loss: 0.0060716\n",
      "\tspeed: 0.1186s/iter; left time: 2531.1249s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.55s\n",
      "Steps: 222 | Train Loss: 0.0068285 Vali Loss: 0.0409141 Test Loss: 0.0554637\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0055226\n",
      "\tspeed: 0.1990s/iter; left time: 4222.2798s\n",
      "\titers: 200, epoch: 5 | loss: 0.0048452\n",
      "\tspeed: 0.1192s/iter; left time: 2515.6343s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.60s\n",
      "Steps: 222 | Train Loss: 0.0052085 Vali Loss: 0.0405766 Test Loss: 0.0553284\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0042709\n",
      "\tspeed: 0.1996s/iter; left time: 4189.3609s\n",
      "\titers: 200, epoch: 6 | loss: 0.0039860\n",
      "\tspeed: 0.1194s/iter; left time: 2494.5140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.66s\n",
      "Steps: 222 | Train Loss: 0.0042853 Vali Loss: 0.0407914 Test Loss: 0.0561981\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0036946\n",
      "\tspeed: 0.2039s/iter; left time: 4235.5437s\n",
      "\titers: 200, epoch: 7 | loss: 0.0036571\n",
      "\tspeed: 0.1187s/iter; left time: 2453.9209s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.61s\n",
      "Steps: 222 | Train Loss: 0.0037219 Vali Loss: 0.0396792 Test Loss: 0.0527121\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0033250\n",
      "\tspeed: 0.2032s/iter; left time: 4175.0595s\n",
      "\titers: 200, epoch: 8 | loss: 0.0031910\n",
      "\tspeed: 0.1188s/iter; left time: 2429.5516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.76s\n",
      "Steps: 222 | Train Loss: 0.0033059 Vali Loss: 0.0397238 Test Loss: 0.0522578\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0029823\n",
      "\tspeed: 0.2009s/iter; left time: 4082.3163s\n",
      "\titers: 200, epoch: 9 | loss: 0.0030154\n",
      "\tspeed: 0.1187s/iter; left time: 2401.1134s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.69s\n",
      "Steps: 222 | Train Loss: 0.0030103 Vali Loss: 0.0397284 Test Loss: 0.0528058\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0027213\n",
      "\tspeed: 0.1985s/iter; left time: 3990.9351s\n",
      "\titers: 200, epoch: 10 | loss: 0.0026023\n",
      "\tspeed: 0.1180s/iter; left time: 2360.8793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.40s\n",
      "Steps: 222 | Train Loss: 0.0027673 Vali Loss: 0.0398414 Test Loss: 0.0522234\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0025320\n",
      "\tspeed: 0.2009s/iter; left time: 3994.8234s\n",
      "\titers: 200, epoch: 11 | loss: 0.0024260\n",
      "\tspeed: 0.1184s/iter; left time: 2341.6970s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.48s\n",
      "Steps: 222 | Train Loss: 0.0025736 Vali Loss: 0.0395068 Test Loss: 0.0516529\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04002630338072777, rmse:0.2000657469034195, mae:0.14279745519161224, rse:0.7086489796638489\n",
      "Intermediate time for DE and pred_len 168: 00h:06m:08.44s\n",
      "Intermediate time for DE: 00h:16m:46.03s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0204524\n",
      "\tspeed: 0.1395s/iter; left time: 3097.9385s\n",
      "\titers: 200, epoch: 1 | loss: 0.0172346\n",
      "\tspeed: 0.1120s/iter; left time: 2476.2464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:25.60s\n",
      "Steps: 223 | Train Loss: 0.0216741 Vali Loss: 0.0224694 Test Loss: 0.0285811\n",
      "Validation loss decreased (inf --> 0.022469).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0165505\n",
      "\tspeed: 0.2103s/iter; left time: 4621.6269s\n",
      "\titers: 200, epoch: 2 | loss: 0.0137054\n",
      "\tspeed: 0.1127s/iter; left time: 2465.9090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.50s\n",
      "Steps: 223 | Train Loss: 0.0156341 Vali Loss: 0.0228663 Test Loss: 0.0294817\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0129808\n",
      "\tspeed: 0.1919s/iter; left time: 4175.8264s\n",
      "\titers: 200, epoch: 3 | loss: 0.0097045\n",
      "\tspeed: 0.1133s/iter; left time: 2454.3169s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:25.45s\n",
      "Steps: 223 | Train Loss: 0.0124850 Vali Loss: 0.0272916 Test Loss: 0.0339417\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0103794\n",
      "\tspeed: 0.1931s/iter; left time: 4158.7084s\n",
      "\titers: 200, epoch: 4 | loss: 0.0078741\n",
      "\tspeed: 0.1144s/iter; left time: 2452.7297s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:25.64s\n",
      "Steps: 223 | Train Loss: 0.0089107 Vali Loss: 0.0299794 Test Loss: 0.0360209\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0060216\n",
      "\tspeed: 0.1969s/iter; left time: 4196.3589s\n",
      "\titers: 200, epoch: 5 | loss: 0.0063107\n",
      "\tspeed: 0.1135s/iter; left time: 2408.2542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.73s\n",
      "Steps: 223 | Train Loss: 0.0065563 Vali Loss: 0.0298951 Test Loss: 0.0369057\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0048502\n",
      "\tspeed: 0.1935s/iter; left time: 4079.1300s\n",
      "\titers: 200, epoch: 6 | loss: 0.0046325\n",
      "\tspeed: 0.1152s/iter; left time: 2417.1862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:25.76s\n",
      "Steps: 223 | Train Loss: 0.0052399 Vali Loss: 0.0298870 Test Loss: 0.0377665\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0045492\n",
      "\tspeed: 0.1954s/iter; left time: 4075.5954s\n",
      "\titers: 200, epoch: 7 | loss: 0.0052110\n",
      "\tspeed: 0.1155s/iter; left time: 2397.1756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:25.80s\n",
      "Steps: 223 | Train Loss: 0.0045226 Vali Loss: 0.0304545 Test Loss: 0.0381390\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0040665\n",
      "\tspeed: 0.1941s/iter; left time: 4005.9379s\n",
      "\titers: 200, epoch: 8 | loss: 0.0039435\n",
      "\tspeed: 0.1138s/iter; left time: 2336.8719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:25.57s\n",
      "Steps: 223 | Train Loss: 0.0040010 Vali Loss: 0.0297942 Test Loss: 0.0374554\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034011\n",
      "\tspeed: 0.1943s/iter; left time: 3966.7927s\n",
      "\titers: 200, epoch: 9 | loss: 0.0032482\n",
      "\tspeed: 0.1128s/iter; left time: 2291.8405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:25.60s\n",
      "Steps: 223 | Train Loss: 0.0036401 Vali Loss: 0.0298510 Test Loss: 0.0378472\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031535\n",
      "\tspeed: 0.1959s/iter; left time: 3955.8457s\n",
      "\titers: 200, epoch: 10 | loss: 0.0030436\n",
      "\tspeed: 0.1144s/iter; left time: 2299.7268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:25.72s\n",
      "Steps: 223 | Train Loss: 0.0033274 Vali Loss: 0.0302574 Test Loss: 0.0372001\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0030076\n",
      "\tspeed: 0.1966s/iter; left time: 3926.1950s\n",
      "\titers: 200, epoch: 11 | loss: 0.0031848\n",
      "\tspeed: 0.1146s/iter; left time: 2277.3600s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:25.87s\n",
      "Steps: 223 | Train Loss: 0.0031077 Vali Loss: 0.0298116 Test Loss: 0.0377274\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.028581099584698677, rmse:0.16905945539474487, mae:0.11745379865169525, rse:0.5832071900367737\n",
      "Intermediate time for GB and pred_len 24: 00h:05m:54.06s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249201\n",
      "\tspeed: 0.1453s/iter; left time: 3210.5184s\n",
      "\titers: 200, epoch: 1 | loss: 0.0207945\n",
      "\tspeed: 0.1171s/iter; left time: 2577.2567s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0265950 Vali Loss: 0.0302113 Test Loss: 0.0427934\n",
      "Validation loss decreased (inf --> 0.030211).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0242963\n",
      "\tspeed: 0.2266s/iter; left time: 4958.5839s\n",
      "\titers: 200, epoch: 2 | loss: 0.0163195\n",
      "\tspeed: 0.1140s/iter; left time: 2483.1500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.74s\n",
      "Steps: 222 | Train Loss: 0.0210160 Vali Loss: 0.0406260 Test Loss: 0.0595929\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0102974\n",
      "\tspeed: 0.1994s/iter; left time: 4317.8766s\n",
      "\titers: 200, epoch: 3 | loss: 0.0083862\n",
      "\tspeed: 0.1161s/iter; left time: 2503.1341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.08s\n",
      "Steps: 222 | Train Loss: 0.0111077 Vali Loss: 0.0442167 Test Loss: 0.0597632\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0075297\n",
      "\tspeed: 0.2042s/iter; left time: 4377.8822s\n",
      "\titers: 200, epoch: 4 | loss: 0.0066034\n",
      "\tspeed: 0.1167s/iter; left time: 2489.0468s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.22s\n",
      "Steps: 222 | Train Loss: 0.0074410 Vali Loss: 0.0445008 Test Loss: 0.0612709\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0057877\n",
      "\tspeed: 0.1990s/iter; left time: 4221.0864s\n",
      "\titers: 200, epoch: 5 | loss: 0.0051656\n",
      "\tspeed: 0.1157s/iter; left time: 2441.9090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.01s\n",
      "Steps: 222 | Train Loss: 0.0057136 Vali Loss: 0.0430807 Test Loss: 0.0606061\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0046578\n",
      "\tspeed: 0.1965s/iter; left time: 4123.7137s\n",
      "\titers: 200, epoch: 6 | loss: 0.0045613\n",
      "\tspeed: 0.1162s/iter; left time: 2427.8245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.02s\n",
      "Steps: 222 | Train Loss: 0.0047347 Vali Loss: 0.0436480 Test Loss: 0.0575669\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0040779\n",
      "\tspeed: 0.1975s/iter; left time: 4101.0427s\n",
      "\titers: 200, epoch: 7 | loss: 0.0038437\n",
      "\tspeed: 0.1164s/iter; left time: 2405.8765s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.14s\n",
      "Steps: 222 | Train Loss: 0.0041591 Vali Loss: 0.0417144 Test Loss: 0.0575785\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0037653\n",
      "\tspeed: 0.1985s/iter; left time: 4079.4096s\n",
      "\titers: 200, epoch: 8 | loss: 0.0038647\n",
      "\tspeed: 0.1171s/iter; left time: 2394.0861s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.08s\n",
      "Steps: 222 | Train Loss: 0.0037220 Vali Loss: 0.0422032 Test Loss: 0.0573435\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034920\n",
      "\tspeed: 0.1997s/iter; left time: 4058.3746s\n",
      "\titers: 200, epoch: 9 | loss: 0.0033256\n",
      "\tspeed: 0.1153s/iter; left time: 2332.6062s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.09s\n",
      "Steps: 222 | Train Loss: 0.0034100 Vali Loss: 0.0411287 Test Loss: 0.0566642\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031117\n",
      "\tspeed: 0.2005s/iter; left time: 4029.8191s\n",
      "\titers: 200, epoch: 10 | loss: 0.0028218\n",
      "\tspeed: 0.1175s/iter; left time: 2351.1313s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.40s\n",
      "Steps: 222 | Train Loss: 0.0031732 Vali Loss: 0.0411667 Test Loss: 0.0564241\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0029950\n",
      "\tspeed: 0.1982s/iter; left time: 3940.4835s\n",
      "\titers: 200, epoch: 11 | loss: 0.0029961\n",
      "\tspeed: 0.1178s/iter; left time: 2330.7597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.34s\n",
      "Steps: 222 | Train Loss: 0.0029687 Vali Loss: 0.0401659 Test Loss: 0.0557463\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04279341548681259, rmse:0.20686569809913635, mae:0.14713023602962494, rse:0.7153703570365906\n",
      "Intermediate time for GB and pred_len 96: 00h:06m:04.48s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0257415\n",
      "\tspeed: 0.1390s/iter; left time: 3071.1425s\n",
      "\titers: 200, epoch: 1 | loss: 0.0241097\n",
      "\tspeed: 0.1193s/iter; left time: 2624.4066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.62s\n",
      "Steps: 222 | Train Loss: 0.0276226 Vali Loss: 0.0318170 Test Loss: 0.0450048\n",
      "Validation loss decreased (inf --> 0.031817).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0237916\n",
      "\tspeed: 0.2492s/iter; left time: 5451.7499s\n",
      "\titers: 200, epoch: 2 | loss: 0.0152756\n",
      "\tspeed: 0.1181s/iter; left time: 2571.1995s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0213474 Vali Loss: 0.0439241 Test Loss: 0.0612773\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0109549\n",
      "\tspeed: 0.1982s/iter; left time: 4293.2388s\n",
      "\titers: 200, epoch: 3 | loss: 0.0101264\n",
      "\tspeed: 0.1178s/iter; left time: 2540.4856s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.37s\n",
      "Steps: 222 | Train Loss: 0.0117174 Vali Loss: 0.0464970 Test Loss: 0.0617225\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0073486\n",
      "\tspeed: 0.2047s/iter; left time: 4387.3031s\n",
      "\titers: 200, epoch: 4 | loss: 0.0063245\n",
      "\tspeed: 0.1186s/iter; left time: 2530.4978s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.70s\n",
      "Steps: 222 | Train Loss: 0.0073780 Vali Loss: 0.0455833 Test Loss: 0.0603723\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059219\n",
      "\tspeed: 0.2018s/iter; left time: 4279.8580s\n",
      "\titers: 200, epoch: 5 | loss: 0.0051964\n",
      "\tspeed: 0.1188s/iter; left time: 2508.7083s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.54s\n",
      "Steps: 222 | Train Loss: 0.0055432 Vali Loss: 0.0437197 Test Loss: 0.0579041\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0045163\n",
      "\tspeed: 0.2018s/iter; left time: 4235.9388s\n",
      "\titers: 200, epoch: 6 | loss: 0.0042522\n",
      "\tspeed: 0.1176s/iter; left time: 2456.6171s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.55s\n",
      "Steps: 222 | Train Loss: 0.0046257 Vali Loss: 0.0429923 Test Loss: 0.0567633\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0039707\n",
      "\tspeed: 0.2008s/iter; left time: 4169.5544s\n",
      "\titers: 200, epoch: 7 | loss: 0.0039244\n",
      "\tspeed: 0.1200s/iter; left time: 2480.8943s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.85s\n",
      "Steps: 222 | Train Loss: 0.0040597 Vali Loss: 0.0432213 Test Loss: 0.0562348\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0039215\n",
      "\tspeed: 0.2041s/iter; left time: 4193.5578s\n",
      "\titers: 200, epoch: 8 | loss: 0.0035443\n",
      "\tspeed: 0.1208s/iter; left time: 2470.3162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:27.05s\n",
      "Steps: 222 | Train Loss: 0.0036566 Vali Loss: 0.0423803 Test Loss: 0.0562973\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034582\n",
      "\tspeed: 0.2065s/iter; left time: 4197.3482s\n",
      "\titers: 200, epoch: 9 | loss: 0.0031870\n",
      "\tspeed: 0.1212s/iter; left time: 2451.0187s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:27.27s\n",
      "Steps: 222 | Train Loss: 0.0033634 Vali Loss: 0.0417200 Test Loss: 0.0547162\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0029790\n",
      "\tspeed: 0.2052s/iter; left time: 4125.3175s\n",
      "\titers: 200, epoch: 10 | loss: 0.0029285\n",
      "\tspeed: 0.1218s/iter; left time: 2435.4878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:27.27s\n",
      "Steps: 222 | Train Loss: 0.0031169 Vali Loss: 0.0414041 Test Loss: 0.0538904\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0028094\n",
      "\tspeed: 0.2040s/iter; left time: 4055.2701s\n",
      "\titers: 200, epoch: 11 | loss: 0.0029949\n",
      "\tspeed: 0.1207s/iter; left time: 2387.4828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:27.10s\n",
      "Steps: 222 | Train Loss: 0.0029408 Vali Loss: 0.0405609 Test Loss: 0.0539439\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.045004863291978836, rmse:0.21214349567890167, mae:0.15204809606075287, rse:0.7355319857597351\n",
      "Intermediate time for GB and pred_len 168: 00h:06m:12.93s\n",
      "Intermediate time for GB: 00h:18m:11.46s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0222541\n",
      "\tspeed: 0.0553s/iter; left time: 1233.7852s\n",
      "\titers: 200, epoch: 1 | loss: 0.0173791\n",
      "\tspeed: 0.0300s/iter; left time: 665.6022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.18s\n",
      "Steps: 224 | Train Loss: 0.0242278 Vali Loss: 0.0146892 Test Loss: 0.0199106\n",
      "Validation loss decreased (inf --> 0.014689).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0093632\n",
      "\tspeed: 0.0665s/iter; left time: 1468.7716s\n",
      "\titers: 200, epoch: 2 | loss: 0.0095968\n",
      "\tspeed: 0.0299s/iter; left time: 657.6159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.05s\n",
      "Steps: 224 | Train Loss: 0.0104013 Vali Loss: 0.0090444 Test Loss: 0.0117678\n",
      "Validation loss decreased (0.014689 --> 0.009044).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0082353\n",
      "\tspeed: 0.0682s/iter; left time: 1490.7586s\n",
      "\titers: 200, epoch: 3 | loss: 0.0082141\n",
      "\tspeed: 0.0285s/iter; left time: 620.8897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.93s\n",
      "Steps: 224 | Train Loss: 0.0086646 Vali Loss: 0.0087289 Test Loss: 0.0114058\n",
      "Validation loss decreased (0.009044 --> 0.008729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0081308\n",
      "\tspeed: 0.0639s/iter; left time: 1382.1534s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084052\n",
      "\tspeed: 0.0299s/iter; left time: 644.3378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.94s\n",
      "Steps: 224 | Train Loss: 0.0081697 Vali Loss: 0.0094098 Test Loss: 0.0121627\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0081944\n",
      "\tspeed: 0.0635s/iter; left time: 1359.3442s\n",
      "\titers: 200, epoch: 5 | loss: 0.0074900\n",
      "\tspeed: 0.0299s/iter; left time: 636.4195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.02s\n",
      "Steps: 224 | Train Loss: 0.0075967 Vali Loss: 0.0089697 Test Loss: 0.0119432\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0067727\n",
      "\tspeed: 0.0617s/iter; left time: 1307.4678s\n",
      "\titers: 200, epoch: 6 | loss: 0.0068486\n",
      "\tspeed: 0.0294s/iter; left time: 619.8549s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 224 | Train Loss: 0.0070614 Vali Loss: 0.0092476 Test Loss: 0.0119057\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0061740\n",
      "\tspeed: 0.0605s/iter; left time: 1267.1797s\n",
      "\titers: 200, epoch: 7 | loss: 0.0062932\n",
      "\tspeed: 0.0287s/iter; left time: 599.2007s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.86s\n",
      "Steps: 224 | Train Loss: 0.0064601 Vali Loss: 0.0094871 Test Loss: 0.0122043\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0064154\n",
      "\tspeed: 0.0604s/iter; left time: 1252.4978s\n",
      "\titers: 200, epoch: 8 | loss: 0.0066724\n",
      "\tspeed: 0.0295s/iter; left time: 608.8793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 224 | Train Loss: 0.0058832 Vali Loss: 0.0096027 Test Loss: 0.0125173\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0051365\n",
      "\tspeed: 0.0594s/iter; left time: 1217.7786s\n",
      "\titers: 200, epoch: 9 | loss: 0.0048415\n",
      "\tspeed: 0.0298s/iter; left time: 609.1436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.81s\n",
      "Steps: 224 | Train Loss: 0.0053815 Vali Loss: 0.0100015 Test Loss: 0.0128093\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049133\n",
      "\tspeed: 0.0611s/iter; left time: 1238.9837s\n",
      "\titers: 200, epoch: 10 | loss: 0.0047802\n",
      "\tspeed: 0.0295s/iter; left time: 594.5728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 224 | Train Loss: 0.0049502 Vali Loss: 0.0102042 Test Loss: 0.0132874\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0047629\n",
      "\tspeed: 0.0627s/iter; left time: 1258.0624s\n",
      "\titers: 200, epoch: 11 | loss: 0.0041915\n",
      "\tspeed: 0.0317s/iter; left time: 633.0375s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 224 | Train Loss: 0.0046080 Vali Loss: 0.0105283 Test Loss: 0.0133615\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0044058\n",
      "\tspeed: 0.0651s/iter; left time: 1291.0467s\n",
      "\titers: 200, epoch: 12 | loss: 0.0043703\n",
      "\tspeed: 0.0310s/iter; left time: 611.7606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 224 | Train Loss: 0.0042848 Vali Loss: 0.0104694 Test Loss: 0.0136671\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0038280\n",
      "\tspeed: 0.0630s/iter; left time: 1235.5566s\n",
      "\titers: 200, epoch: 13 | loss: 0.0037476\n",
      "\tspeed: 0.0304s/iter; left time: 594.0538s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 224 | Train Loss: 0.0040548 Vali Loss: 0.0109012 Test Loss: 0.0137471\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011405767872929573, rmse:0.10679779201745987, mae:0.06919430196285248, rse:0.3142929971218109\n",
      "Intermediate time for ES and pred_len 24: 00h:02m:11.87s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249662\n",
      "\tspeed: 0.0586s/iter; left time: 1307.2795s\n",
      "\titers: 200, epoch: 1 | loss: 0.0213045\n",
      "\tspeed: 0.0304s/iter; left time: 674.4282s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 224 | Train Loss: 0.0275395 Vali Loss: 0.0199210 Test Loss: 0.0259587\n",
      "Validation loss decreased (inf --> 0.019921).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0154199\n",
      "\tspeed: 0.0743s/iter; left time: 1640.5048s\n",
      "\titers: 200, epoch: 2 | loss: 0.0147711\n",
      "\tspeed: 0.0313s/iter; left time: 687.4509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.28s\n",
      "Steps: 224 | Train Loss: 0.0159194 Vali Loss: 0.0164074 Test Loss: 0.0206553\n",
      "Validation loss decreased (0.019921 --> 0.016407).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0128123\n",
      "\tspeed: 0.0699s/iter; left time: 1527.7572s\n",
      "\titers: 200, epoch: 3 | loss: 0.0120069\n",
      "\tspeed: 0.0280s/iter; left time: 608.6671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.79s\n",
      "Steps: 224 | Train Loss: 0.0133727 Vali Loss: 0.0177141 Test Loss: 0.0220083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110963\n",
      "\tspeed: 0.0712s/iter; left time: 1539.1342s\n",
      "\titers: 200, epoch: 4 | loss: 0.0101504\n",
      "\tspeed: 0.0312s/iter; left time: 670.7137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0110040 Vali Loss: 0.0177026 Test Loss: 0.0225919\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0089270\n",
      "\tspeed: 0.0730s/iter; left time: 1562.8129s\n",
      "\titers: 200, epoch: 5 | loss: 0.0081671\n",
      "\tspeed: 0.0320s/iter; left time: 682.5150s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 224 | Train Loss: 0.0087992 Vali Loss: 0.0184645 Test Loss: 0.0240935\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0072723\n",
      "\tspeed: 0.0676s/iter; left time: 1431.4422s\n",
      "\titers: 200, epoch: 6 | loss: 0.0064863\n",
      "\tspeed: 0.0363s/iter; left time: 764.5081s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.65s\n",
      "Steps: 224 | Train Loss: 0.0072933 Vali Loss: 0.0184985 Test Loss: 0.0245565\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064227\n",
      "\tspeed: 0.0715s/iter; left time: 1497.8011s\n",
      "\titers: 200, epoch: 7 | loss: 0.0061511\n",
      "\tspeed: 0.0315s/iter; left time: 656.3951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.89s\n",
      "Steps: 224 | Train Loss: 0.0063212 Vali Loss: 0.0185263 Test Loss: 0.0238103\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0058017\n",
      "\tspeed: 0.0743s/iter; left time: 1541.0346s\n",
      "\titers: 200, epoch: 8 | loss: 0.0057851\n",
      "\tspeed: 0.0317s/iter; left time: 653.1800s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 224 | Train Loss: 0.0056385 Vali Loss: 0.0192424 Test Loss: 0.0244339\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0049183\n",
      "\tspeed: 0.0704s/iter; left time: 1444.4453s\n",
      "\titers: 200, epoch: 9 | loss: 0.0051946\n",
      "\tspeed: 0.0356s/iter; left time: 726.5123s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.74s\n",
      "Steps: 224 | Train Loss: 0.0051166 Vali Loss: 0.0190300 Test Loss: 0.0246217\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049855\n",
      "\tspeed: 0.0706s/iter; left time: 1432.0759s\n",
      "\titers: 200, epoch: 10 | loss: 0.0045004\n",
      "\tspeed: 0.0307s/iter; left time: 619.9271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 224 | Train Loss: 0.0047357 Vali Loss: 0.0190064 Test Loss: 0.0247210\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0043923\n",
      "\tspeed: 0.0738s/iter; left time: 1481.1946s\n",
      "\titers: 200, epoch: 11 | loss: 0.0043126\n",
      "\tspeed: 0.0308s/iter; left time: 615.1021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.79s\n",
      "Steps: 224 | Train Loss: 0.0044053 Vali Loss: 0.0192821 Test Loss: 0.0248229\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0039878\n",
      "\tspeed: 0.0681s/iter; left time: 1351.8309s\n",
      "\titers: 200, epoch: 12 | loss: 0.0041882\n",
      "\tspeed: 0.0362s/iter; left time: 713.6469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0041530 Vali Loss: 0.0190465 Test Loss: 0.0247059\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020655285567045212, rmse:0.14371946454048157, mae:0.0965302586555481, rse:0.42220455408096313\n",
      "Intermediate time for ES and pred_len 96: 00h:02m:15.77s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0244475\n",
      "\tspeed: 0.0552s/iter; left time: 1224.4591s\n",
      "\titers: 200, epoch: 1 | loss: 0.0229005\n",
      "\tspeed: 0.0362s/iter; left time: 799.9027s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.20s\n",
      "Steps: 223 | Train Loss: 0.0284261 Vali Loss: 0.0214357 Test Loss: 0.0273612\n",
      "Validation loss decreased (inf --> 0.021436).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0168547\n",
      "\tspeed: 0.0754s/iter; left time: 1657.8091s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166410\n",
      "\tspeed: 0.0331s/iter; left time: 725.2437s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.89s\n",
      "Steps: 223 | Train Loss: 0.0171577 Vali Loss: 0.0180411 Test Loss: 0.0229071\n",
      "Validation loss decreased (0.021436 --> 0.018041).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0136818\n",
      "\tspeed: 0.0755s/iter; left time: 1641.6418s\n",
      "\titers: 200, epoch: 3 | loss: 0.0134842\n",
      "\tspeed: 0.0317s/iter; left time: 686.7033s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.99s\n",
      "Steps: 223 | Train Loss: 0.0141816 Vali Loss: 0.0183799 Test Loss: 0.0234680\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0115397\n",
      "\tspeed: 0.0722s/iter; left time: 1553.8494s\n",
      "\titers: 200, epoch: 4 | loss: 0.0105017\n",
      "\tspeed: 0.0389s/iter; left time: 833.4224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 223 | Train Loss: 0.0117780 Vali Loss: 0.0189678 Test Loss: 0.0249497\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0095834\n",
      "\tspeed: 0.0745s/iter; left time: 1587.8296s\n",
      "\titers: 200, epoch: 5 | loss: 0.0090101\n",
      "\tspeed: 0.0330s/iter; left time: 700.8299s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.19s\n",
      "Steps: 223 | Train Loss: 0.0094315 Vali Loss: 0.0192081 Test Loss: 0.0266327\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0077858\n",
      "\tspeed: 0.0788s/iter; left time: 1661.0620s\n",
      "\titers: 200, epoch: 6 | loss: 0.0073812\n",
      "\tspeed: 0.0342s/iter; left time: 717.8340s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 223 | Train Loss: 0.0079274 Vali Loss: 0.0197801 Test Loss: 0.0270143\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0065276\n",
      "\tspeed: 0.0739s/iter; left time: 1542.3686s\n",
      "\titers: 200, epoch: 7 | loss: 0.0063776\n",
      "\tspeed: 0.0381s/iter; left time: 791.7813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0069369 Vali Loss: 0.0198990 Test Loss: 0.0276234\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0062215\n",
      "\tspeed: 0.0746s/iter; left time: 1540.6450s\n",
      "\titers: 200, epoch: 8 | loss: 0.0059965\n",
      "\tspeed: 0.0300s/iter; left time: 616.0683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0062075 Vali Loss: 0.0199308 Test Loss: 0.0275127\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0058798\n",
      "\tspeed: 0.0803s/iter; left time: 1638.4948s\n",
      "\titers: 200, epoch: 9 | loss: 0.0054274\n",
      "\tspeed: 0.0366s/iter; left time: 743.2925s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.77s\n",
      "Steps: 223 | Train Loss: 0.0056564 Vali Loss: 0.0199600 Test Loss: 0.0280890\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0053195\n",
      "\tspeed: 0.0687s/iter; left time: 1386.4426s\n",
      "\titers: 200, epoch: 10 | loss: 0.0049607\n",
      "\tspeed: 0.0339s/iter; left time: 681.2269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.88s\n",
      "Steps: 223 | Train Loss: 0.0052265 Vali Loss: 0.0201242 Test Loss: 0.0278490\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0049777\n",
      "\tspeed: 0.0721s/iter; left time: 1439.7230s\n",
      "\titers: 200, epoch: 11 | loss: 0.0049066\n",
      "\tspeed: 0.0295s/iter; left time: 585.5519s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 223 | Train Loss: 0.0048739 Vali Loss: 0.0201201 Test Loss: 0.0282332\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0045771\n",
      "\tspeed: 0.0727s/iter; left time: 1435.5779s\n",
      "\titers: 200, epoch: 12 | loss: 0.0044787\n",
      "\tspeed: 0.0360s/iter; left time: 707.6159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 223 | Train Loss: 0.0045925 Vali Loss: 0.0199313 Test Loss: 0.0279702\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02290712110698223, rmse:0.1513509899377823, mae:0.10324922204017639, rse:0.4446555972099304\n",
      "Intermediate time for ES and pred_len 168: 00h:02m:19.89s\n",
      "Intermediate time for ES: 00h:06m:47.53s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0142817\n",
      "\tspeed: 0.0397s/iter; left time: 893.6377s\n",
      "\titers: 200, epoch: 1 | loss: 0.0109015\n",
      "\tspeed: 0.0191s/iter; left time: 427.7089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.50s\n",
      "Steps: 226 | Train Loss: 0.0166739 Vali Loss: 0.0138271 Test Loss: 0.0167391\n",
      "Validation loss decreased (inf --> 0.013827).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0067945\n",
      "\tspeed: 0.0482s/iter; left time: 1072.7639s\n",
      "\titers: 200, epoch: 2 | loss: 0.0061001\n",
      "\tspeed: 0.0219s/iter; left time: 484.7126s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.25s\n",
      "Steps: 226 | Train Loss: 0.0071401 Vali Loss: 0.0095527 Test Loss: 0.0109669\n",
      "Validation loss decreased (0.013827 --> 0.009553).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0051317\n",
      "\tspeed: 0.0465s/iter; left time: 1025.5800s\n",
      "\titers: 200, epoch: 3 | loss: 0.0056985\n",
      "\tspeed: 0.0248s/iter; left time: 544.5277s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0059504 Vali Loss: 0.0092553 Test Loss: 0.0105850\n",
      "Validation loss decreased (0.009553 --> 0.009255).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0054032\n",
      "\tspeed: 0.0473s/iter; left time: 1032.1579s\n",
      "\titers: 200, epoch: 4 | loss: 0.0049541\n",
      "\tspeed: 0.0246s/iter; left time: 534.6705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0055715 Vali Loss: 0.0093264 Test Loss: 0.0109252\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0057090\n",
      "\tspeed: 0.0438s/iter; left time: 945.6816s\n",
      "\titers: 200, epoch: 5 | loss: 0.0049125\n",
      "\tspeed: 0.0246s/iter; left time: 529.5099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 226 | Train Loss: 0.0051394 Vali Loss: 0.0092770 Test Loss: 0.0106430\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0055766\n",
      "\tspeed: 0.0480s/iter; left time: 1024.9096s\n",
      "\titers: 200, epoch: 6 | loss: 0.0042806\n",
      "\tspeed: 0.0213s/iter; left time: 453.2134s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 226 | Train Loss: 0.0047325 Vali Loss: 0.0091757 Test Loss: 0.0110049\n",
      "Validation loss decreased (0.009255 --> 0.009176).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0041325\n",
      "\tspeed: 0.0575s/iter; left time: 1215.1075s\n",
      "\titers: 200, epoch: 7 | loss: 0.0045434\n",
      "\tspeed: 0.0205s/iter; left time: 431.8695s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.24s\n",
      "Steps: 226 | Train Loss: 0.0042715 Vali Loss: 0.0096620 Test Loss: 0.0119414\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0040274\n",
      "\tspeed: 0.0512s/iter; left time: 1071.2649s\n",
      "\titers: 200, epoch: 8 | loss: 0.0043296\n",
      "\tspeed: 0.0209s/iter; left time: 435.7849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 226 | Train Loss: 0.0038831 Vali Loss: 0.0102222 Test Loss: 0.0123794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0039655\n",
      "\tspeed: 0.0533s/iter; left time: 1102.0029s\n",
      "\titers: 200, epoch: 9 | loss: 0.0035371\n",
      "\tspeed: 0.0199s/iter; left time: 410.6390s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 226 | Train Loss: 0.0035282 Vali Loss: 0.0101897 Test Loss: 0.0123428\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031067\n",
      "\tspeed: 0.0503s/iter; left time: 1029.5440s\n",
      "\titers: 200, epoch: 10 | loss: 0.0034840\n",
      "\tspeed: 0.0195s/iter; left time: 397.1925s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0032746 Vali Loss: 0.0108053 Test Loss: 0.0126818\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0034887\n",
      "\tspeed: 0.0481s/iter; left time: 973.5595s\n",
      "\titers: 200, epoch: 11 | loss: 0.0030701\n",
      "\tspeed: 0.0165s/iter; left time: 332.9588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0030655 Vali Loss: 0.0105460 Test Loss: 0.0124508\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0027485\n",
      "\tspeed: 0.0453s/iter; left time: 907.6802s\n",
      "\titers: 200, epoch: 12 | loss: 0.0027083\n",
      "\tspeed: 0.0162s/iter; left time: 322.1957s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.85s\n",
      "Steps: 226 | Train Loss: 0.0029029 Vali Loss: 0.0104811 Test Loss: 0.0127481\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0024927\n",
      "\tspeed: 0.0400s/iter; left time: 791.7849s\n",
      "\titers: 200, epoch: 13 | loss: 0.0028051\n",
      "\tspeed: 0.0190s/iter; left time: 373.7724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0027675 Vali Loss: 0.0105155 Test Loss: 0.0127166\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0026206\n",
      "\tspeed: 0.0500s/iter; left time: 978.9320s\n",
      "\titers: 200, epoch: 14 | loss: 0.0026182\n",
      "\tspeed: 0.0204s/iter; left time: 397.1433s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0026426 Vali Loss: 0.0107463 Test Loss: 0.0129030\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0025154\n",
      "\tspeed: 0.0433s/iter; left time: 837.1179s\n",
      "\titers: 200, epoch: 15 | loss: 0.0025768\n",
      "\tspeed: 0.0240s/iter; left time: 461.1951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 226 | Train Loss: 0.0025631 Vali Loss: 0.0108644 Test Loss: 0.0129967\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0022232\n",
      "\tspeed: 0.0450s/iter; left time: 859.6571s\n",
      "\titers: 200, epoch: 16 | loss: 0.0022974\n",
      "\tspeed: 0.0249s/iter; left time: 472.9783s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 226 | Train Loss: 0.0024720 Vali Loss: 0.0110153 Test Loss: 0.0128870\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011004881002008915, rmse:0.10490415245294571, mae:0.06105313077569008, rse:0.4047172963619232\n",
      "Intermediate time for FR and pred_len 24: 00h:01m:57.46s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0185340\n",
      "\tspeed: 0.0356s/iter; left time: 796.8574s\n",
      "\titers: 200, epoch: 1 | loss: 0.0147910\n",
      "\tspeed: 0.0194s/iter; left time: 433.2728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.35s\n",
      "Steps: 225 | Train Loss: 0.0200774 Vali Loss: 0.0181967 Test Loss: 0.0233480\n",
      "Validation loss decreased (inf --> 0.018197).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0102094\n",
      "\tspeed: 0.0534s/iter; left time: 1183.3781s\n",
      "\titers: 200, epoch: 2 | loss: 0.0101413\n",
      "\tspeed: 0.0194s/iter; left time: 429.1445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 225 | Train Loss: 0.0114065 Vali Loss: 0.0150723 Test Loss: 0.0191185\n",
      "Validation loss decreased (0.018197 --> 0.015072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0107937\n",
      "\tspeed: 0.0507s/iter; left time: 1113.9931s\n",
      "\titers: 200, epoch: 3 | loss: 0.0090744\n",
      "\tspeed: 0.0190s/iter; left time: 414.2577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 225 | Train Loss: 0.0096025 Vali Loss: 0.0159363 Test Loss: 0.0185294\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0082903\n",
      "\tspeed: 0.0508s/iter; left time: 1103.3885s\n",
      "\titers: 200, epoch: 4 | loss: 0.0068811\n",
      "\tspeed: 0.0193s/iter; left time: 417.8840s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 225 | Train Loss: 0.0081587 Vali Loss: 0.0166399 Test Loss: 0.0192749\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0072266\n",
      "\tspeed: 0.0472s/iter; left time: 1014.5566s\n",
      "\titers: 200, epoch: 5 | loss: 0.0061447\n",
      "\tspeed: 0.0230s/iter; left time: 492.4569s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.42s\n",
      "Steps: 225 | Train Loss: 0.0069639 Vali Loss: 0.0168462 Test Loss: 0.0194454\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0060944\n",
      "\tspeed: 0.0418s/iter; left time: 888.3649s\n",
      "\titers: 200, epoch: 6 | loss: 0.0060795\n",
      "\tspeed: 0.0282s/iter; left time: 597.6238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.56s\n",
      "Steps: 225 | Train Loss: 0.0061028 Vali Loss: 0.0174394 Test Loss: 0.0191661\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0052585\n",
      "\tspeed: 0.0444s/iter; left time: 933.7863s\n",
      "\titers: 200, epoch: 7 | loss: 0.0047111\n",
      "\tspeed: 0.0201s/iter; left time: 420.8287s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.09s\n",
      "Steps: 225 | Train Loss: 0.0054582 Vali Loss: 0.0173450 Test Loss: 0.0193435\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0051130\n",
      "\tspeed: 0.0529s/iter; left time: 1102.6591s\n",
      "\titers: 200, epoch: 8 | loss: 0.0050900\n",
      "\tspeed: 0.0190s/iter; left time: 392.7738s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.83s\n",
      "Steps: 225 | Train Loss: 0.0049748 Vali Loss: 0.0177807 Test Loss: 0.0197438\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0044787\n",
      "\tspeed: 0.0550s/iter; left time: 1133.5448s\n",
      "\titers: 200, epoch: 9 | loss: 0.0054008\n",
      "\tspeed: 0.0195s/iter; left time: 398.8559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0045797 Vali Loss: 0.0180904 Test Loss: 0.0202845\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0043762\n",
      "\tspeed: 0.0532s/iter; left time: 1084.1541s\n",
      "\titers: 200, epoch: 10 | loss: 0.0039011\n",
      "\tspeed: 0.0187s/iter; left time: 378.7681s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.61s\n",
      "Steps: 225 | Train Loss: 0.0042761 Vali Loss: 0.0177933 Test Loss: 0.0203592\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0041078\n",
      "\tspeed: 0.0491s/iter; left time: 988.7858s\n",
      "\titers: 200, epoch: 11 | loss: 0.0040893\n",
      "\tspeed: 0.0255s/iter; left time: 510.8431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 225 | Train Loss: 0.0040168 Vali Loss: 0.0178445 Test Loss: 0.0206380\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0037558\n",
      "\tspeed: 0.0448s/iter; left time: 892.3402s\n",
      "\titers: 200, epoch: 12 | loss: 0.0037573\n",
      "\tspeed: 0.0285s/iter; left time: 564.5831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.79s\n",
      "Steps: 225 | Train Loss: 0.0038152 Vali Loss: 0.0182746 Test Loss: 0.0207126\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019118474796414375, rmse:0.1382695734500885, mae:0.08493250608444214, rse:0.5348634719848633\n",
      "Intermediate time for FR and pred_len 96: 00h:01m:32.65s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0167841\n",
      "\tspeed: 0.0366s/iter; left time: 819.0665s\n",
      "\titers: 200, epoch: 1 | loss: 0.0179857\n",
      "\tspeed: 0.0272s/iter; left time: 606.9146s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 225 | Train Loss: 0.0214065 Vali Loss: 0.0195938 Test Loss: 0.0242914\n",
      "Validation loss decreased (inf --> 0.019594).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0119704\n",
      "\tspeed: 0.0497s/iter; left time: 1101.0654s\n",
      "\titers: 200, epoch: 2 | loss: 0.0114996\n",
      "\tspeed: 0.0275s/iter; left time: 608.1885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.70s\n",
      "Steps: 225 | Train Loss: 0.0127617 Vali Loss: 0.0161654 Test Loss: 0.0199560\n",
      "Validation loss decreased (0.019594 --> 0.016165).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0097979\n",
      "\tspeed: 0.0490s/iter; left time: 1076.5098s\n",
      "\titers: 200, epoch: 3 | loss: 0.0093169\n",
      "\tspeed: 0.0282s/iter; left time: 615.7201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 225 | Train Loss: 0.0106056 Vali Loss: 0.0170234 Test Loss: 0.0204392\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0089465\n",
      "\tspeed: 0.0467s/iter; left time: 1014.8792s\n",
      "\titers: 200, epoch: 4 | loss: 0.0089064\n",
      "\tspeed: 0.0256s/iter; left time: 553.8914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 225 | Train Loss: 0.0090893 Vali Loss: 0.0174842 Test Loss: 0.0206586\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0076368\n",
      "\tspeed: 0.0454s/iter; left time: 975.6675s\n",
      "\titers: 200, epoch: 5 | loss: 0.0076604\n",
      "\tspeed: 0.0252s/iter; left time: 538.5168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 225 | Train Loss: 0.0078856 Vali Loss: 0.0187933 Test Loss: 0.0214456\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0073292\n",
      "\tspeed: 0.0495s/iter; left time: 1053.1073s\n",
      "\titers: 200, epoch: 6 | loss: 0.0062847\n",
      "\tspeed: 0.0197s/iter; left time: 416.5636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.33s\n",
      "Steps: 225 | Train Loss: 0.0069546 Vali Loss: 0.0182947 Test Loss: 0.0217758\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0066456\n",
      "\tspeed: 0.0553s/iter; left time: 1164.5845s\n",
      "\titers: 200, epoch: 7 | loss: 0.0058500\n",
      "\tspeed: 0.0211s/iter; left time: 441.8646s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.15s\n",
      "Steps: 225 | Train Loss: 0.0062480 Vali Loss: 0.0186382 Test Loss: 0.0225218\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0055677\n",
      "\tspeed: 0.0539s/iter; left time: 1123.1297s\n",
      "\titers: 200, epoch: 8 | loss: 0.0053374\n",
      "\tspeed: 0.0210s/iter; left time: 434.5517s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0057114 Vali Loss: 0.0194597 Test Loss: 0.0230711\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0052487\n",
      "\tspeed: 0.0570s/iter; left time: 1173.8884s\n",
      "\titers: 200, epoch: 9 | loss: 0.0048349\n",
      "\tspeed: 0.0210s/iter; left time: 430.1077s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.93s\n",
      "Steps: 225 | Train Loss: 0.0052988 Vali Loss: 0.0192110 Test Loss: 0.0231458\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0050047\n",
      "\tspeed: 0.0575s/iter; left time: 1170.7312s\n",
      "\titers: 200, epoch: 10 | loss: 0.0050389\n",
      "\tspeed: 0.0215s/iter; left time: 435.4071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.04s\n",
      "Steps: 225 | Train Loss: 0.0049720 Vali Loss: 0.0198488 Test Loss: 0.0230945\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0045330\n",
      "\tspeed: 0.0596s/iter; left time: 1200.1550s\n",
      "\titers: 200, epoch: 11 | loss: 0.0046306\n",
      "\tspeed: 0.0208s/iter; left time: 416.1887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 225 | Train Loss: 0.0046707 Vali Loss: 0.0195044 Test Loss: 0.0229342\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0043923\n",
      "\tspeed: 0.0631s/iter; left time: 1256.7383s\n",
      "\titers: 200, epoch: 12 | loss: 0.0038309\n",
      "\tspeed: 0.0208s/iter; left time: 412.1831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.11s\n",
      "Steps: 225 | Train Loss: 0.0044319 Vali Loss: 0.0197984 Test Loss: 0.0232360\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.01995600387454033, rmse:0.14126572012901306, mae:0.08893339335918427, rse:0.5471355319023132\n",
      "Intermediate time for FR and pred_len 168: 00h:01m:40.54s\n",
      "Intermediate time for FR: 00h:05m:10.65s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0264049\n",
      "\tspeed: 0.0401s/iter; left time: 902.2086s\n",
      "\titers: 200, epoch: 1 | loss: 0.0212160\n",
      "\tspeed: 0.0226s/iter; left time: 505.9598s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.71s\n",
      "Steps: 226 | Train Loss: 0.0303445 Vali Loss: 0.0161344 Test Loss: 0.0174399\n",
      "Validation loss decreased (inf --> 0.016134).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0111341\n",
      "\tspeed: 0.0502s/iter; left time: 1119.0532s\n",
      "\titers: 200, epoch: 2 | loss: 0.0105001\n",
      "\tspeed: 0.0208s/iter; left time: 460.7199s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.08s\n",
      "Steps: 226 | Train Loss: 0.0123827 Vali Loss: 0.0101251 Test Loss: 0.0111429\n",
      "Validation loss decreased (0.016134 --> 0.010125).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0094639\n",
      "\tspeed: 0.0547s/iter; left time: 1205.8333s\n",
      "\titers: 200, epoch: 3 | loss: 0.0118124\n",
      "\tspeed: 0.0194s/iter; left time: 425.4621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0103339 Vali Loss: 0.0096896 Test Loss: 0.0108361\n",
      "Validation loss decreased (0.010125 --> 0.009690).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0101394\n",
      "\tspeed: 0.0546s/iter; left time: 1190.7430s\n",
      "\titers: 200, epoch: 4 | loss: 0.0083410\n",
      "\tspeed: 0.0188s/iter; left time: 407.5656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0097526 Vali Loss: 0.0094186 Test Loss: 0.0106146\n",
      "Validation loss decreased (0.009690 --> 0.009419).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0092185\n",
      "\tspeed: 0.0533s/iter; left time: 1152.1456s\n",
      "\titers: 200, epoch: 5 | loss: 0.0083651\n",
      "\tspeed: 0.0180s/iter; left time: 387.8353s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 226 | Train Loss: 0.0092472 Vali Loss: 0.0092419 Test Loss: 0.0106667\n",
      "Validation loss decreased (0.009419 --> 0.009242).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0088084\n",
      "\tspeed: 0.0502s/iter; left time: 1072.5163s\n",
      "\titers: 200, epoch: 6 | loss: 0.0076559\n",
      "\tspeed: 0.0204s/iter; left time: 432.8715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0086846 Vali Loss: 0.0093900 Test Loss: 0.0106002\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0087312\n",
      "\tspeed: 0.0488s/iter; left time: 1031.6167s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092575\n",
      "\tspeed: 0.0210s/iter; left time: 441.0236s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 226 | Train Loss: 0.0082112 Vali Loss: 0.0096978 Test Loss: 0.0107965\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0080053\n",
      "\tspeed: 0.0504s/iter; left time: 1053.6779s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081390\n",
      "\tspeed: 0.0182s/iter; left time: 379.3088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.21s\n",
      "Steps: 226 | Train Loss: 0.0076047 Vali Loss: 0.0100151 Test Loss: 0.0111496\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0066461\n",
      "\tspeed: 0.0481s/iter; left time: 994.6268s\n",
      "\titers: 200, epoch: 9 | loss: 0.0070476\n",
      "\tspeed: 0.0233s/iter; left time: 478.9058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.32s\n",
      "Steps: 226 | Train Loss: 0.0070180 Vali Loss: 0.0101141 Test Loss: 0.0112617\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0057631\n",
      "\tspeed: 0.0453s/iter; left time: 927.9982s\n",
      "\titers: 200, epoch: 10 | loss: 0.0062514\n",
      "\tspeed: 0.0247s/iter; left time: 503.8603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.23s\n",
      "Steps: 226 | Train Loss: 0.0065206 Vali Loss: 0.0103133 Test Loss: 0.0114330\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0062055\n",
      "\tspeed: 0.0424s/iter; left time: 859.1355s\n",
      "\titers: 200, epoch: 11 | loss: 0.0056173\n",
      "\tspeed: 0.0221s/iter; left time: 445.6305s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.15s\n",
      "Steps: 226 | Train Loss: 0.0060727 Vali Loss: 0.0106845 Test Loss: 0.0117234\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0050799\n",
      "\tspeed: 0.0485s/iter; left time: 970.5160s\n",
      "\titers: 200, epoch: 12 | loss: 0.0054049\n",
      "\tspeed: 0.0207s/iter; left time: 411.3019s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.14s\n",
      "Steps: 226 | Train Loss: 0.0057344 Vali Loss: 0.0107858 Test Loss: 0.0115632\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0054517\n",
      "\tspeed: 0.0505s/iter; left time: 999.5847s\n",
      "\titers: 200, epoch: 13 | loss: 0.0050979\n",
      "\tspeed: 0.0186s/iter; left time: 366.9107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 226 | Train Loss: 0.0054612 Vali Loss: 0.0109315 Test Loss: 0.0117036\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0050719\n",
      "\tspeed: 0.0524s/iter; left time: 1025.0226s\n",
      "\titers: 200, epoch: 14 | loss: 0.0055266\n",
      "\tspeed: 0.0189s/iter; left time: 367.8269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.56s\n",
      "Steps: 226 | Train Loss: 0.0051985 Vali Loss: 0.0107567 Test Loss: 0.0119687\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0050930\n",
      "\tspeed: 0.0540s/iter; left time: 1043.3980s\n",
      "\titers: 200, epoch: 15 | loss: 0.0046835\n",
      "\tspeed: 0.0192s/iter; left time: 369.9461s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0050102 Vali Loss: 0.0108887 Test Loss: 0.0118751\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010666660964488983, rmse:0.10327953100204468, mae:0.0619974210858345, rse:0.39024245738983154\n",
      "Intermediate time for IT and pred_len 24: 00h:01m:55.42s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0312227\n",
      "\tspeed: 0.0483s/iter; left time: 1080.9080s\n",
      "\titers: 200, epoch: 1 | loss: 0.0266664\n",
      "\tspeed: 0.0282s/iter; left time: 629.2883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 225 | Train Loss: 0.0356257 Vali Loss: 0.0213617 Test Loss: 0.0232229\n",
      "Validation loss decreased (inf --> 0.021362).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0196660\n",
      "\tspeed: 0.0461s/iter; left time: 1023.0386s\n",
      "\titers: 200, epoch: 2 | loss: 0.0183100\n",
      "\tspeed: 0.0207s/iter; left time: 456.8104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0196914 Vali Loss: 0.0170778 Test Loss: 0.0191014\n",
      "Validation loss decreased (0.021362 --> 0.017078).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0178034\n",
      "\tspeed: 0.0555s/iter; left time: 1219.3081s\n",
      "\titers: 200, epoch: 3 | loss: 0.0181890\n",
      "\tspeed: 0.0198s/iter; left time: 432.1097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 225 | Train Loss: 0.0174670 Vali Loss: 0.0169888 Test Loss: 0.0188168\n",
      "Validation loss decreased (0.017078 --> 0.016989).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0167031\n",
      "\tspeed: 0.0592s/iter; left time: 1286.5769s\n",
      "\titers: 200, epoch: 4 | loss: 0.0140209\n",
      "\tspeed: 0.0206s/iter; left time: 445.7012s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0158114 Vali Loss: 0.0181897 Test Loss: 0.0195597\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0166279\n",
      "\tspeed: 0.0520s/iter; left time: 1118.8445s\n",
      "\titers: 200, epoch: 5 | loss: 0.0126541\n",
      "\tspeed: 0.0208s/iter; left time: 445.7041s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.11s\n",
      "Steps: 225 | Train Loss: 0.0139899 Vali Loss: 0.0198033 Test Loss: 0.0205443\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0126828\n",
      "\tspeed: 0.0552s/iter; left time: 1174.8455s\n",
      "\titers: 200, epoch: 6 | loss: 0.0131766\n",
      "\tspeed: 0.0213s/iter; left time: 451.0288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.98s\n",
      "Steps: 225 | Train Loss: 0.0121957 Vali Loss: 0.0210449 Test Loss: 0.0204667\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0109516\n",
      "\tspeed: 0.0547s/iter; left time: 1150.7185s\n",
      "\titers: 200, epoch: 7 | loss: 0.0098201\n",
      "\tspeed: 0.0210s/iter; left time: 440.7656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 225 | Train Loss: 0.0107617 Vali Loss: 0.0208559 Test Loss: 0.0210789\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0108403\n",
      "\tspeed: 0.0446s/iter; left time: 928.2046s\n",
      "\titers: 200, epoch: 8 | loss: 0.0090046\n",
      "\tspeed: 0.0272s/iter; left time: 562.9741s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 225 | Train Loss: 0.0097365 Vali Loss: 0.0209417 Test Loss: 0.0221892\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0082687\n",
      "\tspeed: 0.0435s/iter; left time: 895.2202s\n",
      "\titers: 200, epoch: 9 | loss: 0.0082735\n",
      "\tspeed: 0.0287s/iter; left time: 589.1983s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0088897 Vali Loss: 0.0214697 Test Loss: 0.0218216\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0087429\n",
      "\tspeed: 0.0459s/iter; left time: 934.2756s\n",
      "\titers: 200, epoch: 10 | loss: 0.0078785\n",
      "\tspeed: 0.0252s/iter; left time: 511.0760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 225 | Train Loss: 0.0082368 Vali Loss: 0.0222755 Test Loss: 0.0228619\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0086819\n",
      "\tspeed: 0.0482s/iter; left time: 970.4437s\n",
      "\titers: 200, epoch: 11 | loss: 0.0084118\n",
      "\tspeed: 0.0212s/iter; left time: 425.5440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.32s\n",
      "Steps: 225 | Train Loss: 0.0077866 Vali Loss: 0.0218232 Test Loss: 0.0222956\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0069241\n",
      "\tspeed: 0.0562s/iter; left time: 1120.4966s\n",
      "\titers: 200, epoch: 12 | loss: 0.0077941\n",
      "\tspeed: 0.0201s/iter; left time: 397.6030s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 225 | Train Loss: 0.0074321 Vali Loss: 0.0221129 Test Loss: 0.0221040\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0076319\n",
      "\tspeed: 0.0500s/iter; left time: 984.5919s\n",
      "\titers: 200, epoch: 13 | loss: 0.0071445\n",
      "\tspeed: 0.0220s/iter; left time: 432.0278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.64s\n",
      "Steps: 225 | Train Loss: 0.0070701 Vali Loss: 0.0216414 Test Loss: 0.0220283\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01881682500243187, rmse:0.13717442750930786, mae:0.08598846942186356, rse:0.5186712741851807\n",
      "Intermediate time for IT and pred_len 96: 00h:01m:47.66s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0296239\n",
      "\tspeed: 0.0476s/iter; left time: 1066.8337s\n",
      "\titers: 200, epoch: 1 | loss: 0.0281595\n",
      "\tspeed: 0.0293s/iter; left time: 653.4324s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.90s\n",
      "Steps: 225 | Train Loss: 0.0368548 Vali Loss: 0.0223018 Test Loss: 0.0240904\n",
      "Validation loss decreased (inf --> 0.022302).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0215188\n",
      "\tspeed: 0.0498s/iter; left time: 1103.9927s\n",
      "\titers: 200, epoch: 2 | loss: 0.0191364\n",
      "\tspeed: 0.0280s/iter; left time: 617.8952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 225 | Train Loss: 0.0213553 Vali Loss: 0.0186788 Test Loss: 0.0198309\n",
      "Validation loss decreased (0.022302 --> 0.018679).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0185150\n",
      "\tspeed: 0.0511s/iter; left time: 1122.2386s\n",
      "\titers: 200, epoch: 3 | loss: 0.0157815\n",
      "\tspeed: 0.0238s/iter; left time: 521.1293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.44s\n",
      "Steps: 225 | Train Loss: 0.0186807 Vali Loss: 0.0192694 Test Loss: 0.0201614\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0160066\n",
      "\tspeed: 0.0511s/iter; left time: 1110.7214s\n",
      "\titers: 200, epoch: 4 | loss: 0.0164357\n",
      "\tspeed: 0.0197s/iter; left time: 426.5529s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.96s\n",
      "Steps: 225 | Train Loss: 0.0167670 Vali Loss: 0.0198088 Test Loss: 0.0203017\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0150016\n",
      "\tspeed: 0.0537s/iter; left time: 1155.5778s\n",
      "\titers: 200, epoch: 5 | loss: 0.0145082\n",
      "\tspeed: 0.0210s/iter; left time: 448.7711s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 225 | Train Loss: 0.0150612 Vali Loss: 0.0214123 Test Loss: 0.0209583\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0135488\n",
      "\tspeed: 0.0539s/iter; left time: 1147.3276s\n",
      "\titers: 200, epoch: 6 | loss: 0.0130257\n",
      "\tspeed: 0.0209s/iter; left time: 442.2027s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.81s\n",
      "Steps: 225 | Train Loss: 0.0135763 Vali Loss: 0.0219544 Test Loss: 0.0217701\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0125295\n",
      "\tspeed: 0.0523s/iter; left time: 1100.0941s\n",
      "\titers: 200, epoch: 7 | loss: 0.0118939\n",
      "\tspeed: 0.0194s/iter; left time: 406.4621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.62s\n",
      "Steps: 225 | Train Loss: 0.0121774 Vali Loss: 0.0224625 Test Loss: 0.0225583\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0113757\n",
      "\tspeed: 0.0457s/iter; left time: 951.1453s\n",
      "\titers: 200, epoch: 8 | loss: 0.0104438\n",
      "\tspeed: 0.0267s/iter; left time: 552.7632s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 225 | Train Loss: 0.0110213 Vali Loss: 0.0227240 Test Loss: 0.0222048\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0101773\n",
      "\tspeed: 0.0443s/iter; left time: 912.6128s\n",
      "\titers: 200, epoch: 9 | loss: 0.0099189\n",
      "\tspeed: 0.0280s/iter; left time: 573.4481s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0101589 Vali Loss: 0.0221913 Test Loss: 0.0226550\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0093672\n",
      "\tspeed: 0.0425s/iter; left time: 866.2472s\n",
      "\titers: 200, epoch: 10 | loss: 0.0089159\n",
      "\tspeed: 0.0202s/iter; left time: 409.3931s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 225 | Train Loss: 0.0094935 Vali Loss: 0.0220188 Test Loss: 0.0229948\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0092734\n",
      "\tspeed: 0.0562s/iter; left time: 1132.6478s\n",
      "\titers: 200, epoch: 11 | loss: 0.0082421\n",
      "\tspeed: 0.0213s/iter; left time: 427.8571s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0089367 Vali Loss: 0.0217495 Test Loss: 0.0225141\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0083151\n",
      "\tspeed: 0.0515s/iter; left time: 1026.8916s\n",
      "\titers: 200, epoch: 12 | loss: 0.0083149\n",
      "\tspeed: 0.0184s/iter; left time: 365.0908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.01s\n",
      "Steps: 225 | Train Loss: 0.0084946 Vali Loss: 0.0219489 Test Loss: 0.0228725\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019830867648124695, rmse:0.14082211256027222, mae:0.09015556424856186, rse:0.5329583883285522\n",
      "Intermediate time for IT and pred_len 168: 00h:01m:38.59s\n",
      "Intermediate time for IT: 00h:05m:21.67s\n",
      "Total time: 00h:52m:17.35s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_MIX_FEATURES.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            model_id = f\"channel_mixing_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>0.1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.0692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.0611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>0.1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.0902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0255  0.1596  0.1060\n",
       "        96        0.0381  0.1953  0.1384\n",
       "        168       0.0400  0.2001  0.1428\n",
       "ES      24        0.0114  0.1068  0.0692\n",
       "        96        0.0207  0.1437  0.0965\n",
       "        168       0.0229  0.1514  0.1032\n",
       "FR      24        0.0110  0.1049  0.0611\n",
       "        96        0.0191  0.1383  0.0849\n",
       "        168       0.0200  0.1413  0.0889\n",
       "GB      24        0.0286  0.1691  0.1175\n",
       "        96        0.0428  0.2069  0.1471\n",
       "        168       0.0450  0.2121  0.1520\n",
       "IT      24        0.0107  0.1033  0.0620\n",
       "        96        0.0188  0.1372  0.0860\n",
       "        168       0.0198  0.1408  0.0902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_MIX_FEATURES.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0234579\n",
      "\tspeed: 0.0700s/iter; left time: 1574.1290s\n",
      "\titers: 200, epoch: 1 | loss: 0.0188763\n",
      "\tspeed: 0.0480s/iter; left time: 1074.2645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 226 | Train Loss: 0.0251614 Vali Loss: 0.0242755 Test Loss: 0.0266506\n",
      "Validation loss decreased (inf --> 0.024276).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0147026\n",
      "\tspeed: 0.0929s/iter; left time: 2069.5066s\n",
      "\titers: 200, epoch: 2 | loss: 0.0127275\n",
      "\tspeed: 0.0476s/iter; left time: 1056.3407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.16s\n",
      "Steps: 226 | Train Loss: 0.0146741 Vali Loss: 0.0213676 Test Loss: 0.0233677\n",
      "Validation loss decreased (0.024276 --> 0.021368).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0132120\n",
      "\tspeed: 0.1003s/iter; left time: 2210.7943s\n",
      "\titers: 200, epoch: 3 | loss: 0.0103937\n",
      "\tspeed: 0.0483s/iter; left time: 1060.1647s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.26s\n",
      "Steps: 226 | Train Loss: 0.0117886 Vali Loss: 0.0237567 Test Loss: 0.0268710\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0099479\n",
      "\tspeed: 0.0928s/iter; left time: 2025.6468s\n",
      "\titers: 200, epoch: 4 | loss: 0.0083373\n",
      "\tspeed: 0.0488s/iter; left time: 1060.7452s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 226 | Train Loss: 0.0090726 Vali Loss: 0.0256334 Test Loss: 0.0279961\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0073438\n",
      "\tspeed: 0.0861s/iter; left time: 1859.5452s\n",
      "\titers: 200, epoch: 5 | loss: 0.0066576\n",
      "\tspeed: 0.0477s/iter; left time: 1025.7056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.09s\n",
      "Steps: 226 | Train Loss: 0.0069540 Vali Loss: 0.0259422 Test Loss: 0.0284075\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0058564\n",
      "\tspeed: 0.0965s/iter; left time: 2062.8319s\n",
      "\titers: 200, epoch: 6 | loss: 0.0050857\n",
      "\tspeed: 0.0493s/iter; left time: 1048.4800s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.29s\n",
      "Steps: 226 | Train Loss: 0.0056569 Vali Loss: 0.0269273 Test Loss: 0.0289203\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0046096\n",
      "\tspeed: 0.0916s/iter; left time: 1936.9642s\n",
      "\titers: 200, epoch: 7 | loss: 0.0043860\n",
      "\tspeed: 0.0493s/iter; left time: 1037.7382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 226 | Train Loss: 0.0047801 Vali Loss: 0.0265904 Test Loss: 0.0292324\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0042168\n",
      "\tspeed: 0.0932s/iter; left time: 1949.7598s\n",
      "\titers: 200, epoch: 8 | loss: 0.0038549\n",
      "\tspeed: 0.0497s/iter; left time: 1034.4960s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 226 | Train Loss: 0.0042179 Vali Loss: 0.0266252 Test Loss: 0.0286232\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0042412\n",
      "\tspeed: 0.0944s/iter; left time: 1952.3947s\n",
      "\titers: 200, epoch: 9 | loss: 0.0034975\n",
      "\tspeed: 0.0483s/iter; left time: 994.2371s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 226 | Train Loss: 0.0037897 Vali Loss: 0.0266672 Test Loss: 0.0288044\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0034196\n",
      "\tspeed: 0.0930s/iter; left time: 1902.4802s\n",
      "\titers: 200, epoch: 10 | loss: 0.0032693\n",
      "\tspeed: 0.0504s/iter; left time: 1026.5745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 226 | Train Loss: 0.0034596 Vali Loss: 0.0265314 Test Loss: 0.0287724\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0032838\n",
      "\tspeed: 0.0972s/iter; left time: 1966.9264s\n",
      "\titers: 200, epoch: 11 | loss: 0.0030819\n",
      "\tspeed: 0.0509s/iter; left time: 1025.8333s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.78s\n",
      "Steps: 226 | Train Loss: 0.0032145 Vali Loss: 0.0266764 Test Loss: 0.0286681\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0028341\n",
      "\tspeed: 0.0893s/iter; left time: 1786.6423s\n",
      "\titers: 200, epoch: 12 | loss: 0.0030430\n",
      "\tspeed: 0.0504s/iter; left time: 1003.7997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 226 | Train Loss: 0.0030139 Vali Loss: 0.0263912 Test Loss: 0.0286644\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02336770109832287, rmse:0.1528649777173996, mae:0.1005844995379448, rse:0.5394816398620605\n",
      "Intermediate time for DE and pred_len 24: 00h:03m:01.06s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0332638\n",
      "\tspeed: 0.0791s/iter; left time: 1771.5424s\n",
      "\titers: 200, epoch: 1 | loss: 0.0300356\n",
      "\tspeed: 0.0502s/iter; left time: 1119.4530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.85s\n",
      "Steps: 225 | Train Loss: 0.0334871 Vali Loss: 0.0348219 Test Loss: 0.0412176\n",
      "Validation loss decreased (inf --> 0.034822).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0224506\n",
      "\tspeed: 0.0969s/iter; left time: 2149.5423s\n",
      "\titers: 200, epoch: 2 | loss: 0.0186753\n",
      "\tspeed: 0.0501s/iter; left time: 1106.3206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0218892 Vali Loss: 0.0342691 Test Loss: 0.0442524\n",
      "Validation loss decreased (0.034822 --> 0.034269).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0144677\n",
      "\tspeed: 0.1107s/iter; left time: 2430.9855s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123060\n",
      "\tspeed: 0.0490s/iter; left time: 1070.5410s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.55s\n",
      "Steps: 225 | Train Loss: 0.0147549 Vali Loss: 0.0370631 Test Loss: 0.0474851\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0108411\n",
      "\tspeed: 0.0979s/iter; left time: 2127.4352s\n",
      "\titers: 200, epoch: 4 | loss: 0.0097878\n",
      "\tspeed: 0.0483s/iter; left time: 1044.4061s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.39s\n",
      "Steps: 225 | Train Loss: 0.0106510 Vali Loss: 0.0379387 Test Loss: 0.0481010\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0081787\n",
      "\tspeed: 0.0963s/iter; left time: 2070.7255s\n",
      "\titers: 200, epoch: 5 | loss: 0.0077351\n",
      "\tspeed: 0.0461s/iter; left time: 986.1043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.12s\n",
      "Steps: 225 | Train Loss: 0.0083025 Vali Loss: 0.0373596 Test Loss: 0.0464266\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0069760\n",
      "\tspeed: 0.0920s/iter; left time: 1958.2338s\n",
      "\titers: 200, epoch: 6 | loss: 0.0065775\n",
      "\tspeed: 0.0505s/iter; left time: 1068.7359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 225 | Train Loss: 0.0068311 Vali Loss: 0.0378817 Test Loss: 0.0463117\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064354\n",
      "\tspeed: 0.0926s/iter; left time: 1949.2758s\n",
      "\titers: 200, epoch: 7 | loss: 0.0056448\n",
      "\tspeed: 0.0511s/iter; left time: 1070.0455s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.55s\n",
      "Steps: 225 | Train Loss: 0.0058802 Vali Loss: 0.0378618 Test Loss: 0.0464821\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0053293\n",
      "\tspeed: 0.1029s/iter; left time: 2143.9633s\n",
      "\titers: 200, epoch: 8 | loss: 0.0054496\n",
      "\tspeed: 0.0506s/iter; left time: 1048.4218s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.68s\n",
      "Steps: 225 | Train Loss: 0.0052071 Vali Loss: 0.0384322 Test Loss: 0.0474449\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0048091\n",
      "\tspeed: 0.0915s/iter; left time: 1884.1641s\n",
      "\titers: 200, epoch: 9 | loss: 0.0047850\n",
      "\tspeed: 0.0495s/iter; left time: 1013.8719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 225 | Train Loss: 0.0046842 Vali Loss: 0.0378488 Test Loss: 0.0462881\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0044565\n",
      "\tspeed: 0.0980s/iter; left time: 1997.3757s\n",
      "\titers: 200, epoch: 10 | loss: 0.0038844\n",
      "\tspeed: 0.0497s/iter; left time: 1007.4344s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 225 | Train Loss: 0.0043068 Vali Loss: 0.0380081 Test Loss: 0.0465702\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0042491\n",
      "\tspeed: 0.0916s/iter; left time: 1845.9031s\n",
      "\titers: 200, epoch: 11 | loss: 0.0040506\n",
      "\tspeed: 0.0505s/iter; left time: 1013.1239s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 225 | Train Loss: 0.0040233 Vali Loss: 0.0386904 Test Loss: 0.0469827\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0037805\n",
      "\tspeed: 0.0998s/iter; left time: 1988.0510s\n",
      "\titers: 200, epoch: 12 | loss: 0.0035495\n",
      "\tspeed: 0.0503s/iter; left time: 998.0157s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.54s\n",
      "Steps: 225 | Train Loss: 0.0037756 Vali Loss: 0.0378433 Test Loss: 0.0466710\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04425235465168953, rmse:0.21036243438720703, mae:0.14291603863239288, rse:0.744935929775238\n",
      "Intermediate time for DE and pred_len 96: 00h:03m:09.12s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0361657\n",
      "\tspeed: 0.0668s/iter; left time: 1496.7550s\n",
      "\titers: 200, epoch: 1 | loss: 0.0303956\n",
      "\tspeed: 0.0504s/iter; left time: 1124.5588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.83s\n",
      "Steps: 225 | Train Loss: 0.0362478 Vali Loss: 0.0360635 Test Loss: 0.0436179\n",
      "Validation loss decreased (inf --> 0.036064).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0248308\n",
      "\tspeed: 0.1080s/iter; left time: 2394.2190s\n",
      "\titers: 200, epoch: 2 | loss: 0.0202720\n",
      "\tspeed: 0.0493s/iter; left time: 1088.1554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:12.10s\n",
      "Steps: 225 | Train Loss: 0.0239077 Vali Loss: 0.0366986 Test Loss: 0.0456352\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0168566\n",
      "\tspeed: 0.0965s/iter; left time: 2119.1420s\n",
      "\titers: 200, epoch: 3 | loss: 0.0140941\n",
      "\tspeed: 0.0500s/iter; left time: 1091.5463s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.64s\n",
      "Steps: 225 | Train Loss: 0.0165090 Vali Loss: 0.0390133 Test Loss: 0.0496497\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0126213\n",
      "\tspeed: 0.1044s/iter; left time: 2268.5671s\n",
      "\titers: 200, epoch: 4 | loss: 0.0113191\n",
      "\tspeed: 0.0512s/iter; left time: 1106.5469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.81s\n",
      "Steps: 225 | Train Loss: 0.0123056 Vali Loss: 0.0389968 Test Loss: 0.0487190\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0098067\n",
      "\tspeed: 0.0983s/iter; left time: 2114.0404s\n",
      "\titers: 200, epoch: 5 | loss: 0.0088960\n",
      "\tspeed: 0.0515s/iter; left time: 1102.7192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.79s\n",
      "Steps: 225 | Train Loss: 0.0096736 Vali Loss: 0.0396343 Test Loss: 0.0498428\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0080175\n",
      "\tspeed: 0.1043s/iter; left time: 2220.0994s\n",
      "\titers: 200, epoch: 6 | loss: 0.0072905\n",
      "\tspeed: 0.0519s/iter; left time: 1099.7504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.83s\n",
      "Steps: 225 | Train Loss: 0.0079707 Vali Loss: 0.0397890 Test Loss: 0.0493932\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0067940\n",
      "\tspeed: 0.0997s/iter; left time: 2098.0576s\n",
      "\titers: 200, epoch: 7 | loss: 0.0064623\n",
      "\tspeed: 0.0517s/iter; left time: 1084.0475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:12.12s\n",
      "Steps: 225 | Train Loss: 0.0069017 Vali Loss: 0.0401710 Test Loss: 0.0496219\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0063115\n",
      "\tspeed: 0.1011s/iter; left time: 2105.5356s\n",
      "\titers: 200, epoch: 8 | loss: 0.0056388\n",
      "\tspeed: 0.0510s/iter; left time: 1057.4030s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.01s\n",
      "Steps: 225 | Train Loss: 0.0060350 Vali Loss: 0.0401194 Test Loss: 0.0490433\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0054513\n",
      "\tspeed: 0.1060s/iter; left time: 2184.4789s\n",
      "\titers: 200, epoch: 9 | loss: 0.0053721\n",
      "\tspeed: 0.0559s/iter; left time: 1147.0101s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.40s\n",
      "Steps: 225 | Train Loss: 0.0054257 Vali Loss: 0.0398992 Test Loss: 0.0493147\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049561\n",
      "\tspeed: 0.0967s/iter; left time: 1970.4992s\n",
      "\titers: 200, epoch: 10 | loss: 0.0048473\n",
      "\tspeed: 0.0504s/iter; left time: 1022.6361s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.71s\n",
      "Steps: 225 | Train Loss: 0.0049978 Vali Loss: 0.0399362 Test Loss: 0.0488129\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0048232\n",
      "\tspeed: 0.1024s/iter; left time: 2064.3837s\n",
      "\titers: 200, epoch: 11 | loss: 0.0045335\n",
      "\tspeed: 0.0513s/iter; left time: 1027.8097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.98s\n",
      "Steps: 225 | Train Loss: 0.0046169 Vali Loss: 0.0395273 Test Loss: 0.0489856\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.0436178483068943, rmse:0.20884886384010315, mae:0.1491432934999466, rse:0.7397594451904297\n",
      "Intermediate time for DE and pred_len 168: 00h:02m:58.36s\n",
      "Intermediate time for DE: 00h:09m:08.55s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0218535\n",
      "\tspeed: 0.0798s/iter; left time: 1796.0078s\n",
      "\titers: 200, epoch: 1 | loss: 0.0183579\n",
      "\tspeed: 0.0503s/iter; left time: 1127.6273s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.70s\n",
      "Steps: 226 | Train Loss: 0.0221939 Vali Loss: 0.0227288 Test Loss: 0.0289717\n",
      "Validation loss decreased (inf --> 0.022729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0139320\n",
      "\tspeed: 0.1053s/iter; left time: 2345.8819s\n",
      "\titers: 200, epoch: 2 | loss: 0.0140528\n",
      "\tspeed: 0.0499s/iter; left time: 1107.2148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.49s\n",
      "Steps: 226 | Train Loss: 0.0145776 Vali Loss: 0.0207217 Test Loss: 0.0265528\n",
      "Validation loss decreased (0.022729 --> 0.020722).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0131557\n",
      "\tspeed: 0.0922s/iter; left time: 2033.5530s\n",
      "\titers: 200, epoch: 3 | loss: 0.0105842\n",
      "\tspeed: 0.0506s/iter; left time: 1109.6279s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 226 | Train Loss: 0.0124672 Vali Loss: 0.0228512 Test Loss: 0.0287703\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0104724\n",
      "\tspeed: 0.0913s/iter; left time: 1992.0171s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084868\n",
      "\tspeed: 0.0460s/iter; left time: 999.2377s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.65s\n",
      "Steps: 226 | Train Loss: 0.0099155 Vali Loss: 0.0245583 Test Loss: 0.0308842\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0084019\n",
      "\tspeed: 0.0905s/iter; left time: 1954.3459s\n",
      "\titers: 200, epoch: 5 | loss: 0.0073181\n",
      "\tspeed: 0.0482s/iter; left time: 1036.9286s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 226 | Train Loss: 0.0077618 Vali Loss: 0.0245623 Test Loss: 0.0308459\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0060664\n",
      "\tspeed: 0.0866s/iter; left time: 1851.6961s\n",
      "\titers: 200, epoch: 6 | loss: 0.0058028\n",
      "\tspeed: 0.0477s/iter; left time: 1015.1849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 226 | Train Loss: 0.0063329 Vali Loss: 0.0250764 Test Loss: 0.0328130\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0056493\n",
      "\tspeed: 0.0916s/iter; left time: 1935.8874s\n",
      "\titers: 200, epoch: 7 | loss: 0.0053948\n",
      "\tspeed: 0.0483s/iter; left time: 1015.6603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.21s\n",
      "Steps: 226 | Train Loss: 0.0054284 Vali Loss: 0.0253184 Test Loss: 0.0329063\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0044056\n",
      "\tspeed: 0.0885s/iter; left time: 1851.6830s\n",
      "\titers: 200, epoch: 8 | loss: 0.0045532\n",
      "\tspeed: 0.0468s/iter; left time: 974.7689s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 226 | Train Loss: 0.0048233 Vali Loss: 0.0254758 Test Loss: 0.0327160\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0040133\n",
      "\tspeed: 0.0913s/iter; left time: 1888.9773s\n",
      "\titers: 200, epoch: 9 | loss: 0.0042028\n",
      "\tspeed: 0.0497s/iter; left time: 1024.3168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.44s\n",
      "Steps: 226 | Train Loss: 0.0044050 Vali Loss: 0.0256798 Test Loss: 0.0325368\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0039573\n",
      "\tspeed: 0.0908s/iter; left time: 1857.6843s\n",
      "\titers: 200, epoch: 10 | loss: 0.0041755\n",
      "\tspeed: 0.0490s/iter; left time: 997.5784s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 226 | Train Loss: 0.0040621 Vali Loss: 0.0253594 Test Loss: 0.0324591\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0037444\n",
      "\tspeed: 0.0904s/iter; left time: 1830.0027s\n",
      "\titers: 200, epoch: 11 | loss: 0.0037421\n",
      "\tspeed: 0.0502s/iter; left time: 1011.4231s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 226 | Train Loss: 0.0037959 Vali Loss: 0.0254270 Test Loss: 0.0329862\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0035340\n",
      "\tspeed: 0.0960s/iter; left time: 1922.0715s\n",
      "\titers: 200, epoch: 12 | loss: 0.0034475\n",
      "\tspeed: 0.0497s/iter; left time: 990.1470s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 226 | Train Loss: 0.0035778 Vali Loss: 0.0253695 Test Loss: 0.0325746\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.026552753522992134, rmse:0.16295015811920166, mae:0.10944870114326477, rse:0.5621318817138672\n",
      "Intermediate time for GB and pred_len 24: 00h:03m:02.28s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0288654\n",
      "\tspeed: 0.0655s/iter; left time: 1468.2221s\n",
      "\titers: 200, epoch: 1 | loss: 0.0257712\n",
      "\tspeed: 0.0481s/iter; left time: 1071.9359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.49s\n",
      "Steps: 225 | Train Loss: 0.0295441 Vali Loss: 0.0322599 Test Loss: 0.0463921\n",
      "Validation loss decreased (inf --> 0.032260).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0204241\n",
      "\tspeed: 0.1087s/iter; left time: 2411.0143s\n",
      "\titers: 200, epoch: 2 | loss: 0.0181788\n",
      "\tspeed: 0.0489s/iter; left time: 1079.3847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.23s\n",
      "Steps: 225 | Train Loss: 0.0211684 Vali Loss: 0.0359143 Test Loss: 0.0490593\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0145193\n",
      "\tspeed: 0.0933s/iter; left time: 2048.7223s\n",
      "\titers: 200, epoch: 3 | loss: 0.0129500\n",
      "\tspeed: 0.0498s/iter; left time: 1089.0880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 225 | Train Loss: 0.0148745 Vali Loss: 0.0380365 Test Loss: 0.0511157\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0108255\n",
      "\tspeed: 0.0910s/iter; left time: 1977.3957s\n",
      "\titers: 200, epoch: 4 | loss: 0.0103050\n",
      "\tspeed: 0.0481s/iter; left time: 1039.3682s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 225 | Train Loss: 0.0110534 Vali Loss: 0.0386122 Test Loss: 0.0516633\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0083075\n",
      "\tspeed: 0.0987s/iter; left time: 2121.2651s\n",
      "\titers: 200, epoch: 5 | loss: 0.0079974\n",
      "\tspeed: 0.0550s/iter; left time: 1176.5774s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.17s\n",
      "Steps: 225 | Train Loss: 0.0087882 Vali Loss: 0.0387466 Test Loss: 0.0523526\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0085979\n",
      "\tspeed: 0.0950s/iter; left time: 2020.9800s\n",
      "\titers: 200, epoch: 6 | loss: 0.0070879\n",
      "\tspeed: 0.0497s/iter; left time: 1051.8523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 225 | Train Loss: 0.0073469 Vali Loss: 0.0378753 Test Loss: 0.0519166\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0068098\n",
      "\tspeed: 0.0964s/iter; left time: 2029.8703s\n",
      "\titers: 200, epoch: 7 | loss: 0.0062420\n",
      "\tspeed: 0.0516s/iter; left time: 1080.4960s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.80s\n",
      "Steps: 225 | Train Loss: 0.0064157 Vali Loss: 0.0382622 Test Loss: 0.0509550\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0059088\n",
      "\tspeed: 0.0940s/iter; left time: 1957.8672s\n",
      "\titers: 200, epoch: 8 | loss: 0.0055143\n",
      "\tspeed: 0.0541s/iter; left time: 1121.3947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.05s\n",
      "Steps: 225 | Train Loss: 0.0057242 Vali Loss: 0.0377223 Test Loss: 0.0509155\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0057496\n",
      "\tspeed: 0.0990s/iter; left time: 2039.3611s\n",
      "\titers: 200, epoch: 9 | loss: 0.0049260\n",
      "\tspeed: 0.0509s/iter; left time: 1042.8847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0052227 Vali Loss: 0.0377632 Test Loss: 0.0501606\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0051351\n",
      "\tspeed: 0.0910s/iter; left time: 1854.3741s\n",
      "\titers: 200, epoch: 10 | loss: 0.0045289\n",
      "\tspeed: 0.0529s/iter; left time: 1072.6055s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.88s\n",
      "Steps: 225 | Train Loss: 0.0048293 Vali Loss: 0.0375795 Test Loss: 0.0502433\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0044681\n",
      "\tspeed: 0.0955s/iter; left time: 1923.7912s\n",
      "\titers: 200, epoch: 11 | loss: 0.0045333\n",
      "\tspeed: 0.0506s/iter; left time: 1015.1115s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.96s\n",
      "Steps: 225 | Train Loss: 0.0044935 Vali Loss: 0.0373866 Test Loss: 0.0508047\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.0463920496404171, rmse:0.21538813412189484, mae:0.1513613760471344, rse:0.7448421716690063\n",
      "Intermediate time for GB and pred_len 96: 00h:02m:50.20s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0311655\n",
      "\tspeed: 0.0808s/iter; left time: 1811.0991s\n",
      "\titers: 200, epoch: 1 | loss: 0.0268520\n",
      "\tspeed: 0.0476s/iter; left time: 1062.3274s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 225 | Train Loss: 0.0315050 Vali Loss: 0.0344559 Test Loss: 0.0494408\n",
      "Validation loss decreased (inf --> 0.034456).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0232563\n",
      "\tspeed: 0.1223s/iter; left time: 2712.4060s\n",
      "\titers: 200, epoch: 2 | loss: 0.0201319\n",
      "\tspeed: 0.0510s/iter; left time: 1125.2679s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0228723 Vali Loss: 0.0356305 Test Loss: 0.0490330\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0164316\n",
      "\tspeed: 0.0927s/iter; left time: 2035.5572s\n",
      "\titers: 200, epoch: 3 | loss: 0.0143577\n",
      "\tspeed: 0.0530s/iter; left time: 1157.0310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.93s\n",
      "Steps: 225 | Train Loss: 0.0164182 Vali Loss: 0.0385980 Test Loss: 0.0549292\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0119379\n",
      "\tspeed: 0.1062s/iter; left time: 2306.9108s\n",
      "\titers: 200, epoch: 4 | loss: 0.0117706\n",
      "\tspeed: 0.0515s/iter; left time: 1113.7389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.76s\n",
      "Steps: 225 | Train Loss: 0.0123429 Vali Loss: 0.0391280 Test Loss: 0.0544823\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0101041\n",
      "\tspeed: 0.0958s/iter; left time: 2059.3053s\n",
      "\titers: 200, epoch: 5 | loss: 0.0096695\n",
      "\tspeed: 0.0529s/iter; left time: 1132.9288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.33s\n",
      "Steps: 225 | Train Loss: 0.0098903 Vali Loss: 0.0375995 Test Loss: 0.0534631\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0084312\n",
      "\tspeed: 0.1035s/iter; left time: 2202.9476s\n",
      "\titers: 200, epoch: 6 | loss: 0.0077957\n",
      "\tspeed: 0.0535s/iter; left time: 1132.6132s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:12.15s\n",
      "Steps: 225 | Train Loss: 0.0083932 Vali Loss: 0.0387505 Test Loss: 0.0536841\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0073348\n",
      "\tspeed: 0.0960s/iter; left time: 2019.8842s\n",
      "\titers: 200, epoch: 7 | loss: 0.0067890\n",
      "\tspeed: 0.0512s/iter; left time: 1072.1064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.94s\n",
      "Steps: 225 | Train Loss: 0.0073746 Vali Loss: 0.0379722 Test Loss: 0.0532853\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0062798\n",
      "\tspeed: 0.0980s/iter; left time: 2040.6019s\n",
      "\titers: 200, epoch: 8 | loss: 0.0067243\n",
      "\tspeed: 0.0510s/iter; left time: 1057.5884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.74s\n",
      "Steps: 225 | Train Loss: 0.0065557 Vali Loss: 0.0379420 Test Loss: 0.0540116\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0063439\n",
      "\tspeed: 0.1052s/iter; left time: 2167.4040s\n",
      "\titers: 200, epoch: 9 | loss: 0.0058767\n",
      "\tspeed: 0.0527s/iter; left time: 1080.9459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.14s\n",
      "Steps: 225 | Train Loss: 0.0059587 Vali Loss: 0.0380186 Test Loss: 0.0534061\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0056824\n",
      "\tspeed: 0.0955s/iter; left time: 1945.2567s\n",
      "\titers: 200, epoch: 10 | loss: 0.0056616\n",
      "\tspeed: 0.0567s/iter; left time: 1149.8234s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.61s\n",
      "Steps: 225 | Train Loss: 0.0055202 Vali Loss: 0.0375020 Test Loss: 0.0525390\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0051364\n",
      "\tspeed: 0.1039s/iter; left time: 2093.7990s\n",
      "\titers: 200, epoch: 11 | loss: 0.0053312\n",
      "\tspeed: 0.0511s/iter; left time: 1024.1143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.96s\n",
      "Steps: 225 | Train Loss: 0.0051564 Vali Loss: 0.0378486 Test Loss: 0.0529896\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04944076016545296, rmse:0.2223527878522873, mae:0.15743298828601837, rse:0.7709290385246277\n",
      "Intermediate time for GB and pred_len 168: 00h:03m:02.16s\n",
      "Intermediate time for GB: 00h:08m:54.64s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0224124\n",
      "\tspeed: 0.0455s/iter; left time: 1024.1005s\n",
      "\titers: 200, epoch: 1 | loss: 0.0196115\n",
      "\tspeed: 0.0171s/iter; left time: 382.8865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0269212 Vali Loss: 0.0150415 Test Loss: 0.0203113\n",
      "Validation loss decreased (inf --> 0.015041).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0107510\n",
      "\tspeed: 0.0480s/iter; left time: 1068.4218s\n",
      "\titers: 200, epoch: 2 | loss: 0.0087262\n",
      "\tspeed: 0.0191s/iter; left time: 423.5362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0108435 Vali Loss: 0.0091117 Test Loss: 0.0119008\n",
      "Validation loss decreased (0.015041 --> 0.009112).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0088294\n",
      "\tspeed: 0.0500s/iter; left time: 1102.0087s\n",
      "\titers: 200, epoch: 3 | loss: 0.0100748\n",
      "\tspeed: 0.0236s/iter; left time: 518.3261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0088790 Vali Loss: 0.0088156 Test Loss: 0.0120870\n",
      "Validation loss decreased (0.009112 --> 0.008816).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0087583\n",
      "\tspeed: 0.0492s/iter; left time: 1073.7082s\n",
      "\titers: 200, epoch: 4 | loss: 0.0078426\n",
      "\tspeed: 0.0201s/iter; left time: 437.1992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.78s\n",
      "Steps: 226 | Train Loss: 0.0082961 Vali Loss: 0.0084341 Test Loss: 0.0115712\n",
      "Validation loss decreased (0.008816 --> 0.008434).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0082415\n",
      "\tspeed: 0.0533s/iter; left time: 1151.3912s\n",
      "\titers: 200, epoch: 5 | loss: 0.0077887\n",
      "\tspeed: 0.0164s/iter; left time: 352.8795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0077767 Vali Loss: 0.0086849 Test Loss: 0.0117333\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0078185\n",
      "\tspeed: 0.0463s/iter; left time: 988.9833s\n",
      "\titers: 200, epoch: 6 | loss: 0.0070359\n",
      "\tspeed: 0.0164s/iter; left time: 348.8947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.35s\n",
      "Steps: 226 | Train Loss: 0.0073781 Vali Loss: 0.0087118 Test Loss: 0.0117897\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0074897\n",
      "\tspeed: 0.0465s/iter; left time: 982.5427s\n",
      "\titers: 200, epoch: 7 | loss: 0.0068850\n",
      "\tspeed: 0.0157s/iter; left time: 331.0021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.03s\n",
      "Steps: 226 | Train Loss: 0.0069519 Vali Loss: 0.0086680 Test Loss: 0.0120939\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0069666\n",
      "\tspeed: 0.0477s/iter; left time: 997.2415s\n",
      "\titers: 200, epoch: 8 | loss: 0.0063023\n",
      "\tspeed: 0.0164s/iter; left time: 341.8813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.04s\n",
      "Steps: 226 | Train Loss: 0.0065128 Vali Loss: 0.0088269 Test Loss: 0.0124773\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0064918\n",
      "\tspeed: 0.0451s/iter; left time: 933.0429s\n",
      "\titers: 200, epoch: 9 | loss: 0.0061917\n",
      "\tspeed: 0.0164s/iter; left time: 337.4504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 226 | Train Loss: 0.0061017 Vali Loss: 0.0089658 Test Loss: 0.0123164\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0054921\n",
      "\tspeed: 0.0469s/iter; left time: 959.8101s\n",
      "\titers: 200, epoch: 10 | loss: 0.0062819\n",
      "\tspeed: 0.0219s/iter; left time: 445.3181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 226 | Train Loss: 0.0057397 Vali Loss: 0.0093366 Test Loss: 0.0131734\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0056405\n",
      "\tspeed: 0.0430s/iter; left time: 870.6743s\n",
      "\titers: 200, epoch: 11 | loss: 0.0046495\n",
      "\tspeed: 0.0235s/iter; left time: 472.7532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.80s\n",
      "Steps: 226 | Train Loss: 0.0054258 Vali Loss: 0.0093886 Test Loss: 0.0132575\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0050808\n",
      "\tspeed: 0.0436s/iter; left time: 871.9838s\n",
      "\titers: 200, epoch: 12 | loss: 0.0050846\n",
      "\tspeed: 0.0249s/iter; left time: 495.5559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 226 | Train Loss: 0.0051052 Vali Loss: 0.0094989 Test Loss: 0.0135827\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0045714\n",
      "\tspeed: 0.0455s/iter; left time: 900.1137s\n",
      "\titers: 200, epoch: 13 | loss: 0.0046720\n",
      "\tspeed: 0.0236s/iter; left time: 464.5946s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.50s\n",
      "Steps: 226 | Train Loss: 0.0048812 Vali Loss: 0.0094084 Test Loss: 0.0140043\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0046412\n",
      "\tspeed: 0.0451s/iter; left time: 881.5763s\n",
      "\titers: 200, epoch: 14 | loss: 0.0043698\n",
      "\tspeed: 0.0204s/iter; left time: 396.6475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 226 | Train Loss: 0.0046494 Vali Loss: 0.0096403 Test Loss: 0.0141297\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01157116424292326, rmse:0.10756934434175491, mae:0.06930112838745117, rse:0.31656357645988464\n",
      "Intermediate time for ES and pred_len 24: 00h:01m:44.69s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0274620\n",
      "\tspeed: 0.0403s/iter; left time: 903.6093s\n",
      "\titers: 200, epoch: 1 | loss: 0.0233340\n",
      "\tspeed: 0.0168s/iter; left time: 373.9195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.16s\n",
      "Steps: 225 | Train Loss: 0.0311444 Vali Loss: 0.0205581 Test Loss: 0.0278780\n",
      "Validation loss decreased (inf --> 0.020558).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0156541\n",
      "\tspeed: 0.0540s/iter; left time: 1198.5014s\n",
      "\titers: 200, epoch: 2 | loss: 0.0156230\n",
      "\tspeed: 0.0168s/iter; left time: 371.3417s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.84s\n",
      "Steps: 225 | Train Loss: 0.0170196 Vali Loss: 0.0159781 Test Loss: 0.0208035\n",
      "Validation loss decreased (0.020558 --> 0.015978).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0142626\n",
      "\tspeed: 0.0513s/iter; left time: 1126.3920s\n",
      "\titers: 200, epoch: 3 | loss: 0.0143844\n",
      "\tspeed: 0.0165s/iter; left time: 360.5415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 225 | Train Loss: 0.0147565 Vali Loss: 0.0153022 Test Loss: 0.0208395\n",
      "Validation loss decreased (0.015978 --> 0.015302).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0129233\n",
      "\tspeed: 0.0488s/iter; left time: 1059.6288s\n",
      "\titers: 200, epoch: 4 | loss: 0.0128198\n",
      "\tspeed: 0.0164s/iter; left time: 354.0143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 225 | Train Loss: 0.0131599 Vali Loss: 0.0156655 Test Loss: 0.0221003\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0118474\n",
      "\tspeed: 0.0515s/iter; left time: 1107.4777s\n",
      "\titers: 200, epoch: 5 | loss: 0.0098400\n",
      "\tspeed: 0.0174s/iter; left time: 372.8807s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 225 | Train Loss: 0.0113209 Vali Loss: 0.0163465 Test Loss: 0.0240946\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0092215\n",
      "\tspeed: 0.0470s/iter; left time: 999.8698s\n",
      "\titers: 200, epoch: 6 | loss: 0.0096176\n",
      "\tspeed: 0.0162s/iter; left time: 342.3958s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0098619 Vali Loss: 0.0163042 Test Loss: 0.0241483\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0097937\n",
      "\tspeed: 0.0499s/iter; left time: 1050.0762s\n",
      "\titers: 200, epoch: 7 | loss: 0.0080033\n",
      "\tspeed: 0.0164s/iter; left time: 343.5197s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.32s\n",
      "Steps: 225 | Train Loss: 0.0087896 Vali Loss: 0.0167270 Test Loss: 0.0244736\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0076690\n",
      "\tspeed: 0.0485s/iter; left time: 1010.0910s\n",
      "\titers: 200, epoch: 8 | loss: 0.0073171\n",
      "\tspeed: 0.0171s/iter; left time: 355.4400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0080117 Vali Loss: 0.0169650 Test Loss: 0.0243798\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0070846\n",
      "\tspeed: 0.0461s/iter; left time: 950.3532s\n",
      "\titers: 200, epoch: 9 | loss: 0.0075421\n",
      "\tspeed: 0.0170s/iter; left time: 349.1597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.58s\n",
      "Steps: 225 | Train Loss: 0.0073967 Vali Loss: 0.0172498 Test Loss: 0.0247350\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0073015\n",
      "\tspeed: 0.0469s/iter; left time: 955.3955s\n",
      "\titers: 200, epoch: 10 | loss: 0.0065840\n",
      "\tspeed: 0.0163s/iter; left time: 330.6696s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.68s\n",
      "Steps: 225 | Train Loss: 0.0069268 Vali Loss: 0.0177244 Test Loss: 0.0248713\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0064723\n",
      "\tspeed: 0.0462s/iter; left time: 931.3291s\n",
      "\titers: 200, epoch: 11 | loss: 0.0066135\n",
      "\tspeed: 0.0164s/iter; left time: 329.7885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.82s\n",
      "Steps: 225 | Train Loss: 0.0065312 Vali Loss: 0.0173557 Test Loss: 0.0249669\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0063455\n",
      "\tspeed: 0.0480s/iter; left time: 956.3885s\n",
      "\titers: 200, epoch: 12 | loss: 0.0061154\n",
      "\tspeed: 0.0162s/iter; left time: 321.4485s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0062166 Vali Loss: 0.0179732 Test Loss: 0.0252495\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0060675\n",
      "\tspeed: 0.0479s/iter; left time: 943.5563s\n",
      "\titers: 200, epoch: 13 | loss: 0.0061311\n",
      "\tspeed: 0.0177s/iter; left time: 346.6826s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.90s\n",
      "Steps: 225 | Train Loss: 0.0059431 Vali Loss: 0.0178869 Test Loss: 0.0250740\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020839489996433258, rmse:0.14435888826847076, mae:0.09703951328992844, rse:0.42408299446105957\n",
      "Intermediate time for ES and pred_len 96: 00h:01m:33.04s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0277074\n",
      "\tspeed: 0.0478s/iter; left time: 1070.9026s\n",
      "\titers: 200, epoch: 1 | loss: 0.0247151\n",
      "\tspeed: 0.0247s/iter; left time: 551.4464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 225 | Train Loss: 0.0327857 Vali Loss: 0.0228405 Test Loss: 0.0301658\n",
      "Validation loss decreased (inf --> 0.022841).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0182268\n",
      "\tspeed: 0.0600s/iter; left time: 1330.2683s\n",
      "\titers: 200, epoch: 2 | loss: 0.0183971\n",
      "\tspeed: 0.0170s/iter; left time: 375.2161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 225 | Train Loss: 0.0186799 Vali Loss: 0.0179325 Test Loss: 0.0228984\n",
      "Validation loss decreased (0.022841 --> 0.017933).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0152040\n",
      "\tspeed: 0.0564s/iter; left time: 1237.5159s\n",
      "\titers: 200, epoch: 3 | loss: 0.0145845\n",
      "\tspeed: 0.0168s/iter; left time: 367.2814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.89s\n",
      "Steps: 225 | Train Loss: 0.0160117 Vali Loss: 0.0180276 Test Loss: 0.0232034\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0137587\n",
      "\tspeed: 0.0492s/iter; left time: 1069.3646s\n",
      "\titers: 200, epoch: 4 | loss: 0.0131766\n",
      "\tspeed: 0.0162s/iter; left time: 350.1691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0140758 Vali Loss: 0.0176767 Test Loss: 0.0228745\n",
      "Validation loss decreased (0.017933 --> 0.017677).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0129491\n",
      "\tspeed: 0.0508s/iter; left time: 1091.4610s\n",
      "\titers: 200, epoch: 5 | loss: 0.0117705\n",
      "\tspeed: 0.0172s/iter; left time: 368.1131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0123849 Vali Loss: 0.0186744 Test Loss: 0.0246194\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0110090\n",
      "\tspeed: 0.0491s/iter; left time: 1044.3217s\n",
      "\titers: 200, epoch: 6 | loss: 0.0100024\n",
      "\tspeed: 0.0163s/iter; left time: 345.8416s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.60s\n",
      "Steps: 225 | Train Loss: 0.0109879 Vali Loss: 0.0187169 Test Loss: 0.0243321\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0105492\n",
      "\tspeed: 0.0516s/iter; left time: 1087.0034s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092284\n",
      "\tspeed: 0.0179s/iter; left time: 375.0773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 225 | Train Loss: 0.0099143 Vali Loss: 0.0193814 Test Loss: 0.0243870\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0092586\n",
      "\tspeed: 0.0525s/iter; left time: 1094.0169s\n",
      "\titers: 200, epoch: 8 | loss: 0.0089153\n",
      "\tspeed: 0.0168s/iter; left time: 349.1782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 225 | Train Loss: 0.0090489 Vali Loss: 0.0190883 Test Loss: 0.0252209\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0082056\n",
      "\tspeed: 0.0545s/iter; left time: 1123.7317s\n",
      "\titers: 200, epoch: 9 | loss: 0.0080735\n",
      "\tspeed: 0.0174s/iter; left time: 356.2632s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 225 | Train Loss: 0.0083603 Vali Loss: 0.0194013 Test Loss: 0.0252273\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0082202\n",
      "\tspeed: 0.0549s/iter; left time: 1119.6085s\n",
      "\titers: 200, epoch: 10 | loss: 0.0073720\n",
      "\tspeed: 0.0169s/iter; left time: 343.1842s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.23s\n",
      "Steps: 225 | Train Loss: 0.0078443 Vali Loss: 0.0196141 Test Loss: 0.0257372\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0069887\n",
      "\tspeed: 0.0570s/iter; left time: 1148.6955s\n",
      "\titers: 200, epoch: 11 | loss: 0.0071465\n",
      "\tspeed: 0.0172s/iter; left time: 343.9165s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.67s\n",
      "Steps: 225 | Train Loss: 0.0073931 Vali Loss: 0.0199717 Test Loss: 0.0256369\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0070513\n",
      "\tspeed: 0.0495s/iter; left time: 986.7180s\n",
      "\titers: 200, epoch: 12 | loss: 0.0067190\n",
      "\tspeed: 0.0182s/iter; left time: 360.8075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.45s\n",
      "Steps: 225 | Train Loss: 0.0070412 Vali Loss: 0.0200850 Test Loss: 0.0260791\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0065328\n",
      "\tspeed: 0.0513s/iter; left time: 1010.1836s\n",
      "\titers: 200, epoch: 13 | loss: 0.0064198\n",
      "\tspeed: 0.0168s/iter; left time: 329.5841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.41s\n",
      "Steps: 225 | Train Loss: 0.0067512 Vali Loss: 0.0202014 Test Loss: 0.0263437\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0063799\n",
      "\tspeed: 0.0555s/iter; left time: 1080.7838s\n",
      "\titers: 200, epoch: 14 | loss: 0.0066022\n",
      "\tspeed: 0.0211s/iter; left time: 408.7836s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 225 | Train Loss: 0.0064871 Vali Loss: 0.0201594 Test Loss: 0.0262867\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02287455089390278, rmse:0.15124334394931793, mae:0.10218845307826996, rse:0.4443393349647522\n",
      "Intermediate time for ES and pred_len 168: 00h:01m:52.09s\n",
      "Intermediate time for ES: 00h:05m:09.83s\n",
      "Total time: 00h:23m:13.02s\n"
     ]
    }
   ],
   "source": [
    "countries = ['DE', 'GB', 'ES']\n",
    "num_cols = [5, 5, 3]\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_168.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            seq_len=168\n",
    "\n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0443</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.2088</td>\n",
       "      <td>0.1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.1574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0234  0.1529  0.1006\n",
       "        96        0.0443  0.2104  0.1429\n",
       "        168       0.0436  0.2088  0.1491\n",
       "ES      24        0.0116  0.1076  0.0693\n",
       "        96        0.0208  0.1444  0.0970\n",
       "        168       0.0229  0.1512  0.1022\n",
       "GB      24        0.0266  0.1630  0.1094\n",
       "        96        0.0464  0.2154  0.1514\n",
       "        168       0.0494  0.2224  0.1574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_168.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_24_ES_PatchTST_custom_ftM_sl96_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0267922\n",
      "\tspeed: 0.0439s/iter; left time: 987.8115s\n",
      "\titers: 200, epoch: 1 | loss: 0.0195768\n",
      "\tspeed: 0.0163s/iter; left time: 364.5006s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.59s\n",
      "Steps: 226 | Train Loss: 0.0295997 Vali Loss: 0.0166912 Test Loss: 0.0213410\n",
      "Validation loss decreased (inf --> 0.016691).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0122195\n",
      "\tspeed: 0.0459s/iter; left time: 1023.3861s\n",
      "\titers: 200, epoch: 2 | loss: 0.0109789\n",
      "\tspeed: 0.0194s/iter; left time: 430.6559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0117965 Vali Loss: 0.0094065 Test Loss: 0.0123087\n",
      "Validation loss decreased (0.016691 --> 0.009406).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0093296\n",
      "\tspeed: 0.0429s/iter; left time: 946.4987s\n",
      "\titers: 200, epoch: 3 | loss: 0.0094486\n",
      "\tspeed: 0.0200s/iter; left time: 439.9887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0094586 Vali Loss: 0.0088756 Test Loss: 0.0117917\n",
      "Validation loss decreased (0.009406 --> 0.008876).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0093523\n",
      "\tspeed: 0.0430s/iter; left time: 937.8723s\n",
      "\titers: 200, epoch: 4 | loss: 0.0088712\n",
      "\tspeed: 0.0234s/iter; left time: 508.1298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0088098 Vali Loss: 0.0086944 Test Loss: 0.0116282\n",
      "Validation loss decreased (0.008876 --> 0.008694).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0086567\n",
      "\tspeed: 0.0417s/iter; left time: 900.3805s\n",
      "\titers: 200, epoch: 5 | loss: 0.0069733\n",
      "\tspeed: 0.0227s/iter; left time: 486.9472s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.51s\n",
      "Steps: 226 | Train Loss: 0.0083599 Vali Loss: 0.0086526 Test Loss: 0.0115494\n",
      "Validation loss decreased (0.008694 --> 0.008653).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0081536\n",
      "\tspeed: 0.0370s/iter; left time: 790.9505s\n",
      "\titers: 200, epoch: 6 | loss: 0.0069282\n",
      "\tspeed: 0.0229s/iter; left time: 487.4311s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.51s\n",
      "Steps: 226 | Train Loss: 0.0080306 Vali Loss: 0.0084352 Test Loss: 0.0112584\n",
      "Validation loss decreased (0.008653 --> 0.008435).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0076914\n",
      "\tspeed: 0.0388s/iter; left time: 819.5598s\n",
      "\titers: 200, epoch: 7 | loss: 0.0077376\n",
      "\tspeed: 0.0221s/iter; left time: 466.1116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 226 | Train Loss: 0.0077208 Vali Loss: 0.0084026 Test Loss: 0.0113544\n",
      "Validation loss decreased (0.008435 --> 0.008403).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0075021\n",
      "\tspeed: 0.0387s/iter; left time: 808.9591s\n",
      "\titers: 200, epoch: 8 | loss: 0.0080163\n",
      "\tspeed: 0.0236s/iter; left time: 491.3997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 226 | Train Loss: 0.0074480 Vali Loss: 0.0085301 Test Loss: 0.0116779\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0070147\n",
      "\tspeed: 0.0357s/iter; left time: 738.4469s\n",
      "\titers: 200, epoch: 9 | loss: 0.0072123\n",
      "\tspeed: 0.0222s/iter; left time: 456.5276s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 226 | Train Loss: 0.0071865 Vali Loss: 0.0083577 Test Loss: 0.0115945\n",
      "Validation loss decreased (0.008403 --> 0.008358).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0069964\n",
      "\tspeed: 0.0389s/iter; left time: 796.8201s\n",
      "\titers: 200, epoch: 10 | loss: 0.0068249\n",
      "\tspeed: 0.0227s/iter; left time: 462.1933s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 226 | Train Loss: 0.0069206 Vali Loss: 0.0087676 Test Loss: 0.0121050\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0071934\n",
      "\tspeed: 0.0353s/iter; left time: 714.0918s\n",
      "\titers: 200, epoch: 11 | loss: 0.0065778\n",
      "\tspeed: 0.0245s/iter; left time: 493.0897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.67s\n",
      "Steps: 226 | Train Loss: 0.0066644 Vali Loss: 0.0085964 Test Loss: 0.0120603\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0072954\n",
      "\tspeed: 0.0358s/iter; left time: 717.4733s\n",
      "\titers: 200, epoch: 12 | loss: 0.0067187\n",
      "\tspeed: 0.0238s/iter; left time: 473.7325s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 226 | Train Loss: 0.0064917 Vali Loss: 0.0086457 Test Loss: 0.0120428\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0058575\n",
      "\tspeed: 0.0363s/iter; left time: 717.7930s\n",
      "\titers: 200, epoch: 13 | loss: 0.0061792\n",
      "\tspeed: 0.0240s/iter; left time: 473.4328s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 226 | Train Loss: 0.0062382 Vali Loss: 0.0085987 Test Loss: 0.0123390\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0060048\n",
      "\tspeed: 0.0364s/iter; left time: 712.2198s\n",
      "\titers: 200, epoch: 14 | loss: 0.0065187\n",
      "\tspeed: 0.0227s/iter; left time: 441.4586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0060652 Vali Loss: 0.0087850 Test Loss: 0.0122426\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0059778\n",
      "\tspeed: 0.0344s/iter; left time: 666.0888s\n",
      "\titers: 200, epoch: 15 | loss: 0.0059392\n",
      "\tspeed: 0.0227s/iter; left time: 435.9288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 226 | Train Loss: 0.0058822 Vali Loss: 0.0089802 Test Loss: 0.0124355\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0056377\n",
      "\tspeed: 0.0376s/iter; left time: 718.4307s\n",
      "\titers: 200, epoch: 16 | loss: 0.0053625\n",
      "\tspeed: 0.0203s/iter; left time: 385.9553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:04.74s\n",
      "Steps: 226 | Train Loss: 0.0057311 Vali Loss: 0.0089195 Test Loss: 0.0126711\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0050651\n",
      "\tspeed: 0.0386s/iter; left time: 729.7615s\n",
      "\titers: 200, epoch: 17 | loss: 0.0050484\n",
      "\tspeed: 0.0187s/iter; left time: 351.8201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 226 | Train Loss: 0.0055950 Vali Loss: 0.0089465 Test Loss: 0.0125136\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0048361\n",
      "\tspeed: 0.0381s/iter; left time: 711.0792s\n",
      "\titers: 200, epoch: 18 | loss: 0.0054747\n",
      "\tspeed: 0.0163s/iter; left time: 302.5829s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:04.17s\n",
      "Steps: 226 | Train Loss: 0.0054673 Vali Loss: 0.0090529 Test Loss: 0.0126415\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0062222\n",
      "\tspeed: 0.0500s/iter; left time: 921.1105s\n",
      "\titers: 200, epoch: 19 | loss: 0.0052610\n",
      "\tspeed: 0.0245s/iter; left time: 448.9730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 226 | Train Loss: 0.0053649 Vali Loss: 0.0089901 Test Loss: 0.0127423\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_24_ES_PatchTST_custom_ftM_sl96_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011594521813094616, rmse:0.10767786204814911, mae:0.06868013739585876, rse:0.3168829381465912\n",
      "Intermediate time for ES and pred_len 24: 00h:02m:06.00s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_96_ES_PatchTST_custom_ftM_sl96_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0343245\n",
      "\tspeed: 0.0541s/iter; left time: 1218.3482s\n",
      "\titers: 200, epoch: 1 | loss: 0.0278460\n",
      "\tspeed: 0.0238s/iter; left time: 532.3629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 226 | Train Loss: 0.0365318 Vali Loss: 0.0239262 Test Loss: 0.0309951\n",
      "Validation loss decreased (inf --> 0.023926).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0185223\n",
      "\tspeed: 0.0487s/iter; left time: 1085.7438s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166251\n",
      "\tspeed: 0.0245s/iter; left time: 542.2201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.91s\n",
      "Steps: 226 | Train Loss: 0.0192041 Vali Loss: 0.0168355 Test Loss: 0.0217300\n",
      "Validation loss decreased (0.023926 --> 0.016835).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0160262\n",
      "\tspeed: 0.0466s/iter; left time: 1028.2645s\n",
      "\titers: 200, epoch: 3 | loss: 0.0156437\n",
      "\tspeed: 0.0268s/iter; left time: 588.3309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 226 | Train Loss: 0.0163757 Vali Loss: 0.0161367 Test Loss: 0.0212674\n",
      "Validation loss decreased (0.016835 --> 0.016137).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0143373\n",
      "\tspeed: 0.0502s/iter; left time: 1095.8093s\n",
      "\titers: 200, epoch: 4 | loss: 0.0144804\n",
      "\tspeed: 0.0275s/iter; left time: 596.7106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0152966 Vali Loss: 0.0164544 Test Loss: 0.0214458\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0147964\n",
      "\tspeed: 0.0493s/iter; left time: 1065.0733s\n",
      "\titers: 200, epoch: 5 | loss: 0.0139811\n",
      "\tspeed: 0.0281s/iter; left time: 604.9082s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 226 | Train Loss: 0.0141186 Vali Loss: 0.0169926 Test Loss: 0.0220698\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0130579\n",
      "\tspeed: 0.0502s/iter; left time: 1071.9956s\n",
      "\titers: 200, epoch: 6 | loss: 0.0136196\n",
      "\tspeed: 0.0255s/iter; left time: 541.5579s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0131513 Vali Loss: 0.0174595 Test Loss: 0.0227895\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0127381\n",
      "\tspeed: 0.0504s/iter; left time: 1065.9463s\n",
      "\titers: 200, epoch: 7 | loss: 0.0121172\n",
      "\tspeed: 0.0274s/iter; left time: 577.3366s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0122806 Vali Loss: 0.0182827 Test Loss: 0.0238469\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0118271\n",
      "\tspeed: 0.0505s/iter; left time: 1057.4361s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116580\n",
      "\tspeed: 0.0279s/iter; left time: 580.2713s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.30s\n",
      "Steps: 226 | Train Loss: 0.0115656 Vali Loss: 0.0178909 Test Loss: 0.0232985\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0110704\n",
      "\tspeed: 0.0488s/iter; left time: 1010.4045s\n",
      "\titers: 200, epoch: 9 | loss: 0.0109119\n",
      "\tspeed: 0.0255s/iter; left time: 524.4640s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.38s\n",
      "Steps: 226 | Train Loss: 0.0109450 Vali Loss: 0.0181275 Test Loss: 0.0236892\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0099459\n",
      "\tspeed: 0.0482s/iter; left time: 985.9528s\n",
      "\titers: 200, epoch: 10 | loss: 0.0107087\n",
      "\tspeed: 0.0228s/iter; left time: 465.0159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 226 | Train Loss: 0.0104373 Vali Loss: 0.0185834 Test Loss: 0.0238976\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0096758\n",
      "\tspeed: 0.0480s/iter; left time: 971.7841s\n",
      "\titers: 200, epoch: 11 | loss: 0.0101523\n",
      "\tspeed: 0.0268s/iter; left time: 540.6212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 226 | Train Loss: 0.0100447 Vali Loss: 0.0185717 Test Loss: 0.0243262\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0096044\n",
      "\tspeed: 0.0490s/iter; left time: 981.5633s\n",
      "\titers: 200, epoch: 12 | loss: 0.0093817\n",
      "\tspeed: 0.0264s/iter; left time: 524.7612s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 226 | Train Loss: 0.0096178 Vali Loss: 0.0189246 Test Loss: 0.0249068\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0095338\n",
      "\tspeed: 0.0471s/iter; left time: 932.7408s\n",
      "\titers: 200, epoch: 13 | loss: 0.0096964\n",
      "\tspeed: 0.0236s/iter; left time: 465.1052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.77s\n",
      "Steps: 226 | Train Loss: 0.0093259 Vali Loss: 0.0187492 Test Loss: 0.0248805\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_96_ES_PatchTST_custom_ftM_sl96_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.02126738242805004, rmse:0.14583340287208557, mae:0.09827333688735962, rse:0.4284146726131439\n",
      "Intermediate time for ES and pred_len 96: 00h:01m:48.87s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_168_ES_PatchTST_custom_ftM_sl96_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0353431\n",
      "\tspeed: 0.0452s/iter; left time: 1013.0117s\n",
      "\titers: 200, epoch: 1 | loss: 0.0268961\n",
      "\tspeed: 0.0226s/iter; left time: 504.3959s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 225 | Train Loss: 0.0376616 Vali Loss: 0.0256392 Test Loss: 0.0331292\n",
      "Validation loss decreased (inf --> 0.025639).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0218692\n",
      "\tspeed: 0.0434s/iter; left time: 963.5274s\n",
      "\titers: 200, epoch: 2 | loss: 0.0194052\n",
      "\tspeed: 0.0259s/iter; left time: 571.7841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.80s\n",
      "Steps: 225 | Train Loss: 0.0208068 Vali Loss: 0.0188844 Test Loss: 0.0242439\n",
      "Validation loss decreased (0.025639 --> 0.018884).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0169687\n",
      "\tspeed: 0.0490s/iter; left time: 1076.1243s\n",
      "\titers: 200, epoch: 3 | loss: 0.0190523\n",
      "\tspeed: 0.0219s/iter; left time: 479.3272s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.46s\n",
      "Steps: 225 | Train Loss: 0.0179892 Vali Loss: 0.0185537 Test Loss: 0.0235893\n",
      "Validation loss decreased (0.018884 --> 0.018554).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0169186\n",
      "\tspeed: 0.0498s/iter; left time: 1082.8328s\n",
      "\titers: 200, epoch: 4 | loss: 0.0175947\n",
      "\tspeed: 0.0211s/iter; left time: 455.6899s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 225 | Train Loss: 0.0167937 Vali Loss: 0.0184703 Test Loss: 0.0235688\n",
      "Validation loss decreased (0.018554 --> 0.018470).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0160075\n",
      "\tspeed: 0.0458s/iter; left time: 984.4546s\n",
      "\titers: 200, epoch: 5 | loss: 0.0160217\n",
      "\tspeed: 0.0264s/iter; left time: 564.3883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.76s\n",
      "Steps: 225 | Train Loss: 0.0157294 Vali Loss: 0.0188542 Test Loss: 0.0238975\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0145555\n",
      "\tspeed: 0.0511s/iter; left time: 1086.5256s\n",
      "\titers: 200, epoch: 6 | loss: 0.0143680\n",
      "\tspeed: 0.0217s/iter; left time: 458.8588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0147236 Vali Loss: 0.0194211 Test Loss: 0.0247843\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0145234\n",
      "\tspeed: 0.0481s/iter; left time: 1011.5290s\n",
      "\titers: 200, epoch: 7 | loss: 0.0138711\n",
      "\tspeed: 0.0291s/iter; left time: 609.5635s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.57s\n",
      "Steps: 225 | Train Loss: 0.0138673 Vali Loss: 0.0195423 Test Loss: 0.0247844\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0137212\n",
      "\tspeed: 0.0466s/iter; left time: 971.0173s\n",
      "\titers: 200, epoch: 8 | loss: 0.0138608\n",
      "\tspeed: 0.0252s/iter; left time: 521.6015s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 225 | Train Loss: 0.0131192 Vali Loss: 0.0199592 Test Loss: 0.0254863\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0122447\n",
      "\tspeed: 0.0462s/iter; left time: 951.0470s\n",
      "\titers: 200, epoch: 9 | loss: 0.0126505\n",
      "\tspeed: 0.0252s/iter; left time: 516.4415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 225 | Train Loss: 0.0124965 Vali Loss: 0.0200359 Test Loss: 0.0256357\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0119075\n",
      "\tspeed: 0.0474s/iter; left time: 966.0763s\n",
      "\titers: 200, epoch: 10 | loss: 0.0112254\n",
      "\tspeed: 0.0265s/iter; left time: 537.4212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 225 | Train Loss: 0.0119626 Vali Loss: 0.0205519 Test Loss: 0.0257522\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0117490\n",
      "\tspeed: 0.0452s/iter; left time: 910.8598s\n",
      "\titers: 200, epoch: 11 | loss: 0.0122202\n",
      "\tspeed: 0.0285s/iter; left time: 571.8669s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0114586 Vali Loss: 0.0203659 Test Loss: 0.0264138\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0113394\n",
      "\tspeed: 0.0491s/iter; left time: 977.8944s\n",
      "\titers: 200, epoch: 12 | loss: 0.0116394\n",
      "\tspeed: 0.0282s/iter; left time: 558.4294s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0110902 Vali Loss: 0.0204172 Test Loss: 0.0262498\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0106933\n",
      "\tspeed: 0.0504s/iter; left time: 993.6019s\n",
      "\titers: 200, epoch: 13 | loss: 0.0108677\n",
      "\tspeed: 0.0267s/iter; left time: 523.3469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.36s\n",
      "Steps: 225 | Train Loss: 0.0107315 Vali Loss: 0.0206860 Test Loss: 0.0266960\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0113744\n",
      "\tspeed: 0.0525s/iter; left time: 1023.3682s\n",
      "\titers: 200, epoch: 14 | loss: 0.0098004\n",
      "\tspeed: 0.0235s/iter; left time: 455.5247s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 225 | Train Loss: 0.0104362 Vali Loss: 0.0206958 Test Loss: 0.0264268\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_168_ES_PatchTST_custom_ftM_sl96_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.023568790405988693, rmse:0.15352129936218262, mae:0.10385473817586899, rse:0.45103174448013306\n",
      "Intermediate time for ES and pred_len 168: 00h:01m:53.63s\n",
      "Intermediate time for ES: 00h:05m:48.49s\n",
      "Total time: 00h:05m:48.50s\n"
     ]
    }
   ],
   "source": [
    "countries = ['ES']\n",
    "num_cols = [3]\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_96.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            seq_len=96\n",
    "\n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.0687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.0983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>0.1039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "ES      24        0.0116  0.1077  0.0687\n",
       "        96        0.0213  0.1458  0.0983\n",
       "        168       0.0236  0.1535  0.1039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_96.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. No patching\n",
    "\n",
    "It runs more than 8 hours on 48GB GPU. Therefore I run it with portions. You can find full results in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0195191\n",
      "\tspeed: 0.4042s/iter; left time: 1770.9342s\n",
      "\titers: 200, epoch: 1 | loss: 0.0178518\n",
      "\tspeed: 0.3824s/iter; left time: 1637.2300s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:01m:25.82s\n",
      "Steps: 224 | Train Loss: 0.0215666 Vali Loss: 0.0120246 Test Loss: 0.0154649\n",
      "Validation loss decreased (inf --> 0.012025).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0110951\n",
      "\tspeed: 0.6152s/iter; left time: 2557.2214s\n",
      "\titers: 200, epoch: 2 | loss: 0.0089168\n",
      "\tspeed: 0.3846s/iter; left time: 1560.2988s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:01m:26.09s\n",
      "Steps: 224 | Train Loss: 0.0114667 Vali Loss: 0.0099340 Test Loss: 0.0124430\n",
      "Validation loss decreased (0.012025 --> 0.009934).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0094385\n",
      "\tspeed: 0.6159s/iter; left time: 2422.2452s\n",
      "\titers: 200, epoch: 3 | loss: 0.0097403\n",
      "\tspeed: 0.3849s/iter; left time: 1475.3067s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:01m:26.32s\n",
      "Steps: 224 | Train Loss: 0.0097346 Vali Loss: 0.0092674 Test Loss: 0.0117505\n",
      "Validation loss decreased (0.009934 --> 0.009267).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0102117\n",
      "\tspeed: 0.6164s/iter; left time: 2286.2182s\n",
      "\titers: 200, epoch: 4 | loss: 0.0096852\n",
      "\tspeed: 0.3853s/iter; left time: 1390.6134s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:01m:26.31s\n",
      "Steps: 224 | Train Loss: 0.0093477 Vali Loss: 0.0093860 Test Loss: 0.0117003\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0082508\n",
      "\tspeed: 0.6137s/iter; left time: 2138.6477s\n",
      "\titers: 200, epoch: 5 | loss: 0.0086889\n",
      "\tspeed: 0.3851s/iter; left time: 1303.6718s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:01m:26.20s\n",
      "Steps: 224 | Train Loss: 0.0089759 Vali Loss: 0.0091834 Test Loss: 0.0113511\n",
      "Validation loss decreased (0.009267 --> 0.009183).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0088070\n",
      "\tspeed: 0.6185s/iter; left time: 2017.0525s\n",
      "\titers: 200, epoch: 6 | loss: 0.0080614\n",
      "\tspeed: 0.3859s/iter; left time: 1219.7309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:01m:26.47s\n",
      "Steps: 224 | Train Loss: 0.0088026 Vali Loss: 0.0089738 Test Loss: 0.0113156\n",
      "Validation loss decreased (0.009183 --> 0.008974).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0104345\n",
      "\tspeed: 0.6165s/iter; left time: 1872.4005s\n",
      "\titers: 200, epoch: 7 | loss: 0.0085016\n",
      "\tspeed: 0.3847s/iter; left time: 1129.7651s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:01m:26.23s\n",
      "Steps: 224 | Train Loss: 0.0085589 Vali Loss: 0.0093118 Test Loss: 0.0118977\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0077006\n",
      "\tspeed: 0.6140s/iter; left time: 1727.0964s\n",
      "\titers: 200, epoch: 8 | loss: 0.0082233\n",
      "\tspeed: 0.3851s/iter; left time: 1044.7248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:01m:26.23s\n",
      "Steps: 224 | Train Loss: 0.0083394 Vali Loss: 0.0090055 Test Loss: 0.0112410\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0093437\n",
      "\tspeed: 0.6147s/iter; left time: 1591.5033s\n",
      "\titers: 200, epoch: 9 | loss: 0.0070935\n",
      "\tspeed: 0.3854s/iter; left time: 959.1466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:01m:26.32s\n",
      "Steps: 224 | Train Loss: 0.0082570 Vali Loss: 0.0089743 Test Loss: 0.0114231\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0086219\n",
      "\tspeed: 0.6158s/iter; left time: 1456.4752s\n",
      "\titers: 200, epoch: 10 | loss: 0.0070904\n",
      "\tspeed: 0.3851s/iter; left time: 872.1611s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:01m:26.29s\n",
      "Steps: 224 | Train Loss: 0.0081209 Vali Loss: 0.0091072 Test Loss: 0.0116036\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0070862\n",
      "\tspeed: 0.6136s/iter; left time: 1313.7240s\n",
      "\titers: 200, epoch: 11 | loss: 0.0075065\n",
      "\tspeed: 0.3846s/iter; left time: 784.9996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:01m:26.16s\n",
      "Steps: 224 | Train Loss: 0.0080325 Vali Loss: 0.0087324 Test Loss: 0.0110522\n",
      "Validation loss decreased (0.008974 --> 0.008732).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0087495\n",
      "\tspeed: 0.6165s/iter; left time: 1181.8267s\n",
      "\titers: 200, epoch: 12 | loss: 0.0090640\n",
      "\tspeed: 0.3854s/iter; left time: 700.3547s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:01m:26.38s\n",
      "Steps: 224 | Train Loss: 0.0078615 Vali Loss: 0.0089089 Test Loss: 0.0114960\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0075404\n",
      "\tspeed: 0.6154s/iter; left time: 1041.8771s\n",
      "\titers: 200, epoch: 13 | loss: 0.0070684\n",
      "\tspeed: 0.3857s/iter; left time: 614.4491s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:01m:26.43s\n",
      "Steps: 224 | Train Loss: 0.0078251 Vali Loss: 0.0085859 Test Loss: 0.0108104\n",
      "Validation loss decreased (0.008732 --> 0.008586).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0074663\n",
      "\tspeed: 0.6153s/iter; left time: 903.9059s\n",
      "\titers: 200, epoch: 14 | loss: 0.0073172\n",
      "\tspeed: 0.3853s/iter; left time: 527.5139s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:01m:26.29s\n",
      "Steps: 224 | Train Loss: 0.0077326 Vali Loss: 0.0087050 Test Loss: 0.0109574\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0067719\n",
      "\tspeed: 0.6149s/iter; left time: 765.5087s\n",
      "\titers: 200, epoch: 15 | loss: 0.0082704\n",
      "\tspeed: 0.3850s/iter; left time: 440.7763s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:01m:26.24s\n",
      "Steps: 224 | Train Loss: 0.0077182 Vali Loss: 0.0086332 Test Loss: 0.0109099\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0083347\n",
      "\tspeed: 0.6141s/iter; left time: 627.0389s\n",
      "\titers: 200, epoch: 16 | loss: 0.0074337\n",
      "\tspeed: 0.3847s/iter; left time: 354.3510s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:01m:26.24s\n",
      "Steps: 224 | Train Loss: 0.0076103 Vali Loss: 0.0084655 Test Loss: 0.0106939\n",
      "Validation loss decreased (0.008586 --> 0.008465).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0079523\n",
      "\tspeed: 0.6166s/iter; left time: 491.4670s\n",
      "\titers: 200, epoch: 17 | loss: 0.0069271\n",
      "\tspeed: 0.3854s/iter; left time: 268.5952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:01m:26.34s\n",
      "Steps: 224 | Train Loss: 0.0075618 Vali Loss: 0.0085241 Test Loss: 0.0107415\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0064623\n",
      "\tspeed: 0.6138s/iter; left time: 351.6913s\n",
      "\titers: 200, epoch: 18 | loss: 0.0071695\n",
      "\tspeed: 0.3850s/iter; left time: 182.1209s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:01m:26.23s\n",
      "Steps: 224 | Train Loss: 0.0075162 Vali Loss: 0.0084254 Test Loss: 0.0107176\n",
      "Validation loss decreased (0.008465 --> 0.008425).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0068357\n",
      "\tspeed: 0.6156s/iter; left time: 214.8378s\n",
      "\titers: 200, epoch: 19 | loss: 0.0076776\n",
      "\tspeed: 0.3851s/iter; left time: 95.8788s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:01m:26.35s\n",
      "Steps: 224 | Train Loss: 0.0074895 Vali Loss: 0.0083774 Test Loss: 0.0105521\n",
      "Validation loss decreased (0.008425 --> 0.008377).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0070090\n",
      "\tspeed: 0.6147s/iter; left time: 76.8408s\n",
      "\titers: 200, epoch: 20 | loss: 0.0075570\n",
      "\tspeed: 0.3853s/iter; left time: 9.6322s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:01m:26.32s\n",
      "Steps: 224 | Train Loss: 0.0074361 Vali Loss: 0.0085109 Test Loss: 0.0108376\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010552062653005123, rmse:0.10272323340177536, mae:0.06482216715812683, rse:0.3023020625114441\n",
      "Intermediate time for ES and pred_len 24: 00h:33m:36.49s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249440\n",
      "\tspeed: 0.4099s/iter; left time: 1795.8819s\n",
      "\titers: 200, epoch: 1 | loss: 0.0234317\n",
      "\tspeed: 0.3855s/iter; left time: 1650.5231s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:01m:26.63s\n",
      "Steps: 224 | Train Loss: 0.0261810 Vali Loss: 0.0183967 Test Loss: 0.0231636\n",
      "Validation loss decreased (inf --> 0.018397).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0182221\n",
      "\tspeed: 0.6192s/iter; left time: 2573.8588s\n",
      "\titers: 200, epoch: 2 | loss: 0.0175976\n",
      "\tspeed: 0.3862s/iter; left time: 1566.9375s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:01m:26.52s\n",
      "Steps: 224 | Train Loss: 0.0182359 Vali Loss: 0.0179768 Test Loss: 0.0219282\n",
      "Validation loss decreased (0.018397 --> 0.017977).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0167149\n",
      "\tspeed: 0.6254s/iter; left time: 2459.5806s\n",
      "\titers: 200, epoch: 3 | loss: 0.0136638\n",
      "\tspeed: 0.3861s/iter; left time: 1479.7868s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:01m:26.51s\n",
      "Steps: 224 | Train Loss: 0.0162843 Vali Loss: 0.0168971 Test Loss: 0.0206574\n",
      "Validation loss decreased (0.017977 --> 0.016897).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0145439\n",
      "\tspeed: 0.6241s/iter; left time: 2314.8453s\n",
      "\titers: 200, epoch: 4 | loss: 0.0169107\n",
      "\tspeed: 0.3873s/iter; left time: 1397.7654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:01m:26.67s\n",
      "Steps: 224 | Train Loss: 0.0158333 Vali Loss: 0.0171990 Test Loss: 0.0215210\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0149616\n",
      "\tspeed: 0.6182s/iter; left time: 2154.3923s\n",
      "\titers: 200, epoch: 5 | loss: 0.0149911\n",
      "\tspeed: 0.3860s/iter; left time: 1306.6464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:01m:26.48s\n",
      "Steps: 224 | Train Loss: 0.0153089 Vali Loss: 0.0167449 Test Loss: 0.0209905\n",
      "Validation loss decreased (0.016897 --> 0.016745).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0163790\n",
      "\tspeed: 0.6190s/iter; left time: 2018.5800s\n",
      "\titers: 200, epoch: 6 | loss: 0.0151860\n",
      "\tspeed: 0.3860s/iter; left time: 1220.2362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:01m:26.43s\n",
      "Steps: 224 | Train Loss: 0.0149298 Vali Loss: 0.0167596 Test Loss: 0.0204989\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0149499\n",
      "\tspeed: 0.6177s/iter; left time: 1876.0995s\n",
      "\titers: 200, epoch: 7 | loss: 0.0139030\n",
      "\tspeed: 0.3860s/iter; left time: 1133.5509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:01m:26.50s\n",
      "Steps: 224 | Train Loss: 0.0147117 Vali Loss: 0.0164430 Test Loss: 0.0204639\n",
      "Validation loss decreased (0.016745 --> 0.016443).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0149081\n",
      "\tspeed: 0.6195s/iter; left time: 1742.7102s\n",
      "\titers: 200, epoch: 8 | loss: 0.0145743\n",
      "\tspeed: 0.3871s/iter; left time: 1050.1298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:01m:26.68s\n",
      "Steps: 224 | Train Loss: 0.0144592 Vali Loss: 0.0162204 Test Loss: 0.0197561\n",
      "Validation loss decreased (0.016443 --> 0.016220).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0146681\n",
      "\tspeed: 0.6221s/iter; left time: 1610.6571s\n",
      "\titers: 200, epoch: 9 | loss: 0.0153187\n",
      "\tspeed: 0.3870s/iter; left time: 963.2005s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:01m:26.87s\n",
      "Steps: 224 | Train Loss: 0.0141569 Vali Loss: 0.0162054 Test Loss: 0.0199452\n",
      "Validation loss decreased (0.016220 --> 0.016205).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0139688\n",
      "\tspeed: 0.6195s/iter; left time: 1465.0114s\n",
      "\titers: 200, epoch: 10 | loss: 0.0133341\n",
      "\tspeed: 0.3869s/iter; left time: 876.3994s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:01m:26.65s\n",
      "Steps: 224 | Train Loss: 0.0139821 Vali Loss: 0.0161435 Test Loss: 0.0199820\n",
      "Validation loss decreased (0.016205 --> 0.016143).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0135381\n",
      "\tspeed: 0.6234s/iter; left time: 1334.6407s\n",
      "\titers: 200, epoch: 11 | loss: 0.0139865\n",
      "\tspeed: 0.3870s/iter; left time: 789.8287s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:01m:26.66s\n",
      "Steps: 224 | Train Loss: 0.0138574 Vali Loss: 0.0160936 Test Loss: 0.0197122\n",
      "Validation loss decreased (0.016143 --> 0.016094).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0125272\n",
      "\tspeed: 0.6243s/iter; left time: 1196.7041s\n",
      "\titers: 200, epoch: 12 | loss: 0.0144413\n",
      "\tspeed: 0.3869s/iter; left time: 702.9611s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:01m:26.71s\n",
      "Steps: 224 | Train Loss: 0.0137525 Vali Loss: 0.0159956 Test Loss: 0.0195931\n",
      "Validation loss decreased (0.016094 --> 0.015996).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0131212\n",
      "\tspeed: 0.6196s/iter; left time: 1048.9764s\n",
      "\titers: 200, epoch: 13 | loss: 0.0132859\n",
      "\tspeed: 0.3867s/iter; left time: 615.9539s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:01m:26.60s\n",
      "Steps: 224 | Train Loss: 0.0135558 Vali Loss: 0.0160421 Test Loss: 0.0197650\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0144332\n",
      "\tspeed: 0.6183s/iter; left time: 908.3111s\n",
      "\titers: 200, epoch: 14 | loss: 0.0137474\n",
      "\tspeed: 0.3865s/iter; left time: 529.0750s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:01m:26.59s\n",
      "Steps: 224 | Train Loss: 0.0134872 Vali Loss: 0.0159760 Test Loss: 0.0194968\n",
      "Validation loss decreased (0.015996 --> 0.015976).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0140534\n",
      "\tspeed: 0.6310s/iter; left time: 785.5979s\n",
      "\titers: 200, epoch: 15 | loss: 0.0133048\n",
      "\tspeed: 0.3865s/iter; left time: 442.5672s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:01m:26.58s\n",
      "Steps: 224 | Train Loss: 0.0133434 Vali Loss: 0.0161070 Test Loss: 0.0195973\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0139255\n",
      "\tspeed: 0.6179s/iter; left time: 630.9237s\n",
      "\titers: 200, epoch: 16 | loss: 0.0130007\n",
      "\tspeed: 0.3865s/iter; left time: 355.9775s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:01m:26.56s\n",
      "Steps: 224 | Train Loss: 0.0132606 Vali Loss: 0.0160394 Test Loss: 0.0196379\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0132931\n",
      "\tspeed: 0.6170s/iter; left time: 491.7486s\n",
      "\titers: 200, epoch: 17 | loss: 0.0127927\n",
      "\tspeed: 0.3865s/iter; left time: 269.4080s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:01m:26.54s\n",
      "Steps: 224 | Train Loss: 0.0131573 Vali Loss: 0.0160593 Test Loss: 0.0194815\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0141886\n",
      "\tspeed: 0.6195s/iter; left time: 354.9520s\n",
      "\titers: 200, epoch: 18 | loss: 0.0120912\n",
      "\tspeed: 0.3865s/iter; left time: 182.8058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:01m:26.62s\n",
      "Steps: 224 | Train Loss: 0.0130748 Vali Loss: 0.0159712 Test Loss: 0.0192531\n",
      "Validation loss decreased (0.015976 --> 0.015971).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0124319\n",
      "\tspeed: 0.6217s/iter; left time: 216.9876s\n",
      "\titers: 200, epoch: 19 | loss: 0.0126571\n",
      "\tspeed: 0.3864s/iter; left time: 96.2177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:01m:26.56s\n",
      "Steps: 224 | Train Loss: 0.0130525 Vali Loss: 0.0161250 Test Loss: 0.0195972\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0134002\n",
      "\tspeed: 0.6169s/iter; left time: 77.1063s\n",
      "\titers: 200, epoch: 20 | loss: 0.0123817\n",
      "\tspeed: 0.3864s/iter; left time: 9.6610s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:01m:26.49s\n",
      "Steps: 224 | Train Loss: 0.0129701 Vali Loss: 0.0160416 Test Loss: 0.0196599\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019253108650445938, rmse:0.13875557482242584, mae:0.09227865189313889, rse:0.4076220989227295\n",
      "Intermediate time for ES and pred_len 96: 00h:33m:50.69s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0265592\n",
      "\tspeed: 0.4121s/iter; left time: 1797.1768s\n",
      "\titers: 200, epoch: 1 | loss: 0.0243586\n",
      "\tspeed: 0.3863s/iter; left time: 1646.0049s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:01m:26.49s\n",
      "Steps: 223 | Train Loss: 0.0273594 Vali Loss: 0.0200295 Test Loss: 0.0249285\n",
      "Validation loss decreased (inf --> 0.020029).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0208427\n",
      "\tspeed: 0.6233s/iter; left time: 2579.4055s\n",
      "\titers: 200, epoch: 2 | loss: 0.0210364\n",
      "\tspeed: 0.3870s/iter; left time: 1562.7388s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:01m:26.26s\n",
      "Steps: 223 | Train Loss: 0.0197896 Vali Loss: 0.0199548 Test Loss: 0.0237666\n",
      "Validation loss decreased (0.020029 --> 0.019955).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0188704\n",
      "\tspeed: 0.6194s/iter; left time: 2425.0667s\n",
      "\titers: 200, epoch: 3 | loss: 0.0189737\n",
      "\tspeed: 0.4546s/iter; left time: 1734.3145s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:01m:38.46s\n",
      "Steps: 223 | Train Loss: 0.0180211 Vali Loss: 0.0194267 Test Loss: 0.0233839\n",
      "Validation loss decreased (0.019955 --> 0.019427).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0169618\n",
      "\tspeed: 0.6717s/iter; left time: 2479.9897s\n",
      "\titers: 200, epoch: 4 | loss: 0.0167444\n",
      "\tspeed: 0.3879s/iter; left time: 1393.2231s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:01m:26.34s\n",
      "Steps: 223 | Train Loss: 0.0175879 Vali Loss: 0.0195176 Test Loss: 0.0237317\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0181750\n",
      "\tspeed: 0.6117s/iter; left time: 2122.1175s\n",
      "\titers: 200, epoch: 5 | loss: 0.0167845\n",
      "\tspeed: 0.3865s/iter; left time: 1302.2230s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:01m:26.24s\n",
      "Steps: 223 | Train Loss: 0.0169361 Vali Loss: 0.0191335 Test Loss: 0.0237855\n",
      "Validation loss decreased (0.019427 --> 0.019134).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0157064\n",
      "\tspeed: 0.6156s/iter; left time: 1998.3674s\n",
      "\titers: 200, epoch: 6 | loss: 0.0180538\n",
      "\tspeed: 0.3872s/iter; left time: 1218.2526s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:01m:26.37s\n",
      "Steps: 223 | Train Loss: 0.0165787 Vali Loss: 0.0187295 Test Loss: 0.0231191\n",
      "Validation loss decreased (0.019134 --> 0.018730).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0166475\n",
      "\tspeed: 0.6146s/iter; left time: 1857.8465s\n",
      "\titers: 200, epoch: 7 | loss: 0.0160672\n",
      "\tspeed: 0.3876s/iter; left time: 1132.9038s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:01m:26.36s\n",
      "Steps: 223 | Train Loss: 0.0161585 Vali Loss: 0.0183947 Test Loss: 0.0226610\n",
      "Validation loss decreased (0.018730 --> 0.018395).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0160268\n",
      "\tspeed: 0.6201s/iter; left time: 1736.2209s\n",
      "\titers: 200, epoch: 8 | loss: 0.0145470\n",
      "\tspeed: 0.3877s/iter; left time: 1046.9027s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:01m:26.44s\n",
      "Steps: 223 | Train Loss: 0.0159654 Vali Loss: 0.0184937 Test Loss: 0.0224739\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0150325\n",
      "\tspeed: 0.6130s/iter; left time: 1579.7983s\n",
      "\titers: 200, epoch: 9 | loss: 0.0163261\n",
      "\tspeed: 0.3872s/iter; left time: 959.0034s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:01m:26.33s\n",
      "Steps: 223 | Train Loss: 0.0157438 Vali Loss: 0.0181651 Test Loss: 0.0220128\n",
      "Validation loss decreased (0.018395 --> 0.018165).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0151650\n",
      "\tspeed: 0.6155s/iter; left time: 1448.9577s\n",
      "\titers: 200, epoch: 10 | loss: 0.0157046\n",
      "\tspeed: 0.3872s/iter; left time: 872.7710s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:01m:26.32s\n",
      "Steps: 223 | Train Loss: 0.0156164 Vali Loss: 0.0180622 Test Loss: 0.0219389\n",
      "Validation loss decreased (0.018165 --> 0.018062).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0162252\n",
      "\tspeed: 0.6160s/iter; left time: 1312.6739s\n",
      "\titers: 200, epoch: 11 | loss: 0.0161423\n",
      "\tspeed: 0.3889s/iter; left time: 789.9386s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:01m:26.58s\n",
      "Steps: 223 | Train Loss: 0.0155445 Vali Loss: 0.0180314 Test Loss: 0.0215321\n",
      "Validation loss decreased (0.018062 --> 0.018031).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0150040\n",
      "\tspeed: 0.6144s/iter; left time: 1172.2758s\n",
      "\titers: 200, epoch: 12 | loss: 0.0156210\n",
      "\tspeed: 0.3867s/iter; left time: 699.1137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:01m:26.26s\n",
      "Steps: 223 | Train Loss: 0.0154035 Vali Loss: 0.0182018 Test Loss: 0.0220669\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0149252\n",
      "\tspeed: 0.6289s/iter; left time: 1059.6735s\n",
      "\titers: 200, epoch: 13 | loss: 0.0144699\n",
      "\tspeed: 0.3877s/iter; left time: 614.4462s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:01m:26.99s\n",
      "Steps: 223 | Train Loss: 0.0152830 Vali Loss: 0.0180043 Test Loss: 0.0217210\n",
      "Validation loss decreased (0.018031 --> 0.018004).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0155531\n",
      "\tspeed: 0.6258s/iter; left time: 914.8969s\n",
      "\titers: 200, epoch: 14 | loss: 0.0156407\n",
      "\tspeed: 0.3872s/iter; left time: 527.3925s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:01m:26.32s\n",
      "Steps: 223 | Train Loss: 0.0151557 Vali Loss: 0.0180213 Test Loss: 0.0216910\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0166781\n",
      "\tspeed: 0.6134s/iter; left time: 759.9576s\n",
      "\titers: 200, epoch: 15 | loss: 0.0160159\n",
      "\tspeed: 0.3876s/iter; left time: 441.4967s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:01m:26.46s\n",
      "Steps: 223 | Train Loss: 0.0150411 Vali Loss: 0.0179539 Test Loss: 0.0217073\n",
      "Validation loss decreased (0.018004 --> 0.017954).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0152969\n",
      "\tspeed: 0.6217s/iter; left time: 631.6563s\n",
      "\titers: 200, epoch: 16 | loss: 0.0135630\n",
      "\tspeed: 0.3873s/iter; left time: 354.7550s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:01m:26.38s\n",
      "Steps: 223 | Train Loss: 0.0149140 Vali Loss: 0.0178565 Test Loss: 0.0215013\n",
      "Validation loss decreased (0.017954 --> 0.017857).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0140472\n",
      "\tspeed: 0.6166s/iter; left time: 488.9443s\n",
      "\titers: 200, epoch: 17 | loss: 0.0152880\n",
      "\tspeed: 0.3877s/iter; left time: 268.6528s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:01m:26.42s\n",
      "Steps: 223 | Train Loss: 0.0148222 Vali Loss: 0.0179699 Test Loss: 0.0216110\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0148507\n",
      "\tspeed: 0.6148s/iter; left time: 350.4527s\n",
      "\titers: 200, epoch: 18 | loss: 0.0164600\n",
      "\tspeed: 0.3875s/iter; left time: 182.1439s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:01m:26.40s\n",
      "Steps: 223 | Train Loss: 0.0147455 Vali Loss: 0.0178199 Test Loss: 0.0213912\n",
      "Validation loss decreased (0.017857 --> 0.017820).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0149923\n",
      "\tspeed: 0.6140s/iter; left time: 213.0419s\n",
      "\titers: 200, epoch: 19 | loss: 0.0143576\n",
      "\tspeed: 0.3861s/iter; left time: 95.3724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:01m:26.13s\n",
      "Steps: 223 | Train Loss: 0.0146880 Vali Loss: 0.0179486 Test Loss: 0.0216664\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0145650\n",
      "\tspeed: 0.6127s/iter; left time: 75.9769s\n",
      "\titers: 200, epoch: 20 | loss: 0.0149989\n",
      "\tspeed: 0.3875s/iter; left time: 9.2989s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:01m:26.38s\n",
      "Steps: 223 | Train Loss: 0.0146028 Vali Loss: 0.0178162 Test Loss: 0.0213934\n",
      "Validation loss decreased (0.017820 --> 0.017816).  Saving model ...\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02139340527355671, rmse:0.14626485109329224, mae:0.0981053039431572, rse:0.4297129511833191\n",
      "Intermediate time for ES and pred_len 168: 00h:33m:58.22s\n",
      "Intermediate time for ES: 01h:41m:25.40s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0130487\n",
      "\tspeed: 0.1511s/iter; left time: 667.9956s\n",
      "\titers: 200, epoch: 1 | loss: 0.0103617\n",
      "\tspeed: 0.1239s/iter; left time: 535.5428s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:28.58s\n",
      "Steps: 226 | Train Loss: 0.0144769 Vali Loss: 0.0124056 Test Loss: 0.0141598\n",
      "Validation loss decreased (inf --> 0.012406).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0082445\n",
      "\tspeed: 0.2080s/iter; left time: 872.4846s\n",
      "\titers: 200, epoch: 2 | loss: 0.0071333\n",
      "\tspeed: 0.1251s/iter; left time: 512.1418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:28.41s\n",
      "Steps: 226 | Train Loss: 0.0077434 Vali Loss: 0.0100425 Test Loss: 0.0115160\n",
      "Validation loss decreased (0.012406 --> 0.010043).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0076254\n",
      "\tspeed: 0.2073s/iter; left time: 822.8809s\n",
      "\titers: 200, epoch: 3 | loss: 0.0089435\n",
      "\tspeed: 0.1243s/iter; left time: 480.7884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:28.23s\n",
      "Steps: 226 | Train Loss: 0.0069315 Vali Loss: 0.0095908 Test Loss: 0.0110281\n",
      "Validation loss decreased (0.010043 --> 0.009591).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0076805\n",
      "\tspeed: 0.2106s/iter; left time: 788.1548s\n",
      "\titers: 200, epoch: 4 | loss: 0.0075063\n",
      "\tspeed: 0.1373s/iter; left time: 500.1599s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:29.94s\n",
      "Steps: 226 | Train Loss: 0.0067137 Vali Loss: 0.0104638 Test Loss: 0.0117844\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0064084\n",
      "\tspeed: 0.2057s/iter; left time: 723.6193s\n",
      "\titers: 200, epoch: 5 | loss: 0.0064071\n",
      "\tspeed: 0.1242s/iter; left time: 424.2606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:28.23s\n",
      "Steps: 226 | Train Loss: 0.0065286 Vali Loss: 0.0098535 Test Loss: 0.0114410\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0075900\n",
      "\tspeed: 0.2256s/iter; left time: 742.4972s\n",
      "\titers: 200, epoch: 6 | loss: 0.0057913\n",
      "\tspeed: 0.1456s/iter; left time: 464.6230s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:32.86s\n",
      "Steps: 226 | Train Loss: 0.0063824 Vali Loss: 0.0095019 Test Loss: 0.0113460\n",
      "Validation loss decreased (0.009591 --> 0.009502).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0057422\n",
      "\tspeed: 0.2129s/iter; left time: 652.6132s\n",
      "\titers: 200, epoch: 7 | loss: 0.0076545\n",
      "\tspeed: 0.1399s/iter; left time: 414.7238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:30.86s\n",
      "Steps: 226 | Train Loss: 0.0062955 Vali Loss: 0.0095683 Test Loss: 0.0111601\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0058789\n",
      "\tspeed: 0.2228s/iter; left time: 632.5032s\n",
      "\titers: 200, epoch: 8 | loss: 0.0063653\n",
      "\tspeed: 0.1526s/iter; left time: 417.8945s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:31.10s\n",
      "Steps: 226 | Train Loss: 0.0061740 Vali Loss: 0.0094667 Test Loss: 0.0111329\n",
      "Validation loss decreased (0.009502 --> 0.009467).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0066320\n",
      "\tspeed: 0.2072s/iter; left time: 541.3679s\n",
      "\titers: 200, epoch: 9 | loss: 0.0070459\n",
      "\tspeed: 0.1247s/iter; left time: 313.3430s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:28.32s\n",
      "Steps: 226 | Train Loss: 0.0061295 Vali Loss: 0.0097573 Test Loss: 0.0115309\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0059691\n",
      "\tspeed: 0.2076s/iter; left time: 495.5522s\n",
      "\titers: 200, epoch: 10 | loss: 0.0064003\n",
      "\tspeed: 0.1263s/iter; left time: 288.8035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:28.61s\n",
      "Steps: 226 | Train Loss: 0.0060785 Vali Loss: 0.0093925 Test Loss: 0.0109733\n",
      "Validation loss decreased (0.009467 --> 0.009392).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0051292\n",
      "\tspeed: 0.2070s/iter; left time: 447.3410s\n",
      "\titers: 200, epoch: 11 | loss: 0.0055100\n",
      "\tspeed: 0.1249s/iter; left time: 257.3534s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:28.33s\n",
      "Steps: 226 | Train Loss: 0.0059735 Vali Loss: 0.0093449 Test Loss: 0.0110144\n",
      "Validation loss decreased (0.009392 --> 0.009345).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0065305\n",
      "\tspeed: 0.2079s/iter; left time: 402.3041s\n",
      "\titers: 200, epoch: 12 | loss: 0.0048937\n",
      "\tspeed: 0.1461s/iter; left time: 268.0112s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:31.06s\n",
      "Steps: 226 | Train Loss: 0.0059678 Vali Loss: 0.0093493 Test Loss: 0.0110368\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0058089\n",
      "\tspeed: 0.2213s/iter; left time: 378.1262s\n",
      "\titers: 200, epoch: 13 | loss: 0.0065166\n",
      "\tspeed: 0.1265s/iter; left time: 203.4598s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:28.72s\n",
      "Steps: 226 | Train Loss: 0.0059069 Vali Loss: 0.0092951 Test Loss: 0.0108905\n",
      "Validation loss decreased (0.009345 --> 0.009295).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0052859\n",
      "\tspeed: 0.2086s/iter; left time: 309.4225s\n",
      "\titers: 200, epoch: 14 | loss: 0.0075063\n",
      "\tspeed: 0.1247s/iter; left time: 172.4812s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:28.46s\n",
      "Steps: 226 | Train Loss: 0.0058505 Vali Loss: 0.0094139 Test Loss: 0.0109936\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0054630\n",
      "\tspeed: 0.2260s/iter; left time: 284.1288s\n",
      "\titers: 200, epoch: 15 | loss: 0.0058545\n",
      "\tspeed: 0.1600s/iter; left time: 185.0958s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:34.47s\n",
      "Steps: 226 | Train Loss: 0.0058097 Vali Loss: 0.0093261 Test Loss: 0.0109611\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0058441\n",
      "\tspeed: 0.2423s/iter; left time: 249.8441s\n",
      "\titers: 200, epoch: 16 | loss: 0.0062263\n",
      "\tspeed: 0.1243s/iter; left time: 115.7596s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:29.58s\n",
      "Steps: 226 | Train Loss: 0.0057721 Vali Loss: 0.0094083 Test Loss: 0.0108882\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0048290\n",
      "\tspeed: 0.2103s/iter; left time: 169.2988s\n",
      "\titers: 200, epoch: 17 | loss: 0.0069202\n",
      "\tspeed: 0.1346s/iter; left time: 94.8587s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:29.89s\n",
      "Steps: 226 | Train Loss: 0.0057287 Vali Loss: 0.0092327 Test Loss: 0.0108260\n",
      "Validation loss decreased (0.009295 --> 0.009233).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0057634\n",
      "\tspeed: 0.2301s/iter; left time: 133.2105s\n",
      "\titers: 200, epoch: 18 | loss: 0.0056466\n",
      "\tspeed: 0.1279s/iter; left time: 61.2775s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:29.59s\n",
      "Steps: 226 | Train Loss: 0.0056984 Vali Loss: 0.0091110 Test Loss: 0.0108564\n",
      "Validation loss decreased (0.009233 --> 0.009111).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0059800\n",
      "\tspeed: 0.2070s/iter; left time: 73.0712s\n",
      "\titers: 200, epoch: 19 | loss: 0.0052861\n",
      "\tspeed: 0.1275s/iter; left time: 32.2481s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:29.03s\n",
      "Steps: 226 | Train Loss: 0.0056953 Vali Loss: 0.0092817 Test Loss: 0.0107927\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0051276\n",
      "\tspeed: 0.2104s/iter; left time: 26.7178s\n",
      "\titers: 200, epoch: 20 | loss: 0.0055541\n",
      "\tspeed: 0.1244s/iter; left time: 3.3599s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:28.27s\n",
      "Steps: 226 | Train Loss: 0.0056517 Vali Loss: 0.0092387 Test Loss: 0.0107835\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010856376960873604, rmse:0.10419394075870514, mae:0.05993499606847763, rse:0.4019773304462433\n",
      "Intermediate time for FR and pred_len 24: 00h:11m:45.54s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0147024\n",
      "\tspeed: 0.1615s/iter; left time: 710.7157s\n",
      "\titers: 200, epoch: 1 | loss: 0.0152049\n",
      "\tspeed: 0.1252s/iter; left time: 538.5728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:29.52s\n",
      "Steps: 225 | Train Loss: 0.0179286 Vali Loss: 0.0170811 Test Loss: 0.0214118\n",
      "Validation loss decreased (inf --> 0.017081).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0113892\n",
      "\tspeed: 0.2080s/iter; left time: 868.7265s\n",
      "\titers: 200, epoch: 2 | loss: 0.0127477\n",
      "\tspeed: 0.1268s/iter; left time: 516.9425s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:28.53s\n",
      "Steps: 225 | Train Loss: 0.0126357 Vali Loss: 0.0162430 Test Loss: 0.0203346\n",
      "Validation loss decreased (0.017081 --> 0.016243).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0126674\n",
      "\tspeed: 0.2114s/iter; left time: 835.3086s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123108\n",
      "\tspeed: 0.1280s/iter; left time: 492.8921s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:28.78s\n",
      "Steps: 225 | Train Loss: 0.0118050 Vali Loss: 0.0156507 Test Loss: 0.0206055\n",
      "Validation loss decreased (0.016243 --> 0.015651).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0113491\n",
      "\tspeed: 0.2127s/iter; left time: 792.5417s\n",
      "\titers: 200, epoch: 4 | loss: 0.0114844\n",
      "\tspeed: 0.1349s/iter; left time: 489.2137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:30.19s\n",
      "Steps: 225 | Train Loss: 0.0113757 Vali Loss: 0.0151757 Test Loss: 0.0200848\n",
      "Validation loss decreased (0.015651 --> 0.015176).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0102961\n",
      "\tspeed: 0.2545s/iter; left time: 890.8570s\n",
      "\titers: 200, epoch: 5 | loss: 0.0125940\n",
      "\tspeed: 0.1501s/iter; left time: 510.5007s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:34.25s\n",
      "Steps: 225 | Train Loss: 0.0109752 Vali Loss: 0.0154549 Test Loss: 0.0202221\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0110599\n",
      "\tspeed: 0.2136s/iter; left time: 699.6178s\n",
      "\titers: 200, epoch: 6 | loss: 0.0100994\n",
      "\tspeed: 0.1244s/iter; left time: 395.0562s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:28.14s\n",
      "Steps: 225 | Train Loss: 0.0107455 Vali Loss: 0.0151377 Test Loss: 0.0201709\n",
      "Validation loss decreased (0.015176 --> 0.015138).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0105998\n",
      "\tspeed: 0.2094s/iter; left time: 638.7615s\n",
      "\titers: 200, epoch: 7 | loss: 0.0122199\n",
      "\tspeed: 0.1248s/iter; left time: 368.3685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:28.46s\n",
      "Steps: 225 | Train Loss: 0.0105227 Vali Loss: 0.0148822 Test Loss: 0.0197665\n",
      "Validation loss decreased (0.015138 --> 0.014882).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0108822\n",
      "\tspeed: 0.2089s/iter; left time: 590.2478s\n",
      "\titers: 200, epoch: 8 | loss: 0.0109945\n",
      "\tspeed: 0.1273s/iter; left time: 346.9206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:28.65s\n",
      "Steps: 225 | Train Loss: 0.0104018 Vali Loss: 0.0152280 Test Loss: 0.0200418\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0099946\n",
      "\tspeed: 0.2095s/iter; left time: 545.0218s\n",
      "\titers: 200, epoch: 9 | loss: 0.0108894\n",
      "\tspeed: 0.1280s/iter; left time: 320.0787s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:28.72s\n",
      "Steps: 225 | Train Loss: 0.0102055 Vali Loss: 0.0145973 Test Loss: 0.0200153\n",
      "Validation loss decreased (0.014882 --> 0.014597).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0097942\n",
      "\tspeed: 0.2070s/iter; left time: 491.7504s\n",
      "\titers: 200, epoch: 10 | loss: 0.0097672\n",
      "\tspeed: 0.1247s/iter; left time: 283.8064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:28.24s\n",
      "Steps: 225 | Train Loss: 0.0101035 Vali Loss: 0.0149185 Test Loss: 0.0200497\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0092038\n",
      "\tspeed: 0.2128s/iter; left time: 457.8325s\n",
      "\titers: 200, epoch: 11 | loss: 0.0101169\n",
      "\tspeed: 0.1456s/iter; left time: 298.7179s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:31.45s\n",
      "Steps: 225 | Train Loss: 0.0099663 Vali Loss: 0.0148763 Test Loss: 0.0202064\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0103183\n",
      "\tspeed: 0.2160s/iter; left time: 416.0914s\n",
      "\titers: 200, epoch: 12 | loss: 0.0095475\n",
      "\tspeed: 0.1247s/iter; left time: 227.6537s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:28.20s\n",
      "Steps: 225 | Train Loss: 0.0099111 Vali Loss: 0.0151120 Test Loss: 0.0204279\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0087962\n",
      "\tspeed: 0.2208s/iter; left time: 375.5851s\n",
      "\titers: 200, epoch: 13 | loss: 0.0110409\n",
      "\tspeed: 0.1251s/iter; left time: 200.2070s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:29.51s\n",
      "Steps: 225 | Train Loss: 0.0098284 Vali Loss: 0.0149348 Test Loss: 0.0203463\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0095532\n",
      "\tspeed: 0.2086s/iter; left time: 307.9121s\n",
      "\titers: 200, epoch: 14 | loss: 0.0089391\n",
      "\tspeed: 0.1255s/iter; left time: 172.7448s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:28.30s\n",
      "Steps: 225 | Train Loss: 0.0097749 Vali Loss: 0.0147762 Test Loss: 0.0201790\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020015263929963112, rmse:0.14147530496120453, mae:0.08575939387083054, rse:0.5472640991210938\n",
      "Intermediate time for FR and pred_len 96: 00h:08m:14.62s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0181483\n",
      "\tspeed: 0.1552s/iter; left time: 683.1972s\n",
      "\titers: 200, epoch: 1 | loss: 0.0172763\n",
      "\tspeed: 0.1303s/iter; left time: 560.4937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:30.38s\n",
      "Steps: 225 | Train Loss: 0.0193621 Vali Loss: 0.0185368 Test Loss: 0.0225658\n",
      "Validation loss decreased (inf --> 0.018537).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0141904\n",
      "\tspeed: 0.2654s/iter; left time: 1108.4793s\n",
      "\titers: 200, epoch: 2 | loss: 0.0126614\n",
      "\tspeed: 0.1431s/iter; left time: 583.2256s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:33.60s\n",
      "Steps: 225 | Train Loss: 0.0141230 Vali Loss: 0.0176849 Test Loss: 0.0215894\n",
      "Validation loss decreased (0.018537 --> 0.017685).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0135257\n",
      "\tspeed: 0.2088s/iter; left time: 824.8487s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123185\n",
      "\tspeed: 0.1307s/iter; left time: 503.4348s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:29.40s\n",
      "Steps: 225 | Train Loss: 0.0133672 Vali Loss: 0.0172216 Test Loss: 0.0222378\n",
      "Validation loss decreased (0.017685 --> 0.017222).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0143941\n",
      "\tspeed: 0.2297s/iter; left time: 855.8799s\n",
      "\titers: 200, epoch: 4 | loss: 0.0125045\n",
      "\tspeed: 0.1250s/iter; left time: 453.3433s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:28.91s\n",
      "Steps: 225 | Train Loss: 0.0131145 Vali Loss: 0.0170706 Test Loss: 0.0219320\n",
      "Validation loss decreased (0.017222 --> 0.017071).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0133954\n",
      "\tspeed: 0.2088s/iter; left time: 731.0332s\n",
      "\titers: 200, epoch: 5 | loss: 0.0122464\n",
      "\tspeed: 0.1249s/iter; left time: 424.6466s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:28.36s\n",
      "Steps: 225 | Train Loss: 0.0126029 Vali Loss: 0.0168675 Test Loss: 0.0215498\n",
      "Validation loss decreased (0.017071 --> 0.016867).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0133064\n",
      "\tspeed: 0.2094s/iter; left time: 686.1242s\n",
      "\titers: 200, epoch: 6 | loss: 0.0123626\n",
      "\tspeed: 0.1260s/iter; left time: 400.1645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:28.59s\n",
      "Steps: 225 | Train Loss: 0.0122548 Vali Loss: 0.0168818 Test Loss: 0.0217501\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0121893\n",
      "\tspeed: 0.2145s/iter; left time: 654.5186s\n",
      "\titers: 200, epoch: 7 | loss: 0.0128906\n",
      "\tspeed: 0.1524s/iter; left time: 449.8610s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:32.60s\n",
      "Steps: 225 | Train Loss: 0.0119784 Vali Loss: 0.0164041 Test Loss: 0.0216603\n",
      "Validation loss decreased (0.016867 --> 0.016404).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0129033\n",
      "\tspeed: 0.2547s/iter; left time: 719.7606s\n",
      "\titers: 200, epoch: 8 | loss: 0.0126196\n",
      "\tspeed: 0.1518s/iter; left time: 413.8278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.38s\n",
      "Steps: 225 | Train Loss: 0.0118114 Vali Loss: 0.0167039 Test Loss: 0.0221317\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0110283\n",
      "\tspeed: 0.2490s/iter; left time: 647.7105s\n",
      "\titers: 200, epoch: 9 | loss: 0.0131031\n",
      "\tspeed: 0.1565s/iter; left time: 391.3209s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:34.86s\n",
      "Steps: 225 | Train Loss: 0.0116214 Vali Loss: 0.0164882 Test Loss: 0.0220004\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0106390\n",
      "\tspeed: 0.2500s/iter; left time: 593.9144s\n",
      "\titers: 200, epoch: 10 | loss: 0.0103403\n",
      "\tspeed: 0.1589s/iter; left time: 361.6155s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:35.14s\n",
      "Steps: 225 | Train Loss: 0.0115014 Vali Loss: 0.0163617 Test Loss: 0.0219452\n",
      "Validation loss decreased (0.016404 --> 0.016362).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0104352\n",
      "\tspeed: 0.2496s/iter; left time: 536.9433s\n",
      "\titers: 200, epoch: 11 | loss: 0.0120406\n",
      "\tspeed: 0.1359s/iter; left time: 278.7026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:31.69s\n",
      "Steps: 225 | Train Loss: 0.0114241 Vali Loss: 0.0164061 Test Loss: 0.0221639\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0100976\n",
      "\tspeed: 0.2467s/iter; left time: 475.1387s\n",
      "\titers: 200, epoch: 12 | loss: 0.0108029\n",
      "\tspeed: 0.1561s/iter; left time: 285.0334s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:34.70s\n",
      "Steps: 225 | Train Loss: 0.0113432 Vali Loss: 0.0163061 Test Loss: 0.0218783\n",
      "Validation loss decreased (0.016362 --> 0.016306).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0123946\n",
      "\tspeed: 0.2500s/iter; left time: 425.2854s\n",
      "\titers: 200, epoch: 13 | loss: 0.0106203\n",
      "\tspeed: 0.1513s/iter; left time: 242.2793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:34.24s\n",
      "Steps: 225 | Train Loss: 0.0112463 Vali Loss: 0.0164862 Test Loss: 0.0220891\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0120560\n",
      "\tspeed: 0.2524s/iter; left time: 372.4722s\n",
      "\titers: 200, epoch: 14 | loss: 0.0114261\n",
      "\tspeed: 0.1545s/iter; left time: 212.6083s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:34.93s\n",
      "Steps: 225 | Train Loss: 0.0111975 Vali Loss: 0.0162412 Test Loss: 0.0219959\n",
      "Validation loss decreased (0.016306 --> 0.016241).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0116879\n",
      "\tspeed: 0.2476s/iter; left time: 309.7735s\n",
      "\titers: 200, epoch: 15 | loss: 0.0111726\n",
      "\tspeed: 0.1573s/iter; left time: 181.0654s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:34.17s\n",
      "Steps: 225 | Train Loss: 0.0111193 Vali Loss: 0.0163175 Test Loss: 0.0220602\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0120678\n",
      "\tspeed: 0.2222s/iter; left time: 227.9839s\n",
      "\titers: 200, epoch: 16 | loss: 0.0114807\n",
      "\tspeed: 0.1461s/iter; left time: 135.2911s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:32.41s\n",
      "Steps: 225 | Train Loss: 0.0110744 Vali Loss: 0.0163514 Test Loss: 0.0223251\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0109318\n",
      "\tspeed: 0.2225s/iter; left time: 178.2290s\n",
      "\titers: 200, epoch: 17 | loss: 0.0104629\n",
      "\tspeed: 0.1409s/iter; left time: 98.7463s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:30.74s\n",
      "Steps: 225 | Train Loss: 0.0110372 Vali Loss: 0.0163676 Test Loss: 0.0223826\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0106376\n",
      "\tspeed: 0.2519s/iter; left time: 145.1035s\n",
      "\titers: 200, epoch: 18 | loss: 0.0119821\n",
      "\tspeed: 0.1519s/iter; left time: 72.3177s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:34.25s\n",
      "Steps: 225 | Train Loss: 0.0109769 Vali Loss: 0.0163234 Test Loss: 0.0221871\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0107207\n",
      "\tspeed: 0.2497s/iter; left time: 87.6505s\n",
      "\titers: 200, epoch: 19 | loss: 0.0112138\n",
      "\tspeed: 0.1565s/iter; left time: 39.2718s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:34.74s\n",
      "Steps: 225 | Train Loss: 0.0109562 Vali Loss: 0.0162882 Test Loss: 0.0222033\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.021995939314365387, rmse:0.14831027388572693, mae:0.09114450216293335, rse:0.5744197368621826\n",
      "Intermediate time for FR and pred_len 168: 00h:12m:19.17s\n",
      "Intermediate time for FR: 00h:32m:19.33s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0243448\n",
      "\tspeed: 0.1652s/iter; left time: 730.3256s\n",
      "\titers: 200, epoch: 1 | loss: 0.0229790\n",
      "\tspeed: 0.1383s/iter; left time: 597.7161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:32.21s\n",
      "Steps: 226 | Train Loss: 0.0268654 Vali Loss: 0.0144127 Test Loss: 0.0152149\n",
      "Validation loss decreased (inf --> 0.014413).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0126385\n",
      "\tspeed: 0.2473s/iter; left time: 1037.3901s\n",
      "\titers: 200, epoch: 2 | loss: 0.0113061\n",
      "\tspeed: 0.1563s/iter; left time: 640.1219s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:34.77s\n",
      "Steps: 226 | Train Loss: 0.0135024 Vali Loss: 0.0112989 Test Loss: 0.0123300\n",
      "Validation loss decreased (0.014413 --> 0.011299).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0124331\n",
      "\tspeed: 0.2475s/iter; left time: 982.1816s\n",
      "\titers: 200, epoch: 3 | loss: 0.0116472\n",
      "\tspeed: 0.1517s/iter; left time: 587.0119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:34.22s\n",
      "Steps: 226 | Train Loss: 0.0119030 Vali Loss: 0.0106734 Test Loss: 0.0116822\n",
      "Validation loss decreased (0.011299 --> 0.010673).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0122854\n",
      "\tspeed: 0.2498s/iter; left time: 934.8984s\n",
      "\titers: 200, epoch: 4 | loss: 0.0143051\n",
      "\tspeed: 0.1539s/iter; left time: 560.5195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:34.85s\n",
      "Steps: 226 | Train Loss: 0.0113319 Vali Loss: 0.0102620 Test Loss: 0.0112144\n",
      "Validation loss decreased (0.010673 --> 0.010262).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0109653\n",
      "\tspeed: 0.2424s/iter; left time: 852.5450s\n",
      "\titers: 200, epoch: 5 | loss: 0.0132881\n",
      "\tspeed: 0.1341s/iter; left time: 458.1996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:31.18s\n",
      "Steps: 226 | Train Loss: 0.0110240 Vali Loss: 0.0102972 Test Loss: 0.0114048\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0113125\n",
      "\tspeed: 0.2372s/iter; left time: 780.5652s\n",
      "\titers: 200, epoch: 6 | loss: 0.0103211\n",
      "\tspeed: 0.1491s/iter; left time: 475.6809s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:33.66s\n",
      "Steps: 226 | Train Loss: 0.0106692 Vali Loss: 0.0098025 Test Loss: 0.0108670\n",
      "Validation loss decreased (0.010262 --> 0.009803).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0103765\n",
      "\tspeed: 0.2515s/iter; left time: 770.8376s\n",
      "\titers: 200, epoch: 7 | loss: 0.0112636\n",
      "\tspeed: 0.1509s/iter; left time: 447.3094s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.23s\n",
      "Steps: 226 | Train Loss: 0.0105300 Vali Loss: 0.0100033 Test Loss: 0.0109814\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0091729\n",
      "\tspeed: 0.2492s/iter; left time: 707.4627s\n",
      "\titers: 200, epoch: 8 | loss: 0.0084527\n",
      "\tspeed: 0.1539s/iter; left time: 421.4469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.61s\n",
      "Steps: 226 | Train Loss: 0.0103527 Vali Loss: 0.0097033 Test Loss: 0.0107587\n",
      "Validation loss decreased (0.009803 --> 0.009703).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0116518\n",
      "\tspeed: 0.2507s/iter; left time: 655.1436s\n",
      "\titers: 200, epoch: 9 | loss: 0.0105640\n",
      "\tspeed: 0.1459s/iter; left time: 366.6337s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:33.55s\n",
      "Steps: 226 | Train Loss: 0.0102292 Vali Loss: 0.0098548 Test Loss: 0.0106564\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0098734\n",
      "\tspeed: 0.2224s/iter; left time: 530.9787s\n",
      "\titers: 200, epoch: 10 | loss: 0.0098391\n",
      "\tspeed: 0.1463s/iter; left time: 334.6955s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:32.04s\n",
      "Steps: 226 | Train Loss: 0.0101449 Vali Loss: 0.0096761 Test Loss: 0.0105900\n",
      "Validation loss decreased (0.009703 --> 0.009676).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0102106\n",
      "\tspeed: 0.2506s/iter; left time: 541.5407s\n",
      "\titers: 200, epoch: 11 | loss: 0.0102552\n",
      "\tspeed: 0.1512s/iter; left time: 311.7200s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:34.48s\n",
      "Steps: 226 | Train Loss: 0.0100208 Vali Loss: 0.0097775 Test Loss: 0.0107950\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0108097\n",
      "\tspeed: 0.2487s/iter; left time: 481.2249s\n",
      "\titers: 200, epoch: 12 | loss: 0.0104917\n",
      "\tspeed: 0.1492s/iter; left time: 273.8000s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:34.06s\n",
      "Steps: 226 | Train Loss: 0.0099346 Vali Loss: 0.0096047 Test Loss: 0.0106282\n",
      "Validation loss decreased (0.009676 --> 0.009605).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0085579\n",
      "\tspeed: 0.2551s/iter; left time: 435.9056s\n",
      "\titers: 200, epoch: 13 | loss: 0.0086822\n",
      "\tspeed: 0.1517s/iter; left time: 244.1550s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:34.90s\n",
      "Steps: 226 | Train Loss: 0.0098653 Vali Loss: 0.0094783 Test Loss: 0.0103315\n",
      "Validation loss decreased (0.009605 --> 0.009478).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0097914\n",
      "\tspeed: 0.2312s/iter; left time: 342.8554s\n",
      "\titers: 200, epoch: 14 | loss: 0.0113469\n",
      "\tspeed: 0.1332s/iter; left time: 184.2712s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:30.74s\n",
      "Steps: 226 | Train Loss: 0.0097909 Vali Loss: 0.0093775 Test Loss: 0.0103079\n",
      "Validation loss decreased (0.009478 --> 0.009378).  Saving model ...\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0094810\n",
      "\tspeed: 0.2434s/iter; left time: 305.9598s\n",
      "\titers: 200, epoch: 15 | loss: 0.0113624\n",
      "\tspeed: 0.1544s/iter; left time: 178.6267s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:34.40s\n",
      "Steps: 226 | Train Loss: 0.0097382 Vali Loss: 0.0095560 Test Loss: 0.0104357\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0104041\n",
      "\tspeed: 0.2474s/iter; left time: 255.0742s\n",
      "\titers: 200, epoch: 16 | loss: 0.0091918\n",
      "\tspeed: 0.1518s/iter; left time: 141.2844s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:34.14s\n",
      "Steps: 226 | Train Loss: 0.0096565 Vali Loss: 0.0094822 Test Loss: 0.0103096\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0116628\n",
      "\tspeed: 0.2490s/iter; left time: 200.4124s\n",
      "\titers: 200, epoch: 17 | loss: 0.0094893\n",
      "\tspeed: 0.1560s/iter; left time: 109.9833s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:34.84s\n",
      "Steps: 226 | Train Loss: 0.0096267 Vali Loss: 0.0093537 Test Loss: 0.0103266\n",
      "Validation loss decreased (0.009378 --> 0.009354).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0102649\n",
      "\tspeed: 0.2432s/iter; left time: 140.7853s\n",
      "\titers: 200, epoch: 18 | loss: 0.0087708\n",
      "\tspeed: 0.1254s/iter; left time: 60.0802s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:30.78s\n",
      "Steps: 226 | Train Loss: 0.0095806 Vali Loss: 0.0094264 Test Loss: 0.0103736\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0095868\n",
      "\tspeed: 0.2412s/iter; left time: 85.1284s\n",
      "\titers: 200, epoch: 19 | loss: 0.0085493\n",
      "\tspeed: 0.1495s/iter; left time: 37.8120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:33.97s\n",
      "Steps: 226 | Train Loss: 0.0095211 Vali Loss: 0.0093145 Test Loss: 0.0102482\n",
      "Validation loss decreased (0.009354 --> 0.009315).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0096340\n",
      "\tspeed: 0.2552s/iter; left time: 32.4072s\n",
      "\titers: 200, epoch: 20 | loss: 0.0095999\n",
      "\tspeed: 0.1497s/iter; left time: 4.0422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:34.21s\n",
      "Steps: 226 | Train Loss: 0.0094857 Vali Loss: 0.0093332 Test Loss: 0.0101544\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01024822797626257, rmse:0.10123353451490402, mae:0.06074071303009987, rse:0.3825116753578186\n",
      "Intermediate time for IT and pred_len 24: 00h:13m:18.33s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0301183\n",
      "\tspeed: 0.1775s/iter; left time: 781.3073s\n",
      "\titers: 200, epoch: 1 | loss: 0.0269664\n",
      "\tspeed: 0.1485s/iter; left time: 638.4962s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:33.53s\n",
      "Steps: 225 | Train Loss: 0.0326885 Vali Loss: 0.0203707 Test Loss: 0.0221248\n",
      "Validation loss decreased (inf --> 0.020371).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0204590\n",
      "\tspeed: 0.2157s/iter; left time: 900.9065s\n",
      "\titers: 200, epoch: 2 | loss: 0.0222128\n",
      "\tspeed: 0.1453s/iter; left time: 592.3631s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:31.61s\n",
      "Steps: 225 | Train Loss: 0.0220685 Vali Loss: 0.0182516 Test Loss: 0.0199305\n",
      "Validation loss decreased (0.020371 --> 0.018252).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0205928\n",
      "\tspeed: 0.2522s/iter; left time: 996.4412s\n",
      "\titers: 200, epoch: 3 | loss: 0.0205219\n",
      "\tspeed: 0.1523s/iter; left time: 586.5661s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:34.54s\n",
      "Steps: 225 | Train Loss: 0.0198721 Vali Loss: 0.0179414 Test Loss: 0.0205862\n",
      "Validation loss decreased (0.018252 --> 0.017941).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0193388\n",
      "\tspeed: 0.2520s/iter; left time: 938.9891s\n",
      "\titers: 200, epoch: 4 | loss: 0.0188175\n",
      "\tspeed: 0.1491s/iter; left time: 540.5583s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:34.08s\n",
      "Steps: 225 | Train Loss: 0.0187379 Vali Loss: 0.0167647 Test Loss: 0.0192748\n",
      "Validation loss decreased (0.017941 --> 0.016765).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0167888\n",
      "\tspeed: 0.2547s/iter; left time: 891.7298s\n",
      "\titers: 200, epoch: 5 | loss: 0.0216136\n",
      "\tspeed: 0.1527s/iter; left time: 519.1987s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:34.82s\n",
      "Steps: 225 | Train Loss: 0.0183412 Vali Loss: 0.0168859 Test Loss: 0.0193811\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0184630\n",
      "\tspeed: 0.2234s/iter; left time: 731.8184s\n",
      "\titers: 200, epoch: 6 | loss: 0.0157104\n",
      "\tspeed: 0.1400s/iter; left time: 444.6285s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:30.50s\n",
      "Steps: 225 | Train Loss: 0.0180643 Vali Loss: 0.0164044 Test Loss: 0.0186259\n",
      "Validation loss decreased (0.016765 --> 0.016404).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0171250\n",
      "\tspeed: 0.2491s/iter; left time: 759.9347s\n",
      "\titers: 200, epoch: 7 | loss: 0.0176361\n",
      "\tspeed: 0.1562s/iter; left time: 460.9156s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.68s\n",
      "Steps: 225 | Train Loss: 0.0178018 Vali Loss: 0.0165095 Test Loss: 0.0187795\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0174236\n",
      "\tspeed: 0.2480s/iter; left time: 700.8354s\n",
      "\titers: 200, epoch: 8 | loss: 0.0183114\n",
      "\tspeed: 0.1521s/iter; left time: 414.5438s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.28s\n",
      "Steps: 225 | Train Loss: 0.0176159 Vali Loss: 0.0161470 Test Loss: 0.0184036\n",
      "Validation loss decreased (0.016404 --> 0.016147).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0173728\n",
      "\tspeed: 0.2541s/iter; left time: 660.8742s\n",
      "\titers: 200, epoch: 9 | loss: 0.0183281\n",
      "\tspeed: 0.1540s/iter; left time: 385.1839s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:34.84s\n",
      "Steps: 225 | Train Loss: 0.0174204 Vali Loss: 0.0161837 Test Loss: 0.0187648\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0181503\n",
      "\tspeed: 0.2255s/iter; left time: 535.7545s\n",
      "\titers: 200, epoch: 10 | loss: 0.0167253\n",
      "\tspeed: 0.1254s/iter; left time: 285.3379s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:28.62s\n",
      "Steps: 225 | Train Loss: 0.0172617 Vali Loss: 0.0161325 Test Loss: 0.0182937\n",
      "Validation loss decreased (0.016147 --> 0.016133).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0158237\n",
      "\tspeed: 0.2069s/iter; left time: 445.0600s\n",
      "\titers: 200, epoch: 11 | loss: 0.0169237\n",
      "\tspeed: 0.1247s/iter; left time: 255.7746s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:28.30s\n",
      "Steps: 225 | Train Loss: 0.0171310 Vali Loss: 0.0159683 Test Loss: 0.0182468\n",
      "Validation loss decreased (0.016133 --> 0.015968).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0191050\n",
      "\tspeed: 0.2079s/iter; left time: 400.3237s\n",
      "\titers: 200, epoch: 12 | loss: 0.0163762\n",
      "\tspeed: 0.1247s/iter; left time: 227.6119s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:28.23s\n",
      "Steps: 225 | Train Loss: 0.0169672 Vali Loss: 0.0160046 Test Loss: 0.0183137\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0166489\n",
      "\tspeed: 0.2086s/iter; left time: 354.8625s\n",
      "\titers: 200, epoch: 13 | loss: 0.0174182\n",
      "\tspeed: 0.1259s/iter; left time: 201.5010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:28.49s\n",
      "Steps: 225 | Train Loss: 0.0168844 Vali Loss: 0.0159117 Test Loss: 0.0182683\n",
      "Validation loss decreased (0.015968 --> 0.015912).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0170752\n",
      "\tspeed: 0.2103s/iter; left time: 310.3465s\n",
      "\titers: 200, epoch: 14 | loss: 0.0175392\n",
      "\tspeed: 0.1259s/iter; left time: 173.2446s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:28.50s\n",
      "Steps: 225 | Train Loss: 0.0167426 Vali Loss: 0.0160454 Test Loss: 0.0181828\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0162328\n",
      "\tspeed: 0.2060s/iter; left time: 257.6706s\n",
      "\titers: 200, epoch: 15 | loss: 0.0161093\n",
      "\tspeed: 0.1258s/iter; left time: 144.8526s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:28.38s\n",
      "Steps: 225 | Train Loss: 0.0166730 Vali Loss: 0.0160248 Test Loss: 0.0183523\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0154793\n",
      "\tspeed: 0.2082s/iter; left time: 213.6553s\n",
      "\titers: 200, epoch: 16 | loss: 0.0169243\n",
      "\tspeed: 0.1252s/iter; left time: 115.9249s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:28.46s\n",
      "Steps: 225 | Train Loss: 0.0165999 Vali Loss: 0.0159579 Test Loss: 0.0183164\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0168139\n",
      "\tspeed: 0.2085s/iter; left time: 167.0029s\n",
      "\titers: 200, epoch: 17 | loss: 0.0175394\n",
      "\tspeed: 0.1245s/iter; left time: 87.2637s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:28.24s\n",
      "Steps: 225 | Train Loss: 0.0165133 Vali Loss: 0.0158993 Test Loss: 0.0181488\n",
      "Validation loss decreased (0.015912 --> 0.015899).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0155911\n",
      "\tspeed: 0.2067s/iter; left time: 119.0761s\n",
      "\titers: 200, epoch: 18 | loss: 0.0171316\n",
      "\tspeed: 0.1251s/iter; left time: 59.5527s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:28.30s\n",
      "Steps: 225 | Train Loss: 0.0164482 Vali Loss: 0.0158756 Test Loss: 0.0183023\n",
      "Validation loss decreased (0.015899 --> 0.015876).  Saving model ...\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0157789\n",
      "\tspeed: 0.2100s/iter; left time: 73.6933s\n",
      "\titers: 200, epoch: 19 | loss: 0.0163923\n",
      "\tspeed: 0.1257s/iter; left time: 31.5412s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:28.48s\n",
      "Steps: 225 | Train Loss: 0.0163893 Vali Loss: 0.0159697 Test Loss: 0.0182470\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0163167\n",
      "\tspeed: 0.2079s/iter; left time: 26.1928s\n",
      "\titers: 200, epoch: 20 | loss: 0.0180438\n",
      "\tspeed: 0.1258s/iter; left time: 3.2699s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:28.41s\n",
      "Steps: 225 | Train Loss: 0.0163222 Vali Loss: 0.0159263 Test Loss: 0.0181796\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018302302807569504, rmse:0.13528600335121155, mae:0.08310361206531525, rse:0.5115309357643127\n",
      "Intermediate time for IT and pred_len 96: 00h:12m:14.02s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0310567\n",
      "\tspeed: 0.1518s/iter; left time: 667.9006s\n",
      "\titers: 200, epoch: 1 | loss: 0.0283692\n",
      "\tspeed: 0.1260s/iter; left time: 541.8447s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:28.83s\n",
      "Steps: 225 | Train Loss: 0.0341425 Vali Loss: 0.0216124 Test Loss: 0.0231160\n",
      "Validation loss decreased (inf --> 0.021612).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0225683\n",
      "\tspeed: 0.2125s/iter; left time: 887.2381s\n",
      "\titers: 200, epoch: 2 | loss: 0.0218985\n",
      "\tspeed: 0.1263s/iter; left time: 514.7070s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:28.53s\n",
      "Steps: 225 | Train Loss: 0.0238823 Vali Loss: 0.0200806 Test Loss: 0.0214945\n",
      "Validation loss decreased (0.021612 --> 0.020081).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0211665\n",
      "\tspeed: 0.2094s/iter; left time: 827.2427s\n",
      "\titers: 200, epoch: 3 | loss: 0.0211268\n",
      "\tspeed: 0.1260s/iter; left time: 485.1794s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:28.52s\n",
      "Steps: 225 | Train Loss: 0.0216721 Vali Loss: 0.0187043 Test Loss: 0.0206154\n",
      "Validation loss decreased (0.020081 --> 0.018704).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0219336\n",
      "\tspeed: 0.2095s/iter; left time: 780.7758s\n",
      "\titers: 200, epoch: 4 | loss: 0.0206096\n",
      "\tspeed: 0.1255s/iter; left time: 455.2306s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:28.44s\n",
      "Steps: 225 | Train Loss: 0.0207172 Vali Loss: 0.0189361 Test Loss: 0.0205644\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0207074\n",
      "\tspeed: 0.2112s/iter; left time: 739.3942s\n",
      "\titers: 200, epoch: 5 | loss: 0.0205898\n",
      "\tspeed: 0.1256s/iter; left time: 427.1883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:28.52s\n",
      "Steps: 225 | Train Loss: 0.0202865 Vali Loss: 0.0184972 Test Loss: 0.0207886\n",
      "Validation loss decreased (0.018704 --> 0.018497).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0206845\n",
      "\tspeed: 0.2087s/iter; left time: 683.6828s\n",
      "\titers: 200, epoch: 6 | loss: 0.0188039\n",
      "\tspeed: 0.1255s/iter; left time: 398.4385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:28.38s\n",
      "Steps: 225 | Train Loss: 0.0199052 Vali Loss: 0.0181741 Test Loss: 0.0200713\n",
      "Validation loss decreased (0.018497 --> 0.018174).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0188000\n",
      "\tspeed: 0.2122s/iter; left time: 647.4822s\n",
      "\titers: 200, epoch: 7 | loss: 0.0204586\n",
      "\tspeed: 0.1259s/iter; left time: 371.6542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:28.63s\n",
      "Steps: 225 | Train Loss: 0.0196300 Vali Loss: 0.0178590 Test Loss: 0.0197266\n",
      "Validation loss decreased (0.018174 --> 0.017859).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0192003\n",
      "\tspeed: 0.2116s/iter; left time: 598.0552s\n",
      "\titers: 200, epoch: 8 | loss: 0.0189526\n",
      "\tspeed: 0.1258s/iter; left time: 342.8997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:28.49s\n",
      "Steps: 225 | Train Loss: 0.0193939 Vali Loss: 0.0182576 Test Loss: 0.0201084\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0203683\n",
      "\tspeed: 0.2093s/iter; left time: 544.4171s\n",
      "\titers: 200, epoch: 9 | loss: 0.0183637\n",
      "\tspeed: 0.1261s/iter; left time: 315.4629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:28.66s\n",
      "Steps: 225 | Train Loss: 0.0191739 Vali Loss: 0.0178613 Test Loss: 0.0197537\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0180912\n",
      "\tspeed: 0.2116s/iter; left time: 502.8547s\n",
      "\titers: 200, epoch: 10 | loss: 0.0195543\n",
      "\tspeed: 0.1260s/iter; left time: 286.7414s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:28.56s\n",
      "Steps: 225 | Train Loss: 0.0190360 Vali Loss: 0.0177946 Test Loss: 0.0196573\n",
      "Validation loss decreased (0.017859 --> 0.017795).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0182367\n",
      "\tspeed: 0.2107s/iter; left time: 453.1540s\n",
      "\titers: 200, epoch: 11 | loss: 0.0190966\n",
      "\tspeed: 0.1260s/iter; left time: 258.5183s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:28.52s\n",
      "Steps: 225 | Train Loss: 0.0189143 Vali Loss: 0.0179626 Test Loss: 0.0195450\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0194587\n",
      "\tspeed: 0.2092s/iter; left time: 402.8617s\n",
      "\titers: 200, epoch: 12 | loss: 0.0180350\n",
      "\tspeed: 0.1257s/iter; left time: 229.4437s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:28.56s\n",
      "Steps: 225 | Train Loss: 0.0188081 Vali Loss: 0.0177290 Test Loss: 0.0194388\n",
      "Validation loss decreased (0.017795 --> 0.017729).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0183054\n",
      "\tspeed: 0.2124s/iter; left time: 361.3455s\n",
      "\titers: 200, epoch: 13 | loss: 0.0174135\n",
      "\tspeed: 0.1257s/iter; left time: 201.1698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:28.55s\n",
      "Steps: 225 | Train Loss: 0.0186785 Vali Loss: 0.0178206 Test Loss: 0.0194642\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0184357\n",
      "\tspeed: 0.2086s/iter; left time: 307.8606s\n",
      "\titers: 200, epoch: 14 | loss: 0.0191077\n",
      "\tspeed: 0.1255s/iter; left time: 172.6878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:28.49s\n",
      "Steps: 225 | Train Loss: 0.0186044 Vali Loss: 0.0177682 Test Loss: 0.0196619\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0174899\n",
      "\tspeed: 0.2093s/iter; left time: 261.7776s\n",
      "\titers: 200, epoch: 15 | loss: 0.0180370\n",
      "\tspeed: 0.1257s/iter; left time: 144.6639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:28.56s\n",
      "Steps: 225 | Train Loss: 0.0185151 Vali Loss: 0.0177840 Test Loss: 0.0194913\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0185360\n",
      "\tspeed: 0.2119s/iter; left time: 217.3935s\n",
      "\titers: 200, epoch: 16 | loss: 0.0188540\n",
      "\tspeed: 0.1260s/iter; left time: 116.6756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:28.65s\n",
      "Steps: 225 | Train Loss: 0.0184078 Vali Loss: 0.0178775 Test Loss: 0.0197945\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0178400\n",
      "\tspeed: 0.2105s/iter; left time: 168.6005s\n",
      "\titers: 200, epoch: 17 | loss: 0.0181476\n",
      "\tspeed: 0.1261s/iter; left time: 88.3857s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:28.57s\n",
      "Steps: 225 | Train Loss: 0.0183320 Vali Loss: 0.0178545 Test Loss: 0.0195108\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019438760355114937, rmse:0.1394229531288147, mae:0.08769001066684723, rse:0.5276631116867065\n",
      "Intermediate time for IT and pred_len 168: 00h:09m:45.11s\n",
      "Intermediate time for IT: 00h:35m:17.46s\n",
      "Total time: 02h:49m:02.19s\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list\n",
    "patchtst_results = []\n",
    "\n",
    "patch_len = 1\n",
    "stride = 1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_patching.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            if seq_len == 512:\n",
    "                batch_size = 64\n",
    "            else:\n",
    "                batch_size = 128\n",
    "                \n",
    "            model_id = f\"no_patching_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --patch_len {patch_len} \\\n",
    "              --stride {stride} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">- P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.1882</td>\n",
       "      <td>0.1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.1939</td>\n",
       "      <td>0.1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1463</td>\n",
       "      <td>0.0981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.0599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.0858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.0911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>0.1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.1353</td>\n",
       "      <td>0.0831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>0.0877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                - P                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0219  0.1480  0.0950\n",
       "        96        0.0354  0.1882  0.1317\n",
       "        168       0.0376  0.1939  0.1365\n",
       "ES      24        0.0106  0.1027  0.0648\n",
       "        96        0.0193  0.1388  0.0923\n",
       "        168       0.0214  0.1463  0.0981\n",
       "FR      24        0.0109  0.1042  0.0599\n",
       "        96        0.0200  0.1415  0.0858\n",
       "        168       0.0220  0.1483  0.0911\n",
       "GB      24        0.0257  0.1604  0.1057\n",
       "        96        0.0403  0.2007  0.1415\n",
       "        168       0.0431  0.2075  0.1470\n",
       "IT      24        0.0102  0.1012  0.0607\n",
       "        96        0.0183  0.1353  0.0831\n",
       "        168       0.0194  0.1394  0.0877"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['- P'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_no_patching.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0237356\n",
      "\tspeed: 0.0630s/iter; left time: 1416.7940s\n",
      "\titers: 200, epoch: 1 | loss: 0.0166661\n",
      "\tspeed: 0.0352s/iter; left time: 787.6332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.13s\n",
      "Steps: 226 | Train Loss: 0.0230923 Vali Loss: 0.0238321 Test Loss: 0.0298794\n",
      "Validation loss decreased (inf --> 0.023832).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0093542\n",
      "\tspeed: 0.0549s/iter; left time: 1222.3188s\n",
      "\titers: 200, epoch: 2 | loss: 0.0079462\n",
      "\tspeed: 0.0307s/iter; left time: 681.6420s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.84s\n",
      "Steps: 226 | Train Loss: 0.0099565 Vali Loss: 0.0107128 Test Loss: 0.0120591\n",
      "Validation loss decreased (0.023832 --> 0.010713).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0080557\n",
      "\tspeed: 0.0575s/iter; left time: 1267.1122s\n",
      "\titers: 200, epoch: 3 | loss: 0.0073414\n",
      "\tspeed: 0.0325s/iter; left time: 714.3936s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0072211 Vali Loss: 0.0097881 Test Loss: 0.0109410\n",
      "Validation loss decreased (0.010713 --> 0.009788).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0072077\n",
      "\tspeed: 0.0547s/iter; left time: 1193.3030s\n",
      "\titers: 200, epoch: 4 | loss: 0.0052147\n",
      "\tspeed: 0.0241s/iter; left time: 523.1088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0066755 Vali Loss: 0.0095442 Test Loss: 0.0107035\n",
      "Validation loss decreased (0.009788 --> 0.009544).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0054814\n",
      "\tspeed: 0.0541s/iter; left time: 1167.6294s\n",
      "\titers: 200, epoch: 5 | loss: 0.0061579\n",
      "\tspeed: 0.0251s/iter; left time: 539.1269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0063989 Vali Loss: 0.0092577 Test Loss: 0.0103496\n",
      "Validation loss decreased (0.009544 --> 0.009258).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0062205\n",
      "\tspeed: 0.0579s/iter; left time: 1237.9695s\n",
      "\titers: 200, epoch: 6 | loss: 0.0067256\n",
      "\tspeed: 0.0344s/iter; left time: 730.9275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 226 | Train Loss: 0.0062187 Vali Loss: 0.0091027 Test Loss: 0.0102357\n",
      "Validation loss decreased (0.009258 --> 0.009103).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0060843\n",
      "\tspeed: 0.0654s/iter; left time: 1382.3597s\n",
      "\titers: 200, epoch: 7 | loss: 0.0056078\n",
      "\tspeed: 0.0286s/iter; left time: 602.7052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.73s\n",
      "Steps: 226 | Train Loss: 0.0060805 Vali Loss: 0.0089617 Test Loss: 0.0101414\n",
      "Validation loss decreased (0.009103 --> 0.008962).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0050162\n",
      "\tspeed: 0.0539s/iter; left time: 1127.4219s\n",
      "\titers: 200, epoch: 8 | loss: 0.0051762\n",
      "\tspeed: 0.0289s/iter; left time: 601.8684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.76s\n",
      "Steps: 226 | Train Loss: 0.0059466 Vali Loss: 0.0089266 Test Loss: 0.0100621\n",
      "Validation loss decreased (0.008962 --> 0.008927).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0043858\n",
      "\tspeed: 0.0627s/iter; left time: 1297.6389s\n",
      "\titers: 200, epoch: 9 | loss: 0.0063614\n",
      "\tspeed: 0.0331s/iter; left time: 681.7924s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.80s\n",
      "Steps: 226 | Train Loss: 0.0058522 Vali Loss: 0.0088418 Test Loss: 0.0099862\n",
      "Validation loss decreased (0.008927 --> 0.008842).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0063100\n",
      "\tspeed: 0.0556s/iter; left time: 1137.8350s\n",
      "\titers: 200, epoch: 10 | loss: 0.0061891\n",
      "\tspeed: 0.0296s/iter; left time: 602.3603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.86s\n",
      "Steps: 226 | Train Loss: 0.0057647 Vali Loss: 0.0088251 Test Loss: 0.0100708\n",
      "Validation loss decreased (0.008842 --> 0.008825).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0067617\n",
      "\tspeed: 0.0538s/iter; left time: 1089.2481s\n",
      "\titers: 200, epoch: 11 | loss: 0.0066305\n",
      "\tspeed: 0.0268s/iter; left time: 540.6986s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 226 | Train Loss: 0.0056819 Vali Loss: 0.0087484 Test Loss: 0.0100197\n",
      "Validation loss decreased (0.008825 --> 0.008748).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0060824\n",
      "\tspeed: 0.0518s/iter; left time: 1036.1625s\n",
      "\titers: 200, epoch: 12 | loss: 0.0058182\n",
      "\tspeed: 0.0270s/iter; left time: 538.3675s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 226 | Train Loss: 0.0055992 Vali Loss: 0.0087655 Test Loss: 0.0100026\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0064740\n",
      "\tspeed: 0.0519s/iter; left time: 1027.6831s\n",
      "\titers: 200, epoch: 13 | loss: 0.0052136\n",
      "\tspeed: 0.0275s/iter; left time: 541.6869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0055461 Vali Loss: 0.0087297 Test Loss: 0.0100234\n",
      "Validation loss decreased (0.008748 --> 0.008730).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0054593\n",
      "\tspeed: 0.0544s/iter; left time: 1063.8500s\n",
      "\titers: 200, epoch: 14 | loss: 0.0066011\n",
      "\tspeed: 0.0277s/iter; left time: 538.9120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.52s\n",
      "Steps: 226 | Train Loss: 0.0055118 Vali Loss: 0.0087377 Test Loss: 0.0100422\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0048896\n",
      "\tspeed: 0.0587s/iter; left time: 1134.3386s\n",
      "\titers: 200, epoch: 15 | loss: 0.0061342\n",
      "\tspeed: 0.0299s/iter; left time: 575.3251s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 226 | Train Loss: 0.0054799 Vali Loss: 0.0087377 Test Loss: 0.0099943\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0055380\n",
      "\tspeed: 0.0584s/iter; left time: 1115.4190s\n",
      "\titers: 200, epoch: 16 | loss: 0.0058325\n",
      "\tspeed: 0.0248s/iter; left time: 471.3572s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 226 | Train Loss: 0.0054355 Vali Loss: 0.0086340 Test Loss: 0.0099511\n",
      "Validation loss decreased (0.008730 --> 0.008634).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0056160\n",
      "\tspeed: 0.0604s/iter; left time: 1141.4406s\n",
      "\titers: 200, epoch: 17 | loss: 0.0052510\n",
      "\tspeed: 0.0298s/iter; left time: 559.4422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0054082 Vali Loss: 0.0086247 Test Loss: 0.0099653\n",
      "Validation loss decreased (0.008634 --> 0.008625).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0053355\n",
      "\tspeed: 0.0577s/iter; left time: 1075.9685s\n",
      "\titers: 200, epoch: 18 | loss: 0.0048924\n",
      "\tspeed: 0.0280s/iter; left time: 519.5674s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.90s\n",
      "Steps: 226 | Train Loss: 0.0053938 Vali Loss: 0.0086484 Test Loss: 0.0099769\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0055976\n",
      "\tspeed: 0.0601s/iter; left time: 1108.6693s\n",
      "\titers: 200, epoch: 19 | loss: 0.0050728\n",
      "\tspeed: 0.0374s/iter; left time: 684.8314s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.46s\n",
      "Steps: 226 | Train Loss: 0.0053646 Vali Loss: 0.0086138 Test Loss: 0.0099554\n",
      "Validation loss decreased (0.008625 --> 0.008614).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0056121\n",
      "\tspeed: 0.0606s/iter; left time: 1102.4992s\n",
      "\titers: 200, epoch: 20 | loss: 0.0047978\n",
      "\tspeed: 0.0271s/iter; left time: 491.5696s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.10s\n",
      "Steps: 226 | Train Loss: 0.0053453 Vali Loss: 0.0086795 Test Loss: 0.0099502\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0052046\n",
      "\tspeed: 0.0548s/iter; left time: 985.3313s\n",
      "\titers: 200, epoch: 21 | loss: 0.0055206\n",
      "\tspeed: 0.0274s/iter; left time: 489.2986s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0053154 Vali Loss: 0.0086463 Test Loss: 0.0099572\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0047986\n",
      "\tspeed: 0.0528s/iter; left time: 938.0809s\n",
      "\titers: 200, epoch: 22 | loss: 0.0047735\n",
      "\tspeed: 0.0283s/iter; left time: 500.1586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.06s\n",
      "Steps: 226 | Train Loss: 0.0052983 Vali Loss: 0.0086482 Test Loss: 0.0099361\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0054964\n",
      "\tspeed: 0.0587s/iter; left time: 1029.2893s\n",
      "\titers: 200, epoch: 23 | loss: 0.0057613\n",
      "\tspeed: 0.0235s/iter; left time: 410.0671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 226 | Train Loss: 0.0052910 Vali Loss: 0.0086885 Test Loss: 0.0099816\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0051943\n",
      "\tspeed: 0.0497s/iter; left time: 860.1582s\n",
      "\titers: 200, epoch: 24 | loss: 0.0046766\n",
      "\tspeed: 0.0264s/iter; left time: 453.8627s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 226 | Train Loss: 0.0052728 Vali Loss: 0.0086597 Test Loss: 0.0099599\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0048268\n",
      "\tspeed: 0.0502s/iter; left time: 857.0807s\n",
      "\titers: 200, epoch: 25 | loss: 0.0055762\n",
      "\tspeed: 0.0287s/iter; left time: 487.7496s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 226 | Train Loss: 0.0052606 Vali Loss: 0.0086038 Test Loss: 0.0099352\n",
      "Validation loss decreased (0.008614 --> 0.008604).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0054199\n",
      "\tspeed: 0.0624s/iter; left time: 1051.8108s\n",
      "\titers: 200, epoch: 26 | loss: 0.0047658\n",
      "\tspeed: 0.0317s/iter; left time: 530.4635s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.51s\n",
      "Steps: 226 | Train Loss: 0.0052556 Vali Loss: 0.0086673 Test Loss: 0.0099702\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0054049\n",
      "\tspeed: 0.0532s/iter; left time: 883.9730s\n",
      "\titers: 200, epoch: 27 | loss: 0.0052203\n",
      "\tspeed: 0.0294s/iter; left time: 485.7770s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:06.80s\n",
      "Steps: 226 | Train Loss: 0.0052431 Vali Loss: 0.0086526 Test Loss: 0.0099488\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0051072\n",
      "\tspeed: 0.0580s/iter; left time: 950.9519s\n",
      "\titers: 200, epoch: 28 | loss: 0.0047350\n",
      "\tspeed: 0.0249s/iter; left time: 405.5559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 226 | Train Loss: 0.0052365 Vali Loss: 0.0086557 Test Loss: 0.0099640\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0052623\n",
      "\tspeed: 0.0535s/iter; left time: 865.1883s\n",
      "\titers: 200, epoch: 29 | loss: 0.0066183\n",
      "\tspeed: 0.0315s/iter; left time: 506.6605s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 226 | Train Loss: 0.0052327 Vali Loss: 0.0086441 Test Loss: 0.0099638\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0041976\n",
      "\tspeed: 0.0562s/iter; left time: 895.9772s\n",
      "\titers: 200, epoch: 30 | loss: 0.0064455\n",
      "\tspeed: 0.0327s/iter; left time: 517.6012s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 226 | Train Loss: 0.0052169 Vali Loss: 0.0086797 Test Loss: 0.0099548\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0047889\n",
      "\tspeed: 0.0549s/iter; left time: 863.2743s\n",
      "\titers: 200, epoch: 31 | loss: 0.0054666\n",
      "\tspeed: 0.0303s/iter; left time: 472.7757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 226 | Train Loss: 0.0052050 Vali Loss: 0.0086171 Test Loss: 0.0099502\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0052385\n",
      "\tspeed: 0.0594s/iter; left time: 920.9677s\n",
      "\titers: 200, epoch: 32 | loss: 0.0052754\n",
      "\tspeed: 0.0264s/iter; left time: 405.8018s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.77s\n",
      "Steps: 226 | Train Loss: 0.0052094 Vali Loss: 0.0086470 Test Loss: 0.0099568\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0048150\n",
      "\tspeed: 0.0542s/iter; left time: 827.8754s\n",
      "\titers: 200, epoch: 33 | loss: 0.0051399\n",
      "\tspeed: 0.0270s/iter; left time: 409.0242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.54s\n",
      "Steps: 226 | Train Loss: 0.0052055 Vali Loss: 0.0086455 Test Loss: 0.0099734\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0053405\n",
      "\tspeed: 0.0544s/iter; left time: 818.6350s\n",
      "\titers: 200, epoch: 34 | loss: 0.0056786\n",
      "\tspeed: 0.0247s/iter; left time: 369.4467s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 226 | Train Loss: 0.0052045 Vali Loss: 0.0086125 Test Loss: 0.0099662\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0055080\n",
      "\tspeed: 0.0579s/iter; left time: 858.3640s\n",
      "\titers: 200, epoch: 35 | loss: 0.0048850\n",
      "\tspeed: 0.0247s/iter; left time: 363.5174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.73s\n",
      "Steps: 226 | Train Loss: 0.0051874 Vali Loss: 0.0086392 Test Loss: 0.0099633\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009935157373547554, rmse:0.0996752604842186, mae:0.056855376809835434, rse:0.38454437255859375\n",
      "Intermediate time for FR and pred_len 24: 00h:05m:09.03s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0254300\n",
      "\tspeed: 0.0466s/iter; left time: 1044.5209s\n",
      "\titers: 200, epoch: 1 | loss: 0.0226345\n",
      "\tspeed: 0.0276s/iter; left time: 616.1690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 225 | Train Loss: 0.0254588 Vali Loss: 0.0273043 Test Loss: 0.0352626\n",
      "Validation loss decreased (inf --> 0.027304).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0142459\n",
      "\tspeed: 0.0601s/iter; left time: 1333.5906s\n",
      "\titers: 200, epoch: 2 | loss: 0.0128824\n",
      "\tspeed: 0.0366s/iter; left time: 807.4442s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.98s\n",
      "Steps: 225 | Train Loss: 0.0145184 Vali Loss: 0.0163889 Test Loss: 0.0206476\n",
      "Validation loss decreased (0.027304 --> 0.016389).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0115962\n",
      "\tspeed: 0.0549s/iter; left time: 1205.2686s\n",
      "\titers: 200, epoch: 3 | loss: 0.0122913\n",
      "\tspeed: 0.0310s/iter; left time: 677.5091s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.54s\n",
      "Steps: 225 | Train Loss: 0.0118228 Vali Loss: 0.0154873 Test Loss: 0.0198312\n",
      "Validation loss decreased (0.016389 --> 0.015487).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120922\n",
      "\tspeed: 0.0623s/iter; left time: 1353.8541s\n",
      "\titers: 200, epoch: 4 | loss: 0.0113413\n",
      "\tspeed: 0.0300s/iter; left time: 647.8548s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0112676 Vali Loss: 0.0152080 Test Loss: 0.0196720\n",
      "Validation loss decreased (0.015487 --> 0.015208).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0102608\n",
      "\tspeed: 0.0648s/iter; left time: 1393.2321s\n",
      "\titers: 200, epoch: 5 | loss: 0.0104193\n",
      "\tspeed: 0.0377s/iter; left time: 806.3203s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.41s\n",
      "Steps: 225 | Train Loss: 0.0109707 Vali Loss: 0.0150341 Test Loss: 0.0193699\n",
      "Validation loss decreased (0.015208 --> 0.015034).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0108786\n",
      "\tspeed: 0.0612s/iter; left time: 1303.0145s\n",
      "\titers: 200, epoch: 6 | loss: 0.0100191\n",
      "\tspeed: 0.0305s/iter; left time: 645.2362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 225 | Train Loss: 0.0106828 Vali Loss: 0.0148039 Test Loss: 0.0193114\n",
      "Validation loss decreased (0.015034 --> 0.014804).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0110777\n",
      "\tspeed: 0.0646s/iter; left time: 1359.5216s\n",
      "\titers: 200, epoch: 7 | loss: 0.0127516\n",
      "\tspeed: 0.0340s/iter; left time: 712.6072s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 225 | Train Loss: 0.0104120 Vali Loss: 0.0146961 Test Loss: 0.0191919\n",
      "Validation loss decreased (0.014804 --> 0.014696).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0092179\n",
      "\tspeed: 0.0595s/iter; left time: 1240.1219s\n",
      "\titers: 200, epoch: 8 | loss: 0.0089943\n",
      "\tspeed: 0.0282s/iter; left time: 584.1662s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.91s\n",
      "Steps: 225 | Train Loss: 0.0101818 Vali Loss: 0.0145038 Test Loss: 0.0191868\n",
      "Validation loss decreased (0.014696 --> 0.014504).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0098994\n",
      "\tspeed: 0.0581s/iter; left time: 1196.0263s\n",
      "\titers: 200, epoch: 9 | loss: 0.0097919\n",
      "\tspeed: 0.0307s/iter; left time: 629.9677s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.97s\n",
      "Steps: 225 | Train Loss: 0.0100057 Vali Loss: 0.0145380 Test Loss: 0.0191176\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0096034\n",
      "\tspeed: 0.0547s/iter; left time: 1114.2371s\n",
      "\titers: 200, epoch: 10 | loss: 0.0100670\n",
      "\tspeed: 0.0273s/iter; left time: 552.5516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 225 | Train Loss: 0.0098874 Vali Loss: 0.0145757 Test Loss: 0.0192410\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0098913\n",
      "\tspeed: 0.0588s/iter; left time: 1185.8312s\n",
      "\titers: 200, epoch: 11 | loss: 0.0097004\n",
      "\tspeed: 0.0306s/iter; left time: 614.5099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.30s\n",
      "Steps: 225 | Train Loss: 0.0097905 Vali Loss: 0.0145232 Test Loss: 0.0191619\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0081249\n",
      "\tspeed: 0.0618s/iter; left time: 1231.0164s\n",
      "\titers: 200, epoch: 12 | loss: 0.0093403\n",
      "\tspeed: 0.0350s/iter; left time: 694.0710s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.23s\n",
      "Steps: 225 | Train Loss: 0.0096884 Vali Loss: 0.0146157 Test Loss: 0.0192267\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0094197\n",
      "\tspeed: 0.0696s/iter; left time: 1370.3090s\n",
      "\titers: 200, epoch: 13 | loss: 0.0094363\n",
      "\tspeed: 0.0310s/iter; left time: 607.9622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 225 | Train Loss: 0.0095827 Vali Loss: 0.0145366 Test Loss: 0.0191297\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0100738\n",
      "\tspeed: 0.0626s/iter; left time: 1218.8052s\n",
      "\titers: 200, epoch: 14 | loss: 0.0103495\n",
      "\tspeed: 0.0325s/iter; left time: 630.1532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0094880 Vali Loss: 0.0146680 Test Loss: 0.0193109\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0093224\n",
      "\tspeed: 0.0582s/iter; left time: 1119.8090s\n",
      "\titers: 200, epoch: 15 | loss: 0.0091487\n",
      "\tspeed: 0.0327s/iter; left time: 626.3071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.72s\n",
      "Steps: 225 | Train Loss: 0.0093999 Vali Loss: 0.0145867 Test Loss: 0.0193278\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0093353\n",
      "\tspeed: 0.0560s/iter; left time: 1065.4500s\n",
      "\titers: 200, epoch: 16 | loss: 0.0092124\n",
      "\tspeed: 0.0249s/iter; left time: 471.7984s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.30s\n",
      "Steps: 225 | Train Loss: 0.0093282 Vali Loss: 0.0145779 Test Loss: 0.0191555\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0087824\n",
      "\tspeed: 0.0517s/iter; left time: 972.2200s\n",
      "\titers: 200, epoch: 17 | loss: 0.0086737\n",
      "\tspeed: 0.0283s/iter; left time: 529.2222s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 225 | Train Loss: 0.0092395 Vali Loss: 0.0146747 Test Loss: 0.0192166\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0088149\n",
      "\tspeed: 0.0655s/iter; left time: 1216.3936s\n",
      "\titers: 200, epoch: 18 | loss: 0.0099954\n",
      "\tspeed: 0.0264s/iter; left time: 487.3639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0091820 Vali Loss: 0.0147064 Test Loss: 0.0192700\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019186770543456078, rmse:0.13851632177829742, mae:0.08384789526462555, rse:0.5358179211616516\n",
      "Intermediate time for FR and pred_len 96: 00h:02m:53.05s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0269169\n",
      "\tspeed: 0.0525s/iter; left time: 1177.1305s\n",
      "\titers: 200, epoch: 1 | loss: 0.0220217\n",
      "\tspeed: 0.0244s/iter; left time: 543.9332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.12s\n",
      "Steps: 225 | Train Loss: 0.0276227 Vali Loss: 0.0292230 Test Loss: 0.0370285\n",
      "Validation loss decreased (inf --> 0.029223).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0149196\n",
      "\tspeed: 0.0560s/iter; left time: 1241.4105s\n",
      "\titers: 200, epoch: 2 | loss: 0.0117432\n",
      "\tspeed: 0.0302s/iter; left time: 666.2404s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 225 | Train Loss: 0.0159841 Vali Loss: 0.0176442 Test Loss: 0.0221663\n",
      "Validation loss decreased (0.029223 --> 0.017644).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0130947\n",
      "\tspeed: 0.0558s/iter; left time: 1225.4648s\n",
      "\titers: 200, epoch: 3 | loss: 0.0118973\n",
      "\tspeed: 0.0245s/iter; left time: 534.9855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 225 | Train Loss: 0.0131677 Vali Loss: 0.0168402 Test Loss: 0.0214203\n",
      "Validation loss decreased (0.017644 --> 0.016840).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120702\n",
      "\tspeed: 0.0516s/iter; left time: 1120.5532s\n",
      "\titers: 200, epoch: 4 | loss: 0.0153237\n",
      "\tspeed: 0.0275s/iter; left time: 595.6620s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 225 | Train Loss: 0.0126011 Vali Loss: 0.0166417 Test Loss: 0.0214589\n",
      "Validation loss decreased (0.016840 --> 0.016642).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0122577\n",
      "\tspeed: 0.0591s/iter; left time: 1271.3524s\n",
      "\titers: 200, epoch: 5 | loss: 0.0114579\n",
      "\tspeed: 0.0310s/iter; left time: 663.5821s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.94s\n",
      "Steps: 225 | Train Loss: 0.0122536 Vali Loss: 0.0165372 Test Loss: 0.0212060\n",
      "Validation loss decreased (0.016642 --> 0.016537).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0122982\n",
      "\tspeed: 0.0553s/iter; left time: 1177.0684s\n",
      "\titers: 200, epoch: 6 | loss: 0.0108476\n",
      "\tspeed: 0.0239s/iter; left time: 505.1855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 225 | Train Loss: 0.0119525 Vali Loss: 0.0163623 Test Loss: 0.0210994\n",
      "Validation loss decreased (0.016537 --> 0.016362).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0103870\n",
      "\tspeed: 0.0535s/iter; left time: 1125.2335s\n",
      "\titers: 200, epoch: 7 | loss: 0.0120801\n",
      "\tspeed: 0.0255s/iter; left time: 534.3685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 225 | Train Loss: 0.0116693 Vali Loss: 0.0163048 Test Loss: 0.0210180\n",
      "Validation loss decreased (0.016362 --> 0.016305).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0126737\n",
      "\tspeed: 0.0701s/iter; left time: 1458.8931s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116746\n",
      "\tspeed: 0.0261s/iter; left time: 540.1608s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.55s\n",
      "Steps: 225 | Train Loss: 0.0114354 Vali Loss: 0.0161684 Test Loss: 0.0209229\n",
      "Validation loss decreased (0.016305 --> 0.016168).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0114568\n",
      "\tspeed: 0.0590s/iter; left time: 1215.5890s\n",
      "\titers: 200, epoch: 9 | loss: 0.0103613\n",
      "\tspeed: 0.0258s/iter; left time: 528.1320s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 225 | Train Loss: 0.0112451 Vali Loss: 0.0162233 Test Loss: 0.0209913\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0106115\n",
      "\tspeed: 0.0555s/iter; left time: 1131.7468s\n",
      "\titers: 200, epoch: 10 | loss: 0.0107473\n",
      "\tspeed: 0.0241s/iter; left time: 489.1191s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.23s\n",
      "Steps: 225 | Train Loss: 0.0110886 Vali Loss: 0.0161682 Test Loss: 0.0208051\n",
      "Validation loss decreased (0.016168 --> 0.016168).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0095515\n",
      "\tspeed: 0.0610s/iter; left time: 1228.3944s\n",
      "\titers: 200, epoch: 11 | loss: 0.0108994\n",
      "\tspeed: 0.0301s/iter; left time: 604.2637s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.24s\n",
      "Steps: 225 | Train Loss: 0.0109432 Vali Loss: 0.0162758 Test Loss: 0.0210103\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0102318\n",
      "\tspeed: 0.0624s/iter; left time: 1243.9470s\n",
      "\titers: 200, epoch: 12 | loss: 0.0117106\n",
      "\tspeed: 0.0308s/iter; left time: 611.2043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.24s\n",
      "Steps: 225 | Train Loss: 0.0108397 Vali Loss: 0.0163587 Test Loss: 0.0211157\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0103174\n",
      "\tspeed: 0.0552s/iter; left time: 1087.1800s\n",
      "\titers: 200, epoch: 13 | loss: 0.0115475\n",
      "\tspeed: 0.0374s/iter; left time: 732.2872s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 225 | Train Loss: 0.0107320 Vali Loss: 0.0163240 Test Loss: 0.0209834\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0106057\n",
      "\tspeed: 0.0588s/iter; left time: 1145.3310s\n",
      "\titers: 200, epoch: 14 | loss: 0.0110867\n",
      "\tspeed: 0.0269s/iter; left time: 520.3227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.88s\n",
      "Steps: 225 | Train Loss: 0.0106457 Vali Loss: 0.0163372 Test Loss: 0.0208973\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0099765\n",
      "\tspeed: 0.0594s/iter; left time: 1143.2446s\n",
      "\titers: 200, epoch: 15 | loss: 0.0116516\n",
      "\tspeed: 0.0275s/iter; left time: 526.7832s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.46s\n",
      "Steps: 225 | Train Loss: 0.0105807 Vali Loss: 0.0163443 Test Loss: 0.0209021\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0096081\n",
      "\tspeed: 0.0465s/iter; left time: 885.4802s\n",
      "\titers: 200, epoch: 16 | loss: 0.0116073\n",
      "\tspeed: 0.0250s/iter; left time: 472.4955s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.82s\n",
      "Steps: 225 | Train Loss: 0.0105108 Vali Loss: 0.0164842 Test Loss: 0.0211343\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0115879\n",
      "\tspeed: 0.0568s/iter; left time: 1067.3560s\n",
      "\titers: 200, epoch: 17 | loss: 0.0106102\n",
      "\tspeed: 0.0367s/iter; left time: 685.6871s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.47s\n",
      "Steps: 225 | Train Loss: 0.0104455 Vali Loss: 0.0163789 Test Loss: 0.0211149\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0106645\n",
      "\tspeed: 0.0610s/iter; left time: 1133.4480s\n",
      "\titers: 200, epoch: 18 | loss: 0.0108051\n",
      "\tspeed: 0.0297s/iter; left time: 548.4628s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.31s\n",
      "Steps: 225 | Train Loss: 0.0103969 Vali Loss: 0.0164130 Test Loss: 0.0211276\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0107501\n",
      "\tspeed: 0.0599s/iter; left time: 1098.6225s\n",
      "\titers: 200, epoch: 19 | loss: 0.0097857\n",
      "\tspeed: 0.0263s/iter; left time: 479.3413s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:06.84s\n",
      "Steps: 225 | Train Loss: 0.0103496 Vali Loss: 0.0164557 Test Loss: 0.0210654\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0101434\n",
      "\tspeed: 0.0603s/iter; left time: 1093.2678s\n",
      "\titers: 200, epoch: 20 | loss: 0.0112589\n",
      "\tspeed: 0.0242s/iter; left time: 435.6044s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0103155 Vali Loss: 0.0165480 Test Loss: 0.0210437\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.020805105566978455, rmse:0.14423975348472595, mae:0.08963675051927567, rse:0.5586541891098022\n",
      "Intermediate time for FR and pred_len 168: 00h:03m:00.74s\n",
      "Intermediate time for FR: 00h:11m:02.82s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0430156\n",
      "\tspeed: 0.0542s/iter; left time: 1219.3584s\n",
      "\titers: 200, epoch: 1 | loss: 0.0372121\n",
      "\tspeed: 0.0293s/iter; left time: 655.9818s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 226 | Train Loss: 0.0441541 Vali Loss: 0.0310717 Test Loss: 0.0345077\n",
      "Validation loss decreased (inf --> 0.031072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0152964\n",
      "\tspeed: 0.0515s/iter; left time: 1146.2866s\n",
      "\titers: 200, epoch: 2 | loss: 0.0113698\n",
      "\tspeed: 0.0255s/iter; left time: 565.9941s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.99s\n",
      "Steps: 226 | Train Loss: 0.0179926 Vali Loss: 0.0115744 Test Loss: 0.0126363\n",
      "Validation loss decreased (0.031072 --> 0.011574).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0125631\n",
      "\tspeed: 0.0575s/iter; left time: 1268.2801s\n",
      "\titers: 200, epoch: 3 | loss: 0.0114366\n",
      "\tspeed: 0.0266s/iter; left time: 583.1139s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.77s\n",
      "Steps: 226 | Train Loss: 0.0120551 Vali Loss: 0.0104010 Test Loss: 0.0114574\n",
      "Validation loss decreased (0.011574 --> 0.010401).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110885\n",
      "\tspeed: 0.0596s/iter; left time: 1300.6696s\n",
      "\titers: 200, epoch: 4 | loss: 0.0099184\n",
      "\tspeed: 0.0321s/iter; left time: 696.3817s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 226 | Train Loss: 0.0110524 Vali Loss: 0.0100088 Test Loss: 0.0111228\n",
      "Validation loss decreased (0.010401 --> 0.010009).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0098978\n",
      "\tspeed: 0.0611s/iter; left time: 1318.8958s\n",
      "\titers: 200, epoch: 5 | loss: 0.0116828\n",
      "\tspeed: 0.0310s/iter; left time: 666.8761s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.26s\n",
      "Steps: 226 | Train Loss: 0.0105525 Vali Loss: 0.0096835 Test Loss: 0.0107585\n",
      "Validation loss decreased (0.010009 --> 0.009683).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0114763\n",
      "\tspeed: 0.0553s/iter; left time: 1182.6344s\n",
      "\titers: 200, epoch: 6 | loss: 0.0109357\n",
      "\tspeed: 0.0339s/iter; left time: 721.2530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.39s\n",
      "Steps: 226 | Train Loss: 0.0102527 Vali Loss: 0.0095776 Test Loss: 0.0106026\n",
      "Validation loss decreased (0.009683 --> 0.009578).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0090321\n",
      "\tspeed: 0.0540s/iter; left time: 1142.2472s\n",
      "\titers: 200, epoch: 7 | loss: 0.0095975\n",
      "\tspeed: 0.0292s/iter; left time: 614.2884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 226 | Train Loss: 0.0099702 Vali Loss: 0.0094630 Test Loss: 0.0105136\n",
      "Validation loss decreased (0.009578 --> 0.009463).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0093714\n",
      "\tspeed: 0.0597s/iter; left time: 1247.9077s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081001\n",
      "\tspeed: 0.0391s/iter; left time: 814.6955s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.21s\n",
      "Steps: 226 | Train Loss: 0.0097985 Vali Loss: 0.0093102 Test Loss: 0.0103237\n",
      "Validation loss decreased (0.009463 --> 0.009310).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0094263\n",
      "\tspeed: 0.0521s/iter; left time: 1077.9675s\n",
      "\titers: 200, epoch: 9 | loss: 0.0099736\n",
      "\tspeed: 0.0325s/iter; left time: 669.7630s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.82s\n",
      "Steps: 226 | Train Loss: 0.0096688 Vali Loss: 0.0092797 Test Loss: 0.0102676\n",
      "Validation loss decreased (0.009310 --> 0.009280).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0085524\n",
      "\tspeed: 0.0598s/iter; left time: 1223.3745s\n",
      "\titers: 200, epoch: 10 | loss: 0.0101026\n",
      "\tspeed: 0.0242s/iter; left time: 492.8865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.74s\n",
      "Steps: 226 | Train Loss: 0.0095466 Vali Loss: 0.0093089 Test Loss: 0.0102406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0096203\n",
      "\tspeed: 0.0528s/iter; left time: 1068.9342s\n",
      "\titers: 200, epoch: 11 | loss: 0.0088058\n",
      "\tspeed: 0.0258s/iter; left time: 519.7897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 226 | Train Loss: 0.0094569 Vali Loss: 0.0092299 Test Loss: 0.0102465\n",
      "Validation loss decreased (0.009280 --> 0.009230).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0088285\n",
      "\tspeed: 0.0531s/iter; left time: 1062.7348s\n",
      "\titers: 200, epoch: 12 | loss: 0.0094550\n",
      "\tspeed: 0.0307s/iter; left time: 611.8542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.53s\n",
      "Steps: 226 | Train Loss: 0.0093679 Vali Loss: 0.0092287 Test Loss: 0.0101918\n",
      "Validation loss decreased (0.009230 --> 0.009229).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0094994\n",
      "\tspeed: 0.0581s/iter; left time: 1149.4300s\n",
      "\titers: 200, epoch: 13 | loss: 0.0095642\n",
      "\tspeed: 0.0254s/iter; left time: 499.4905s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 226 | Train Loss: 0.0093120 Vali Loss: 0.0091062 Test Loss: 0.0100900\n",
      "Validation loss decreased (0.009229 --> 0.009106).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0090722\n",
      "\tspeed: 0.0507s/iter; left time: 992.2097s\n",
      "\titers: 200, epoch: 14 | loss: 0.0097731\n",
      "\tspeed: 0.0283s/iter; left time: 550.1150s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0092447 Vali Loss: 0.0091206 Test Loss: 0.0100921\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0097098\n",
      "\tspeed: 0.0505s/iter; left time: 976.0067s\n",
      "\titers: 200, epoch: 15 | loss: 0.0098209\n",
      "\tspeed: 0.0253s/iter; left time: 487.1089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.06s\n",
      "Steps: 226 | Train Loss: 0.0091855 Vali Loss: 0.0090958 Test Loss: 0.0100805\n",
      "Validation loss decreased (0.009106 --> 0.009096).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0091442\n",
      "\tspeed: 0.0566s/iter; left time: 1081.6922s\n",
      "\titers: 200, epoch: 16 | loss: 0.0073901\n",
      "\tspeed: 0.0308s/iter; left time: 585.8950s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 226 | Train Loss: 0.0091526 Vali Loss: 0.0091489 Test Loss: 0.0100649\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0086832\n",
      "\tspeed: 0.0500s/iter; left time: 943.4212s\n",
      "\titers: 200, epoch: 17 | loss: 0.0081065\n",
      "\tspeed: 0.0297s/iter; left time: 557.3880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.65s\n",
      "Steps: 226 | Train Loss: 0.0091120 Vali Loss: 0.0090676 Test Loss: 0.0100418\n",
      "Validation loss decreased (0.009096 --> 0.009068).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0089915\n",
      "\tspeed: 0.0553s/iter; left time: 1031.9393s\n",
      "\titers: 200, epoch: 18 | loss: 0.0104773\n",
      "\tspeed: 0.0270s/iter; left time: 500.9996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.55s\n",
      "Steps: 226 | Train Loss: 0.0090761 Vali Loss: 0.0090888 Test Loss: 0.0100091\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0087572\n",
      "\tspeed: 0.0504s/iter; left time: 929.6214s\n",
      "\titers: 200, epoch: 19 | loss: 0.0087940\n",
      "\tspeed: 0.0245s/iter; left time: 448.4910s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.71s\n",
      "Steps: 226 | Train Loss: 0.0090523 Vali Loss: 0.0090600 Test Loss: 0.0100221\n",
      "Validation loss decreased (0.009068 --> 0.009060).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0086708\n",
      "\tspeed: 0.0492s/iter; left time: 896.2677s\n",
      "\titers: 200, epoch: 20 | loss: 0.0089390\n",
      "\tspeed: 0.0255s/iter; left time: 462.4140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 226 | Train Loss: 0.0090191 Vali Loss: 0.0090712 Test Loss: 0.0099884\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0102910\n",
      "\tspeed: 0.0480s/iter; left time: 863.6423s\n",
      "\titers: 200, epoch: 21 | loss: 0.0078968\n",
      "\tspeed: 0.0260s/iter; left time: 465.2913s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.86s\n",
      "Steps: 226 | Train Loss: 0.0089772 Vali Loss: 0.0090484 Test Loss: 0.0099763\n",
      "Validation loss decreased (0.009060 --> 0.009048).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0083771\n",
      "\tspeed: 0.0547s/iter; left time: 971.6334s\n",
      "\titers: 200, epoch: 22 | loss: 0.0077762\n",
      "\tspeed: 0.0240s/iter; left time: 423.3853s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:06.56s\n",
      "Steps: 226 | Train Loss: 0.0089653 Vali Loss: 0.0090280 Test Loss: 0.0099728\n",
      "Validation loss decreased (0.009048 --> 0.009028).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0089312\n",
      "\tspeed: 0.0583s/iter; left time: 1022.5282s\n",
      "\titers: 200, epoch: 23 | loss: 0.0089807\n",
      "\tspeed: 0.0264s/iter; left time: 459.6983s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 226 | Train Loss: 0.0089423 Vali Loss: 0.0090344 Test Loss: 0.0099611\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0078890\n",
      "\tspeed: 0.0566s/iter; left time: 978.6115s\n",
      "\titers: 200, epoch: 24 | loss: 0.0085228\n",
      "\tspeed: 0.0320s/iter; left time: 550.6104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:07.33s\n",
      "Steps: 226 | Train Loss: 0.0089327 Vali Loss: 0.0090320 Test Loss: 0.0099370\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0097201\n",
      "\tspeed: 0.0547s/iter; left time: 934.0444s\n",
      "\titers: 200, epoch: 25 | loss: 0.0089450\n",
      "\tspeed: 0.0244s/iter; left time: 414.1116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0089084 Vali Loss: 0.0090175 Test Loss: 0.0099251\n",
      "Validation loss decreased (0.009028 --> 0.009017).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0097886\n",
      "\tspeed: 0.0583s/iter; left time: 982.0724s\n",
      "\titers: 200, epoch: 26 | loss: 0.0076452\n",
      "\tspeed: 0.0284s/iter; left time: 476.0494s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.07s\n",
      "Steps: 226 | Train Loss: 0.0089052 Vali Loss: 0.0090332 Test Loss: 0.0099195\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0091809\n",
      "\tspeed: 0.0491s/iter; left time: 817.0285s\n",
      "\titers: 200, epoch: 27 | loss: 0.0093014\n",
      "\tspeed: 0.0246s/iter; left time: 405.7418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:05.79s\n",
      "Steps: 226 | Train Loss: 0.0088916 Vali Loss: 0.0090062 Test Loss: 0.0099227\n",
      "Validation loss decreased (0.009017 --> 0.009006).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0083350\n",
      "\tspeed: 0.0564s/iter; left time: 925.2712s\n",
      "\titers: 200, epoch: 28 | loss: 0.0081989\n",
      "\tspeed: 0.0256s/iter; left time: 418.0152s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 226 | Train Loss: 0.0088817 Vali Loss: 0.0090501 Test Loss: 0.0099505\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0090654\n",
      "\tspeed: 0.0501s/iter; left time: 810.3152s\n",
      "\titers: 200, epoch: 29 | loss: 0.0101934\n",
      "\tspeed: 0.0266s/iter; left time: 427.6180s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 226 | Train Loss: 0.0088651 Vali Loss: 0.0090426 Test Loss: 0.0099366\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0100033\n",
      "\tspeed: 0.0529s/iter; left time: 843.7943s\n",
      "\titers: 200, epoch: 30 | loss: 0.0091661\n",
      "\tspeed: 0.0236s/iter; left time: 374.7100s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 226 | Train Loss: 0.0088670 Vali Loss: 0.0090467 Test Loss: 0.0099255\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0078916\n",
      "\tspeed: 0.0529s/iter; left time: 832.2571s\n",
      "\titers: 200, epoch: 31 | loss: 0.0098773\n",
      "\tspeed: 0.0276s/iter; left time: 430.8198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:06.37s\n",
      "Steps: 226 | Train Loss: 0.0088527 Vali Loss: 0.0090145 Test Loss: 0.0099058\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0085965\n",
      "\tspeed: 0.0513s/iter; left time: 794.7831s\n",
      "\titers: 200, epoch: 32 | loss: 0.0095884\n",
      "\tspeed: 0.0244s/iter; left time: 375.5542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0088357 Vali Loss: 0.0090503 Test Loss: 0.0099251\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0078262\n",
      "\tspeed: 0.0540s/iter; left time: 824.3251s\n",
      "\titers: 200, epoch: 33 | loss: 0.0090609\n",
      "\tspeed: 0.0273s/iter; left time: 414.7048s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.70s\n",
      "Steps: 226 | Train Loss: 0.0088254 Vali Loss: 0.0090212 Test Loss: 0.0099334\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0095772\n",
      "\tspeed: 0.0596s/iter; left time: 896.4411s\n",
      "\titers: 200, epoch: 34 | loss: 0.0093180\n",
      "\tspeed: 0.0309s/iter; left time: 461.4606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.22s\n",
      "Steps: 226 | Train Loss: 0.0088139 Vali Loss: 0.0090119 Test Loss: 0.0099190\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0089763\n",
      "\tspeed: 0.0459s/iter; left time: 680.6396s\n",
      "\titers: 200, epoch: 35 | loss: 0.0093619\n",
      "\tspeed: 0.0242s/iter; left time: 355.4157s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:05.59s\n",
      "Steps: 226 | Train Loss: 0.0088183 Vali Loss: 0.0090063 Test Loss: 0.0099132\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0080900\n",
      "\tspeed: 0.0515s/iter; left time: 750.9620s\n",
      "\titers: 200, epoch: 36 | loss: 0.0077684\n",
      "\tspeed: 0.0273s/iter; left time: 396.2352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0088260 Vali Loss: 0.0090389 Test Loss: 0.0099133\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0091542\n",
      "\tspeed: 0.0482s/iter; left time: 692.9952s\n",
      "\titers: 200, epoch: 37 | loss: 0.0082184\n",
      "\tspeed: 0.0232s/iter; left time: 330.3205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:05.63s\n",
      "Steps: 226 | Train Loss: 0.0088172 Vali Loss: 0.0090156 Test Loss: 0.0099207\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009922743774950504, rmse:0.09961297363042831, mae:0.05834198370575905, rse:0.3763883709907532\n",
      "Intermediate time for IT and pred_len 24: 00h:05m:12.70s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0478222\n",
      "\tspeed: 0.0504s/iter; left time: 1128.4772s\n",
      "\titers: 200, epoch: 1 | loss: 0.0381344\n",
      "\tspeed: 0.0344s/iter; left time: 766.1421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.08s\n",
      "Steps: 225 | Train Loss: 0.0474403 Vali Loss: 0.0352382 Test Loss: 0.0391712\n",
      "Validation loss decreased (inf --> 0.035238).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0253392\n",
      "\tspeed: 0.0644s/iter; left time: 1427.8207s\n",
      "\titers: 200, epoch: 2 | loss: 0.0213449\n",
      "\tspeed: 0.0344s/iter; left time: 759.6715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 225 | Train Loss: 0.0254757 Vali Loss: 0.0179431 Test Loss: 0.0199769\n",
      "Validation loss decreased (0.035238 --> 0.017943).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0204113\n",
      "\tspeed: 0.0595s/iter; left time: 1305.1059s\n",
      "\titers: 200, epoch: 3 | loss: 0.0188128\n",
      "\tspeed: 0.0384s/iter; left time: 838.0136s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.12s\n",
      "Steps: 225 | Train Loss: 0.0195972 Vali Loss: 0.0166414 Test Loss: 0.0186526\n",
      "Validation loss decreased (0.017943 --> 0.016641).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0188461\n",
      "\tspeed: 0.0688s/iter; left time: 1494.3791s\n",
      "\titers: 200, epoch: 4 | loss: 0.0188044\n",
      "\tspeed: 0.0381s/iter; left time: 823.9522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 225 | Train Loss: 0.0185892 Vali Loss: 0.0163968 Test Loss: 0.0184007\n",
      "Validation loss decreased (0.016641 --> 0.016397).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0179484\n",
      "\tspeed: 0.0653s/iter; left time: 1402.9415s\n",
      "\titers: 200, epoch: 5 | loss: 0.0178978\n",
      "\tspeed: 0.0309s/iter; left time: 661.3506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.71s\n",
      "Steps: 225 | Train Loss: 0.0180306 Vali Loss: 0.0161476 Test Loss: 0.0181733\n",
      "Validation loss decreased (0.016397 --> 0.016148).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0198851\n",
      "\tspeed: 0.0607s/iter; left time: 1290.9299s\n",
      "\titers: 200, epoch: 6 | loss: 0.0186755\n",
      "\tspeed: 0.0389s/iter; left time: 823.1098s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 225 | Train Loss: 0.0176643 Vali Loss: 0.0160395 Test Loss: 0.0180845\n",
      "Validation loss decreased (0.016148 --> 0.016040).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0174389\n",
      "\tspeed: 0.0582s/iter; left time: 1224.4866s\n",
      "\titers: 200, epoch: 7 | loss: 0.0180951\n",
      "\tspeed: 0.0243s/iter; left time: 509.7486s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 225 | Train Loss: 0.0173723 Vali Loss: 0.0158992 Test Loss: 0.0178840\n",
      "Validation loss decreased (0.016040 --> 0.015899).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0174347\n",
      "\tspeed: 0.0541s/iter; left time: 1126.1975s\n",
      "\titers: 200, epoch: 8 | loss: 0.0167886\n",
      "\tspeed: 0.0322s/iter; left time: 666.6907s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.79s\n",
      "Steps: 225 | Train Loss: 0.0171305 Vali Loss: 0.0158986 Test Loss: 0.0178638\n",
      "Validation loss decreased (0.015899 --> 0.015899).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0182739\n",
      "\tspeed: 0.0615s/iter; left time: 1266.8376s\n",
      "\titers: 200, epoch: 9 | loss: 0.0169673\n",
      "\tspeed: 0.0300s/iter; left time: 615.0006s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 225 | Train Loss: 0.0169175 Vali Loss: 0.0158338 Test Loss: 0.0178399\n",
      "Validation loss decreased (0.015899 --> 0.015834).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0168277\n",
      "\tspeed: 0.0633s/iter; left time: 1289.8898s\n",
      "\titers: 200, epoch: 10 | loss: 0.0179330\n",
      "\tspeed: 0.0273s/iter; left time: 553.6355s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.52s\n",
      "Steps: 225 | Train Loss: 0.0167715 Vali Loss: 0.0157845 Test Loss: 0.0177933\n",
      "Validation loss decreased (0.015834 --> 0.015784).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0163170\n",
      "\tspeed: 0.0645s/iter; left time: 1299.0787s\n",
      "\titers: 200, epoch: 11 | loss: 0.0163759\n",
      "\tspeed: 0.0258s/iter; left time: 516.3500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0166167 Vali Loss: 0.0157842 Test Loss: 0.0178167\n",
      "Validation loss decreased (0.015784 --> 0.015784).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0151263\n",
      "\tspeed: 0.0646s/iter; left time: 1286.8668s\n",
      "\titers: 200, epoch: 12 | loss: 0.0166397\n",
      "\tspeed: 0.0265s/iter; left time: 524.7317s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 225 | Train Loss: 0.0165070 Vali Loss: 0.0157736 Test Loss: 0.0178441\n",
      "Validation loss decreased (0.015784 --> 0.015774).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0164019\n",
      "\tspeed: 0.0620s/iter; left time: 1221.5179s\n",
      "\titers: 200, epoch: 13 | loss: 0.0159479\n",
      "\tspeed: 0.0319s/iter; left time: 625.8896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0163756 Vali Loss: 0.0157088 Test Loss: 0.0178032\n",
      "Validation loss decreased (0.015774 --> 0.015709).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0154372\n",
      "\tspeed: 0.0603s/iter; left time: 1173.8247s\n",
      "\titers: 200, epoch: 14 | loss: 0.0160886\n",
      "\tspeed: 0.0308s/iter; left time: 596.5937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0162828 Vali Loss: 0.0158301 Test Loss: 0.0179277\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0154287\n",
      "\tspeed: 0.0599s/iter; left time: 1154.0704s\n",
      "\titers: 200, epoch: 15 | loss: 0.0153177\n",
      "\tspeed: 0.0334s/iter; left time: 638.7949s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.15s\n",
      "Steps: 225 | Train Loss: 0.0161906 Vali Loss: 0.0157809 Test Loss: 0.0179186\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0172052\n",
      "\tspeed: 0.0558s/iter; left time: 1061.0893s\n",
      "\titers: 200, epoch: 16 | loss: 0.0152363\n",
      "\tspeed: 0.0263s/iter; left time: 497.8089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.96s\n",
      "Steps: 225 | Train Loss: 0.0161114 Vali Loss: 0.0158190 Test Loss: 0.0179464\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0186068\n",
      "\tspeed: 0.0673s/iter; left time: 1266.2018s\n",
      "\titers: 200, epoch: 17 | loss: 0.0161077\n",
      "\tspeed: 0.0351s/iter; left time: 656.3230s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.37s\n",
      "Steps: 225 | Train Loss: 0.0160482 Vali Loss: 0.0158060 Test Loss: 0.0179885\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0147823\n",
      "\tspeed: 0.0538s/iter; left time: 1000.1613s\n",
      "\titers: 200, epoch: 18 | loss: 0.0150528\n",
      "\tspeed: 0.0307s/iter; left time: 567.6914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.88s\n",
      "Steps: 225 | Train Loss: 0.0159649 Vali Loss: 0.0157838 Test Loss: 0.0179825\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0155928\n",
      "\tspeed: 0.0593s/iter; left time: 1088.2279s\n",
      "\titers: 200, epoch: 19 | loss: 0.0147181\n",
      "\tspeed: 0.0312s/iter; left time: 569.2792s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.43s\n",
      "Steps: 225 | Train Loss: 0.0159203 Vali Loss: 0.0158155 Test Loss: 0.0180364\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0158121\n",
      "\tspeed: 0.0637s/iter; left time: 1155.4831s\n",
      "\titers: 200, epoch: 20 | loss: 0.0156867\n",
      "\tspeed: 0.0345s/iter; left time: 622.5498s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 225 | Train Loss: 0.0158178 Vali Loss: 0.0158000 Test Loss: 0.0179819\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0155895\n",
      "\tspeed: 0.0548s/iter; left time: 980.2308s\n",
      "\titers: 200, epoch: 21 | loss: 0.0158620\n",
      "\tspeed: 0.0289s/iter; left time: 515.0808s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.98s\n",
      "Steps: 225 | Train Loss: 0.0157976 Vali Loss: 0.0157882 Test Loss: 0.0180150\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0161253\n",
      "\tspeed: 0.0623s/iter; left time: 1100.6162s\n",
      "\titers: 200, epoch: 22 | loss: 0.0149056\n",
      "\tspeed: 0.0300s/iter; left time: 526.7779s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.76s\n",
      "Steps: 225 | Train Loss: 0.0157508 Vali Loss: 0.0158112 Test Loss: 0.0180414\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0168003\n",
      "\tspeed: 0.0554s/iter; left time: 966.6790s\n",
      "\titers: 200, epoch: 23 | loss: 0.0160291\n",
      "\tspeed: 0.0355s/iter; left time: 615.7245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 225 | Train Loss: 0.0157218 Vali Loss: 0.0158285 Test Loss: 0.0180852\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.017803212627768517, rmse:0.1334286779165268, mae:0.08180394023656845, rse:0.5045081973075867\n",
      "Intermediate time for IT and pred_len 96: 00h:03m:42.65s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0506961\n",
      "\tspeed: 0.0676s/iter; left time: 1514.8791s\n",
      "\titers: 200, epoch: 1 | loss: 0.0434178\n",
      "\tspeed: 0.0344s/iter; left time: 768.0979s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.71s\n",
      "Steps: 225 | Train Loss: 0.0503334 Vali Loss: 0.0368931 Test Loss: 0.0408223\n",
      "Validation loss decreased (inf --> 0.036893).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0257164\n",
      "\tspeed: 0.0888s/iter; left time: 1970.1584s\n",
      "\titers: 200, epoch: 2 | loss: 0.0217644\n",
      "\tspeed: 0.0489s/iter; left time: 1080.6051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.16s\n",
      "Steps: 225 | Train Loss: 0.0272302 Vali Loss: 0.0194705 Test Loss: 0.0210699\n",
      "Validation loss decreased (0.036893 --> 0.019471).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0223614\n",
      "\tspeed: 0.0886s/iter; left time: 1945.3772s\n",
      "\titers: 200, epoch: 3 | loss: 0.0198506\n",
      "\tspeed: 0.0486s/iter; left time: 1062.0087s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.21s\n",
      "Steps: 225 | Train Loss: 0.0215152 Vali Loss: 0.0184542 Test Loss: 0.0200383\n",
      "Validation loss decreased (0.019471 --> 0.018454).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0190125\n",
      "\tspeed: 0.0902s/iter; left time: 1959.1852s\n",
      "\titers: 200, epoch: 4 | loss: 0.0211241\n",
      "\tspeed: 0.0493s/iter; left time: 1065.3879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.31s\n",
      "Steps: 225 | Train Loss: 0.0204801 Vali Loss: 0.0180584 Test Loss: 0.0196614\n",
      "Validation loss decreased (0.018454 --> 0.018058).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0185759\n",
      "\tspeed: 0.0885s/iter; left time: 1902.8078s\n",
      "\titers: 200, epoch: 5 | loss: 0.0203622\n",
      "\tspeed: 0.0486s/iter; left time: 1040.6218s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.22s\n",
      "Steps: 225 | Train Loss: 0.0198778 Vali Loss: 0.0178510 Test Loss: 0.0194241\n",
      "Validation loss decreased (0.018058 --> 0.017851).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0208315\n",
      "\tspeed: 0.0883s/iter; left time: 1879.2563s\n",
      "\titers: 200, epoch: 6 | loss: 0.0172729\n",
      "\tspeed: 0.0481s/iter; left time: 1017.8850s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 225 | Train Loss: 0.0194679 Vali Loss: 0.0177488 Test Loss: 0.0192593\n",
      "Validation loss decreased (0.017851 --> 0.017749).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0172482\n",
      "\tspeed: 0.0869s/iter; left time: 1829.4538s\n",
      "\titers: 200, epoch: 7 | loss: 0.0186308\n",
      "\tspeed: 0.0483s/iter; left time: 1012.9089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 225 | Train Loss: 0.0191360 Vali Loss: 0.0177661 Test Loss: 0.0191440\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0193071\n",
      "\tspeed: 0.0871s/iter; left time: 1814.1306s\n",
      "\titers: 200, epoch: 8 | loss: 0.0185706\n",
      "\tspeed: 0.0471s/iter; left time: 976.9506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 225 | Train Loss: 0.0189352 Vali Loss: 0.0177398 Test Loss: 0.0190398\n",
      "Validation loss decreased (0.017749 --> 0.017740).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0183653\n",
      "\tspeed: 0.0858s/iter; left time: 1767.1173s\n",
      "\titers: 200, epoch: 9 | loss: 0.0181597\n",
      "\tspeed: 0.0474s/iter; left time: 972.1877s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.03s\n",
      "Steps: 225 | Train Loss: 0.0187318 Vali Loss: 0.0178138 Test Loss: 0.0190202\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0187022\n",
      "\tspeed: 0.0872s/iter; left time: 1776.6623s\n",
      "\titers: 200, epoch: 10 | loss: 0.0177875\n",
      "\tspeed: 0.0482s/iter; left time: 976.9009s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.10s\n",
      "Steps: 225 | Train Loss: 0.0185414 Vali Loss: 0.0178180 Test Loss: 0.0189794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0175327\n",
      "\tspeed: 0.0856s/iter; left time: 1725.4260s\n",
      "\titers: 200, epoch: 11 | loss: 0.0181863\n",
      "\tspeed: 0.0488s/iter; left time: 977.7620s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.15s\n",
      "Steps: 225 | Train Loss: 0.0183754 Vali Loss: 0.0177803 Test Loss: 0.0190018\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0174661\n",
      "\tspeed: 0.0874s/iter; left time: 1741.2727s\n",
      "\titers: 200, epoch: 12 | loss: 0.0184091\n",
      "\tspeed: 0.0480s/iter; left time: 950.8208s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 225 | Train Loss: 0.0182399 Vali Loss: 0.0178374 Test Loss: 0.0189758\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0181057\n",
      "\tspeed: 0.0844s/iter; left time: 1663.4012s\n",
      "\titers: 200, epoch: 13 | loss: 0.0172303\n",
      "\tspeed: 0.0485s/iter; left time: 949.9773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.08s\n",
      "Steps: 225 | Train Loss: 0.0180831 Vali Loss: 0.0178951 Test Loss: 0.0190489\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0180981\n",
      "\tspeed: 0.0875s/iter; left time: 1703.5181s\n",
      "\titers: 200, epoch: 14 | loss: 0.0179996\n",
      "\tspeed: 0.0483s/iter; left time: 935.7908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.04s\n",
      "Steps: 225 | Train Loss: 0.0179621 Vali Loss: 0.0179250 Test Loss: 0.0190570\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0168550\n",
      "\tspeed: 0.0876s/iter; left time: 1687.2333s\n",
      "\titers: 200, epoch: 15 | loss: 0.0188148\n",
      "\tspeed: 0.0477s/iter; left time: 913.8198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.08s\n",
      "Steps: 225 | Train Loss: 0.0178600 Vali Loss: 0.0180246 Test Loss: 0.0190814\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0184088\n",
      "\tspeed: 0.0878s/iter; left time: 1670.6955s\n",
      "\titers: 200, epoch: 16 | loss: 0.0172866\n",
      "\tspeed: 0.0479s/iter; left time: 907.0227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.13s\n",
      "Steps: 225 | Train Loss: 0.0177449 Vali Loss: 0.0178746 Test Loss: 0.0191260\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0191272\n",
      "\tspeed: 0.0876s/iter; left time: 1646.0430s\n",
      "\titers: 200, epoch: 17 | loss: 0.0176109\n",
      "\tspeed: 0.0475s/iter; left time: 889.1229s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.01s\n",
      "Steps: 225 | Train Loss: 0.0176496 Vali Loss: 0.0180301 Test Loss: 0.0191746\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0176716\n",
      "\tspeed: 0.0869s/iter; left time: 1613.4237s\n",
      "\titers: 200, epoch: 18 | loss: 0.0185039\n",
      "\tspeed: 0.0480s/iter; left time: 886.6051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 225 | Train Loss: 0.0175396 Vali Loss: 0.0180294 Test Loss: 0.0192170\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019039802253246307, rmse:0.13798479735851288, mae:0.08742713183164597, rse:0.5222201943397522\n",
      "Intermediate time for IT and pred_len 168: 00h:04m:13.33s\n",
      "Intermediate time for IT: 00h:13m:08.68s\n",
      "Total time: 00h:24m:11.50s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_decomposition.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == \"DE\" and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "                \n",
    "            model_id = f\"ts_decomp_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --decomposition 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Decomposition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.0921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.1374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1362</td>\n",
       "      <td>0.0902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.0967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>0.0569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.0838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.0896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>0.1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.0818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.0874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            Decomposition                \n",
       "Metrics                    MSE    RMSE     MAE\n",
       "Country Pred_len                              \n",
       "DE      24              0.0212  0.1455  0.0921\n",
       "        96              0.0350  0.1870  0.1285\n",
       "        168             0.0376  0.1940  0.1374\n",
       "ES      24              0.0100  0.0998  0.0622\n",
       "        96              0.0185  0.1362  0.0902\n",
       "        168             0.0209  0.1446  0.0967\n",
       "FR      24              0.0099  0.0997  0.0569\n",
       "        96              0.0192  0.1385  0.0838\n",
       "        168             0.0208  0.1442  0.0896\n",
       "GB      24              0.0257  0.1604  0.1056\n",
       "        96              0.0442  0.2102  0.1458\n",
       "        168             0.0433  0.2082  0.1481\n",
       "IT      24              0.0099  0.0996  0.0583\n",
       "        96              0.0178  0.1334  0.0818\n",
       "        168             0.0190  0.1380  0.0874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['Decomposition'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_decomposition.csv'))\n",
    "patchtst_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
