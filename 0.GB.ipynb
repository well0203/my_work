{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/GB_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_outliers(df, col, lower_b=True, upper_b=True):\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if lower_b == True:\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "    if upper_b == True:\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "    if lower_b == False:\n",
    "        lower_bound = None\n",
    "    if upper_b == False:\n",
    "        upper_bound = None\n",
    "\n",
    "    df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = truncate_outliers(df, 'GB_UKM_load_actual_entsoe_transparency', lower_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/GB_data_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['GB']\n",
    "num_cols = [5]\n",
    "seq_lens = [512]\n",
    "\n",
    "model = \"PatchTST\"\n",
    "loss = \"MSE\"\n",
    "itr=1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_revin.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0265027\n",
      "\tspeed: 0.2294s/iter; left time: 5092.2824s\n",
      "\titers: 200, epoch: 1 | loss: 0.0251007\n",
      "\tspeed: 0.1974s/iter; left time: 4363.3776s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:45.25s\n",
      "Steps: 223 | Train Loss: 0.0289925 Vali Loss: 0.0287373 Test Loss: 0.0392917\n",
      "Validation loss decreased (inf --> 0.028737).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0161534\n",
      "\tspeed: 0.3346s/iter; left time: 7352.8966s\n",
      "\titers: 200, epoch: 2 | loss: 0.0151043\n",
      "\tspeed: 0.2015s/iter; left time: 4408.3458s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:45.36s\n",
      "Steps: 223 | Train Loss: 0.0159772 Vali Loss: 0.0200053 Test Loss: 0.0257209\n",
      "Validation loss decreased (0.028737 --> 0.020005).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0147951\n",
      "\tspeed: 0.3233s/iter; left time: 7034.0547s\n",
      "\titers: 200, epoch: 3 | loss: 0.0140800\n",
      "\tspeed: 0.2048s/iter; left time: 4435.4769s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:45.63s\n",
      "Steps: 223 | Train Loss: 0.0141499 Vali Loss: 0.0192924 Test Loss: 0.0253132\n",
      "Validation loss decreased (0.020005 --> 0.019292).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0150833\n",
      "\tspeed: 0.3194s/iter; left time: 6877.4776s\n",
      "\titers: 200, epoch: 4 | loss: 0.0160355\n",
      "\tspeed: 0.2018s/iter; left time: 4325.3560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:44.44s\n",
      "Steps: 223 | Train Loss: 0.0137518 Vali Loss: 0.0194898 Test Loss: 0.0253258\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0125888\n",
      "\tspeed: 0.3287s/iter; left time: 7005.2287s\n",
      "\titers: 200, epoch: 5 | loss: 0.0125418\n",
      "\tspeed: 0.1992s/iter; left time: 4225.8636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:45.12s\n",
      "Steps: 223 | Train Loss: 0.0134877 Vali Loss: 0.0192790 Test Loss: 0.0254325\n",
      "Validation loss decreased (0.019292 --> 0.019279).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0121389\n",
      "\tspeed: 0.3303s/iter; left time: 6963.9304s\n",
      "\titers: 200, epoch: 6 | loss: 0.0150013\n",
      "\tspeed: 0.2041s/iter; left time: 4282.2698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:45.70s\n",
      "Steps: 223 | Train Loss: 0.0133018 Vali Loss: 0.0190845 Test Loss: 0.0249459\n",
      "Validation loss decreased (0.019279 --> 0.019085).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0132609\n",
      "\tspeed: 0.3290s/iter; left time: 6864.5902s\n",
      "\titers: 200, epoch: 7 | loss: 0.0122648\n",
      "\tspeed: 0.2034s/iter; left time: 4222.2325s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:45.50s\n",
      "Steps: 223 | Train Loss: 0.0131548 Vali Loss: 0.0191955 Test Loss: 0.0252381\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0141194\n",
      "\tspeed: 0.3340s/iter; left time: 6894.1110s\n",
      "\titers: 200, epoch: 8 | loss: 0.0127646\n",
      "\tspeed: 0.2012s/iter; left time: 4132.4303s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:45.00s\n",
      "Steps: 223 | Train Loss: 0.0130708 Vali Loss: 0.0189013 Test Loss: 0.0249659\n",
      "Validation loss decreased (0.019085 --> 0.018901).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0131821\n",
      "\tspeed: 0.3321s/iter; left time: 6779.9021s\n",
      "\titers: 200, epoch: 9 | loss: 0.0120102\n",
      "\tspeed: 0.1992s/iter; left time: 4047.8665s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:45.08s\n",
      "Steps: 223 | Train Loss: 0.0129469 Vali Loss: 0.0189183 Test Loss: 0.0246164\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0122071\n",
      "\tspeed: 0.3296s/iter; left time: 6655.7649s\n",
      "\titers: 200, epoch: 10 | loss: 0.0140594\n",
      "\tspeed: 0.2036s/iter; left time: 4091.8566s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:45.63s\n",
      "Steps: 223 | Train Loss: 0.0128515 Vali Loss: 0.0189848 Test Loss: 0.0247979\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0143610\n",
      "\tspeed: 0.3259s/iter; left time: 6509.1639s\n",
      "\titers: 200, epoch: 11 | loss: 0.0131748\n",
      "\tspeed: 0.2022s/iter; left time: 4017.1950s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:45.43s\n",
      "Steps: 223 | Train Loss: 0.0127449 Vali Loss: 0.0188822 Test Loss: 0.0251732\n",
      "Validation loss decreased (0.018901 --> 0.018882).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0105144\n",
      "\tspeed: 0.3322s/iter; left time: 6560.6609s\n",
      "\titers: 200, epoch: 12 | loss: 0.0144839\n",
      "\tspeed: 0.2023s/iter; left time: 3974.2329s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:45.67s\n",
      "Steps: 223 | Train Loss: 0.0126781 Vali Loss: 0.0189142 Test Loss: 0.0246183\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0121141\n",
      "\tspeed: 0.3293s/iter; left time: 6429.0900s\n",
      "\titers: 200, epoch: 13 | loss: 0.0133965\n",
      "\tspeed: 0.1992s/iter; left time: 3870.2664s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:45.11s\n",
      "Steps: 223 | Train Loss: 0.0126196 Vali Loss: 0.0189258 Test Loss: 0.0246682\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0124054\n",
      "\tspeed: 0.3274s/iter; left time: 6318.5488s\n",
      "\titers: 200, epoch: 14 | loss: 0.0124091\n",
      "\tspeed: 0.2036s/iter; left time: 3909.2487s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:45.54s\n",
      "Steps: 223 | Train Loss: 0.0125549 Vali Loss: 0.0189255 Test Loss: 0.0247454\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0123658\n",
      "\tspeed: 0.3258s/iter; left time: 6216.0777s\n",
      "\titers: 200, epoch: 15 | loss: 0.0121659\n",
      "\tspeed: 0.2016s/iter; left time: 3826.6529s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:45.40s\n",
      "Steps: 223 | Train Loss: 0.0124956 Vali Loss: 0.0188231 Test Loss: 0.0244495\n",
      "Validation loss decreased (0.018882 --> 0.018823).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0117624\n",
      "\tspeed: 0.3238s/iter; left time: 6105.1571s\n",
      "\titers: 200, epoch: 16 | loss: 0.0113316\n",
      "\tspeed: 0.2031s/iter; left time: 3810.2698s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:45.07s\n",
      "Steps: 223 | Train Loss: 0.0124439 Vali Loss: 0.0188531 Test Loss: 0.0245404\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0137128\n",
      "\tspeed: 0.3297s/iter; left time: 6142.5536s\n",
      "\titers: 200, epoch: 17 | loss: 0.0124194\n",
      "\tspeed: 0.2043s/iter; left time: 3785.9745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:45.75s\n",
      "Steps: 223 | Train Loss: 0.0124042 Vali Loss: 0.0187859 Test Loss: 0.0245493\n",
      "Validation loss decreased (0.018823 --> 0.018786).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0125534\n",
      "\tspeed: 0.3333s/iter; left time: 6135.6930s\n",
      "\titers: 200, epoch: 18 | loss: 0.0122139\n",
      "\tspeed: 0.2035s/iter; left time: 3726.2518s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:45.58s\n",
      "Steps: 223 | Train Loss: 0.0123595 Vali Loss: 0.0188351 Test Loss: 0.0246896\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0124088\n",
      "\tspeed: 0.3281s/iter; left time: 5966.5142s\n",
      "\titers: 200, epoch: 19 | loss: 0.0138830\n",
      "\tspeed: 0.1810s/iter; left time: 3273.5565s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:41.94s\n",
      "Steps: 223 | Train Loss: 0.0123274 Vali Loss: 0.0189098 Test Loss: 0.0245892\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0123822\n",
      "\tspeed: 0.2458s/iter; left time: 4416.1672s\n",
      "\titers: 200, epoch: 20 | loss: 0.0100983\n",
      "\tspeed: 0.1514s/iter; left time: 2703.8607s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:33.84s\n",
      "Steps: 223 | Train Loss: 0.0122708 Vali Loss: 0.0189085 Test Loss: 0.0245858\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0110936\n",
      "\tspeed: 0.2487s/iter; left time: 4411.4754s\n",
      "\titers: 200, epoch: 21 | loss: 0.0137585\n",
      "\tspeed: 0.1499s/iter; left time: 2643.7300s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:33.83s\n",
      "Steps: 223 | Train Loss: 0.0122343 Vali Loss: 0.0188904 Test Loss: 0.0246416\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0121044\n",
      "\tspeed: 0.2477s/iter; left time: 4338.6473s\n",
      "\titers: 200, epoch: 22 | loss: 0.0123542\n",
      "\tspeed: 0.1518s/iter; left time: 2644.3153s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:34.09s\n",
      "Steps: 223 | Train Loss: 0.0122031 Vali Loss: 0.0188184 Test Loss: 0.0246698\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0133064\n",
      "\tspeed: 0.2450s/iter; left time: 4237.4510s\n",
      "\titers: 200, epoch: 23 | loss: 0.0106579\n",
      "\tspeed: 0.1524s/iter; left time: 2620.6067s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:34.12s\n",
      "Steps: 223 | Train Loss: 0.0121892 Vali Loss: 0.0188000 Test Loss: 0.0246145\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0125715\n",
      "\tspeed: 0.2439s/iter; left time: 4164.5757s\n",
      "\titers: 200, epoch: 24 | loss: 0.0127193\n",
      "\tspeed: 0.1515s/iter; left time: 2571.5771s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:33.69s\n",
      "Steps: 223 | Train Loss: 0.0121651 Vali Loss: 0.0188571 Test Loss: 0.0246809\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0128545\n",
      "\tspeed: 0.2504s/iter; left time: 4218.3415s\n",
      "\titers: 200, epoch: 25 | loss: 0.0118437\n",
      "\tspeed: 0.1474s/iter; left time: 2468.3015s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:33.63s\n",
      "Steps: 223 | Train Loss: 0.0121565 Vali Loss: 0.0189216 Test Loss: 0.0246473\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0113622\n",
      "\tspeed: 0.2491s/iter; left time: 4140.9006s\n",
      "\titers: 200, epoch: 26 | loss: 0.0115188\n",
      "\tspeed: 0.1517s/iter; left time: 2506.7645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:34.05s\n",
      "Steps: 223 | Train Loss: 0.0121280 Vali Loss: 0.0189415 Test Loss: 0.0246623\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0128001\n",
      "\tspeed: 0.2428s/iter; left time: 3981.9653s\n",
      "\titers: 200, epoch: 27 | loss: 0.0128085\n",
      "\tspeed: 0.1513s/iter; left time: 2465.8289s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:33.45s\n",
      "Steps: 223 | Train Loss: 0.0120977 Vali Loss: 0.0188481 Test Loss: 0.0246160\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.024549338966608047, rmse:0.15668228268623352, mae:0.10350367426872253, rse:0.5449948906898499\n",
      "Intermediate time for GB and pred_len 24: 00h:22m:13.89s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0315760\n",
      "\tspeed: 0.1792s/iter; left time: 3960.3168s\n",
      "\titers: 200, epoch: 1 | loss: 0.0260716\n",
      "\tspeed: 0.1526s/iter; left time: 3357.8429s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:34.26s\n",
      "Steps: 222 | Train Loss: 0.0314290 Vali Loss: 0.0328159 Test Loss: 0.0469043\n",
      "Validation loss decreased (inf --> 0.032816).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0229935\n",
      "\tspeed: 0.2495s/iter; left time: 5458.2791s\n",
      "\titers: 200, epoch: 2 | loss: 0.0242903\n",
      "\tspeed: 0.1519s/iter; left time: 3308.9580s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:33.89s\n",
      "Steps: 222 | Train Loss: 0.0231359 Vali Loss: 0.0287751 Test Loss: 0.0403347\n",
      "Validation loss decreased (0.032816 --> 0.028775).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0212913\n",
      "\tspeed: 0.2498s/iter; left time: 5409.9755s\n",
      "\titers: 200, epoch: 3 | loss: 0.0224801\n",
      "\tspeed: 0.1472s/iter; left time: 3174.1336s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:33.38s\n",
      "Steps: 222 | Train Loss: 0.0215724 Vali Loss: 0.0286149 Test Loss: 0.0410265\n",
      "Validation loss decreased (0.028775 --> 0.028615).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0193154\n",
      "\tspeed: 0.2487s/iter; left time: 5331.6997s\n",
      "\titers: 200, epoch: 4 | loss: 0.0216382\n",
      "\tspeed: 0.1529s/iter; left time: 3262.1456s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:34.01s\n",
      "Steps: 222 | Train Loss: 0.0211449 Vali Loss: 0.0284505 Test Loss: 0.0410261\n",
      "Validation loss decreased (0.028615 --> 0.028451).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0213219\n",
      "\tspeed: 0.2468s/iter; left time: 5235.4657s\n",
      "\titers: 200, epoch: 5 | loss: 0.0220371\n",
      "\tspeed: 0.1519s/iter; left time: 3206.5128s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:33.99s\n",
      "Steps: 222 | Train Loss: 0.0208245 Vali Loss: 0.0282347 Test Loss: 0.0402066\n",
      "Validation loss decreased (0.028451 --> 0.028235).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0199007\n",
      "\tspeed: 0.2477s/iter; left time: 5199.2213s\n",
      "\titers: 200, epoch: 6 | loss: 0.0204234\n",
      "\tspeed: 0.1523s/iter; left time: 3181.9468s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:33.67s\n",
      "Steps: 222 | Train Loss: 0.0205529 Vali Loss: 0.0283002 Test Loss: 0.0408061\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0195169\n",
      "\tspeed: 0.2506s/iter; left time: 5203.9552s\n",
      "\titers: 200, epoch: 7 | loss: 0.0198141\n",
      "\tspeed: 0.1465s/iter; left time: 3028.2576s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:33.37s\n",
      "Steps: 222 | Train Loss: 0.0202949 Vali Loss: 0.0284169 Test Loss: 0.0404694\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0206108\n",
      "\tspeed: 0.2474s/iter; left time: 5083.7914s\n",
      "\titers: 200, epoch: 8 | loss: 0.0197101\n",
      "\tspeed: 0.1531s/iter; left time: 3130.0307s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.08s\n",
      "Steps: 222 | Train Loss: 0.0200492 Vali Loss: 0.0285237 Test Loss: 0.0404855\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0190199\n",
      "\tspeed: 0.2469s/iter; left time: 5019.0308s\n",
      "\titers: 200, epoch: 9 | loss: 0.0195537\n",
      "\tspeed: 0.1530s/iter; left time: 3094.3461s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:33.78s\n",
      "Steps: 222 | Train Loss: 0.0198195 Vali Loss: 0.0285747 Test Loss: 0.0406383\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0194279\n",
      "\tspeed: 0.2483s/iter; left time: 4992.2551s\n",
      "\titers: 200, epoch: 10 | loss: 0.0191261\n",
      "\tspeed: 0.1483s/iter; left time: 2965.8248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:33.47s\n",
      "Steps: 222 | Train Loss: 0.0196048 Vali Loss: 0.0285005 Test Loss: 0.0411416\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0190283\n",
      "\tspeed: 0.2509s/iter; left time: 4987.3438s\n",
      "\titers: 200, epoch: 11 | loss: 0.0186413\n",
      "\tspeed: 0.1500s/iter; left time: 2967.5365s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:33.83s\n",
      "Steps: 222 | Train Loss: 0.0193664 Vali Loss: 0.0287495 Test Loss: 0.0415420\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0180051\n",
      "\tspeed: 0.2513s/iter; left time: 4940.7899s\n",
      "\titers: 200, epoch: 12 | loss: 0.0181843\n",
      "\tspeed: 0.1529s/iter; left time: 2990.6341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:34.02s\n",
      "Steps: 222 | Train Loss: 0.0191550 Vali Loss: 0.0286770 Test Loss: 0.0415046\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0177059\n",
      "\tspeed: 0.2444s/iter; left time: 4750.6707s\n",
      "\titers: 200, epoch: 13 | loss: 0.0195095\n",
      "\tspeed: 0.1526s/iter; left time: 2950.7537s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:33.61s\n",
      "Steps: 222 | Train Loss: 0.0189459 Vali Loss: 0.0287584 Test Loss: 0.0423306\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0189065\n",
      "\tspeed: 0.2503s/iter; left time: 4808.8563s\n",
      "\titers: 200, epoch: 14 | loss: 0.0186706\n",
      "\tspeed: 0.1455s/iter; left time: 2780.7382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:33.33s\n",
      "Steps: 222 | Train Loss: 0.0187665 Vali Loss: 0.0290294 Test Loss: 0.0423234\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0205582\n",
      "\tspeed: 0.2510s/iter; left time: 4767.7059s\n",
      "\titers: 200, epoch: 15 | loss: 0.0195843\n",
      "\tspeed: 0.1530s/iter; left time: 2890.1065s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:34.16s\n",
      "Steps: 222 | Train Loss: 0.0186139 Vali Loss: 0.0289920 Test Loss: 0.0426694\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04020659253001213, rmse:0.20051582157611847, mae:0.142020583152771, rse:0.699314534664154\n",
      "Intermediate time for GB and pred_len 96: 00h:10m:13.26s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0296356\n",
      "\tspeed: 0.1806s/iter; left time: 3992.5194s\n",
      "\titers: 200, epoch: 1 | loss: 0.0287718\n",
      "\tspeed: 0.1509s/iter; left time: 3319.1439s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:34.00s\n",
      "Steps: 222 | Train Loss: 0.0322291 Vali Loss: 0.0338487 Test Loss: 0.0484028\n",
      "Validation loss decreased (inf --> 0.033849).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0232499\n",
      "\tspeed: 0.2513s/iter; left time: 5498.9143s\n",
      "\titers: 200, epoch: 2 | loss: 0.0232826\n",
      "\tspeed: 0.1534s/iter; left time: 3341.3752s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:34.22s\n",
      "Steps: 222 | Train Loss: 0.0245919 Vali Loss: 0.0303544 Test Loss: 0.0427659\n",
      "Validation loss decreased (0.033849 --> 0.030354).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0234374\n",
      "\tspeed: 0.2461s/iter; left time: 5330.5269s\n",
      "\titers: 200, epoch: 3 | loss: 0.0244926\n",
      "\tspeed: 0.1529s/iter; left time: 3296.7460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:33.68s\n",
      "Steps: 222 | Train Loss: 0.0230832 Vali Loss: 0.0300844 Test Loss: 0.0427573\n",
      "Validation loss decreased (0.030354 --> 0.030084).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0242307\n",
      "\tspeed: 0.2530s/iter; left time: 5422.4374s\n",
      "\titers: 200, epoch: 4 | loss: 0.0212251\n",
      "\tspeed: 0.1491s/iter; left time: 3181.8903s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:33.68s\n",
      "Steps: 222 | Train Loss: 0.0225980 Vali Loss: 0.0299937 Test Loss: 0.0427167\n",
      "Validation loss decreased (0.030084 --> 0.029994).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0220736\n",
      "\tspeed: 0.2529s/iter; left time: 5365.1938s\n",
      "\titers: 200, epoch: 5 | loss: 0.0232161\n",
      "\tspeed: 0.1474s/iter; left time: 3112.3360s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:33.57s\n",
      "Steps: 222 | Train Loss: 0.0221819 Vali Loss: 0.0303117 Test Loss: 0.0432081\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0207133\n",
      "\tspeed: 0.2526s/iter; left time: 5302.8358s\n",
      "\titers: 200, epoch: 6 | loss: 0.0218739\n",
      "\tspeed: 0.1505s/iter; left time: 3144.9407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:34.02s\n",
      "Steps: 222 | Train Loss: 0.0217686 Vali Loss: 0.0305694 Test Loss: 0.0433750\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0210360\n",
      "\tspeed: 0.2517s/iter; left time: 5227.7507s\n",
      "\titers: 200, epoch: 7 | loss: 0.0207920\n",
      "\tspeed: 0.1576s/iter; left time: 3257.6188s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:34.77s\n",
      "Steps: 222 | Train Loss: 0.0213849 Vali Loss: 0.0308112 Test Loss: 0.0445731\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0203465\n",
      "\tspeed: 0.2558s/iter; left time: 5255.8389s\n",
      "\titers: 200, epoch: 8 | loss: 0.0195119\n",
      "\tspeed: 0.1547s/iter; left time: 3162.3226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:34.69s\n",
      "Steps: 222 | Train Loss: 0.0210066 Vali Loss: 0.0309717 Test Loss: 0.0446415\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0208949\n",
      "\tspeed: 0.2592s/iter; left time: 5267.5677s\n",
      "\titers: 200, epoch: 9 | loss: 0.0205876\n",
      "\tspeed: 0.1576s/iter; left time: 3187.5327s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:34.55s\n",
      "Steps: 222 | Train Loss: 0.0206804 Vali Loss: 0.0312313 Test Loss: 0.0441510\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0201517\n",
      "\tspeed: 0.2521s/iter; left time: 5067.0481s\n",
      "\titers: 200, epoch: 10 | loss: 0.0210667\n",
      "\tspeed: 0.1579s/iter; left time: 3158.4988s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:35.14s\n",
      "Steps: 222 | Train Loss: 0.0203697 Vali Loss: 0.0312947 Test Loss: 0.0443421\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0213249\n",
      "\tspeed: 0.2557s/iter; left time: 5084.0207s\n",
      "\titers: 200, epoch: 11 | loss: 0.0196188\n",
      "\tspeed: 0.1579s/iter; left time: 3122.5341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:34.97s\n",
      "Steps: 222 | Train Loss: 0.0200198 Vali Loss: 0.0315035 Test Loss: 0.0443597\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0212380\n",
      "\tspeed: 0.2583s/iter; left time: 5077.9610s\n",
      "\titers: 200, epoch: 12 | loss: 0.0194634\n",
      "\tspeed: 0.1534s/iter; left time: 3000.5714s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:34.69s\n",
      "Steps: 222 | Train Loss: 0.0197339 Vali Loss: 0.0318247 Test Loss: 0.0447579\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0186138\n",
      "\tspeed: 0.2569s/iter; left time: 4993.8452s\n",
      "\titers: 200, epoch: 13 | loss: 0.0199512\n",
      "\tspeed: 0.1573s/iter; left time: 3041.2405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:34.80s\n",
      "Steps: 222 | Train Loss: 0.0194446 Vali Loss: 0.0319720 Test Loss: 0.0459761\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0178414\n",
      "\tspeed: 0.2538s/iter; left time: 4876.4838s\n",
      "\titers: 200, epoch: 14 | loss: 0.0188426\n",
      "\tspeed: 0.1579s/iter; left time: 3018.4586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:35.13s\n",
      "Steps: 222 | Train Loss: 0.0192130 Vali Loss: 0.0320775 Test Loss: 0.0456569\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04271668940782547, rmse:0.20668016374111176, mae:0.14791306853294373, rse:0.7228701710700989\n",
      "Intermediate time for GB and pred_len 168: 00h:09m:45.57s\n",
      "Intermediate time for GB: 00h:42m:12.71s\n",
      "Total time: 00h:42m:12.72s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"no_outliers_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data_new.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>0.1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.2005</td>\n",
       "      <td>0.1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "GB      24        0.0245  0.1567  0.1035\n",
       "        96        0.0402  0.2005  0.1420\n",
       "        168       0.0427  0.2067  0.1479"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False).round(4)\n",
    "patchtst_df.to_csv(os.path.join(path, 'GB_patchtst_512.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_no_revin_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.1244073\n",
      "\tspeed: 0.0749s/iter; left time: 1662.0936s\n",
      "\titers: 200, epoch: 1 | loss: 0.1176813\n",
      "\tspeed: 0.0466s/iter; left time: 1029.0412s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 223 | Train Loss: 0.1267753 Vali Loss: 0.0940013 Test Loss: 0.1089936\n",
      "Validation loss decreased (inf --> 0.094001).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0384000\n",
      "\tspeed: 0.0853s/iter; left time: 1875.8094s\n",
      "\titers: 200, epoch: 2 | loss: 0.0266127\n",
      "\tspeed: 0.0465s/iter; left time: 1018.1141s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.66s\n",
      "Steps: 223 | Train Loss: 0.0454170 Vali Loss: 0.0249390 Test Loss: 0.0346667\n",
      "Validation loss decreased (0.094001 --> 0.024939).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0237448\n",
      "\tspeed: 0.0850s/iter; left time: 1848.1967s\n",
      "\titers: 200, epoch: 3 | loss: 0.0198930\n",
      "\tspeed: 0.0467s/iter; left time: 1011.5216s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.64s\n",
      "Steps: 223 | Train Loss: 0.0219407 Vali Loss: 0.0212463 Test Loss: 0.0277511\n",
      "Validation loss decreased (0.024939 --> 0.021246).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0208119\n",
      "\tspeed: 0.0844s/iter; left time: 1818.3062s\n",
      "\titers: 200, epoch: 4 | loss: 0.0200961\n",
      "\tspeed: 0.0468s/iter; left time: 1002.0697s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.66s\n",
      "Steps: 223 | Train Loss: 0.0182399 Vali Loss: 0.0210393 Test Loss: 0.0276593\n",
      "Validation loss decreased (0.021246 --> 0.021039).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0150578\n",
      "\tspeed: 0.0846s/iter; left time: 1802.8697s\n",
      "\titers: 200, epoch: 5 | loss: 0.0163067\n",
      "\tspeed: 0.0470s/iter; left time: 997.3114s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.72s\n",
      "Steps: 223 | Train Loss: 0.0166892 Vali Loss: 0.0207341 Test Loss: 0.0281381\n",
      "Validation loss decreased (0.021039 --> 0.020734).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0142145\n",
      "\tspeed: 0.0850s/iter; left time: 1792.0456s\n",
      "\titers: 200, epoch: 6 | loss: 0.0175426\n",
      "\tspeed: 0.0463s/iter; left time: 972.3974s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.64s\n",
      "Steps: 223 | Train Loss: 0.0158944 Vali Loss: 0.0207792 Test Loss: 0.0284950\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0156249\n",
      "\tspeed: 0.0814s/iter; left time: 1698.8864s\n",
      "\titers: 200, epoch: 7 | loss: 0.0142546\n",
      "\tspeed: 0.0472s/iter; left time: 980.9013s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.75s\n",
      "Steps: 223 | Train Loss: 0.0155555 Vali Loss: 0.0206535 Test Loss: 0.0284376\n",
      "Validation loss decreased (0.020734 --> 0.020654).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0165259\n",
      "\tspeed: 0.0860s/iter; left time: 1774.5886s\n",
      "\titers: 200, epoch: 8 | loss: 0.0152326\n",
      "\tspeed: 0.0472s/iter; left time: 968.5643s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.77s\n",
      "Steps: 223 | Train Loss: 0.0153165 Vali Loss: 0.0207704 Test Loss: 0.0276095\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0161124\n",
      "\tspeed: 0.0846s/iter; left time: 1726.3933s\n",
      "\titers: 200, epoch: 9 | loss: 0.0140249\n",
      "\tspeed: 0.0472s/iter; left time: 958.8026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.76s\n",
      "Steps: 223 | Train Loss: 0.0150784 Vali Loss: 0.0203086 Test Loss: 0.0268710\n",
      "Validation loss decreased (0.020654 --> 0.020309).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0143674\n",
      "\tspeed: 0.0853s/iter; left time: 1722.3466s\n",
      "\titers: 200, epoch: 10 | loss: 0.0157508\n",
      "\tspeed: 0.0475s/iter; left time: 954.3449s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.82s\n",
      "Steps: 223 | Train Loss: 0.0148455 Vali Loss: 0.0201842 Test Loss: 0.0270238\n",
      "Validation loss decreased (0.020309 --> 0.020184).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0164965\n",
      "\tspeed: 0.0863s/iter; left time: 1722.5095s\n",
      "\titers: 200, epoch: 11 | loss: 0.0158760\n",
      "\tspeed: 0.0478s/iter; left time: 948.8596s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0148584 Vali Loss: 0.0204785 Test Loss: 0.0271726\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0121544\n",
      "\tspeed: 0.0848s/iter; left time: 1675.2354s\n",
      "\titers: 200, epoch: 12 | loss: 0.0158793\n",
      "\tspeed: 0.0475s/iter; left time: 932.5540s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.82s\n",
      "Steps: 223 | Train Loss: 0.0146100 Vali Loss: 0.0200453 Test Loss: 0.0266486\n",
      "Validation loss decreased (0.020184 --> 0.020045).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0137476\n",
      "\tspeed: 0.0858s/iter; left time: 1676.1882s\n",
      "\titers: 200, epoch: 13 | loss: 0.0149794\n",
      "\tspeed: 0.0475s/iter; left time: 923.3602s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0144587 Vali Loss: 0.0200264 Test Loss: 0.0266688\n",
      "Validation loss decreased (0.020045 --> 0.020026).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0141716\n",
      "\tspeed: 0.0856s/iter; left time: 1651.6939s\n",
      "\titers: 200, epoch: 14 | loss: 0.0140526\n",
      "\tspeed: 0.0475s/iter; left time: 911.8136s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0144077 Vali Loss: 0.0202406 Test Loss: 0.0269560\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0143215\n",
      "\tspeed: 0.0847s/iter; left time: 1615.3578s\n",
      "\titers: 200, epoch: 15 | loss: 0.0136560\n",
      "\tspeed: 0.0475s/iter; left time: 900.7778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.81s\n",
      "Steps: 223 | Train Loss: 0.0142961 Vali Loss: 0.0198701 Test Loss: 0.0266428\n",
      "Validation loss decreased (0.020026 --> 0.019870).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0136261\n",
      "\tspeed: 0.0858s/iter; left time: 1618.7503s\n",
      "\titers: 200, epoch: 16 | loss: 0.0127296\n",
      "\tspeed: 0.0475s/iter; left time: 890.6676s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 223 | Train Loss: 0.0142301 Vali Loss: 0.0201080 Test Loss: 0.0274499\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0148282\n",
      "\tspeed: 0.0862s/iter; left time: 1606.6090s\n",
      "\titers: 200, epoch: 17 | loss: 0.0132890\n",
      "\tspeed: 0.0476s/iter; left time: 882.6745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0141759 Vali Loss: 0.0198568 Test Loss: 0.0266295\n",
      "Validation loss decreased (0.019870 --> 0.019857).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0139410\n",
      "\tspeed: 0.0858s/iter; left time: 1580.3258s\n",
      "\titers: 200, epoch: 18 | loss: 0.0142552\n",
      "\tspeed: 0.0476s/iter; left time: 871.7514s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0140903 Vali Loss: 0.0201418 Test Loss: 0.0271959\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0133088\n",
      "\tspeed: 0.0849s/iter; left time: 1543.7251s\n",
      "\titers: 200, epoch: 19 | loss: 0.0154610\n",
      "\tspeed: 0.0475s/iter; left time: 859.7683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0140211 Vali Loss: 0.0201959 Test Loss: 0.0270690\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0139708\n",
      "\tspeed: 0.0857s/iter; left time: 1539.8229s\n",
      "\titers: 200, epoch: 20 | loss: 0.0123683\n",
      "\tspeed: 0.0476s/iter; left time: 850.1343s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 223 | Train Loss: 0.0139802 Vali Loss: 0.0200129 Test Loss: 0.0270160\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0130937\n",
      "\tspeed: 0.0852s/iter; left time: 1511.9598s\n",
      "\titers: 200, epoch: 21 | loss: 0.0157320\n",
      "\tspeed: 0.0475s/iter; left time: 838.7522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0139475 Vali Loss: 0.0198937 Test Loss: 0.0268523\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0137721\n",
      "\tspeed: 0.0857s/iter; left time: 1500.5826s\n",
      "\titers: 200, epoch: 22 | loss: 0.0144720\n",
      "\tspeed: 0.0476s/iter; left time: 828.9468s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0139068 Vali Loss: 0.0200308 Test Loss: 0.0272258\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0150069\n",
      "\tspeed: 0.0853s/iter; left time: 1475.4119s\n",
      "\titers: 200, epoch: 23 | loss: 0.0121667\n",
      "\tspeed: 0.0477s/iter; left time: 819.5160s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0138664 Vali Loss: 0.0198748 Test Loss: 0.0270324\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0141130\n",
      "\tspeed: 0.0860s/iter; left time: 1468.1255s\n",
      "\titers: 200, epoch: 24 | loss: 0.0143699\n",
      "\tspeed: 0.0476s/iter; left time: 807.8973s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 223 | Train Loss: 0.0138457 Vali Loss: 0.0199568 Test Loss: 0.0268451\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0144105\n",
      "\tspeed: 0.0862s/iter; left time: 1452.3313s\n",
      "\titers: 200, epoch: 25 | loss: 0.0134898\n",
      "\tspeed: 0.0476s/iter; left time: 796.8914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 223 | Train Loss: 0.0137632 Vali Loss: 0.0200222 Test Loss: 0.0271989\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0126074\n",
      "\tspeed: 0.0852s/iter; left time: 1416.2256s\n",
      "\titers: 200, epoch: 26 | loss: 0.0131049\n",
      "\tspeed: 0.0476s/iter; left time: 786.4616s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:10.84s\n",
      "Steps: 223 | Train Loss: 0.0137622 Vali Loss: 0.0199821 Test Loss: 0.0271711\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0144456\n",
      "\tspeed: 0.0850s/iter; left time: 1394.1905s\n",
      "\titers: 200, epoch: 27 | loss: 0.0144747\n",
      "\tspeed: 0.0476s/iter; left time: 776.8368s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 223 | Train Loss: 0.0137394 Vali Loss: 0.0199936 Test Loss: 0.0273226\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_no_revin_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02662954293191433, rmse:0.16318561136722565, mae:0.10998011380434036, rse:0.5676156878471375\n",
      "Intermediate time for GB and pred_len 24: 00h:06m:08.90s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_no_revin_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1277484\n",
      "\tspeed: 0.0761s/iter; left time: 1681.0916s\n",
      "\titers: 200, epoch: 1 | loss: 0.1105688\n",
      "\tspeed: 0.0474s/iter; left time: 1042.5530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.18s\n",
      "Steps: 222 | Train Loss: 0.1275293 Vali Loss: 0.0996873 Test Loss: 0.1169330\n",
      "Validation loss decreased (inf --> 0.099687).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0396113\n",
      "\tspeed: 0.0902s/iter; left time: 1973.0911s\n",
      "\titers: 200, epoch: 2 | loss: 0.0316610\n",
      "\tspeed: 0.0477s/iter; left time: 1038.1517s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.85s\n",
      "Steps: 222 | Train Loss: 0.0477875 Vali Loss: 0.0329726 Test Loss: 0.0469587\n",
      "Validation loss decreased (0.099687 --> 0.032973).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0259816\n",
      "\tspeed: 0.0880s/iter; left time: 1905.5810s\n",
      "\titers: 200, epoch: 3 | loss: 0.0258741\n",
      "\tspeed: 0.0479s/iter; left time: 1031.7230s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0271180 Vali Loss: 0.0306961 Test Loss: 0.0461644\n",
      "Validation loss decreased (0.032973 --> 0.030696).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0224891\n",
      "\tspeed: 0.0878s/iter; left time: 1881.0279s\n",
      "\titers: 200, epoch: 4 | loss: 0.0245912\n",
      "\tspeed: 0.0478s/iter; left time: 1019.5808s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.86s\n",
      "Steps: 222 | Train Loss: 0.0241611 Vali Loss: 0.0310721 Test Loss: 0.0510838\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0248933\n",
      "\tspeed: 0.0874s/iter; left time: 1853.3116s\n",
      "\titers: 200, epoch: 5 | loss: 0.0234952\n",
      "\tspeed: 0.0479s/iter; left time: 1010.2657s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 222 | Train Loss: 0.0232249 Vali Loss: 0.0314988 Test Loss: 0.0510794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0227747\n",
      "\tspeed: 0.0869s/iter; left time: 1825.0550s\n",
      "\titers: 200, epoch: 6 | loss: 0.0229812\n",
      "\tspeed: 0.0479s/iter; left time: 1000.5854s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 222 | Train Loss: 0.0227524 Vali Loss: 0.0309934 Test Loss: 0.0464433\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0225947\n",
      "\tspeed: 0.0870s/iter; left time: 1806.1535s\n",
      "\titers: 200, epoch: 7 | loss: 0.0214749\n",
      "\tspeed: 0.0479s/iter; left time: 990.3233s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 222 | Train Loss: 0.0223699 Vali Loss: 0.0304921 Test Loss: 0.0436644\n",
      "Validation loss decreased (0.030696 --> 0.030492).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0223484\n",
      "\tspeed: 0.0878s/iter; left time: 1803.9380s\n",
      "\titers: 200, epoch: 8 | loss: 0.0223088\n",
      "\tspeed: 0.0479s/iter; left time: 979.5104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0220757 Vali Loss: 0.0304858 Test Loss: 0.0445817\n",
      "Validation loss decreased (0.030492 --> 0.030486).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0217350\n",
      "\tspeed: 0.0879s/iter; left time: 1786.3577s\n",
      "\titers: 200, epoch: 9 | loss: 0.0211748\n",
      "\tspeed: 0.0479s/iter; left time: 969.1913s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0218478 Vali Loss: 0.0298603 Test Loss: 0.0428153\n",
      "Validation loss decreased (0.030486 --> 0.029860).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0210856\n",
      "\tspeed: 0.0904s/iter; left time: 1817.0286s\n",
      "\titers: 200, epoch: 10 | loss: 0.0210287\n",
      "\tspeed: 0.0480s/iter; left time: 960.3638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0216425 Vali Loss: 0.0306545 Test Loss: 0.0445956\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0210099\n",
      "\tspeed: 0.0871s/iter; left time: 1731.3773s\n",
      "\titers: 200, epoch: 11 | loss: 0.0200109\n",
      "\tspeed: 0.0479s/iter; left time: 946.5862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0215405 Vali Loss: 0.0297202 Test Loss: 0.0420539\n",
      "Validation loss decreased (0.029860 --> 0.029720).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0214709\n",
      "\tspeed: 0.0869s/iter; left time: 1708.8436s\n",
      "\titers: 200, epoch: 12 | loss: 0.0205379\n",
      "\tspeed: 0.0479s/iter; left time: 936.8241s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0213686 Vali Loss: 0.0301132 Test Loss: 0.0428274\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0192222\n",
      "\tspeed: 0.0876s/iter; left time: 1702.8857s\n",
      "\titers: 200, epoch: 13 | loss: 0.0211983\n",
      "\tspeed: 0.0479s/iter; left time: 926.5020s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 222 | Train Loss: 0.0212856 Vali Loss: 0.0303281 Test Loss: 0.0432985\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0204989\n",
      "\tspeed: 0.0868s/iter; left time: 1667.2094s\n",
      "\titers: 200, epoch: 14 | loss: 0.0211937\n",
      "\tspeed: 0.0481s/iter; left time: 919.1514s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0212257 Vali Loss: 0.0301415 Test Loss: 0.0426247\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0236250\n",
      "\tspeed: 0.0866s/iter; left time: 1645.6826s\n",
      "\titers: 200, epoch: 15 | loss: 0.0221896\n",
      "\tspeed: 0.0479s/iter; left time: 904.5638s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.88s\n",
      "Steps: 222 | Train Loss: 0.0210869 Vali Loss: 0.0300319 Test Loss: 0.0423191\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0209516\n",
      "\tspeed: 0.0861s/iter; left time: 1616.5395s\n",
      "\titers: 200, epoch: 16 | loss: 0.0186915\n",
      "\tspeed: 0.0479s/iter; left time: 894.8879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0209981 Vali Loss: 0.0303171 Test Loss: 0.0429077\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0201732\n",
      "\tspeed: 0.0869s/iter; left time: 1612.7192s\n",
      "\titers: 200, epoch: 17 | loss: 0.0209619\n",
      "\tspeed: 0.0480s/iter; left time: 884.9696s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.90s\n",
      "Steps: 222 | Train Loss: 0.0209122 Vali Loss: 0.0307355 Test Loss: 0.0434550\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0222549\n",
      "\tspeed: 0.0869s/iter; left time: 1591.8212s\n",
      "\titers: 200, epoch: 18 | loss: 0.0199029\n",
      "\tspeed: 0.0480s/iter; left time: 874.2991s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0208540 Vali Loss: 0.0302686 Test Loss: 0.0431604\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0193379\n",
      "\tspeed: 0.0869s/iter; left time: 1572.6425s\n",
      "\titers: 200, epoch: 19 | loss: 0.0223832\n",
      "\tspeed: 0.0480s/iter; left time: 863.4604s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.89s\n",
      "Steps: 222 | Train Loss: 0.0207686 Vali Loss: 0.0303039 Test Loss: 0.0429730\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0196274\n",
      "\tspeed: 0.0872s/iter; left time: 1559.8350s\n",
      "\titers: 200, epoch: 20 | loss: 0.0206015\n",
      "\tspeed: 0.0479s/iter; left time: 851.5192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0207312 Vali Loss: 0.0301241 Test Loss: 0.0424815\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0198095\n",
      "\tspeed: 0.0866s/iter; left time: 1529.9866s\n",
      "\titers: 200, epoch: 21 | loss: 0.0227735\n",
      "\tspeed: 0.0480s/iter; left time: 842.8760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:10.87s\n",
      "Steps: 222 | Train Loss: 0.0206810 Vali Loss: 0.0301876 Test Loss: 0.0427990\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_no_revin_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04205385968089104, rmse:0.2050703763961792, mae:0.14662711322307587, rse:0.7151989340782166\n",
      "Intermediate time for GB and pred_len 96: 00h:04m:56.41s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_outliers_no_revin_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=0, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_outliers_no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1237701\n",
      "\tspeed: 0.0777s/iter; left time: 1716.7565s\n",
      "\titers: 200, epoch: 1 | loss: 0.1164568\n",
      "\tspeed: 0.0476s/iter; left time: 1048.3253s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.28s\n",
      "Steps: 222 | Train Loss: 0.1279938 Vali Loss: 0.1003439 Test Loss: 0.1168319\n",
      "Validation loss decreased (inf --> 0.100344).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0414207\n",
      "\tspeed: 0.0885s/iter; left time: 1936.2712s\n",
      "\titers: 200, epoch: 2 | loss: 0.0309294\n",
      "\tspeed: 0.0476s/iter; left time: 1037.6290s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 222 | Train Loss: 0.0481190 Vali Loss: 0.0343527 Test Loss: 0.0490068\n",
      "Validation loss decreased (0.100344 --> 0.034353).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0287404\n",
      "\tspeed: 0.0913s/iter; left time: 1978.3668s\n",
      "\titers: 200, epoch: 3 | loss: 0.0275198\n",
      "\tspeed: 0.0481s/iter; left time: 1036.8234s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 222 | Train Loss: 0.0279552 Vali Loss: 0.0322467 Test Loss: 0.0512701\n",
      "Validation loss decreased (0.034353 --> 0.032247).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0268840\n",
      "\tspeed: 0.0913s/iter; left time: 1957.8268s\n",
      "\titers: 200, epoch: 4 | loss: 0.0240252\n",
      "\tspeed: 0.0480s/iter; left time: 1024.8803s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 222 | Train Loss: 0.0254077 Vali Loss: 0.0327636 Test Loss: 0.0536329\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0233899\n",
      "\tspeed: 0.0893s/iter; left time: 1895.2890s\n",
      "\titers: 200, epoch: 5 | loss: 0.0252921\n",
      "\tspeed: 0.0479s/iter; left time: 1010.6618s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:10.99s\n",
      "Steps: 222 | Train Loss: 0.0246268 Vali Loss: 0.0333259 Test Loss: 0.0539267\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0226340\n",
      "\tspeed: 0.0890s/iter; left time: 1868.4848s\n",
      "\titers: 200, epoch: 6 | loss: 0.0243396\n",
      "\tspeed: 0.0481s/iter; left time: 1004.6214s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.99s\n",
      "Steps: 222 | Train Loss: 0.0241429 Vali Loss: 0.0325398 Test Loss: 0.0486072\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0234801\n",
      "\tspeed: 0.0908s/iter; left time: 1885.3515s\n",
      "\titers: 200, epoch: 7 | loss: 0.0236296\n",
      "\tspeed: 0.0482s/iter; left time: 996.1790s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.03s\n",
      "Steps: 222 | Train Loss: 0.0237094 Vali Loss: 0.0326204 Test Loss: 0.0464402\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0234830\n",
      "\tspeed: 0.0884s/iter; left time: 1815.8246s\n",
      "\titers: 200, epoch: 8 | loss: 0.0215452\n",
      "\tspeed: 0.0481s/iter; left time: 983.4910s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 222 | Train Loss: 0.0233615 Vali Loss: 0.0330032 Test Loss: 0.0461395\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0232363\n",
      "\tspeed: 0.0885s/iter; left time: 1798.4050s\n",
      "\titers: 200, epoch: 9 | loss: 0.0230493\n",
      "\tspeed: 0.0481s/iter; left time: 972.6111s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 222 | Train Loss: 0.0231210 Vali Loss: 0.0320860 Test Loss: 0.0435966\n",
      "Validation loss decreased (0.032247 --> 0.032086).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0227674\n",
      "\tspeed: 0.0889s/iter; left time: 1787.5593s\n",
      "\titers: 200, epoch: 10 | loss: 0.0230295\n",
      "\tspeed: 0.0481s/iter; left time: 961.4519s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 222 | Train Loss: 0.0229645 Vali Loss: 0.0321824 Test Loss: 0.0431622\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0241527\n",
      "\tspeed: 0.0885s/iter; left time: 1759.0596s\n",
      "\titers: 200, epoch: 11 | loss: 0.0232794\n",
      "\tspeed: 0.0481s/iter; left time: 951.1272s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:10.91s\n",
      "Steps: 222 | Train Loss: 0.0228119 Vali Loss: 0.0323835 Test Loss: 0.0437122\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0241883\n",
      "\tspeed: 0.0880s/iter; left time: 1729.1273s\n",
      "\titers: 200, epoch: 12 | loss: 0.0225999\n",
      "\tspeed: 0.0480s/iter; left time: 938.4961s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0226900 Vali Loss: 0.0326535 Test Loss: 0.0442543\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0212769\n",
      "\tspeed: 0.0880s/iter; left time: 1710.8001s\n",
      "\titers: 200, epoch: 13 | loss: 0.0227571\n",
      "\tspeed: 0.0480s/iter; left time: 928.6161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 222 | Train Loss: 0.0225336 Vali Loss: 0.0325103 Test Loss: 0.0440546\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0208707\n",
      "\tspeed: 0.0879s/iter; left time: 1688.0395s\n",
      "\titers: 200, epoch: 14 | loss: 0.0224494\n",
      "\tspeed: 0.0480s/iter; left time: 917.5247s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:10.96s\n",
      "Steps: 222 | Train Loss: 0.0224375 Vali Loss: 0.0330522 Test Loss: 0.0451421\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0230468\n",
      "\tspeed: 0.0880s/iter; left time: 1671.9936s\n",
      "\titers: 200, epoch: 15 | loss: 0.0212044\n",
      "\tspeed: 0.0481s/iter; left time: 909.4586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:10.94s\n",
      "Steps: 222 | Train Loss: 0.0223891 Vali Loss: 0.0324046 Test Loss: 0.0438901\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0217093\n",
      "\tspeed: 0.0880s/iter; left time: 1650.9228s\n",
      "\titers: 200, epoch: 16 | loss: 0.0222104\n",
      "\tspeed: 0.0480s/iter; left time: 896.3471s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0223561 Vali Loss: 0.0327737 Test Loss: 0.0449965\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0240783\n",
      "\tspeed: 0.0882s/iter; left time: 1636.3138s\n",
      "\titers: 200, epoch: 17 | loss: 0.0236116\n",
      "\tspeed: 0.0480s/iter; left time: 885.4251s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 222 | Train Loss: 0.0221888 Vali Loss: 0.0326069 Test Loss: 0.0444371\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0231026\n",
      "\tspeed: 0.0883s/iter; left time: 1617.7693s\n",
      "\titers: 200, epoch: 18 | loss: 0.0220676\n",
      "\tspeed: 0.0480s/iter; left time: 874.6553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:10.93s\n",
      "Steps: 222 | Train Loss: 0.0221504 Vali Loss: 0.0323706 Test Loss: 0.0439783\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0210069\n",
      "\tspeed: 0.0878s/iter; left time: 1589.4847s\n",
      "\titers: 200, epoch: 19 | loss: 0.0220666\n",
      "\tspeed: 0.0481s/iter; left time: 866.0427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:10.92s\n",
      "Steps: 222 | Train Loss: 0.0220821 Vali Loss: 0.0327219 Test Loss: 0.0447983\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_outliers_no_revin_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04359659552574158, rmse:0.20879797637462616, mae:0.15036597847938538, rse:0.7302772402763367\n",
      "Intermediate time for GB and pred_len 168: 00h:04m:32.60s\n",
      "Intermediate time for GB: 00h:15m:37.92s\n",
      "Total time: 00h:15m:37.96s\n"
     ]
    }
   ],
   "source": [
    "patchtst_results = []\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"no_outliers_no_revin_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data_new.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --revin 0 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026630</td>\n",
       "      <td>0.163186</td>\n",
       "      <td>0.109980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0.042054</td>\n",
       "      <td>0.205070</td>\n",
       "      <td>0.146627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.208798</td>\n",
       "      <td>0.150366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Iteration       MSE      RMSE       MAE\n",
       "Country Pred_len                                         \n",
       "GB      24                1  0.026630  0.163186  0.109980\n",
       "        96                1  0.042054  0.205070  0.146627\n",
       "        168               1  0.043597  0.208798  0.150366"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_results_into_df(patchtst_results, if_loss_fnc=False, itr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/informer/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic variables\n",
    "seq_len = 96\n",
    "model = \"Informer\"\n",
    "itr = 1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_gb_no_outliers.log\"\n",
    "\n",
    "# Parameters for tuning\n",
    "lr = 0.0001\n",
    "#n_heads = 16\n",
    "e_layers = 2\n",
    "d_layers = 1\n",
    "loss = \"MSE\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Lists to store the results\n",
    "informer_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_24_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0772443\n",
      "\tspeed: 0.3727s/iter; left time: 6715.6062s\n",
      "\titers: 200, epoch: 1 | loss: 0.0665340\n",
      "\tspeed: 0.2383s/iter; left time: 4269.8613s\n",
      "\titers: 300, epoch: 1 | loss: 0.0574210\n",
      "\tspeed: 0.2651s/iter; left time: 4723.7911s\n",
      "\titers: 400, epoch: 1 | loss: 0.0559277\n",
      "\tspeed: 0.2690s/iter; left time: 4766.0906s\n",
      "\titers: 500, epoch: 1 | loss: 0.0479041\n",
      "\tspeed: 0.2627s/iter; left time: 4628.2347s\n",
      "\titers: 600, epoch: 1 | loss: 0.0463593\n",
      "\tspeed: 0.2675s/iter; left time: 4686.8457s\n",
      "\titers: 700, epoch: 1 | loss: 0.0443947\n",
      "\tspeed: 0.2681s/iter; left time: 4670.6560s\n",
      "\titers: 800, epoch: 1 | loss: 0.0387929\n",
      "\tspeed: 0.2605s/iter; left time: 4511.9840s\n",
      "\titers: 900, epoch: 1 | loss: 0.0373599\n",
      "\tspeed: 0.2702s/iter; left time: 4652.9213s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:04m:06.68s\n",
      "Steps: 906 | Train Loss: 0.0596497 Vali Loss: 0.0437670 Test Loss: 0.0612638\n",
      "Validation loss decreased (inf --> 0.043767).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0317354\n",
      "\tspeed: 0.8435s/iter; left time: 14436.8990s\n",
      "\titers: 200, epoch: 2 | loss: 0.0268772\n",
      "\tspeed: 0.2675s/iter; left time: 4551.2884s\n",
      "\titers: 300, epoch: 2 | loss: 0.0155954\n",
      "\tspeed: 0.2633s/iter; left time: 4453.1947s\n",
      "\titers: 400, epoch: 2 | loss: 0.0169977\n",
      "\tspeed: 0.2496s/iter; left time: 4197.2884s\n",
      "\titers: 500, epoch: 2 | loss: 0.0201362\n",
      "\tspeed: 0.2676s/iter; left time: 4473.3862s\n",
      "\titers: 600, epoch: 2 | loss: 0.0128809\n",
      "\tspeed: 0.2628s/iter; left time: 4366.3521s\n",
      "\titers: 700, epoch: 2 | loss: 0.0234461\n",
      "\tspeed: 0.2678s/iter; left time: 4423.2021s\n",
      "\titers: 800, epoch: 2 | loss: 0.0200652\n",
      "\tspeed: 0.2647s/iter; left time: 4344.7978s\n",
      "\titers: 900, epoch: 2 | loss: 0.0135218\n",
      "\tspeed: 0.2603s/iter; left time: 4246.0161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:03m:58.57s\n",
      "Steps: 906 | Train Loss: 0.0202198 Vali Loss: 0.0222250 Test Loss: 0.0311381\n",
      "Validation loss decreased (0.043767 --> 0.022225).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0161514\n",
      "\tspeed: 0.8418s/iter; left time: 13645.2619s\n",
      "\titers: 200, epoch: 3 | loss: 0.0153901\n",
      "\tspeed: 0.2667s/iter; left time: 4296.6235s\n",
      "\titers: 300, epoch: 3 | loss: 0.0174698\n",
      "\tspeed: 0.2662s/iter; left time: 4261.4076s\n",
      "\titers: 400, epoch: 3 | loss: 0.0135069\n",
      "\tspeed: 0.2425s/iter; left time: 3857.5744s\n",
      "\titers: 500, epoch: 3 | loss: 0.0147586\n",
      "\tspeed: 0.2721s/iter; left time: 4301.6572s\n",
      "\titers: 600, epoch: 3 | loss: 0.0133049\n",
      "\tspeed: 0.2624s/iter; left time: 4122.7786s\n",
      "\titers: 700, epoch: 3 | loss: 0.0149954\n",
      "\tspeed: 0.2633s/iter; left time: 4109.1282s\n",
      "\titers: 800, epoch: 3 | loss: 0.0108741\n",
      "\tspeed: 0.2682s/iter; left time: 4159.7454s\n",
      "\titers: 900, epoch: 3 | loss: 0.0146226\n",
      "\tspeed: 0.2584s/iter; left time: 3981.8122s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:03m:57.61s\n",
      "Steps: 906 | Train Loss: 0.0151053 Vali Loss: 0.0220440 Test Loss: 0.0312617\n",
      "Validation loss decreased (0.022225 --> 0.022044).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0122296\n",
      "\tspeed: 0.8420s/iter; left time: 12885.6067s\n",
      "\titers: 200, epoch: 4 | loss: 0.0158181\n",
      "\tspeed: 0.2611s/iter; left time: 3970.0934s\n",
      "\titers: 300, epoch: 4 | loss: 0.0147865\n",
      "\tspeed: 0.2722s/iter; left time: 4111.5584s\n",
      "\titers: 400, epoch: 4 | loss: 0.0166783\n",
      "\tspeed: 0.2651s/iter; left time: 3977.2682s\n",
      "\titers: 500, epoch: 4 | loss: 0.0202066\n",
      "\tspeed: 0.2630s/iter; left time: 3919.0709s\n",
      "\titers: 600, epoch: 4 | loss: 0.0158609\n",
      "\tspeed: 0.2700s/iter; left time: 3997.0793s\n",
      "\titers: 700, epoch: 4 | loss: 0.0120254\n",
      "\tspeed: 0.2585s/iter; left time: 3801.2918s\n",
      "\titers: 800, epoch: 4 | loss: 0.0131718\n",
      "\tspeed: 0.2675s/iter; left time: 3906.0659s\n",
      "\titers: 900, epoch: 4 | loss: 0.0123550\n",
      "\tspeed: 0.2698s/iter; left time: 3912.7812s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:04m:00.81s\n",
      "Steps: 906 | Train Loss: 0.0139492 Vali Loss: 0.0227810 Test Loss: 0.0311168\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0180273\n",
      "\tspeed: 0.8366s/iter; left time: 12044.8312s\n",
      "\titers: 200, epoch: 5 | loss: 0.0118298\n",
      "\tspeed: 0.2589s/iter; left time: 3701.3364s\n",
      "\titers: 300, epoch: 5 | loss: 0.0114985\n",
      "\tspeed: 0.2482s/iter; left time: 3523.0795s\n",
      "\titers: 400, epoch: 5 | loss: 0.0176274\n",
      "\tspeed: 0.2823s/iter; left time: 3979.9314s\n",
      "\titers: 500, epoch: 5 | loss: 0.0107142\n",
      "\tspeed: 0.2752s/iter; left time: 3852.4108s\n",
      "\titers: 600, epoch: 5 | loss: 0.0116690\n",
      "\tspeed: 0.2808s/iter; left time: 3902.5007s\n",
      "\titers: 700, epoch: 5 | loss: 0.0149052\n",
      "\tspeed: 0.2828s/iter; left time: 3901.2693s\n",
      "\titers: 800, epoch: 5 | loss: 0.0152470\n",
      "\tspeed: 0.2754s/iter; left time: 3771.8216s\n",
      "\titers: 900, epoch: 5 | loss: 0.0127046\n",
      "\tspeed: 0.2730s/iter; left time: 3712.1529s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:04m:06.57s\n",
      "Steps: 906 | Train Loss: 0.0130435 Vali Loss: 0.0243139 Test Loss: 0.0332743\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0119907\n",
      "\tspeed: 0.8882s/iter; left time: 11983.3310s\n",
      "\titers: 200, epoch: 6 | loss: 0.0128953\n",
      "\tspeed: 0.2844s/iter; left time: 3808.4743s\n",
      "\titers: 300, epoch: 6 | loss: 0.0111791\n",
      "\tspeed: 0.2741s/iter; left time: 3642.6758s\n",
      "\titers: 400, epoch: 6 | loss: 0.0130701\n",
      "\tspeed: 0.2725s/iter; left time: 3595.0991s\n",
      "\titers: 500, epoch: 6 | loss: 0.0139359\n",
      "\tspeed: 0.2833s/iter; left time: 3708.6411s\n",
      "\titers: 600, epoch: 6 | loss: 0.0105510\n",
      "\tspeed: 0.2722s/iter; left time: 3536.3513s\n",
      "\titers: 700, epoch: 6 | loss: 0.0118018\n",
      "\tspeed: 0.2791s/iter; left time: 3598.0050s\n",
      "\titers: 800, epoch: 6 | loss: 0.0098152\n",
      "\tspeed: 0.2832s/iter; left time: 3622.0938s\n",
      "\titers: 900, epoch: 6 | loss: 0.0158376\n",
      "\tspeed: 0.2790s/iter; left time: 3540.8553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:04m:12.77s\n",
      "Steps: 906 | Train Loss: 0.0121461 Vali Loss: 0.0260641 Test Loss: 0.0357538\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0108146\n",
      "\tspeed: 0.8873s/iter; left time: 11167.1747s\n",
      "\titers: 200, epoch: 7 | loss: 0.0118945\n",
      "\tspeed: 0.2778s/iter; left time: 3467.7360s\n",
      "\titers: 300, epoch: 7 | loss: 0.0115980\n",
      "\tspeed: 0.2848s/iter; left time: 3526.8623s\n",
      "\titers: 400, epoch: 7 | loss: 0.0122997\n",
      "\tspeed: 0.2240s/iter; left time: 2751.2369s\n",
      "\titers: 500, epoch: 7 | loss: 0.0103911\n",
      "\tspeed: 0.2665s/iter; left time: 3247.2211s\n",
      "\titers: 600, epoch: 7 | loss: 0.0076208\n",
      "\tspeed: 0.1537s/iter; left time: 1856.9477s\n",
      "\titers: 700, epoch: 7 | loss: 0.0111306\n",
      "\tspeed: 0.2560s/iter; left time: 3067.7745s\n",
      "\titers: 800, epoch: 7 | loss: 0.0103596\n",
      "\tspeed: 0.2711s/iter; left time: 3222.5555s\n",
      "\titers: 900, epoch: 7 | loss: 0.0102603\n",
      "\tspeed: 0.2601s/iter; left time: 3065.6723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:03m:49.00s\n",
      "Steps: 906 | Train Loss: 0.0111766 Vali Loss: 0.0229357 Test Loss: 0.0333902\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0101313\n",
      "\tspeed: 0.8360s/iter; left time: 9763.1855s\n",
      "\titers: 200, epoch: 8 | loss: 0.0094223\n",
      "\tspeed: 0.2570s/iter; left time: 2975.7650s\n",
      "\titers: 300, epoch: 8 | loss: 0.0103423\n",
      "\tspeed: 0.2654s/iter; left time: 3046.8799s\n",
      "\titers: 400, epoch: 8 | loss: 0.0107818\n",
      "\tspeed: 0.2691s/iter; left time: 3061.9460s\n",
      "\titers: 500, epoch: 8 | loss: 0.0091713\n",
      "\tspeed: 0.2646s/iter; left time: 2984.1928s\n",
      "\titers: 600, epoch: 8 | loss: 0.0108418\n",
      "\tspeed: 0.2660s/iter; left time: 2973.8777s\n",
      "\titers: 700, epoch: 8 | loss: 0.0100406\n",
      "\tspeed: 0.2656s/iter; left time: 2942.4679s\n",
      "\titers: 800, epoch: 8 | loss: 0.0093689\n",
      "\tspeed: 0.2619s/iter; left time: 2875.7539s\n",
      "\titers: 900, epoch: 8 | loss: 0.0084888\n",
      "\tspeed: 0.2660s/iter; left time: 2894.2810s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:04m:00.02s\n",
      "Steps: 906 | Train Loss: 0.0101429 Vali Loss: 0.0250434 Test Loss: 0.0369369\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_24_GB_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.031276024878025055, rmse:0.17685028910636902, mae:0.1218523159623146, rse:0.6148375272750854\n",
      "Intermediate time for GB and pred_len 24: 00h:40m:22.33s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_96_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0888242\n",
      "\tspeed: 0.3087s/iter; left time: 5550.7764s\n",
      "\titers: 200, epoch: 1 | loss: 0.0705890\n",
      "\tspeed: 0.2707s/iter; left time: 4840.1257s\n",
      "\titers: 300, epoch: 1 | loss: 0.0684523\n",
      "\tspeed: 0.2637s/iter; left time: 4689.7115s\n",
      "\titers: 400, epoch: 1 | loss: 0.0652739\n",
      "\tspeed: 0.2676s/iter; left time: 4731.7548s\n",
      "\titers: 500, epoch: 1 | loss: 0.0586714\n",
      "\tspeed: 0.2723s/iter; left time: 4786.9711s\n",
      "\titers: 600, epoch: 1 | loss: 0.0544381\n",
      "\tspeed: 0.2607s/iter; left time: 4556.9620s\n",
      "\titers: 700, epoch: 1 | loss: 0.0506051\n",
      "\tspeed: 0.2712s/iter; left time: 4714.4472s\n",
      "\titers: 800, epoch: 1 | loss: 0.0533382\n",
      "\tspeed: 0.2615s/iter; left time: 4519.1328s\n",
      "\titers: 900, epoch: 1 | loss: 0.0501792\n",
      "\tspeed: 0.2666s/iter; left time: 4579.6338s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:04m:03.42s\n",
      "Steps: 904 | Train Loss: 0.0655034 Vali Loss: 0.0554776 Test Loss: 0.0856784\n",
      "Validation loss decreased (inf --> 0.055478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0354850\n",
      "\tspeed: 0.8196s/iter; left time: 13996.4660s\n",
      "\titers: 200, epoch: 2 | loss: 0.0330975\n",
      "\tspeed: 0.2700s/iter; left time: 4584.1034s\n",
      "\titers: 300, epoch: 2 | loss: 0.0290257\n",
      "\tspeed: 0.2646s/iter; left time: 4465.0309s\n",
      "\titers: 400, epoch: 2 | loss: 0.0290518\n",
      "\tspeed: 0.2640s/iter; left time: 4429.8032s\n",
      "\titers: 500, epoch: 2 | loss: 0.0271741\n",
      "\tspeed: 0.2704s/iter; left time: 4508.6885s\n",
      "\titers: 600, epoch: 2 | loss: 0.0304372\n",
      "\tspeed: 0.2647s/iter; left time: 4387.8123s\n",
      "\titers: 700, epoch: 2 | loss: 0.0243599\n",
      "\tspeed: 0.2663s/iter; left time: 4387.9320s\n",
      "\titers: 800, epoch: 2 | loss: 0.0261583\n",
      "\tspeed: 0.2708s/iter; left time: 4434.8400s\n",
      "\titers: 900, epoch: 2 | loss: 0.0309401\n",
      "\tspeed: 0.2648s/iter; left time: 4309.7408s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:04m:01.23s\n",
      "Steps: 904 | Train Loss: 0.0307574 Vali Loss: 0.0388125 Test Loss: 0.0588348\n",
      "Validation loss decreased (0.055478 --> 0.038813).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0227409\n",
      "\tspeed: 0.8052s/iter; left time: 13022.0482s\n",
      "\titers: 200, epoch: 3 | loss: 0.0249779\n",
      "\tspeed: 0.2634s/iter; left time: 4234.0357s\n",
      "\titers: 300, epoch: 3 | loss: 0.0234726\n",
      "\tspeed: 0.2708s/iter; left time: 4325.5399s\n",
      "\titers: 400, epoch: 3 | loss: 0.0235839\n",
      "\tspeed: 0.2601s/iter; left time: 4127.9864s\n",
      "\titers: 500, epoch: 3 | loss: 0.0225263\n",
      "\tspeed: 0.2659s/iter; left time: 4193.3534s\n",
      "\titers: 600, epoch: 3 | loss: 0.0255288\n",
      "\tspeed: 0.2680s/iter; left time: 4200.7289s\n",
      "\titers: 700, epoch: 3 | loss: 0.0223419\n",
      "\tspeed: 0.2629s/iter; left time: 4094.6485s\n",
      "\titers: 800, epoch: 3 | loss: 0.0206777\n",
      "\tspeed: 0.2646s/iter; left time: 4093.6883s\n",
      "\titers: 900, epoch: 3 | loss: 0.0217625\n",
      "\tspeed: 0.2677s/iter; left time: 4116.0916s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:03m:59.33s\n",
      "Steps: 904 | Train Loss: 0.0231717 Vali Loss: 0.0329526 Test Loss: 0.0498644\n",
      "Validation loss decreased (0.038813 --> 0.032953).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0244380\n",
      "\tspeed: 0.8176s/iter; left time: 12483.8907s\n",
      "\titers: 200, epoch: 4 | loss: 0.0217591\n",
      "\tspeed: 0.2641s/iter; left time: 4006.1975s\n",
      "\titers: 300, epoch: 4 | loss: 0.0263575\n",
      "\tspeed: 0.2723s/iter; left time: 4102.9564s\n",
      "\titers: 400, epoch: 4 | loss: 0.0192778\n",
      "\tspeed: 0.2621s/iter; left time: 3923.1585s\n",
      "\titers: 500, epoch: 4 | loss: 0.0190167\n",
      "\tspeed: 0.2618s/iter; left time: 3892.4955s\n",
      "\titers: 600, epoch: 4 | loss: 0.0213376\n",
      "\tspeed: 0.2717s/iter; left time: 4013.0626s\n",
      "\titers: 700, epoch: 4 | loss: 0.0180658\n",
      "\tspeed: 0.2636s/iter; left time: 3866.3738s\n",
      "\titers: 800, epoch: 4 | loss: 0.0215900\n",
      "\tspeed: 0.2653s/iter; left time: 3865.7433s\n",
      "\titers: 900, epoch: 4 | loss: 0.0223565\n",
      "\tspeed: 0.2715s/iter; left time: 3927.6662s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:04m:01.43s\n",
      "Steps: 904 | Train Loss: 0.0205717 Vali Loss: 0.0343880 Test Loss: 0.0540990\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0184004\n",
      "\tspeed: 0.8028s/iter; left time: 11531.8754s\n",
      "\titers: 200, epoch: 5 | loss: 0.0186673\n",
      "\tspeed: 0.2592s/iter; left time: 3696.7759s\n",
      "\titers: 300, epoch: 5 | loss: 0.0169819\n",
      "\tspeed: 0.2514s/iter; left time: 3561.1577s\n",
      "\titers: 400, epoch: 5 | loss: 0.0202467\n",
      "\tspeed: 0.2586s/iter; left time: 3637.2880s\n",
      "\titers: 500, epoch: 5 | loss: 0.0175338\n",
      "\tspeed: 0.2517s/iter; left time: 3515.1361s\n",
      "\titers: 600, epoch: 5 | loss: 0.0207296\n",
      "\tspeed: 0.2661s/iter; left time: 3688.8451s\n",
      "\titers: 700, epoch: 5 | loss: 0.0230584\n",
      "\tspeed: 0.2523s/iter; left time: 3472.2257s\n",
      "\titers: 800, epoch: 5 | loss: 0.0220162\n",
      "\tspeed: 0.2607s/iter; left time: 3562.6503s\n",
      "\titers: 900, epoch: 5 | loss: 0.0194697\n",
      "\tspeed: 0.2671s/iter; left time: 3623.3071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:03m:55.19s\n",
      "Steps: 904 | Train Loss: 0.0186928 Vali Loss: 0.0351914 Test Loss: 0.0550853\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0176253\n",
      "\tspeed: 0.8006s/iter; left time: 10776.3316s\n",
      "\titers: 200, epoch: 6 | loss: 0.0151356\n",
      "\tspeed: 0.2593s/iter; left time: 3463.9499s\n",
      "\titers: 300, epoch: 6 | loss: 0.0163791\n",
      "\tspeed: 0.2410s/iter; left time: 3196.0743s\n",
      "\titers: 400, epoch: 6 | loss: 0.0177617\n",
      "\tspeed: 0.2624s/iter; left time: 3454.0192s\n",
      "\titers: 500, epoch: 6 | loss: 0.0156806\n",
      "\tspeed: 0.2543s/iter; left time: 3321.4350s\n",
      "\titers: 600, epoch: 6 | loss: 0.0171245\n",
      "\tspeed: 0.2706s/iter; left time: 3507.8579s\n",
      "\titers: 700, epoch: 6 | loss: 0.0156903\n",
      "\tspeed: 0.2605s/iter; left time: 3349.9496s\n",
      "\titers: 800, epoch: 6 | loss: 0.0161198\n",
      "\tspeed: 0.2610s/iter; left time: 3330.7061s\n",
      "\titers: 900, epoch: 6 | loss: 0.0172870\n",
      "\tspeed: 0.2695s/iter; left time: 3412.7534s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:03m:55.92s\n",
      "Steps: 904 | Train Loss: 0.0166939 Vali Loss: 0.0374359 Test Loss: 0.0612993\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0145877\n",
      "\tspeed: 0.7917s/iter; left time: 9941.9049s\n",
      "\titers: 200, epoch: 7 | loss: 0.0141682\n",
      "\tspeed: 0.2613s/iter; left time: 3255.2061s\n",
      "\titers: 300, epoch: 7 | loss: 0.0137427\n",
      "\tspeed: 0.2491s/iter; left time: 3078.4363s\n",
      "\titers: 400, epoch: 7 | loss: 0.0157436\n",
      "\tspeed: 0.2679s/iter; left time: 3283.4278s\n",
      "\titers: 500, epoch: 7 | loss: 0.0166169\n",
      "\tspeed: 0.2654s/iter; left time: 3226.0390s\n",
      "\titers: 600, epoch: 7 | loss: 0.0160536\n",
      "\tspeed: 0.2635s/iter; left time: 3177.2840s\n",
      "\titers: 700, epoch: 7 | loss: 0.0167722\n",
      "\tspeed: 0.2666s/iter; left time: 3187.8338s\n",
      "\titers: 800, epoch: 7 | loss: 0.0134341\n",
      "\tspeed: 0.2729s/iter; left time: 3236.1702s\n",
      "\titers: 900, epoch: 7 | loss: 0.0141654\n",
      "\tspeed: 0.2791s/iter; left time: 3280.8884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:04m:00.83s\n",
      "Steps: 904 | Train Loss: 0.0148130 Vali Loss: 0.0393599 Test Loss: 0.0670252\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0146132\n",
      "\tspeed: 0.8522s/iter; left time: 9931.2495s\n",
      "\titers: 200, epoch: 8 | loss: 0.0132051\n",
      "\tspeed: 0.2803s/iter; left time: 3238.1488s\n",
      "\titers: 300, epoch: 8 | loss: 0.0128555\n",
      "\tspeed: 0.2729s/iter; left time: 3125.3513s\n",
      "\titers: 400, epoch: 8 | loss: 0.0109993\n",
      "\tspeed: 0.2800s/iter; left time: 3178.6752s\n",
      "\titers: 500, epoch: 8 | loss: 0.0143175\n",
      "\tspeed: 0.2798s/iter; left time: 3148.0893s\n",
      "\titers: 600, epoch: 8 | loss: 0.0144558\n",
      "\tspeed: 0.2781s/iter; left time: 3101.1244s\n",
      "\titers: 700, epoch: 8 | loss: 0.0126671\n",
      "\tspeed: 0.2824s/iter; left time: 3120.9997s\n",
      "\titers: 800, epoch: 8 | loss: 0.0143060\n",
      "\tspeed: 0.2706s/iter; left time: 2964.1303s\n",
      "\titers: 900, epoch: 8 | loss: 0.0140634\n",
      "\tspeed: 0.2783s/iter; left time: 3020.4729s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:04m:11.67s\n",
      "Steps: 904 | Train Loss: 0.0131424 Vali Loss: 0.0388623 Test Loss: 0.0613639\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_96_GB_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04987471550703049, rmse:0.22332647442817688, mae:0.16049449145793915, rse:0.7788684964179993\n",
      "Intermediate time for GB and pred_len 96: 00h:39m:56.31s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_96_168_GB', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data_new.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0829070\n",
      "\tspeed: 0.3236s/iter; left time: 5805.7733s\n",
      "\titers: 200, epoch: 1 | loss: 0.0759974\n",
      "\tspeed: 0.2875s/iter; left time: 5128.8972s\n",
      "\titers: 300, epoch: 1 | loss: 0.0631866\n",
      "\tspeed: 0.2790s/iter; left time: 4949.6128s\n",
      "\titers: 400, epoch: 1 | loss: 0.0604318\n",
      "\tspeed: 0.2817s/iter; left time: 4969.4839s\n",
      "\titers: 500, epoch: 1 | loss: 0.0626196\n",
      "\tspeed: 0.2978s/iter; left time: 5224.2360s\n",
      "\titers: 600, epoch: 1 | loss: 0.0595129\n",
      "\tspeed: 0.2818s/iter; left time: 4914.0308s\n",
      "\titers: 700, epoch: 1 | loss: 0.0533847\n",
      "\tspeed: 0.2938s/iter; left time: 5094.6498s\n",
      "\titers: 800, epoch: 1 | loss: 0.0519579\n",
      "\tspeed: 0.2043s/iter; left time: 3522.0502s\n",
      "\titers: 900, epoch: 1 | loss: 0.0505214\n",
      "\tspeed: 0.2707s/iter; left time: 4639.4111s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:04m:10.60s\n",
      "Steps: 902 | Train Loss: 0.0663622 Vali Loss: 0.0573445 Test Loss: 0.0869258\n",
      "Validation loss decreased (inf --> 0.057345).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0426472\n",
      "\tspeed: 0.8747s/iter; left time: 14903.5027s\n",
      "\titers: 200, epoch: 2 | loss: 0.0349004\n",
      "\tspeed: 0.2649s/iter; left time: 4486.3675s\n",
      "\titers: 300, epoch: 2 | loss: 0.0358213\n",
      "\tspeed: 0.2771s/iter; left time: 4665.8182s\n",
      "\titers: 400, epoch: 2 | loss: 0.0284545\n",
      "\tspeed: 0.2914s/iter; left time: 4878.3336s\n",
      "\titers: 500, epoch: 2 | loss: 0.0290951\n",
      "\tspeed: 0.2840s/iter; left time: 4725.9033s\n",
      "\titers: 600, epoch: 2 | loss: 0.0292360\n",
      "\tspeed: 0.2774s/iter; left time: 4588.7344s\n",
      "\titers: 700, epoch: 2 | loss: 0.0296331\n",
      "\tspeed: 0.2751s/iter; left time: 4522.5252s\n",
      "\titers: 800, epoch: 2 | loss: 0.0293463\n",
      "\tspeed: 0.2639s/iter; left time: 4311.3517s\n",
      "\titers: 900, epoch: 2 | loss: 0.0310534\n",
      "\tspeed: 0.2890s/iter; left time: 4693.8015s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:04m:11.05s\n",
      "Steps: 902 | Train Loss: 0.0336614 Vali Loss: 0.0411186 Test Loss: 0.0646942\n",
      "Validation loss decreased (0.057345 --> 0.041119).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0284420\n",
      "\tspeed: 0.8690s/iter; left time: 14023.0488s\n",
      "\titers: 200, epoch: 3 | loss: 0.0262493\n",
      "\tspeed: 0.2673s/iter; left time: 4286.1706s\n",
      "\titers: 300, epoch: 3 | loss: 0.0307209\n",
      "\tspeed: 0.2760s/iter; left time: 4398.5954s\n",
      "\titers: 400, epoch: 3 | loss: 0.0275573\n",
      "\tspeed: 0.2674s/iter; left time: 4234.9414s\n",
      "\titers: 500, epoch: 3 | loss: 0.0267769\n",
      "\tspeed: 0.2874s/iter; left time: 4522.3216s\n",
      "\titers: 600, epoch: 3 | loss: 0.0320355\n",
      "\tspeed: 0.2806s/iter; left time: 4387.9822s\n",
      "\titers: 700, epoch: 3 | loss: 0.0264259\n",
      "\tspeed: 0.2665s/iter; left time: 4140.8910s\n",
      "\titers: 800, epoch: 3 | loss: 0.0263252\n",
      "\tspeed: 0.2682s/iter; left time: 4140.0376s\n",
      "\titers: 900, epoch: 3 | loss: 0.0251511\n",
      "\tspeed: 0.2728s/iter; left time: 4184.5189s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:04m:06.29s\n",
      "Steps: 902 | Train Loss: 0.0276223 Vali Loss: 0.0444757 Test Loss: 0.0666012\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0279506\n",
      "\tspeed: 0.8046s/iter; left time: 12258.1385s\n",
      "\titers: 200, epoch: 4 | loss: 0.0244704\n",
      "\tspeed: 0.2614s/iter; left time: 3956.2973s\n",
      "\titers: 300, epoch: 4 | loss: 0.0270438\n",
      "\tspeed: 0.2741s/iter; left time: 4120.5976s\n",
      "\titers: 400, epoch: 4 | loss: 0.0258560\n",
      "\tspeed: 0.2734s/iter; left time: 4082.5867s\n",
      "\titers: 500, epoch: 4 | loss: 0.0235558\n",
      "\tspeed: 0.2703s/iter; left time: 4009.4923s\n",
      "\titers: 600, epoch: 4 | loss: 0.0234683\n",
      "\tspeed: 0.2744s/iter; left time: 4044.0085s\n",
      "\titers: 700, epoch: 4 | loss: 0.0255238\n",
      "\tspeed: 0.2678s/iter; left time: 3919.2300s\n",
      "\titers: 800, epoch: 4 | loss: 0.0224685\n",
      "\tspeed: 0.2661s/iter; left time: 3867.1654s\n",
      "\titers: 900, epoch: 4 | loss: 0.0197155\n",
      "\tspeed: 0.2741s/iter; left time: 3956.5351s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:04m:04.38s\n",
      "Steps: 902 | Train Loss: 0.0249035 Vali Loss: 0.0395032 Test Loss: 0.0601001\n",
      "Validation loss decreased (0.041119 --> 0.039503).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0205600\n",
      "\tspeed: 0.8167s/iter; left time: 11705.9142s\n",
      "\titers: 200, epoch: 5 | loss: 0.0222585\n",
      "\tspeed: 0.2692s/iter; left time: 3831.8011s\n",
      "\titers: 300, epoch: 5 | loss: 0.0204342\n",
      "\tspeed: 0.2715s/iter; left time: 3836.4578s\n",
      "\titers: 400, epoch: 5 | loss: 0.0201808\n",
      "\tspeed: 0.2761s/iter; left time: 3874.6729s\n",
      "\titers: 500, epoch: 5 | loss: 0.0186784\n",
      "\tspeed: 0.2708s/iter; left time: 3773.3721s\n",
      "\titers: 600, epoch: 5 | loss: 0.0189903\n",
      "\tspeed: 0.2642s/iter; left time: 3654.8504s\n",
      "\titers: 700, epoch: 5 | loss: 0.0173614\n",
      "\tspeed: 0.2768s/iter; left time: 3800.6247s\n",
      "\titers: 800, epoch: 5 | loss: 0.0206758\n",
      "\tspeed: 0.2635s/iter; left time: 3592.5581s\n",
      "\titers: 900, epoch: 5 | loss: 0.0192645\n",
      "\tspeed: 0.2691s/iter; left time: 3642.0248s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:04m:04.48s\n",
      "Steps: 902 | Train Loss: 0.0199944 Vali Loss: 0.0393270 Test Loss: 0.0635765\n",
      "Validation loss decreased (0.039503 --> 0.039327).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0184959\n",
      "\tspeed: 0.8206s/iter; left time: 11021.4758s\n",
      "\titers: 200, epoch: 6 | loss: 0.0191773\n",
      "\tspeed: 0.2745s/iter; left time: 3659.1330s\n",
      "\titers: 300, epoch: 6 | loss: 0.0157848\n",
      "\tspeed: 0.2712s/iter; left time: 3588.7343s\n",
      "\titers: 400, epoch: 6 | loss: 0.0155282\n",
      "\tspeed: 0.2659s/iter; left time: 3491.6229s\n",
      "\titers: 500, epoch: 6 | loss: 0.0208430\n",
      "\tspeed: 0.2765s/iter; left time: 3603.4308s\n",
      "\titers: 600, epoch: 6 | loss: 0.0166869\n",
      "\tspeed: 0.2644s/iter; left time: 3418.8943s\n",
      "\titers: 700, epoch: 6 | loss: 0.0139565\n",
      "\tspeed: 0.2676s/iter; left time: 3433.9497s\n",
      "\titers: 800, epoch: 6 | loss: 0.0158319\n",
      "\tspeed: 0.2762s/iter; left time: 3516.1406s\n",
      "\titers: 900, epoch: 6 | loss: 0.0162191\n",
      "\tspeed: 0.2666s/iter; left time: 3367.2817s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:04m:04.19s\n",
      "Steps: 902 | Train Loss: 0.0168447 Vali Loss: 0.0407798 Test Loss: 0.0686161\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0182228\n",
      "\tspeed: 0.8136s/iter; left time: 10194.2105s\n",
      "\titers: 200, epoch: 7 | loss: 0.0153983\n",
      "\tspeed: 0.2749s/iter; left time: 3416.6844s\n",
      "\titers: 300, epoch: 7 | loss: 0.0165821\n",
      "\tspeed: 0.2675s/iter; left time: 3298.4555s\n",
      "\titers: 400, epoch: 7 | loss: 0.0142608\n",
      "\tspeed: 0.2681s/iter; left time: 3278.0748s\n",
      "\titers: 500, epoch: 7 | loss: 0.0149419\n",
      "\tspeed: 0.2762s/iter; left time: 3349.8928s\n",
      "\titers: 600, epoch: 7 | loss: 0.0129064\n",
      "\tspeed: 0.2689s/iter; left time: 3234.8303s\n",
      "\titers: 700, epoch: 7 | loss: 0.0153925\n",
      "\tspeed: 0.2523s/iter; left time: 3009.7630s\n",
      "\titers: 800, epoch: 7 | loss: 0.0133468\n",
      "\tspeed: 0.2765s/iter; left time: 3271.0560s\n",
      "\titers: 900, epoch: 7 | loss: 0.0167193\n",
      "\tspeed: 0.2689s/iter; left time: 3153.7408s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:04m:03.07s\n",
      "Steps: 902 | Train Loss: 0.0150303 Vali Loss: 0.0400188 Test Loss: 0.0703250\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0122531\n",
      "\tspeed: 0.8156s/iter; left time: 9483.4300s\n",
      "\titers: 200, epoch: 8 | loss: 0.0139316\n",
      "\tspeed: 0.2720s/iter; left time: 3135.4067s\n",
      "\titers: 300, epoch: 8 | loss: 0.0149266\n",
      "\tspeed: 0.2759s/iter; left time: 3152.5301s\n",
      "\titers: 400, epoch: 8 | loss: 0.0128494\n",
      "\tspeed: 0.2684s/iter; left time: 3039.8617s\n",
      "\titers: 500, epoch: 8 | loss: 0.0130582\n",
      "\tspeed: 0.2777s/iter; left time: 3118.0325s\n",
      "\titers: 600, epoch: 8 | loss: 0.0130779\n",
      "\tspeed: 0.2647s/iter; left time: 2944.7871s\n",
      "\titers: 700, epoch: 8 | loss: 0.0123101\n",
      "\tspeed: 0.2686s/iter; left time: 2961.9898s\n",
      "\titers: 800, epoch: 8 | loss: 0.0121390\n",
      "\tspeed: 0.2768s/iter; left time: 3024.1037s\n",
      "\titers: 900, epoch: 8 | loss: 0.0128848\n",
      "\tspeed: 0.2689s/iter; left time: 2911.1647s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:04m:05.10s\n",
      "Steps: 902 | Train Loss: 0.0136662 Vali Loss: 0.0412201 Test Loss: 0.0707636\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0120258\n",
      "\tspeed: 0.8183s/iter; left time: 8775.9540s\n",
      "\titers: 200, epoch: 9 | loss: 0.0127251\n",
      "\tspeed: 0.2648s/iter; left time: 2813.3408s\n",
      "\titers: 300, epoch: 9 | loss: 0.0137979\n",
      "\tspeed: 0.2765s/iter; left time: 2910.6547s\n",
      "\titers: 400, epoch: 9 | loss: 0.0108363\n",
      "\tspeed: 0.2622s/iter; left time: 2732.9840s\n",
      "\titers: 500, epoch: 9 | loss: 0.0133659\n",
      "\tspeed: 0.2752s/iter; left time: 2841.6704s\n",
      "\titers: 600, epoch: 9 | loss: 0.0118990\n",
      "\tspeed: 0.2752s/iter; left time: 2814.2267s\n",
      "\titers: 700, epoch: 9 | loss: 0.0125034\n",
      "\tspeed: 0.2589s/iter; left time: 2621.6624s\n",
      "\titers: 800, epoch: 9 | loss: 0.0125648\n",
      "\tspeed: 0.2688s/iter; left time: 2695.2000s\n",
      "\titers: 900, epoch: 9 | loss: 0.0115932\n",
      "\tspeed: 0.2763s/iter; left time: 2742.0644s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:04m:03.62s\n",
      "Steps: 902 | Train Loss: 0.0123701 Vali Loss: 0.0418743 Test Loss: 0.0690959\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0103409\n",
      "\tspeed: 0.8524s/iter; left time: 8372.8290s\n",
      "\titers: 200, epoch: 10 | loss: 0.0119286\n",
      "\tspeed: 0.2845s/iter; left time: 2766.6478s\n",
      "\titers: 300, epoch: 10 | loss: 0.0125193\n",
      "\tspeed: 0.2829s/iter; left time: 2722.6511s\n",
      "\titers: 400, epoch: 10 | loss: 0.0110763\n",
      "\tspeed: 0.2681s/iter; left time: 2552.7214s\n",
      "\titers: 500, epoch: 10 | loss: 0.0119144\n",
      "\tspeed: 0.2822s/iter; left time: 2659.1301s\n",
      "\titers: 600, epoch: 10 | loss: 0.0104541\n",
      "\tspeed: 0.2878s/iter; left time: 2682.7920s\n",
      "\titers: 700, epoch: 10 | loss: 0.0118452\n",
      "\tspeed: 0.2819s/iter; left time: 2600.2202s\n",
      "\titers: 800, epoch: 10 | loss: 0.0105715\n",
      "\tspeed: 0.2842s/iter; left time: 2592.8282s\n",
      "\titers: 900, epoch: 10 | loss: 0.0128959\n",
      "\tspeed: 0.2887s/iter; left time: 2604.6856s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:04m:15.58s\n",
      "Steps: 902 | Train Loss: 0.0113523 Vali Loss: 0.0413917 Test Loss: 0.0694536\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_96_168_GB_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.06352294236421585, rmse:0.25203758478164673, mae:0.17717456817626953, rse:0.8814232349395752\n",
      "Intermediate time for GB and pred_len 168: 00h:50m:57.71s\n",
      "Intermediate time for GB: 02h:11m:16.35s\n",
      "Total time: 02h:11m:16.36s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data_new.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --label_len 48 \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --d_layers {d_layers} \\\n",
    "              --factor 5 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --dec_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --dropout 0.1 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --overlapping_windows \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                informer_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "informer_df = convert_results_into_df(informer_results, if_loss_fnc=False, itr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.1219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.1772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Iteration     MSE    RMSE     MAE\n",
       "Country Pred_len                                   \n",
       "GB      24                1  0.0313  0.1769  0.1219\n",
       "        96                1  0.0499  0.2233  0.1605\n",
       "        168               1  0.0635  0.2520  0.1772"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'results/informer'\n",
    "\n",
    "informer_df.to_csv(os.path.join(path, 'GB_informer.csv'))\n",
    "informer_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
