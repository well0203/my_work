
=== Starting experiments for country: DE ===


=== Starting experiments for pred_len: 24 ===

[2024-11-16 13:35:46,795] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,816] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 143885
val 31085
test 31085
train 143885
train 143885
val 31085
train 143885
val 31085
test 31085
val 31085
test 31085
test 31085
[2024-11-16 13:35:49,712] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-16 13:35:50,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-16 13:35:51,000] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-16 13:35:51,351] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-16 13:35:51,352] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 171.88 GB, percent = 17.1%
[2024-11-16 13:35:51,671] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-16 13:35:51,672] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.23 GB, percent = 17.1%
[2024-11-16 13:35:51,672] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-16 13:35:51,876] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-16 13:35:51,877] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,877] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.55 GB, percent = 17.1%
[2024-11-16 13:35:51,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-16 13:35:51,878] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f53b3468590>
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-16 13:35:51,880] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0632429
	speed: 0.3523s/iter; left time: 31645.8063s
	iters: 200, epoch: 1 | loss: 0.0507106
	speed: 0.3228s/iter; left time: 28965.5795s
	iters: 300, epoch: 1 | loss: 0.0394340
	speed: 0.3229s/iter; left time: 28935.6049s
	iters: 400, epoch: 1 | loss: 0.0388287
	speed: 0.3229s/iter; left time: 28909.7832s
	iters: 500, epoch: 1 | loss: 0.0251836
	speed: 0.3229s/iter; left time: 28876.4997s
	iters: 600, epoch: 1 | loss: 0.0225685
	speed: 0.3230s/iter; left time: 28851.5746s
	iters: 700, epoch: 1 | loss: 0.0203688
	speed: 0.3229s/iter; left time: 28813.8099s
	iters: 800, epoch: 1 | loss: 0.0230455
	speed: 0.3229s/iter; left time: 28778.3263s
	iters: 900, epoch: 1 | loss: 0.0221143
	speed: 0.3229s/iter; left time: 28746.0771s
	iters: 1000, epoch: 1 | loss: 0.0220149
	speed: 0.3229s/iter; left time: 28708.3550s
	iters: 1100, epoch: 1 | loss: 0.0171128
	speed: 0.3231s/iter; left time: 28698.6023s
Epoch: 1 cost time: 00h:06m:04.06s
Epoch: 1 | Train Loss: 0.0333040 Vali Loss: 0.0229411 Test Loss: 0.0251673
Validation loss decreased (inf --> 0.022941).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0113808
	speed: 1.2685s/iter; left time: 108237.0048s
	iters: 200, epoch: 2 | loss: 0.0146446
	speed: 0.3227s/iter; left time: 27499.5702s
	iters: 300, epoch: 2 | loss: 0.0160543
	speed: 0.3230s/iter; left time: 27499.5493s
	iters: 400, epoch: 2 | loss: 0.0154046
	speed: 0.3232s/iter; left time: 27476.3707s
	iters: 500, epoch: 2 | loss: 0.0173704
	speed: 0.3231s/iter; left time: 27435.1440s
	iters: 600, epoch: 2 | loss: 0.0362225
	speed: 0.3230s/iter; left time: 27397.2340s
	iters: 700, epoch: 2 | loss: 0.0231417
	speed: 0.2851s/iter; left time: 24156.7660s
	iters: 800, epoch: 2 | loss: 0.0158735
	speed: 0.2628s/iter; left time: 22235.2269s
	iters: 900, epoch: 2 | loss: 0.0282127
	speed: 0.3228s/iter; left time: 27280.8594s
	iters: 1000, epoch: 2 | loss: 0.0127809
	speed: 0.3231s/iter; left time: 27274.1887s
	iters: 1100, epoch: 2 | loss: 0.0152286
	speed: 0.3232s/iter; left time: 27251.1327s
Epoch: 2 cost time: 00h:05m:53.62s
Epoch: 2 | Train Loss: 0.0189258 Vali Loss: 0.0213288 Test Loss: 0.0228869
Validation loss decreased (0.022941 --> 0.021329).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0147866
	speed: 1.2393s/iter; left time: 100172.1175s
	iters: 200, epoch: 3 | loss: 0.0251924
	speed: 0.3226s/iter; left time: 26042.7383s
	iters: 300, epoch: 3 | loss: 0.0179662
	speed: 0.3218s/iter; left time: 25948.7631s
	iters: 400, epoch: 3 | loss: 0.0170750
	speed: 0.3227s/iter; left time: 25983.4054s
	iters: 500, epoch: 3 | loss: 0.0222653
	speed: 0.3229s/iter; left time: 25973.3680s
	iters: 600, epoch: 3 | loss: 0.0223925
	speed: 0.3228s/iter; left time: 25928.9061s
	iters: 700, epoch: 3 | loss: 0.0192496
	speed: 0.3228s/iter; left time: 25901.0748s
	iters: 800, epoch: 3 | loss: 0.0108626
	speed: 0.3234s/iter; left time: 25916.4432s
	iters: 900, epoch: 3 | loss: 0.0230040
	speed: 0.3236s/iter; left time: 25894.3792s
	iters: 1000, epoch: 3 | loss: 0.0219360
	speed: 0.3237s/iter; left time: 25870.1387s
	iters: 1100, epoch: 3 | loss: 0.0159164
	speed: 0.3236s/iter; left time: 25828.7384s
Epoch: 3 cost time: 00h:06m:03.50s
Epoch: 3 | Train Loss: 0.0180037 Vali Loss: 0.0207199 Test Loss: 0.0221680
Validation loss decreased (0.021329 --> 0.020720).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0195420
	speed: 1.2593s/iter; left time: 96126.8123s
	iters: 200, epoch: 4 | loss: 0.0174730
	speed: 0.3234s/iter; left time: 24654.0084s
	iters: 300, epoch: 4 | loss: 0.0160964
	speed: 0.3236s/iter; left time: 24636.3340s
	iters: 400, epoch: 4 | loss: 0.0249365
	speed: 0.3238s/iter; left time: 24619.7518s
	iters: 500, epoch: 4 | loss: 0.0150605
	speed: 0.3236s/iter; left time: 24570.3229s
	iters: 600, epoch: 4 | loss: 0.0123712
	speed: 0.3235s/iter; left time: 24532.5339s
	iters: 700, epoch: 4 | loss: 0.0285275
	speed: 0.3234s/iter; left time: 24494.2035s
	iters: 800, epoch: 4 | loss: 0.0258730
	speed: 0.3235s/iter; left time: 24465.4872s
	iters: 900, epoch: 4 | loss: 0.0145841
	speed: 0.3233s/iter; left time: 24420.9361s
	iters: 1000, epoch: 4 | loss: 0.0179337
	speed: 0.3231s/iter; left time: 24372.6274s
	iters: 1100, epoch: 4 | loss: 0.0190074
	speed: 0.3234s/iter; left time: 24362.5242s
Epoch: 4 cost time: 00h:06m:04.01s
Epoch: 4 | Train Loss: 0.0173761 Vali Loss: 0.0201705 Test Loss: 0.0218201
Validation loss decreased (0.020720 --> 0.020170).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0248572
	speed: 1.2281s/iter; left time: 88221.7166s
	iters: 200, epoch: 5 | loss: 0.0181087
	speed: 0.3236s/iter; left time: 23212.2930s
	iters: 300, epoch: 5 | loss: 0.0130525
	speed: 0.3238s/iter; left time: 23198.4974s
	iters: 400, epoch: 5 | loss: 0.0138662
	speed: 0.3239s/iter; left time: 23173.0373s
	iters: 500, epoch: 5 | loss: 0.0155967
	speed: 0.3239s/iter; left time: 23135.2303s
	iters: 600, epoch: 5 | loss: 0.0198725
	speed: 0.3238s/iter; left time: 23095.7626s
	iters: 700, epoch: 5 | loss: 0.0202901
	speed: 0.3207s/iter; left time: 22844.8257s
	iters: 800, epoch: 5 | loss: 0.0220147
	speed: 0.3239s/iter; left time: 23043.2874s
	iters: 900, epoch: 5 | loss: 0.0173096
	speed: 0.3239s/iter; left time: 23006.4260s
	iters: 1000, epoch: 5 | loss: 0.0179298
	speed: 0.3237s/iter; left time: 22962.7114s
	iters: 1100, epoch: 5 | loss: 0.0144313
	speed: 0.3237s/iter; left time: 22930.7100s
Epoch: 5 cost time: 00h:06m:04.12s
Epoch: 5 | Train Loss: 0.0169353 Vali Loss: 0.0197934 Test Loss: 0.0216913
Validation loss decreased (0.020170 --> 0.019793).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0178052
	speed: 1.2141s/iter; left time: 81761.1781s
	iters: 200, epoch: 6 | loss: 0.0166879
	speed: 0.3222s/iter; left time: 21668.1074s
	iters: 300, epoch: 6 | loss: 0.0215266
	speed: 0.3225s/iter; left time: 21656.1161s
	iters: 400, epoch: 6 | loss: 0.0158959
	speed: 0.3230s/iter; left time: 21652.2129s
	iters: 500, epoch: 6 | loss: 0.0225539
	speed: 0.3235s/iter; left time: 21656.3385s
	iters: 600, epoch: 6 | loss: 0.0142451
	speed: 0.3239s/iter; left time: 21647.6084s
	iters: 700, epoch: 6 | loss: 0.0146011
	speed: 0.3237s/iter; left time: 21605.6043s
	iters: 800, epoch: 6 | loss: 0.0156253
	speed: 0.3240s/iter; left time: 21589.3386s
	iters: 900, epoch: 6 | loss: 0.0157494
	speed: 0.3240s/iter; left time: 21556.3283s
	iters: 1000, epoch: 6 | loss: 0.0117435
	speed: 0.3239s/iter; left time: 21519.5069s
	iters: 1100, epoch: 6 | loss: 0.0189988
	speed: 0.3229s/iter; left time: 21419.9093s
Epoch: 6 cost time: 00h:06m:03.76s
Epoch: 6 | Train Loss: 0.0165692 Vali Loss: 0.0196932 Test Loss: 0.0218816
Validation loss decreased (0.019793 --> 0.019693).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0172309
	speed: 1.2322s/iter; left time: 77437.5813s
	iters: 200, epoch: 7 | loss: 0.0123486
	speed: 0.3239s/iter; left time: 20322.1704s
	iters: 300, epoch: 7 | loss: 0.0212684
	speed: 0.3223s/iter; left time: 20192.9327s
	iters: 400, epoch: 7 | loss: 0.0195458
	speed: 0.3229s/iter; left time: 20195.5811s
	iters: 500, epoch: 7 | loss: 0.0102791
	speed: 0.3243s/iter; left time: 20252.8605s
	iters: 600, epoch: 7 | loss: 0.0223481
	speed: 0.3239s/iter; left time: 20191.7938s
	iters: 700, epoch: 7 | loss: 0.0160351
	speed: 0.3206s/iter; left time: 19956.1453s
	iters: 800, epoch: 7 | loss: 0.0161180
	speed: 0.3227s/iter; left time: 20056.8657s
	iters: 900, epoch: 7 | loss: 0.0166668
	speed: 0.3230s/iter; left time: 20038.1523s
	iters: 1000, epoch: 7 | loss: 0.0117904
	speed: 0.3215s/iter; left time: 19912.8472s
	iters: 1100, epoch: 7 | loss: 0.0165729
	speed: 0.3235s/iter; left time: 20004.3691s
Epoch: 7 cost time: 00h:06m:03.53s
Epoch: 7 | Train Loss: 0.0163646 Vali Loss: 0.0194863 Test Loss: 0.0215178
Validation loss decreased (0.019693 --> 0.019486).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0151283
	speed: 1.2214s/iter; left time: 71268.5130s
	iters: 200, epoch: 8 | loss: 0.0133968
	speed: 0.3230s/iter; left time: 18813.9660s
	iters: 300, epoch: 8 | loss: 0.0154707
	speed: 0.3212s/iter; left time: 18676.8243s
	iters: 400, epoch: 8 | loss: 0.0145500
	speed: 0.3225s/iter; left time: 18721.9795s
	iters: 500, epoch: 8 | loss: 0.0161144
	speed: 0.3240s/iter; left time: 18774.5140s
	iters: 600, epoch: 8 | loss: 0.0167301
	speed: 0.3225s/iter; left time: 18654.1833s
	iters: 700, epoch: 8 | loss: 0.0157079
	speed: 0.3231s/iter; left time: 18655.8284s
	iters: 800, epoch: 8 | loss: 0.0133750
	speed: 0.3224s/iter; left time: 18585.1511s
	iters: 900, epoch: 8 | loss: 0.0157046
	speed: 0.3231s/iter; left time: 18591.7227s
	iters: 1000, epoch: 8 | loss: 0.0103165
	speed: 0.3238s/iter; left time: 18602.5103s
	iters: 1100, epoch: 8 | loss: 0.0139662
	speed: 0.3239s/iter; left time: 18576.8395s
Epoch: 8 cost time: 00h:06m:03.11s
Epoch: 8 | Train Loss: 0.0162527 Vali Loss: 0.0196475 Test Loss: 0.0218817
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0137342
	speed: 1.1821s/iter; left time: 63658.6641s
	iters: 200, epoch: 9 | loss: 0.0193019
	speed: 0.3220s/iter; left time: 17308.2726s
	iters: 300, epoch: 9 | loss: 0.0143334
	speed: 0.3204s/iter; left time: 17192.0739s
	iters: 400, epoch: 9 | loss: 0.0120264
	speed: 0.3234s/iter; left time: 17319.3794s
	iters: 500, epoch: 9 | loss: 0.0147567
	speed: 0.3234s/iter; left time: 17284.7771s
	iters: 600, epoch: 9 | loss: 0.0199759
	speed: 0.3231s/iter; left time: 17240.7187s
	iters: 700, epoch: 9 | loss: 0.0136194
	speed: 0.3227s/iter; left time: 17182.8881s
	iters: 800, epoch: 9 | loss: 0.0103802
	speed: 0.3214s/iter; left time: 17082.7926s
	iters: 900, epoch: 9 | loss: 0.0166926
	speed: 0.3182s/iter; left time: 16881.1642s
	iters: 1000, epoch: 9 | loss: 0.0198049
	speed: 0.3226s/iter; left time: 17081.0444s
	iters: 1100, epoch: 9 | loss: 0.0279420
	speed: 0.3218s/iter; left time: 17006.9184s
Epoch: 9 cost time: 00h:06m:02.41s
Epoch: 9 | Train Loss: 0.0158304 Vali Loss: 0.0192229 Test Loss: 0.0215448
Validation loss decreased (0.019486 --> 0.019223).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0203107
	speed: 1.2472s/iter; left time: 61555.7164s
	iters: 200, epoch: 10 | loss: 0.0133896
	speed: 0.3223s/iter; left time: 15875.1190s
	iters: 300, epoch: 10 | loss: 0.0116833
	speed: 0.3217s/iter; left time: 15813.3395s
	iters: 400, epoch: 10 | loss: 0.0166726
	speed: 0.3182s/iter; left time: 15611.8300s
	iters: 500, epoch: 10 | loss: 0.0108126
	speed: 0.3180s/iter; left time: 15567.2747s
	iters: 600, epoch: 10 | loss: 0.0193221
	speed: 0.3178s/iter; left time: 15524.8963s
	iters: 700, epoch: 10 | loss: 0.0144544
	speed: 0.3169s/iter; left time: 15452.7183s
	iters: 800, epoch: 10 | loss: 0.0147057
	speed: 0.3200s/iter; left time: 15569.0658s
	iters: 900, epoch: 10 | loss: 0.0103039
	speed: 0.3205s/iter; left time: 15564.3570s
	iters: 1000, epoch: 10 | loss: 0.0158291
	speed: 0.3228s/iter; left time: 15641.0012s
	iters: 1100, epoch: 10 | loss: 0.0099726
	speed: 0.3181s/iter; left time: 15384.2536s
Epoch: 10 cost time: 00h:06m:00.19s
Epoch: 10 | Train Loss: 0.0154770 Vali Loss: 0.0195692 Test Loss: 0.0220183
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0165156
	speed: 1.1787s/iter; left time: 52878.7841s
	iters: 200, epoch: 11 | loss: 0.0128128
	speed: 0.3211s/iter; left time: 14370.7279s
	iters: 300, epoch: 11 | loss: 0.0199038
	speed: 0.3201s/iter; left time: 14297.0739s
	iters: 400, epoch: 11 | loss: 0.0113736
	speed: 0.3203s/iter; left time: 14271.6793s
	iters: 500, epoch: 11 | loss: 0.0177854
	speed: 0.3196s/iter; left time: 14208.5477s
	iters: 600, epoch: 11 | loss: 0.0116605
	speed: 0.3232s/iter; left time: 14337.9315s
	iters: 700, epoch: 11 | loss: 0.0130346
	speed: 0.3210s/iter; left time: 14208.8342s
	iters: 800, epoch: 11 | loss: 0.0176324
	speed: 0.3143s/iter; left time: 13877.8484s
	iters: 900, epoch: 11 | loss: 0.0118758
	speed: 0.3189s/iter; left time: 14051.7649s
	iters: 1000, epoch: 11 | loss: 0.0107230
	speed: 0.3171s/iter; left time: 13941.7038s
	iters: 1100, epoch: 11 | loss: 0.0178455
	speed: 0.3166s/iter; left time: 13886.0899s
Epoch: 11 cost time: 00h:05m:59.43s
Epoch: 11 | Train Loss: 0.0154121 Vali Loss: 0.0196424 Test Loss: 0.0221752
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0175937
	speed: 1.1792s/iter; left time: 47596.4800s
	iters: 200, epoch: 12 | loss: 0.0126353
	speed: 0.3231s/iter; left time: 13009.9612s
	iters: 300, epoch: 12 | loss: 0.0206016
	speed: 0.3230s/iter; left time: 12972.9080s
	iters: 400, epoch: 12 | loss: 0.0102996
	speed: 0.3227s/iter; left time: 12928.5097s
	iters: 500, epoch: 12 | loss: 0.0157677
	speed: 0.3226s/iter; left time: 12894.6865s
	iters: 600, epoch: 12 | loss: 0.0146827
	speed: 0.3226s/iter; left time: 12861.4432s
	iters: 700, epoch: 12 | loss: 0.0145983
	speed: 0.3228s/iter; left time: 12835.6238s
	iters: 800, epoch: 12 | loss: 0.0172842
	speed: 0.3222s/iter; left time: 12781.1348s
	iters: 900, epoch: 12 | loss: 0.0198558
	speed: 0.3228s/iter; left time: 12771.4169s
	iters: 1000, epoch: 12 | loss: 0.0173477
	speed: 0.3226s/iter; left time: 12732.5014s
	iters: 1100, epoch: 12 | loss: 0.0088205
	speed: 0.3228s/iter; left time: 12705.5370s
Epoch: 12 cost time: 00h:06m:03.24s
Epoch: 12 | Train Loss: 0.0152097 Vali Loss: 0.0197840 Test Loss: 0.0225309
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0192575
	speed: 1.1781s/iter; left time: 42257.1033s
	iters: 200, epoch: 13 | loss: 0.0178487
	speed: 0.3215s/iter; left time: 11500.5318s
	iters: 300, epoch: 13 | loss: 0.0284491
	speed: 0.3226s/iter; left time: 11507.3652s
	iters: 400, epoch: 13 | loss: 0.0159005
	speed: 0.3221s/iter; left time: 11456.8257s
	iters: 500, epoch: 13 | loss: 0.0128495
	speed: 0.3220s/iter; left time: 11421.6517s
	iters: 600, epoch: 13 | loss: 0.0117158
	speed: 0.3228s/iter; left time: 11418.6962s
	iters: 700, epoch: 13 | loss: 0.0135272
	speed: 0.3228s/iter; left time: 11383.7663s
	iters: 800, epoch: 13 | loss: 0.0158590
	speed: 0.3227s/iter; left time: 11348.2563s
	iters: 900, epoch: 13 | loss: 0.0106493
	speed: 0.3223s/iter; left time: 11301.7743s
	iters: 1000, epoch: 13 | loss: 0.0102547
	speed: 0.3196s/iter; left time: 11176.2883s
	iters: 1100, epoch: 13 | loss: 0.0108302
	speed: 0.3185s/iter; left time: 11107.3901s
Epoch: 13 cost time: 00h:06m:02.09s
Epoch: 13 | Train Loss: 0.0150879 Vali Loss: 0.0196984 Test Loss: 0.0227838
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0201575
	speed: 1.1755s/iter; left time: 36877.6420s
	iters: 200, epoch: 14 | loss: 0.0167995
	speed: 0.3194s/iter; left time: 9989.9229s
	iters: 300, epoch: 14 | loss: 0.0204233
	speed: 0.3225s/iter; left time: 10052.4890s
	iters: 400, epoch: 14 | loss: 0.0163709
	speed: 0.3188s/iter; left time: 9906.4489s
	iters: 500, epoch: 14 | loss: 0.0143940
	speed: 0.3171s/iter; left time: 9821.4001s
	iters: 600, epoch: 14 | loss: 0.0164356
	speed: 0.3179s/iter; left time: 9814.6520s
	iters: 700, epoch: 14 | loss: 0.0193533
	speed: 0.3170s/iter; left time: 9756.5390s
	iters: 800, epoch: 14 | loss: 0.0155888
	speed: 0.3165s/iter; left time: 9707.3937s
	iters: 900, epoch: 14 | loss: 0.0123037
	speed: 0.3178s/iter; left time: 9716.9589s
	iters: 1000, epoch: 14 | loss: 0.0190936
	speed: 0.3170s/iter; left time: 9660.8142s
	iters: 1100, epoch: 14 | loss: 0.0183230
	speed: 0.3205s/iter; left time: 9733.8511s
Epoch: 14 cost time: 00h:05m:58.40s
Epoch: 14 | Train Loss: 0.0148230 Vali Loss: 0.0200994 Test Loss: 0.0234704
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Intermediate time for DE and pred_len 24: 01h:44m:15.62s



=== Starting experiments for pred_len: 96 ===

[2024-11-13 10:29:16,024] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 10:29:16,029] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 10:29:16,062] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 10:29:16,062] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 10:29:17,107] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 10:29:17,107] [INFO] [comm.py:637:init_distributed] cdb=None
train 142645
val 30725
test 30725
train 142645
train 142645
train 142645
val 30725
val 30725
val 30725
test 30725
test 30725
test 30725
[2024-11-13 10:29:18,892] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 10:29:19,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 10:29:19,817] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 10:29:19,817] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 10:29:19,818] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 10:29:19,818] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 10:29:19,818] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 10:29:19,818] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 10:29:20,237] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 10:29:20,238] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 10:29:20,238] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.62 GB, percent = 2.5%
[2024-11-13 10:29:20,447] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 10:29:20,448] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 10:29:20,448] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.08 GB, percent = 2.6%
[2024-11-13 10:29:20,448] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 10:29:20,643] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 10:29:20,644] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 10:29:20,644] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.39 GB, percent = 2.6%
[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 10:29:20,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 10:29:20,645] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6db7bd38d0>
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 10:29:20,646] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 10:29:20,647] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 10:29:20,647] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0446114
	speed: 0.1660s/iter; left time: 14783.4114s
	iters: 200, epoch: 1 | loss: 0.0507687
	speed: 0.1377s/iter; left time: 12247.6350s
	iters: 300, epoch: 1 | loss: 0.0448839
	speed: 0.1369s/iter; left time: 12158.9270s
	iters: 400, epoch: 1 | loss: 0.0454857
	speed: 0.1368s/iter; left time: 12142.5777s
	iters: 500, epoch: 1 | loss: 0.0477529
	speed: 0.1369s/iter; left time: 12135.3471s
	iters: 600, epoch: 1 | loss: 0.0380543
	speed: 0.1394s/iter; left time: 12341.3727s
	iters: 700, epoch: 1 | loss: 0.0391503
	speed: 0.1378s/iter; left time: 12191.2799s
	iters: 800, epoch: 1 | loss: 0.0337921
	speed: 0.1376s/iter; left time: 12160.0810s
	iters: 900, epoch: 1 | loss: 0.0199492
	speed: 0.1385s/iter; left time: 12218.6009s
	iters: 1000, epoch: 1 | loss: 0.0314866
	speed: 0.1376s/iter; left time: 12127.1685s
	iters: 1100, epoch: 1 | loss: 0.0306014
	speed: 0.1371s/iter; left time: 12067.6042s
Epoch: 1 cost time: 00h:02m:34.53s
Epoch: 1 | Train Loss: 0.0429309 Vali Loss: 0.0322865 Test Loss: 0.0373079
Validation loss decreased (inf --> 0.032287).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0302605
	speed: 0.5005s/iter; left time: 42335.2406s
	iters: 200, epoch: 2 | loss: 0.0323583
	speed: 0.1425s/iter; left time: 12038.4969s
	iters: 300, epoch: 2 | loss: 0.0289049
	speed: 0.1392s/iter; left time: 11745.1487s
	iters: 400, epoch: 2 | loss: 0.0225016
	speed: 0.1412s/iter; left time: 11901.7324s
	iters: 500, epoch: 2 | loss: 0.0280473
	speed: 0.1380s/iter; left time: 11620.8339s
	iters: 600, epoch: 2 | loss: 0.0231738
	speed: 0.1359s/iter; left time: 11427.4173s
	iters: 700, epoch: 2 | loss: 0.0285885
	speed: 0.1376s/iter; left time: 11553.4280s
	iters: 800, epoch: 2 | loss: 0.0226014
	speed: 0.1382s/iter; left time: 11588.6630s
	iters: 900, epoch: 2 | loss: 0.0252432
	speed: 0.1367s/iter; left time: 11453.7880s
	iters: 1000, epoch: 2 | loss: 0.0303803
	speed: 0.1359s/iter; left time: 11374.1162s
	iters: 1100, epoch: 2 | loss: 0.0346701
	speed: 0.1369s/iter; left time: 11444.2170s
Epoch: 2 cost time: 00h:02m:34.58s
Epoch: 2 | Train Loss: 0.0267929 Vali Loss: 0.0310637 Test Loss: 0.0355101
Validation loss decreased (0.032287 --> 0.031064).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0272676
	speed: 0.5265s/iter; left time: 42183.8046s
	iters: 200, epoch: 3 | loss: 0.0320771
	speed: 0.1363s/iter; left time: 10906.2099s
	iters: 300, epoch: 3 | loss: 0.0189163
	speed: 0.1355s/iter; left time: 10832.6138s
	iters: 400, epoch: 3 | loss: 0.0244222
	speed: 0.1371s/iter; left time: 10945.0419s
	iters: 500, epoch: 3 | loss: 0.0268812
	speed: 0.1347s/iter; left time: 10736.1242s
	iters: 600, epoch: 3 | loss: 0.0218725
	speed: 0.1348s/iter; left time: 10732.4763s
	iters: 700, epoch: 3 | loss: 0.0201757
	speed: 0.1373s/iter; left time: 10915.5663s
	iters: 800, epoch: 3 | loss: 0.0237758
	speed: 0.1380s/iter; left time: 10962.3137s
	iters: 900, epoch: 3 | loss: 0.0272236
	speed: 0.1360s/iter; left time: 10789.2366s
	iters: 1000, epoch: 3 | loss: 0.0253749
	speed: 0.1365s/iter; left time: 10816.0596s
	iters: 1100, epoch: 3 | loss: 0.0313581
	speed: 0.1383s/iter; left time: 10942.4425s
Epoch: 3 cost time: 00h:02m:33.15s
Epoch: 3 | Train Loss: 0.0256442 Vali Loss: 0.0304424 Test Loss: 0.0349693
Validation loss decreased (0.031064 --> 0.030442).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0234055
	speed: 0.5104s/iter; left time: 38623.6626s
	iters: 200, epoch: 4 | loss: 0.0379499
	speed: 0.1372s/iter; left time: 10365.9854s
	iters: 300, epoch: 4 | loss: 0.0260023
	speed: 0.1367s/iter; left time: 10313.3352s
	iters: 400, epoch: 4 | loss: 0.0309342
	speed: 0.1371s/iter; left time: 10334.6951s
	iters: 500, epoch: 4 | loss: 0.0230162
	speed: 0.1363s/iter; left time: 10257.9880s
	iters: 600, epoch: 4 | loss: 0.0329382
	speed: 0.1361s/iter; left time: 10232.0065s
	iters: 700, epoch: 4 | loss: 0.0270466
	speed: 0.1351s/iter; left time: 10139.0892s
	iters: 800, epoch: 4 | loss: 0.0279899
	speed: 0.1389s/iter; left time: 10415.5284s
	iters: 900, epoch: 4 | loss: 0.0265847
	speed: 0.1338s/iter; left time: 10018.1953s
	iters: 1000, epoch: 4 | loss: 0.0298538
	speed: 0.1343s/iter; left time: 10041.2317s
	iters: 1100, epoch: 4 | loss: 0.0180375
	speed: 0.1348s/iter; left time: 10064.6278s
Epoch: 4 cost time: 00h:02m:32.42s
Epoch: 4 | Train Loss: 0.0250050 Vali Loss: 0.0307257 Test Loss: 0.0347905
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0198935
	speed: 0.4449s/iter; left time: 31682.8080s
	iters: 200, epoch: 5 | loss: 0.0202480
	speed: 0.1360s/iter; left time: 9669.3895s
	iters: 300, epoch: 5 | loss: 0.0214222
	speed: 0.1334s/iter; left time: 9471.0439s
	iters: 400, epoch: 5 | loss: 0.0283736
	speed: 0.1345s/iter; left time: 9539.0708s
	iters: 500, epoch: 5 | loss: 0.0473016
	speed: 0.1343s/iter; left time: 9508.0877s
	iters: 600, epoch: 5 | loss: 0.0202376
	speed: 0.1332s/iter; left time: 9421.8602s
	iters: 700, epoch: 5 | loss: 0.0211350
	speed: 0.1337s/iter; left time: 9439.6364s
	iters: 800, epoch: 5 | loss: 0.0247196
	speed: 0.1334s/iter; left time: 9405.3486s
	iters: 900, epoch: 5 | loss: 0.0205404
	speed: 0.1339s/iter; left time: 9427.7018s
	iters: 1000, epoch: 5 | loss: 0.0239363
	speed: 0.1360s/iter; left time: 9563.8871s
	iters: 1100, epoch: 5 | loss: 0.0213725
	speed: 0.1334s/iter; left time: 9366.0544s
Epoch: 5 cost time: 00h:02m:30.44s
Epoch: 5 | Train Loss: 0.0242875 Vali Loss: 0.0304751 Test Loss: 0.0348097
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0309512
	speed: 0.4661s/iter; left time: 31115.2195s
	iters: 200, epoch: 6 | loss: 0.0205533
	speed: 0.1372s/iter; left time: 9145.8859s
	iters: 300, epoch: 6 | loss: 0.0224231
	speed: 0.1357s/iter; left time: 9029.5893s
	iters: 400, epoch: 6 | loss: 0.0260528
	speed: 0.1338s/iter; left time: 8894.1612s
	iters: 500, epoch: 6 | loss: 0.0180285
	speed: 0.1339s/iter; left time: 8884.8938s
	iters: 600, epoch: 6 | loss: 0.0195526
	speed: 0.1332s/iter; left time: 8823.3560s
	iters: 700, epoch: 6 | loss: 0.0172730
	speed: 0.1328s/iter; left time: 8785.3991s
	iters: 800, epoch: 6 | loss: 0.0267439
	speed: 0.1335s/iter; left time: 8818.1725s
	iters: 900, epoch: 6 | loss: 0.0241353
	speed: 0.1352s/iter; left time: 8916.8592s
	iters: 1000, epoch: 6 | loss: 0.0157862
	speed: 0.1336s/iter; left time: 8797.1455s
	iters: 1100, epoch: 6 | loss: 0.0216071
	speed: 0.1334s/iter; left time: 8773.5081s
Epoch: 6 cost time: 00h:02m:30.73s
Epoch: 6 | Train Loss: 0.0236184 Vali Loss: 0.0298692 Test Loss: 0.0346515
Validation loss decreased (0.030442 --> 0.029869).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0280138
	speed: 0.5151s/iter; left time: 32089.1125s
	iters: 200, epoch: 7 | loss: 0.0166881
	speed: 0.1330s/iter; left time: 8274.2779s
	iters: 300, epoch: 7 | loss: 0.0210482
	speed: 0.1334s/iter; left time: 8282.1030s
	iters: 400, epoch: 7 | loss: 0.0262818
	speed: 0.1351s/iter; left time: 8379.1070s
	iters: 500, epoch: 7 | loss: 0.0175326
	speed: 0.1336s/iter; left time: 8269.3489s
	iters: 600, epoch: 7 | loss: 0.0228772
	speed: 0.1363s/iter; left time: 8424.8114s
	iters: 700, epoch: 7 | loss: 0.0226232
	speed: 0.1348s/iter; left time: 8318.8391s
	iters: 800, epoch: 7 | loss: 0.0138826
	speed: 0.1341s/iter; left time: 8259.0888s
	iters: 900, epoch: 7 | loss: 0.0142537
	speed: 0.1340s/iter; left time: 8240.6064s
	iters: 1000, epoch: 7 | loss: 0.0185196
	speed: 0.1343s/iter; left time: 8247.4938s
	iters: 1100, epoch: 7 | loss: 0.0187493
	speed: 0.1354s/iter; left time: 8297.4674s
Epoch: 7 cost time: 00h:02m:30.26s
Epoch: 7 | Train Loss: 0.0231316 Vali Loss: 0.0302424 Test Loss: 0.0350993
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0193801
	speed: 0.4514s/iter; left time: 26110.7881s
	iters: 200, epoch: 8 | loss: 0.0185916
	speed: 0.1344s/iter; left time: 7759.8646s
	iters: 300, epoch: 8 | loss: 0.0213563
	speed: 0.1354s/iter; left time: 7803.5777s
	iters: 400, epoch: 8 | loss: 0.0218396
	speed: 0.1318s/iter; left time: 7584.4618s
	iters: 500, epoch: 8 | loss: 0.0290579
	speed: 0.1355s/iter; left time: 7782.1843s
	iters: 600, epoch: 8 | loss: 0.0331087
	speed: 0.1510s/iter; left time: 8655.8468s
	iters: 700, epoch: 8 | loss: 0.0227459
	speed: 0.1348s/iter; left time: 7713.7551s
	iters: 800, epoch: 8 | loss: 0.0252364
	speed: 0.1323s/iter; left time: 7560.6869s
	iters: 900, epoch: 8 | loss: 0.0219207
	speed: 0.1318s/iter; left time: 7517.8268s
	iters: 1000, epoch: 8 | loss: 0.0333456
	speed: 0.1335s/iter; left time: 7604.4913s
	iters: 1100, epoch: 8 | loss: 0.0219904
	speed: 0.1341s/iter; left time: 7623.0934s
Epoch: 8 cost time: 00h:02m:31.45s
Epoch: 8 | Train Loss: 0.0225641 Vali Loss: 0.0309571 Test Loss: 0.0370625
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0179517
	speed: 0.4581s/iter; left time: 24458.1708s
	iters: 200, epoch: 9 | loss: 0.0256749
	speed: 0.1337s/iter; left time: 7125.7395s
	iters: 300, epoch: 9 | loss: 0.0194726
	speed: 0.1347s/iter; left time: 7164.2771s
	iters: 400, epoch: 9 | loss: 0.0239984
	speed: 0.1338s/iter; left time: 7100.6239s
	iters: 500, epoch: 9 | loss: 0.0159293
	speed: 0.1345s/iter; left time: 7125.6781s
	iters: 600, epoch: 9 | loss: 0.0192116
	speed: 0.1330s/iter; left time: 7034.6980s
	iters: 700, epoch: 9 | loss: 0.0197610
	speed: 0.1326s/iter; left time: 7001.6594s
	iters: 800, epoch: 9 | loss: 0.0186225
	speed: 0.1327s/iter; left time: 6989.3650s
	iters: 900, epoch: 9 | loss: 0.0202066
	speed: 0.1332s/iter; left time: 7002.1728s
	iters: 1000, epoch: 9 | loss: 0.0174803
	speed: 0.1329s/iter; left time: 6974.2119s
	iters: 1100, epoch: 9 | loss: 0.0254039
	speed: 0.1322s/iter; left time: 6924.9990s
Epoch: 9 cost time: 00h:02m:29.02s
Epoch: 9 | Train Loss: 0.0215361 Vali Loss: 0.0309942 Test Loss: 0.0375069
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0238908
	speed: 0.4433s/iter; left time: 21687.4134s
	iters: 200, epoch: 10 | loss: 0.0189626
	speed: 0.1328s/iter; left time: 6482.5742s
	iters: 300, epoch: 10 | loss: 0.0185715
	speed: 0.1334s/iter; left time: 6499.3279s
	iters: 400, epoch: 10 | loss: 0.0208660
	speed: 0.1517s/iter; left time: 7376.7239s
	iters: 500, epoch: 10 | loss: 0.0276698
	speed: 0.1340s/iter; left time: 6500.9074s
	iters: 600, epoch: 10 | loss: 0.0254691
	speed: 0.1332s/iter; left time: 6452.5390s
	iters: 700, epoch: 10 | loss: 0.0252488
	speed: 0.1334s/iter; left time: 6448.5702s
	iters: 800, epoch: 10 | loss: 0.0194837
	speed: 0.1330s/iter; left time: 6412.6507s
	iters: 900, epoch: 10 | loss: 0.0210351
	speed: 0.1319s/iter; left time: 6347.9444s
	iters: 1000, epoch: 10 | loss: 0.0163288
	speed: 0.1336s/iter; left time: 6414.8274s
	iters: 1100, epoch: 10 | loss: 0.0178875
	speed: 0.1337s/iter; left time: 6407.5426s
Epoch: 10 cost time: 00h:02m:30.98s
Epoch: 10 | Train Loss: 0.0208566 Vali Loss: 0.0317293 Test Loss: 0.0390418
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0179506
	speed: 0.4443s/iter; left time: 19756.4008s
	iters: 200, epoch: 11 | loss: 0.0186568
	speed: 0.1374s/iter; left time: 6095.1041s
	iters: 300, epoch: 11 | loss: 0.0261067
	speed: 0.1360s/iter; left time: 6022.0215s
	iters: 400, epoch: 11 | loss: 0.0201972
	speed: 0.1411s/iter; left time: 6233.8208s
	iters: 500, epoch: 11 | loss: 0.0227132
	speed: 0.1445s/iter; left time: 6368.5941s
	iters: 600, epoch: 11 | loss: 0.0168747
	speed: 0.1320s/iter; left time: 5806.0565s
	iters: 700, epoch: 11 | loss: 0.0244474
	speed: 0.1326s/iter; left time: 5819.1693s
	iters: 800, epoch: 11 | loss: 0.0158708
	speed: 0.1348s/iter; left time: 5898.1910s
	iters: 900, epoch: 11 | loss: 0.0233177
	speed: 0.1324s/iter; left time: 5779.9168s
	iters: 1000, epoch: 11 | loss: 0.0231835
	speed: 0.1352s/iter; left time: 5889.8572s
	iters: 1100, epoch: 11 | loss: 0.0227261
	speed: 0.1359s/iter; left time: 5909.5680s
Epoch: 11 cost time: 00h:02m:32.88s
Epoch: 11 | Train Loss: 0.0203122 Vali Loss: 0.0328566 Test Loss: 0.0383958
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Intermediate time for DE and pred_len 96: 00h:34m:11.77s


=== Starting experiments for pred_len: 168 ===

[2024-11-13 11:03:27,894] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:03:27,932] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:03:27,932] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:03:27,937] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:03:29,019] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:03:29,020] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 142285
val 30365
test 30365
train 142285
train 142285
val 30365
val 30365
train 142285
test 30365
test 30365
val 30365
test 30365
[2024-11-13 11:03:30,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 11:03:31,995] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 11:03:31,996] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 11:03:31,997] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 11:03:31,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 11:03:31,998] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 11:03:31,998] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 11:03:31,998] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 11:03:32,394] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 11:03:32,395] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 11:03:32,395] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 28.76 GB, percent = 2.9%
[2024-11-13 11:03:32,558] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 11:03:32,559] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 11:03:32,559] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.09 GB, percent = 2.9%
[2024-11-13 11:03:32,559] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 11:03:32,705] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 11:03:32,705] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 11:03:32,705] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.52 GB, percent = 2.9%
[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 11:03:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 11:03:32,706] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2acedd5b90>
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 11:03:32,707] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 11:03:32,708] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 11:03:32,708] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0359368
	speed: 0.1722s/iter; left time: 15298.7937s
	iters: 200, epoch: 1 | loss: 0.0448534
	speed: 0.1391s/iter; left time: 12339.8766s
	iters: 300, epoch: 1 | loss: 0.0433941
	speed: 0.1450s/iter; left time: 12845.9561s
	iters: 400, epoch: 1 | loss: 0.0418473
	speed: 0.1493s/iter; left time: 13217.9844s
	iters: 500, epoch: 1 | loss: 0.0473000
	speed: 0.1431s/iter; left time: 12653.4435s
	iters: 600, epoch: 1 | loss: 0.0449542
	speed: 0.1420s/iter; left time: 12543.6537s
	iters: 700, epoch: 1 | loss: 0.0429755
	speed: 0.1420s/iter; left time: 12530.1423s
	iters: 800, epoch: 1 | loss: 0.0494640
	speed: 0.1402s/iter; left time: 12352.5080s
	iters: 900, epoch: 1 | loss: 0.0413226
	speed: 0.1414s/iter; left time: 12443.0984s
	iters: 1000, epoch: 1 | loss: 0.0430699
	speed: 0.1437s/iter; left time: 12634.2753s
	iters: 1100, epoch: 1 | loss: 0.0416704
	speed: 0.1491s/iter; left time: 13095.6824s
Epoch: 1 cost time: 00h:02m:40.67s
Epoch: 1 | Train Loss: 0.0463432 Vali Loss: 0.0367494 Test Loss: 0.0425076
Validation loss decreased (inf --> 0.036749).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0301424
	speed: 0.5470s/iter; left time: 46156.3133s
	iters: 200, epoch: 2 | loss: 0.0237679
	speed: 0.1380s/iter; left time: 11629.3456s
	iters: 300, epoch: 2 | loss: 0.0273275
	speed: 0.1396s/iter; left time: 11747.6284s
	iters: 400, epoch: 2 | loss: 0.0363315
	speed: 0.1532s/iter; left time: 12877.1144s
	iters: 500, epoch: 2 | loss: 0.0213703
	speed: 0.1408s/iter; left time: 11825.7740s
	iters: 600, epoch: 2 | loss: 0.0212502
	speed: 0.1375s/iter; left time: 11532.5011s
	iters: 700, epoch: 2 | loss: 0.0261255
	speed: 0.1380s/iter; left time: 11560.4512s
	iters: 800, epoch: 2 | loss: 0.0245793
	speed: 0.1376s/iter; left time: 11510.0097s
	iters: 900, epoch: 2 | loss: 0.0246260
	speed: 0.1379s/iter; left time: 11528.3340s
	iters: 1000, epoch: 2 | loss: 0.0303873
	speed: 0.1396s/iter; left time: 11652.7868s
	iters: 1100, epoch: 2 | loss: 0.0278004
	speed: 0.1860s/iter; left time: 15507.0476s
Epoch: 2 cost time: 00h:02m:40.92s
Epoch: 2 | Train Loss: 0.0293619 Vali Loss: 0.0321608 Test Loss: 0.0374062
Validation loss decreased (0.036749 --> 0.032161).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0327742
	speed: 0.7670s/iter; left time: 61302.4705s
	iters: 200, epoch: 3 | loss: 0.0426615
	speed: 0.1419s/iter; left time: 11325.9891s
	iters: 300, epoch: 3 | loss: 0.0251191
	speed: 0.1383s/iter; left time: 11030.4376s
	iters: 400, epoch: 3 | loss: 0.0265200
	speed: 0.1394s/iter; left time: 11102.7374s
	iters: 500, epoch: 3 | loss: 0.0287092
	speed: 0.1381s/iter; left time: 10984.0975s
	iters: 600, epoch: 3 | loss: 0.0326744
	speed: 0.1406s/iter; left time: 11164.0744s
	iters: 700, epoch: 3 | loss: 0.0272740
	speed: 0.1370s/iter; left time: 10870.6939s
	iters: 800, epoch: 3 | loss: 0.0219435
	speed: 0.1365s/iter; left time: 10810.8142s
	iters: 900, epoch: 3 | loss: 0.0302642
	speed: 0.1479s/iter; left time: 11706.2359s
	iters: 1000, epoch: 3 | loss: 0.0288475
	speed: 0.1450s/iter; left time: 11456.6996s
	iters: 1100, epoch: 3 | loss: 0.0385880
	speed: 0.1409s/iter; left time: 11124.5729s
Epoch: 3 cost time: 00h:02m:37.21s
Epoch: 3 | Train Loss: 0.0275353 Vali Loss: 0.0315600 Test Loss: 0.0373444
Validation loss decreased (0.032161 --> 0.031560).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0265749
	speed: 0.5563s/iter; left time: 41990.0144s
	iters: 200, epoch: 4 | loss: 0.0307901
	speed: 0.1382s/iter; left time: 10415.4185s
	iters: 300, epoch: 4 | loss: 0.0240945
	speed: 0.1376s/iter; left time: 10359.3495s
	iters: 400, epoch: 4 | loss: 0.0293844
	speed: 0.1484s/iter; left time: 11155.4229s
	iters: 500, epoch: 4 | loss: 0.0243979
	speed: 0.1698s/iter; left time: 12746.2554s
	iters: 600, epoch: 4 | loss: 0.0342317
	speed: 0.1387s/iter; left time: 10401.2931s
	iters: 700, epoch: 4 | loss: 0.0235715
	speed: 0.1576s/iter; left time: 11803.1621s
	iters: 800, epoch: 4 | loss: 0.0298171
	speed: 0.1488s/iter; left time: 11124.8708s
	iters: 900, epoch: 4 | loss: 0.0243449
	speed: 0.1443s/iter; left time: 10777.9346s
	iters: 1000, epoch: 4 | loss: 0.0236856
	speed: 0.1365s/iter; left time: 10178.4176s
	iters: 1100, epoch: 4 | loss: 0.0263085
	speed: 0.1393s/iter; left time: 10372.4091s
Epoch: 4 cost time: 00h:02m:42.83s
Epoch: 4 | Train Loss: 0.0266994 Vali Loss: 0.0314476 Test Loss: 0.0374681
Validation loss decreased (0.031560 --> 0.031448).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0282643
	speed: 0.5089s/iter; left time: 36154.2422s
	iters: 200, epoch: 5 | loss: 0.0272768
	speed: 0.1390s/iter; left time: 9859.2993s
	iters: 300, epoch: 5 | loss: 0.0217751
	speed: 0.1361s/iter; left time: 9639.1548s
	iters: 400, epoch: 5 | loss: 0.0190624
	speed: 0.1354s/iter; left time: 9574.5745s
	iters: 500, epoch: 5 | loss: 0.0376097
	speed: 0.1360s/iter; left time: 9604.8419s
	iters: 600, epoch: 5 | loss: 0.0226067
	speed: 0.1356s/iter; left time: 9566.3206s
	iters: 700, epoch: 5 | loss: 0.0193413
	speed: 0.1353s/iter; left time: 9528.9752s
	iters: 800, epoch: 5 | loss: 0.0192956
	speed: 0.1354s/iter; left time: 9525.3985s
	iters: 900, epoch: 5 | loss: 0.0285773
	speed: 0.1371s/iter; left time: 9626.8953s
	iters: 1000, epoch: 5 | loss: 0.0276020
	speed: 0.1358s/iter; left time: 9522.4454s
	iters: 1100, epoch: 5 | loss: 0.0244862
	speed: 0.1366s/iter; left time: 9568.9091s
Epoch: 5 cost time: 00h:02m:31.96s
Epoch: 5 | Train Loss: 0.0259062 Vali Loss: 0.0320041 Test Loss: 0.0369165
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0214580
	speed: 0.4321s/iter; left time: 28773.2660s
	iters: 200, epoch: 6 | loss: 0.0234301
	speed: 0.1380s/iter; left time: 9177.8334s
	iters: 300, epoch: 6 | loss: 0.0200381
	speed: 0.1366s/iter; left time: 9070.3838s
	iters: 400, epoch: 6 | loss: 0.0311940
	speed: 0.1360s/iter; left time: 9013.8870s
	iters: 500, epoch: 6 | loss: 0.0301950
	speed: 0.1356s/iter; left time: 8977.4810s
	iters: 600, epoch: 6 | loss: 0.0203328
	speed: 0.1355s/iter; left time: 8953.2152s
	iters: 700, epoch: 6 | loss: 0.0225864
	speed: 0.1355s/iter; left time: 8939.1808s
	iters: 800, epoch: 6 | loss: 0.0181251
	speed: 0.1355s/iter; left time: 8927.4561s
	iters: 900, epoch: 6 | loss: 0.0198194
	speed: 0.1363s/iter; left time: 8967.0378s
	iters: 1000, epoch: 6 | loss: 0.0240496
	speed: 0.1372s/iter; left time: 9014.9313s
	iters: 1100, epoch: 6 | loss: 0.0215907
	speed: 0.1351s/iter; left time: 8863.7746s
Epoch: 6 cost time: 00h:02m:32.10s
Epoch: 6 | Train Loss: 0.0249147 Vali Loss: 0.0328208 Test Loss: 0.0379578
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0212492
	speed: 0.4438s/iter; left time: 27578.1033s
	iters: 200, epoch: 7 | loss: 0.0209108
	speed: 0.1347s/iter; left time: 8360.3285s
	iters: 300, epoch: 7 | loss: 0.0251728
	speed: 0.1354s/iter; left time: 8384.9002s
	iters: 400, epoch: 7 | loss: 0.0205974
	speed: 0.1346s/iter; left time: 8325.8712s
	iters: 500, epoch: 7 | loss: 0.0166121
	speed: 0.1346s/iter; left time: 8309.2218s
	iters: 600, epoch: 7 | loss: 0.0294926
	speed: 0.1347s/iter; left time: 8302.5436s
	iters: 700, epoch: 7 | loss: 0.0198359
	speed: 0.1350s/iter; left time: 8310.0735s
	iters: 800, epoch: 7 | loss: 0.0242572
	speed: 0.1352s/iter; left time: 8304.4934s
	iters: 900, epoch: 7 | loss: 0.0213706
	speed: 0.1349s/iter; left time: 8277.2322s
	iters: 1000, epoch: 7 | loss: 0.0236055
	speed: 0.1352s/iter; left time: 8280.2130s
	iters: 1100, epoch: 7 | loss: 0.0265674
	speed: 0.1349s/iter; left time: 8250.0903s
Epoch: 7 cost time: 00h:02m:30.45s
Epoch: 7 | Train Loss: 0.0237292 Vali Loss: 0.0337015 Test Loss: 0.0404406
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0256783
	speed: 0.4228s/iter; left time: 24393.0934s
	iters: 200, epoch: 8 | loss: 0.0266715
	speed: 0.1351s/iter; left time: 7779.5734s
	iters: 300, epoch: 8 | loss: 0.0207095
	speed: 0.1362s/iter; left time: 7832.3826s
	iters: 400, epoch: 8 | loss: 0.0248391
	speed: 0.1352s/iter; left time: 7761.6284s
	iters: 500, epoch: 8 | loss: 0.0196725
	speed: 0.1351s/iter; left time: 7740.5252s
	iters: 600, epoch: 8 | loss: 0.0184297
	speed: 0.1356s/iter; left time: 7753.9399s
	iters: 700, epoch: 8 | loss: 0.0277658
	speed: 0.1359s/iter; left time: 7761.8067s
	iters: 800, epoch: 8 | loss: 0.0265737
	speed: 0.1367s/iter; left time: 7793.6799s
	iters: 900, epoch: 8 | loss: 0.0243952
	speed: 0.1349s/iter; left time: 7675.3084s
	iters: 1000, epoch: 8 | loss: 0.0238605
	speed: 0.1349s/iter; left time: 7659.9294s
	iters: 1100, epoch: 8 | loss: 0.0290543
	speed: 0.1349s/iter; left time: 7649.6008s
Epoch: 8 cost time: 00h:02m:31.05s
Epoch: 8 | Train Loss: 0.0225662 Vali Loss: 0.0353154 Test Loss: 0.0423327
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0227777
	speed: 0.4248s/iter; left time: 22623.1170s
	iters: 200, epoch: 9 | loss: 0.0262419
	speed: 0.1364s/iter; left time: 7249.8248s
	iters: 300, epoch: 9 | loss: 0.0207251
	speed: 0.1370s/iter; left time: 7270.5946s
	iters: 400, epoch: 9 | loss: 0.0197221
	speed: 0.1358s/iter; left time: 7192.1330s
	iters: 500, epoch: 9 | loss: 0.0167211
	speed: 0.1353s/iter; left time: 7151.8524s
	iters: 600, epoch: 9 | loss: 0.0236003
	speed: 0.1357s/iter; left time: 7161.1481s
	iters: 700, epoch: 9 | loss: 0.0170809
	speed: 0.1347s/iter; left time: 7092.8895s
	iters: 800, epoch: 9 | loss: 0.0209369
	speed: 0.1344s/iter; left time: 7062.4339s
	iters: 900, epoch: 9 | loss: 0.0263193
	speed: 0.1481s/iter; left time: 7767.3616s
	iters: 1000, epoch: 9 | loss: 0.0252886
	speed: 0.1777s/iter; left time: 9305.0562s
	iters: 1100, epoch: 9 | loss: 0.0198990
	speed: 0.1418s/iter; left time: 7411.8532s
Epoch: 9 cost time: 00h:02m:37.36s
Epoch: 9 | Train Loss: 0.0214407 Vali Loss: 0.0361602 Test Loss: 0.0435897
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Intermediate time for DE and pred_len 168: 00h:29m:20.95s

Intermediate time for DE: 01h:43m:22.52s


=== Starting experiments for country: GB ===


=== Starting experiments for pred_len: 24 ===

[2024-11-13 11:32:47,356] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:32:47,364] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:32:47,366] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:32:47,373] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 11:32:48,369] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:32:48,369] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:32:48,370] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:32:48,370] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 11:32:48,370] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 143005
val 31085
test 31085
train 143005
train 143005
train 143005
val 31085
val 31085
val 31085
test 31085
test 31085
test 31085
[2024-11-13 11:32:50,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 11:32:51,137] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 11:32:51,138] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 11:32:51,138] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 11:32:51,139] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 11:32:51,139] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 11:32:51,140] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 11:32:51,140] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 11:32:51,461] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 11:32:51,462] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 11:32:51,466] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.52 GB, percent = 3.0%
[2024-11-13 11:32:51,628] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 11:32:51,628] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 11:32:51,629] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.79 GB, percent = 3.1%
[2024-11-13 11:32:51,629] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 11:32:51,778] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 11:32:51,779] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 11:32:51,779] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.09 GB, percent = 3.1%
[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 11:32:51,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 11:32:51,780] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5fb44d2ed0>
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 11:32:51,780] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 11:32:51,781] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 11:32:51,781] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0445508
	speed: 0.1689s/iter; left time: 15079.9524s
	iters: 200, epoch: 1 | loss: 0.0364530
	speed: 0.1673s/iter; left time: 14916.7352s
	iters: 300, epoch: 1 | loss: 0.0445299
	speed: 0.1678s/iter; left time: 14942.4247s
	iters: 400, epoch: 1 | loss: 0.0314661
	speed: 0.1670s/iter; left time: 14852.4109s
	iters: 500, epoch: 1 | loss: 0.0345089
	speed: 0.1688s/iter; left time: 14996.9240s
	iters: 600, epoch: 1 | loss: 0.0283519
	speed: 0.1704s/iter; left time: 15126.5485s
	iters: 700, epoch: 1 | loss: 0.0160935
	speed: 0.1562s/iter; left time: 13847.6493s
	iters: 800, epoch: 1 | loss: 0.0235337
	speed: 0.1582s/iter; left time: 14010.2982s
	iters: 900, epoch: 1 | loss: 0.0168510
	speed: 0.1689s/iter; left time: 14936.8908s
	iters: 1000, epoch: 1 | loss: 0.0143824
	speed: 0.1665s/iter; left time: 14713.4831s
	iters: 1100, epoch: 1 | loss: 0.0171138
	speed: 0.1690s/iter; left time: 14916.8498s
Epoch: 1 cost time: 00h:03m:04.57s
Epoch: 1 | Train Loss: 0.0295342 Vali Loss: 0.0213105 Test Loss: 0.0279612
Validation loss decreased (inf --> 0.021310).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0167756
	speed: 0.6396s/iter; left time: 54235.3679s
	iters: 200, epoch: 2 | loss: 0.0156655
	speed: 0.1383s/iter; left time: 11713.1563s
	iters: 300, epoch: 2 | loss: 0.0240314
	speed: 0.1398s/iter; left time: 11828.6467s
	iters: 400, epoch: 2 | loss: 0.0155516
	speed: 0.1391s/iter; left time: 11749.0185s
	iters: 500, epoch: 2 | loss: 0.0197155
	speed: 0.1631s/iter; left time: 13763.8673s
	iters: 600, epoch: 2 | loss: 0.0126239
	speed: 0.1368s/iter; left time: 11533.9759s
	iters: 700, epoch: 2 | loss: 0.0161524
	speed: 0.1373s/iter; left time: 11562.3026s
	iters: 800, epoch: 2 | loss: 0.0159542
	speed: 0.1377s/iter; left time: 11578.6068s
	iters: 900, epoch: 2 | loss: 0.0152068
	speed: 0.1375s/iter; left time: 11547.2452s
	iters: 1000, epoch: 2 | loss: 0.0121104
	speed: 0.1370s/iter; left time: 11494.6049s
	iters: 1100, epoch: 2 | loss: 0.0178026
	speed: 0.1372s/iter; left time: 11496.7225s
Epoch: 2 cost time: 00h:02m:37.20s
Epoch: 2 | Train Loss: 0.0173583 Vali Loss: 0.0201293 Test Loss: 0.0262493
Validation loss decreased (0.021310 --> 0.020129).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0216605
	speed: 0.6332s/iter; left time: 50859.1483s
	iters: 200, epoch: 3 | loss: 0.0127704
	speed: 0.1516s/iter; left time: 12159.6553s
	iters: 300, epoch: 3 | loss: 0.0188288
	speed: 0.1481s/iter; left time: 11867.3880s
	iters: 400, epoch: 3 | loss: 0.0127714
	speed: 0.1499s/iter; left time: 11999.0089s
	iters: 500, epoch: 3 | loss: 0.0111119
	speed: 0.1500s/iter; left time: 11986.5586s
	iters: 600, epoch: 3 | loss: 0.0135245
	speed: 0.1521s/iter; left time: 12140.7001s
	iters: 700, epoch: 3 | loss: 0.0192609
	speed: 0.1467s/iter; left time: 11697.7410s
	iters: 800, epoch: 3 | loss: 0.0223024
	speed: 0.1472s/iter; left time: 11720.5749s
	iters: 900, epoch: 3 | loss: 0.0113499
	speed: 0.1518s/iter; left time: 12068.2996s
	iters: 1000, epoch: 3 | loss: 0.0146333
	speed: 0.1515s/iter; left time: 12036.7716s
	iters: 1100, epoch: 3 | loss: 0.0131600
	speed: 0.1501s/iter; left time: 11906.7998s
Epoch: 3 cost time: 00h:02m:48.42s
Epoch: 3 | Train Loss: 0.0163123 Vali Loss: 0.0200448 Test Loss: 0.0264302
Validation loss decreased (0.020129 --> 0.020045).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0187275
	speed: 0.5255s/iter; left time: 39864.9755s
	iters: 200, epoch: 4 | loss: 0.0202246
	speed: 0.1358s/iter; left time: 10284.4427s
	iters: 300, epoch: 4 | loss: 0.0182328
	speed: 0.1394s/iter; left time: 10548.3262s
	iters: 400, epoch: 4 | loss: 0.0135744
	speed: 0.1797s/iter; left time: 13581.2004s
	iters: 500, epoch: 4 | loss: 0.0195635
	speed: 0.1685s/iter; left time: 12716.0054s
	iters: 600, epoch: 4 | loss: 0.0121597
	speed: 0.1414s/iter; left time: 10655.6937s
	iters: 700, epoch: 4 | loss: 0.0168482
	speed: 0.1511s/iter; left time: 11372.1743s
	iters: 800, epoch: 4 | loss: 0.0135412
	speed: 0.1427s/iter; left time: 10727.5482s
	iters: 900, epoch: 4 | loss: 0.0165432
	speed: 0.1479s/iter; left time: 11099.6962s
	iters: 1000, epoch: 4 | loss: 0.0164242
	speed: 0.1476s/iter; left time: 11060.4945s
	iters: 1100, epoch: 4 | loss: 0.0161591
	speed: 0.1453s/iter; left time: 10875.8727s
Epoch: 4 cost time: 00h:02m:46.79s
Epoch: 4 | Train Loss: 0.0159980 Vali Loss: 0.0201996 Test Loss: 0.0260893
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0141485
	speed: 0.4704s/iter; left time: 33581.9524s
	iters: 200, epoch: 5 | loss: 0.0138484
	speed: 0.1479s/iter; left time: 10541.3037s
	iters: 300, epoch: 5 | loss: 0.0135508
	speed: 0.1406s/iter; left time: 10007.6190s
	iters: 400, epoch: 5 | loss: 0.0143307
	speed: 0.1481s/iter; left time: 10524.7770s
	iters: 500, epoch: 5 | loss: 0.0136095
	speed: 0.1464s/iter; left time: 10391.2816s
	iters: 600, epoch: 5 | loss: 0.0170796
	speed: 0.1447s/iter; left time: 10254.4810s
	iters: 700, epoch: 5 | loss: 0.0158733
	speed: 0.1441s/iter; left time: 10199.7434s
	iters: 800, epoch: 5 | loss: 0.0140419
	speed: 0.1465s/iter; left time: 10355.5123s
	iters: 900, epoch: 5 | loss: 0.0146981
	speed: 0.1459s/iter; left time: 10297.4784s
	iters: 1000, epoch: 5 | loss: 0.0159149
	speed: 0.1657s/iter; left time: 11679.7014s
	iters: 1100, epoch: 5 | loss: 0.0173821
	speed: 0.1356s/iter; left time: 9543.6569s
Epoch: 5 cost time: 00h:02m:44.14s
Epoch: 5 | Train Loss: 0.0158847 Vali Loss: 0.0195334 Test Loss: 0.0254453
Validation loss decreased (0.020045 --> 0.019533).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0143947
	speed: 0.5572s/iter; left time: 37285.3580s
	iters: 200, epoch: 6 | loss: 0.0158171
	speed: 0.1436s/iter; left time: 9598.3176s
	iters: 300, epoch: 6 | loss: 0.0155350
	speed: 0.1425s/iter; left time: 9505.8236s
	iters: 400, epoch: 6 | loss: 0.0205618
	speed: 0.1475s/iter; left time: 9825.6910s
	iters: 500, epoch: 6 | loss: 0.0150704
	speed: 0.1468s/iter; left time: 9765.8414s
	iters: 600, epoch: 6 | loss: 0.0142039
	speed: 0.1409s/iter; left time: 9357.1907s
	iters: 700, epoch: 6 | loss: 0.0139274
	speed: 0.1466s/iter; left time: 9719.8030s
	iters: 800, epoch: 6 | loss: 0.0114122
	speed: 0.1391s/iter; left time: 9212.6672s
	iters: 900, epoch: 6 | loss: 0.0197675
	speed: 0.1466s/iter; left time: 9696.4502s
	iters: 1000, epoch: 6 | loss: 0.0182349
	speed: 0.1440s/iter; left time: 9506.8018s
	iters: 1100, epoch: 6 | loss: 0.0153248
	speed: 0.1451s/iter; left time: 9561.9837s
Epoch: 6 cost time: 00h:02m:42.22s
Epoch: 6 | Train Loss: 0.0155889 Vali Loss: 0.0196016 Test Loss: 0.0255106
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0122040
	speed: 0.4647s/iter; left time: 29020.8853s
	iters: 200, epoch: 7 | loss: 0.0146544
	speed: 0.1464s/iter; left time: 9127.8423s
	iters: 300, epoch: 7 | loss: 0.0146292
	speed: 0.1468s/iter; left time: 9140.4399s
	iters: 400, epoch: 7 | loss: 0.0161332
	speed: 0.1527s/iter; left time: 9487.8659s
	iters: 500, epoch: 7 | loss: 0.0123958
	speed: 0.1769s/iter; left time: 10979.2061s
	iters: 600, epoch: 7 | loss: 0.0120419
	speed: 0.1528s/iter; left time: 9468.6101s
	iters: 700, epoch: 7 | loss: 0.0091025
	speed: 0.1497s/iter; left time: 9256.7827s
	iters: 800, epoch: 7 | loss: 0.0131330
	speed: 0.1454s/iter; left time: 8976.2784s
	iters: 900, epoch: 7 | loss: 0.0155955
	speed: 0.1426s/iter; left time: 8790.2632s
	iters: 1000, epoch: 7 | loss: 0.0199920
	speed: 0.1477s/iter; left time: 9090.2497s
	iters: 1100, epoch: 7 | loss: 0.0171997
	speed: 0.1468s/iter; left time: 9019.8215s
Epoch: 7 cost time: 00h:02m:48.28s
Epoch: 7 | Train Loss: 0.0154535 Vali Loss: 0.0196389 Test Loss: 0.0258326
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0115717
	speed: 0.4594s/iter; left time: 26635.7757s
	iters: 200, epoch: 8 | loss: 0.0151119
	speed: 0.1439s/iter; left time: 8331.4829s
	iters: 300, epoch: 8 | loss: 0.0155807
	speed: 0.1419s/iter; left time: 8202.2757s
	iters: 400, epoch: 8 | loss: 0.0152087
	speed: 0.1442s/iter; left time: 8316.6771s
	iters: 500, epoch: 8 | loss: 0.0133925
	speed: 0.1481s/iter; left time: 8528.4769s
	iters: 600, epoch: 8 | loss: 0.0131203
	speed: 0.1452s/iter; left time: 8349.3047s
	iters: 700, epoch: 8 | loss: 0.0103167
	speed: 0.1489s/iter; left time: 8547.3725s
	iters: 800, epoch: 8 | loss: 0.0162530
	speed: 0.1362s/iter; left time: 7802.3516s
	iters: 900, epoch: 8 | loss: 0.0113713
	speed: 0.1336s/iter; left time: 7641.3838s
	iters: 1000, epoch: 8 | loss: 0.0115047
	speed: 0.1334s/iter; left time: 7613.9256s
	iters: 1100, epoch: 8 | loss: 0.0225006
	speed: 0.1340s/iter; left time: 7638.0302s
Epoch: 8 cost time: 00h:02m:38.24s
Epoch: 8 | Train Loss: 0.0151374 Vali Loss: 0.0196318 Test Loss: 0.0257646
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0119662
	speed: 0.4323s/iter; left time: 23136.4566s
	iters: 200, epoch: 9 | loss: 0.0165778
	speed: 0.1415s/iter; left time: 7558.6949s
	iters: 300, epoch: 9 | loss: 0.0131666
	speed: 0.1729s/iter; left time: 9220.9088s
	iters: 400, epoch: 9 | loss: 0.0152484
	speed: 0.1743s/iter; left time: 9273.2876s
	iters: 500, epoch: 9 | loss: 0.0153882
	speed: 0.1357s/iter; left time: 7210.5692s
	iters: 600, epoch: 9 | loss: 0.0150025
	speed: 0.1538s/iter; left time: 8152.9610s
	iters: 700, epoch: 9 | loss: 0.0164857
	speed: 0.1407s/iter; left time: 7443.6514s
	iters: 800, epoch: 9 | loss: 0.0132324
	speed: 0.1478s/iter; left time: 7803.9538s
	iters: 900, epoch: 9 | loss: 0.0190716
	speed: 0.1481s/iter; left time: 7805.7896s
	iters: 1000, epoch: 9 | loss: 0.0120974
	speed: 0.1443s/iter; left time: 7592.4004s
	iters: 1100, epoch: 9 | loss: 0.0095748
	speed: 0.1460s/iter; left time: 7669.7458s
Epoch: 9 cost time: 00h:02m:47.24s
Epoch: 9 | Train Loss: 0.0148546 Vali Loss: 0.0195053 Test Loss: 0.0259596
Validation loss decreased (0.019533 --> 0.019505).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0140567
	speed: 0.6005s/iter; left time: 29452.7297s
	iters: 200, epoch: 10 | loss: 0.0134446
	speed: 0.1445s/iter; left time: 7074.5708s
	iters: 300, epoch: 10 | loss: 0.0100605
	speed: 0.1473s/iter; left time: 7194.5783s
	iters: 400, epoch: 10 | loss: 0.0239360
	speed: 0.1429s/iter; left time: 6964.2641s
	iters: 500, epoch: 10 | loss: 0.0136836
	speed: 0.1463s/iter; left time: 7115.9583s
	iters: 600, epoch: 10 | loss: 0.0134052
	speed: 0.1414s/iter; left time: 6863.0326s
	iters: 700, epoch: 10 | loss: 0.0111096
	speed: 0.1770s/iter; left time: 8576.8994s
	iters: 800, epoch: 10 | loss: 0.0095586
	speed: 0.1656s/iter; left time: 8006.5191s
	iters: 900, epoch: 10 | loss: 0.0141145
	speed: 0.1395s/iter; left time: 6732.0777s
	iters: 1000, epoch: 10 | loss: 0.0113207
	speed: 0.1505s/iter; left time: 7248.4576s
	iters: 1100, epoch: 10 | loss: 0.0197075
	speed: 0.1438s/iter; left time: 6910.6822s
Epoch: 10 cost time: 00h:02m:47.56s
Epoch: 10 | Train Loss: 0.0145774 Vali Loss: 0.0199446 Test Loss: 0.0268420
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0142818
	speed: 0.4627s/iter; left time: 20626.4121s
	iters: 200, epoch: 11 | loss: 0.0155543
	speed: 0.1447s/iter; left time: 6437.4328s
	iters: 300, epoch: 11 | loss: 0.0120371
	speed: 0.1429s/iter; left time: 6343.1596s
	iters: 400, epoch: 11 | loss: 0.0102055
	speed: 0.1406s/iter; left time: 6225.9999s
	iters: 500, epoch: 11 | loss: 0.0138252
	speed: 0.1434s/iter; left time: 6337.3687s
	iters: 600, epoch: 11 | loss: 0.0128423
	speed: 0.1398s/iter; left time: 6160.6946s
	iters: 700, epoch: 11 | loss: 0.0203481
	speed: 0.1427s/iter; left time: 6273.9398s
	iters: 800, epoch: 11 | loss: 0.0165933
	speed: 0.1444s/iter; left time: 6336.4041s
	iters: 900, epoch: 11 | loss: 0.0200436
	speed: 0.1412s/iter; left time: 6181.4347s
	iters: 1000, epoch: 11 | loss: 0.0186101
	speed: 0.1473s/iter; left time: 6435.7428s
	iters: 1100, epoch: 11 | loss: 0.0099120
	speed: 0.1396s/iter; left time: 6084.1253s
Epoch: 11 cost time: 00h:02m:39.88s
Epoch: 11 | Train Loss: 0.0143655 Vali Loss: 0.0198954 Test Loss: 0.0276270
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0114732
	speed: 0.4985s/iter; left time: 19997.8949s
	iters: 200, epoch: 12 | loss: 0.0146292
	speed: 0.1533s/iter; left time: 6132.4669s
	iters: 300, epoch: 12 | loss: 0.0205463
	speed: 0.1450s/iter; left time: 5786.8678s
	iters: 400, epoch: 12 | loss: 0.0147206
	speed: 0.1491s/iter; left time: 5934.5702s
	iters: 500, epoch: 12 | loss: 0.0103157
	speed: 0.1519s/iter; left time: 6030.5000s
	iters: 600, epoch: 12 | loss: 0.0151519
	speed: 0.1441s/iter; left time: 5709.7368s
	iters: 700, epoch: 12 | loss: 0.0115998
	speed: 0.1447s/iter; left time: 5718.7836s
	iters: 800, epoch: 12 | loss: 0.0137087
	speed: 0.1386s/iter; left time: 5462.2397s
	iters: 900, epoch: 12 | loss: 0.0197190
	speed: 0.1449s/iter; left time: 5696.6512s
	iters: 1000, epoch: 12 | loss: 0.0094789
	speed: 0.1448s/iter; left time: 5677.7864s
	iters: 1100, epoch: 12 | loss: 0.0175112
	speed: 0.1419s/iter; left time: 5549.5587s
Epoch: 12 cost time: 00h:02m:42.25s
Epoch: 12 | Train Loss: 0.0140272 Vali Loss: 0.0196622 Test Loss: 0.0274446
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0165435
	speed: 0.4616s/iter; left time: 16453.3358s
	iters: 200, epoch: 13 | loss: 0.0082239
	speed: 0.1422s/iter; left time: 5055.3934s
	iters: 300, epoch: 13 | loss: 0.0136536
	speed: 0.1434s/iter; left time: 5082.6446s
	iters: 400, epoch: 13 | loss: 0.0121044
	speed: 0.1565s/iter; left time: 5533.1552s
	iters: 500, epoch: 13 | loss: 0.0146031
	speed: 0.1759s/iter; left time: 6198.7555s
	iters: 600, epoch: 13 | loss: 0.0114206
	speed: 0.1473s/iter; left time: 5175.3620s
	iters: 700, epoch: 13 | loss: 0.0122001
	speed: 0.1502s/iter; left time: 5264.4591s
	iters: 800, epoch: 13 | loss: 0.0092386
	speed: 0.1477s/iter; left time: 5163.0182s
	iters: 900, epoch: 13 | loss: 0.0119760
	speed: 0.1398s/iter; left time: 4870.8015s
	iters: 1000, epoch: 13 | loss: 0.0124882
	speed: 0.1474s/iter; left time: 5122.1298s
	iters: 1100, epoch: 13 | loss: 0.0165476
	speed: 0.1435s/iter; left time: 4971.0180s
Epoch: 13 cost time: 00h:02m:47.22s
Epoch: 13 | Train Loss: 0.0137605 Vali Loss: 0.0195286 Test Loss: 0.0274269
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0123071
	speed: 0.4582s/iter; left time: 14286.8493s
	iters: 200, epoch: 14 | loss: 0.0137600
	speed: 0.1445s/iter; left time: 4490.1253s
	iters: 300, epoch: 14 | loss: 0.0163909
	speed: 0.1445s/iter; left time: 4477.7194s
	iters: 400, epoch: 14 | loss: 0.0137735
	speed: 0.1435s/iter; left time: 4431.6945s
	iters: 500, epoch: 14 | loss: 0.0141575
	speed: 0.1471s/iter; left time: 4526.3406s
	iters: 600, epoch: 14 | loss: 0.0134898
	speed: 0.1418s/iter; left time: 4349.2324s
	iters: 700, epoch: 14 | loss: 0.0115834
	speed: 0.1480s/iter; left time: 4526.1229s
	iters: 800, epoch: 14 | loss: 0.0097802
	speed: 0.1404s/iter; left time: 4279.3233s
	iters: 900, epoch: 14 | loss: 0.0110015
	speed: 0.1468s/iter; left time: 4457.9061s
	iters: 1000, epoch: 14 | loss: 0.0159150
	speed: 0.1759s/iter; left time: 5325.3338s
	iters: 1100, epoch: 14 | loss: 0.0127477
	speed: 0.1632s/iter; left time: 4923.7249s
Epoch: 14 cost time: 00h:02m:46.93s
Epoch: 14 | Train Loss: 0.0135975 Vali Loss: 0.0199316 Test Loss: 0.0280450
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036
Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036
Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036
Scaled mse:0.02595963142812252, rmse:0.16111992299556732, mae:0.10740001499652863, rse:0.5551244020462036
Intermediate time for GB and pred_len 24: 00h:47m:05.58s


=== Starting experiments for pred_len: 96 ===

[2024-11-13 12:19:54,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:19:54,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:19:54,320] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:19:54,328] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:19:55,344] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:19:55,346] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:19:55,346] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 142645
val 30725
test 30725
train 142645
train 142645
val 30725
val 30725
test 30725
test 30725
train 142645
val 30725
test 30725
[2024-11-13 12:19:57,695] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 12:19:58,674] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 12:19:58,675] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 12:19:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 12:19:58,675] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 12:19:59,007] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 12:19:59,008] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 12:19:59,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.27 GB, percent = 2.9%
[2024-11-13 12:19:59,175] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 12:19:59,176] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 12:19:59,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.63 GB, percent = 2.9%
[2024-11-13 12:19:59,176] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 12:19:59,317] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 12:19:59,318] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 12:19:59,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 29.9 GB, percent = 3.0%
[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 12:19:59,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 12:19:59,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 12:19:59,319] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 12:19:59,319] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fee46d773d0>
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 12:19:59,320] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 12:19:59,321] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 12:19:59,321] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0431685
	speed: 0.1758s/iter; left time: 15654.4433s
	iters: 200, epoch: 1 | loss: 0.0412425
	speed: 0.1474s/iter; left time: 13112.2237s
	iters: 300, epoch: 1 | loss: 0.0381312
	speed: 0.1495s/iter; left time: 13278.8728s
	iters: 400, epoch: 1 | loss: 0.0340931
	speed: 0.1471s/iter; left time: 13055.5807s
	iters: 500, epoch: 1 | loss: 0.0292018
	speed: 0.1484s/iter; left time: 13154.3788s
	iters: 600, epoch: 1 | loss: 0.0356489
	speed: 0.1485s/iter; left time: 13145.4687s
	iters: 700, epoch: 1 | loss: 0.0240586
	speed: 0.1500s/iter; left time: 13265.3475s
	iters: 800, epoch: 1 | loss: 0.0366761
	speed: 0.1513s/iter; left time: 13365.8592s
	iters: 900, epoch: 1 | loss: 0.0273430
	speed: 0.1496s/iter; left time: 13205.1400s
	iters: 1000, epoch: 1 | loss: 0.0331169
	speed: 0.1460s/iter; left time: 12869.0423s
	iters: 1100, epoch: 1 | loss: 0.0259525
	speed: 0.1393s/iter; left time: 12263.0066s
Epoch: 1 cost time: 00h:02m:45.73s
Epoch: 1 | Train Loss: 0.0362132 Vali Loss: 0.0313898 Test Loss: 0.0448382
Validation loss decreased (inf --> 0.031390).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0258306
	speed: 0.5717s/iter; left time: 48360.6021s
	iters: 200, epoch: 2 | loss: 0.0334581
	speed: 0.1566s/iter; left time: 13230.5393s
	iters: 300, epoch: 2 | loss: 0.0252429
	speed: 0.1482s/iter; left time: 12507.3714s
	iters: 400, epoch: 2 | loss: 0.0185966
	speed: 0.1514s/iter; left time: 12760.8535s
	iters: 500, epoch: 2 | loss: 0.0246332
	speed: 0.1430s/iter; left time: 12040.1216s
	iters: 600, epoch: 2 | loss: 0.0231118
	speed: 0.1479s/iter; left time: 12431.8229s
	iters: 700, epoch: 2 | loss: 0.0219791
	speed: 0.1509s/iter; left time: 12671.5795s
	iters: 800, epoch: 2 | loss: 0.0198914
	speed: 0.1448s/iter; left time: 12148.5088s
	iters: 900, epoch: 2 | loss: 0.0247300
	speed: 0.1471s/iter; left time: 12327.7538s
	iters: 1000, epoch: 2 | loss: 0.0231949
	speed: 0.1481s/iter; left time: 12395.4467s
	iters: 1100, epoch: 2 | loss: 0.0246845
	speed: 0.1459s/iter; left time: 12197.9972s
Epoch: 2 cost time: 00h:02m:48.81s
Epoch: 2 | Train Loss: 0.0239260 Vali Loss: 0.0285503 Test Loss: 0.0405891
Validation loss decreased (0.031390 --> 0.028550).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0197042
	speed: 0.5781s/iter; left time: 46320.3913s
	iters: 200, epoch: 3 | loss: 0.0239601
	speed: 0.1454s/iter; left time: 11636.4902s
	iters: 300, epoch: 3 | loss: 0.0183007
	speed: 0.1429s/iter; left time: 11419.4287s
	iters: 400, epoch: 3 | loss: 0.0246140
	speed: 0.1638s/iter; left time: 13074.1593s
	iters: 500, epoch: 3 | loss: 0.0191527
	speed: 0.1760s/iter; left time: 14028.3477s
	iters: 600, epoch: 3 | loss: 0.0159092
	speed: 0.1406s/iter; left time: 11197.5995s
	iters: 700, epoch: 3 | loss: 0.0165353
	speed: 0.1554s/iter; left time: 12356.1520s
	iters: 800, epoch: 3 | loss: 0.0242688
	speed: 0.1414s/iter; left time: 11227.4441s
	iters: 900, epoch: 3 | loss: 0.0225599
	speed: 0.1484s/iter; left time: 11769.1555s
	iters: 1000, epoch: 3 | loss: 0.0251363
	speed: 0.1501s/iter; left time: 11894.0352s
	iters: 1100, epoch: 3 | loss: 0.0191519
	speed: 0.1436s/iter; left time: 11363.0548s
Epoch: 3 cost time: 00h:02m:47.74s
Epoch: 3 | Train Loss: 0.0226770 Vali Loss: 0.0285465 Test Loss: 0.0406802
Validation loss decreased (0.028550 --> 0.028547).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0234475
	speed: 0.5714s/iter; left time: 43234.9674s
	iters: 200, epoch: 4 | loss: 0.0300897
	speed: 0.1446s/iter; left time: 10923.7429s
	iters: 300, epoch: 4 | loss: 0.0228381
	speed: 0.1427s/iter; left time: 10767.7970s
	iters: 400, epoch: 4 | loss: 0.0283763
	speed: 0.1485s/iter; left time: 11189.0675s
	iters: 500, epoch: 4 | loss: 0.0187848
	speed: 0.1460s/iter; left time: 10989.6127s
	iters: 600, epoch: 4 | loss: 0.0241174
	speed: 0.1433s/iter; left time: 10775.4521s
	iters: 700, epoch: 4 | loss: 0.0249683
	speed: 0.1419s/iter; left time: 10653.7677s
	iters: 800, epoch: 4 | loss: 0.0259485
	speed: 0.1730s/iter; left time: 12971.5867s
	iters: 900, epoch: 4 | loss: 0.0209155
	speed: 0.1697s/iter; left time: 12701.7659s
	iters: 1000, epoch: 4 | loss: 0.0263899
	speed: 0.1388s/iter; left time: 10379.1844s
	iters: 1100, epoch: 4 | loss: 0.0173570
	speed: 0.1495s/iter; left time: 11164.1992s
Epoch: 4 cost time: 00h:02m:47.23s
Epoch: 4 | Train Loss: 0.0224530 Vali Loss: 0.0287918 Test Loss: 0.0410931
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0178938
	speed: 0.4623s/iter; left time: 32919.4405s
	iters: 200, epoch: 5 | loss: 0.0188043
	speed: 0.1400s/iter; left time: 9956.5457s
	iters: 300, epoch: 5 | loss: 0.0216615
	speed: 0.1446s/iter; left time: 10268.4174s
	iters: 400, epoch: 5 | loss: 0.0233857
	speed: 0.1430s/iter; left time: 10141.0748s
	iters: 500, epoch: 5 | loss: 0.0290505
	speed: 0.1413s/iter; left time: 10005.8098s
	iters: 600, epoch: 5 | loss: 0.0166351
	speed: 0.1410s/iter; left time: 9967.6144s
	iters: 700, epoch: 5 | loss: 0.0162357
	speed: 0.1436s/iter; left time: 10137.5615s
	iters: 800, epoch: 5 | loss: 0.0181134
	speed: 0.1482s/iter; left time: 10448.4301s
	iters: 900, epoch: 5 | loss: 0.0203686
	speed: 0.1451s/iter; left time: 10219.6400s
	iters: 1000, epoch: 5 | loss: 0.0181668
	speed: 0.1450s/iter; left time: 10197.7090s
	iters: 1100, epoch: 5 | loss: 0.0223588
	speed: 0.1440s/iter; left time: 10112.0823s
Epoch: 5 cost time: 00h:02m:40.72s
Epoch: 5 | Train Loss: 0.0219209 Vali Loss: 0.0294841 Test Loss: 0.0420929
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0236786
	speed: 0.4999s/iter; left time: 33373.5461s
	iters: 200, epoch: 6 | loss: 0.0201914
	speed: 0.1455s/iter; left time: 9697.6561s
	iters: 300, epoch: 6 | loss: 0.0241761
	speed: 0.1462s/iter; left time: 9730.7753s
	iters: 400, epoch: 6 | loss: 0.0242343
	speed: 0.1419s/iter; left time: 9429.5514s
	iters: 500, epoch: 6 | loss: 0.0198615
	speed: 0.1505s/iter; left time: 9985.8626s
	iters: 600, epoch: 6 | loss: 0.0203659
	speed: 0.1428s/iter; left time: 9462.1292s
	iters: 700, epoch: 6 | loss: 0.0144106
	speed: 0.1396s/iter; left time: 9236.1745s
	iters: 800, epoch: 6 | loss: 0.0290639
	speed: 0.1423s/iter; left time: 9398.0438s
	iters: 900, epoch: 6 | loss: 0.0208715
	speed: 0.1430s/iter; left time: 9432.2366s
	iters: 1000, epoch: 6 | loss: 0.0160980
	speed: 0.1420s/iter; left time: 9351.1661s
	iters: 1100, epoch: 6 | loss: 0.0191238
	speed: 0.1429s/iter; left time: 9397.4567s
Epoch: 6 cost time: 00h:02m:41.67s
Epoch: 6 | Train Loss: 0.0211830 Vali Loss: 0.0289895 Test Loss: 0.0423584
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0238875
	speed: 0.4596s/iter; left time: 28635.5410s
	iters: 200, epoch: 7 | loss: 0.0230123
	speed: 0.1411s/iter; left time: 8775.3995s
	iters: 300, epoch: 7 | loss: 0.0194589
	speed: 0.1392s/iter; left time: 8644.8621s
	iters: 400, epoch: 7 | loss: 0.0207183
	speed: 0.1760s/iter; left time: 10910.7722s
	iters: 500, epoch: 7 | loss: 0.0176361
	speed: 0.1695s/iter; left time: 10490.1977s
	iters: 600, epoch: 7 | loss: 0.0229180
	speed: 0.1340s/iter; left time: 8279.7777s
	iters: 700, epoch: 7 | loss: 0.0217962
	speed: 0.1533s/iter; left time: 9456.2479s
	iters: 800, epoch: 7 | loss: 0.0188232
	speed: 0.1396s/iter; left time: 8596.6262s
	iters: 900, epoch: 7 | loss: 0.0143067
	speed: 0.1454s/iter; left time: 8940.9797s
	iters: 1000, epoch: 7 | loss: 0.0162435
	speed: 0.1444s/iter; left time: 8863.3899s
	iters: 1100, epoch: 7 | loss: 0.0184402
	speed: 0.1458s/iter; left time: 8937.3963s
Epoch: 7 cost time: 00h:02m:45.57s
Epoch: 7 | Train Loss: 0.0204943 Vali Loss: 0.0291370 Test Loss: 0.0436702
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0156560
	speed: 0.4491s/iter; left time: 25975.3082s
	iters: 200, epoch: 8 | loss: 0.0162347
	speed: 0.1441s/iter; left time: 8321.3504s
	iters: 300, epoch: 8 | loss: 0.0205112
	speed: 0.1436s/iter; left time: 8278.8374s
	iters: 400, epoch: 8 | loss: 0.0194970
	speed: 0.1491s/iter; left time: 8581.8332s
	iters: 500, epoch: 8 | loss: 0.0208870
	speed: 0.1405s/iter; left time: 8072.3808s
	iters: 600, epoch: 8 | loss: 0.0229489
	speed: 0.1467s/iter; left time: 8412.1015s
	iters: 700, epoch: 8 | loss: 0.0246555
	speed: 0.1415s/iter; left time: 8101.8741s
	iters: 800, epoch: 8 | loss: 0.0203001
	speed: 0.1408s/iter; left time: 8044.6193s
	iters: 900, epoch: 8 | loss: 0.0155339
	speed: 0.1752s/iter; left time: 9991.2409s
	iters: 1000, epoch: 8 | loss: 0.0228974
	speed: 0.1624s/iter; left time: 9246.3945s
	iters: 1100, epoch: 8 | loss: 0.0213769
	speed: 0.1399s/iter; left time: 7950.4401s
Epoch: 8 cost time: 00h:02m:45.05s
Epoch: 8 | Train Loss: 0.0197084 Vali Loss: 0.0296450 Test Loss: 0.0439350
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515
Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515
Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515
Scaled mse:0.04068015143275261, rmse:0.20169320702552795, mae:0.14278638362884521, rse:0.6974509954452515
Intermediate time for GB and pred_len 96: 00h:27m:06.62s


=== Starting experiments for pred_len: 168 ===

[2024-11-13 12:47:00,827] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:47:00,827] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:47:00,828] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:47:00,829] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 12:47:01,852] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 12:47:01,852] [INFO] [comm.py:637:init_distributed] cdb=None
train 142285
val 30365
test 30365
train 142285
train 142285
train 142285
val 30365
val 30365
val 30365
test 30365
test 30365
test 30365
[2024-11-13 12:47:03,961] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 12:47:04,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 12:47:04,463] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 12:47:04,463] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 12:47:04,463] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 12:47:04,463] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 12:47:04,464] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 12:47:04,464] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 12:47:04,866] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 12:47:04,867] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 12:47:04,867] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.11 GB, percent = 3.0%
[2024-11-13 12:47:05,320] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 12:47:05,320] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 12:47:05,320] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.81 GB, percent = 3.1%
[2024-11-13 12:47:05,321] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 12:47:05,531] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 12:47:05,531] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 12:47:05,531] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.32 GB, percent = 3.1%
[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 12:47:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 12:47:05,532] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f60468b89d0>
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 12:47:05,533] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 12:47:05,534] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 12:47:05,534] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0378977
	speed: 0.1759s/iter; left time: 15622.6922s
	iters: 200, epoch: 1 | loss: 0.0419873
	speed: 0.1489s/iter; left time: 13210.5809s
	iters: 300, epoch: 1 | loss: 0.0356091
	speed: 0.1491s/iter; left time: 13215.5034s
	iters: 400, epoch: 1 | loss: 0.0364937
	speed: 0.1476s/iter; left time: 13063.3147s
	iters: 500, epoch: 1 | loss: 0.0337698
	speed: 0.1496s/iter; left time: 13231.5394s
	iters: 600, epoch: 1 | loss: 0.0345404
	speed: 0.1550s/iter; left time: 13692.6434s
	iters: 700, epoch: 1 | loss: 0.0367745
	speed: 0.1469s/iter; left time: 12960.7442s
	iters: 800, epoch: 1 | loss: 0.0408153
	speed: 0.1521s/iter; left time: 13403.7619s
	iters: 900, epoch: 1 | loss: 0.0269480
	speed: 0.1458s/iter; left time: 12830.5565s
	iters: 1000, epoch: 1 | loss: 0.0305300
	speed: 0.1492s/iter; left time: 13113.5783s
	iters: 1100, epoch: 1 | loss: 0.0259015
	speed: 0.1835s/iter; left time: 16113.5196s
Epoch: 1 cost time: 00h:02m:50.95s
Epoch: 1 | Train Loss: 0.0362847 Vali Loss: 0.0319874 Test Loss: 0.0458686
Validation loss decreased (inf --> 0.031987).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0231011
	speed: 0.6749s/iter; left time: 56943.6581s
	iters: 200, epoch: 2 | loss: 0.0233680
	speed: 0.1475s/iter; left time: 12431.8754s
	iters: 300, epoch: 2 | loss: 0.0231609
	speed: 0.1489s/iter; left time: 12532.0878s
	iters: 400, epoch: 2 | loss: 0.0254489
	speed: 0.1460s/iter; left time: 12279.0582s
	iters: 500, epoch: 2 | loss: 0.0178552
	speed: 0.1473s/iter; left time: 12365.7776s
	iters: 600, epoch: 2 | loss: 0.0188664
	speed: 0.1472s/iter; left time: 12344.5553s
	iters: 700, epoch: 2 | loss: 0.0252296
	speed: 0.1491s/iter; left time: 12493.4521s
	iters: 800, epoch: 2 | loss: 0.0281855
	speed: 0.1493s/iter; left time: 12495.0919s
	iters: 900, epoch: 2 | loss: 0.0223051
	speed: 0.1481s/iter; left time: 12379.3665s
	iters: 1000, epoch: 2 | loss: 0.0262830
	speed: 0.1503s/iter; left time: 12549.6521s
	iters: 1100, epoch: 2 | loss: 0.0261961
	speed: 0.1507s/iter; left time: 12567.3697s
Epoch: 2 cost time: 00h:02m:46.41s
Epoch: 2 | Train Loss: 0.0253058 Vali Loss: 0.0303148 Test Loss: 0.0436852
Validation loss decreased (0.031987 --> 0.030315).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0261015
	speed: 0.6465s/iter; left time: 51674.9232s
	iters: 200, epoch: 3 | loss: 0.0404731
	speed: 0.1518s/iter; left time: 12115.0665s
	iters: 300, epoch: 3 | loss: 0.0243075
	speed: 0.1485s/iter; left time: 11837.8628s
	iters: 400, epoch: 3 | loss: 0.0239190
	speed: 0.1505s/iter; left time: 11986.0432s
	iters: 500, epoch: 3 | loss: 0.0216389
	speed: 0.1492s/iter; left time: 11862.7518s
	iters: 600, epoch: 3 | loss: 0.0287541
	speed: 0.1470s/iter; left time: 11674.5047s
	iters: 700, epoch: 3 | loss: 0.0206204
	speed: 0.1460s/iter; left time: 11584.7796s
	iters: 800, epoch: 3 | loss: 0.0211433
	speed: 0.1478s/iter; left time: 11709.8314s
	iters: 900, epoch: 3 | loss: 0.0259170
	speed: 0.1490s/iter; left time: 11787.0177s
	iters: 1000, epoch: 3 | loss: 0.0205409
	speed: 0.1468s/iter; left time: 11604.5434s
	iters: 1100, epoch: 3 | loss: 0.0319823
	speed: 0.1464s/iter; left time: 11557.4452s
Epoch: 3 cost time: 00h:02m:45.28s
Epoch: 3 | Train Loss: 0.0244032 Vali Loss: 0.0301797 Test Loss: 0.0434866
Validation loss decreased (0.030315 --> 0.030180).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0256963
	speed: 0.5675s/iter; left time: 42833.9604s
	iters: 200, epoch: 4 | loss: 0.0290772
	speed: 0.1536s/iter; left time: 11581.1860s
	iters: 300, epoch: 4 | loss: 0.0239815
	speed: 0.1784s/iter; left time: 13432.1521s
	iters: 400, epoch: 4 | loss: 0.0218931
	speed: 0.1553s/iter; left time: 11676.0520s
	iters: 500, epoch: 4 | loss: 0.0247705
	speed: 0.1521s/iter; left time: 11416.5307s
	iters: 600, epoch: 4 | loss: 0.0266350
	speed: 0.1482s/iter; left time: 11116.1300s
	iters: 700, epoch: 4 | loss: 0.0252275
	speed: 0.1489s/iter; left time: 11153.3651s
	iters: 800, epoch: 4 | loss: 0.0297814
	speed: 0.1506s/iter; left time: 11259.0229s
	iters: 900, epoch: 4 | loss: 0.0179410
	speed: 0.1444s/iter; left time: 10785.9688s
	iters: 1000, epoch: 4 | loss: 0.0197732
	speed: 0.1458s/iter; left time: 10874.2652s
	iters: 1100, epoch: 4 | loss: 0.0223960
	speed: 0.1485s/iter; left time: 11061.1264s
Epoch: 4 cost time: 00h:02m:49.57s
Epoch: 4 | Train Loss: 0.0238908 Vali Loss: 0.0309684 Test Loss: 0.0434581
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0283829
	speed: 0.4510s/iter; left time: 32034.2971s
	iters: 200, epoch: 5 | loss: 0.0219900
	speed: 0.1470s/iter; left time: 10424.5901s
	iters: 300, epoch: 5 | loss: 0.0191130
	speed: 0.1471s/iter; left time: 10423.3004s
	iters: 400, epoch: 5 | loss: 0.0148625
	speed: 0.1521s/iter; left time: 10756.7540s
	iters: 500, epoch: 5 | loss: 0.0246373
	speed: 0.1452s/iter; left time: 10254.4069s
	iters: 600, epoch: 5 | loss: 0.0179953
	speed: 0.1469s/iter; left time: 10361.4160s
	iters: 700, epoch: 5 | loss: 0.0196560
	speed: 0.1573s/iter; left time: 11078.0661s
	iters: 800, epoch: 5 | loss: 0.0215981
	speed: 0.1793s/iter; left time: 12609.0967s
	iters: 900, epoch: 5 | loss: 0.0253618
	speed: 0.1521s/iter; left time: 10684.7874s
	iters: 1000, epoch: 5 | loss: 0.0208310
	speed: 0.1538s/iter; left time: 10790.4714s
	iters: 1100, epoch: 5 | loss: 0.0218497
	speed: 0.1474s/iter; left time: 10326.7091s
Epoch: 5 cost time: 00h:02m:49.61s
Epoch: 5 | Train Loss: 0.0230881 Vali Loss: 0.0315672 Test Loss: 0.0440530
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0195508
	speed: 0.4564s/iter; left time: 30393.5712s
	iters: 200, epoch: 6 | loss: 0.0208991
	speed: 0.1454s/iter; left time: 9665.8534s
	iters: 300, epoch: 6 | loss: 0.0208607
	speed: 0.1391s/iter; left time: 9233.0083s
	iters: 400, epoch: 6 | loss: 0.0259936
	speed: 0.1463s/iter; left time: 9701.5206s
	iters: 500, epoch: 6 | loss: 0.0239710
	speed: 0.1449s/iter; left time: 9594.2853s
	iters: 600, epoch: 6 | loss: 0.0177536
	speed: 0.1448s/iter; left time: 9572.2872s
	iters: 700, epoch: 6 | loss: 0.0203063
	speed: 0.1469s/iter; left time: 9696.4900s
	iters: 800, epoch: 6 | loss: 0.0190913
	speed: 0.1462s/iter; left time: 9634.5712s
	iters: 900, epoch: 6 | loss: 0.0180605
	speed: 0.1487s/iter; left time: 9782.7205s
	iters: 1000, epoch: 6 | loss: 0.0214202
	speed: 0.1496s/iter; left time: 9827.1739s
	iters: 1100, epoch: 6 | loss: 0.0192209
	speed: 0.1452s/iter; left time: 9522.7678s
Epoch: 6 cost time: 00h:02m:42.59s
Epoch: 6 | Train Loss: 0.0222028 Vali Loss: 0.0321920 Test Loss: 0.0460837
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0182597
	speed: 0.4976s/iter; left time: 30926.3723s
	iters: 200, epoch: 7 | loss: 0.0198322
	speed: 0.1501s/iter; left time: 9315.1383s
	iters: 300, epoch: 7 | loss: 0.0240528
	speed: 0.1445s/iter; left time: 8953.1140s
	iters: 400, epoch: 7 | loss: 0.0182669
	speed: 0.1529s/iter; left time: 9453.0261s
	iters: 500, epoch: 7 | loss: 0.0143159
	speed: 0.1428s/iter; left time: 8814.4324s
	iters: 600, epoch: 7 | loss: 0.0193958
	speed: 0.1451s/iter; left time: 8943.3694s
	iters: 700, epoch: 7 | loss: 0.0183734
	speed: 0.1472s/iter; left time: 9057.0261s
	iters: 800, epoch: 7 | loss: 0.0228030
	speed: 0.1458s/iter; left time: 8955.9752s
	iters: 900, epoch: 7 | loss: 0.0241459
	speed: 0.1442s/iter; left time: 8844.6763s
	iters: 1000, epoch: 7 | loss: 0.0196830
	speed: 0.1438s/iter; left time: 8807.0666s
	iters: 1100, epoch: 7 | loss: 0.0201751
	speed: 0.1454s/iter; left time: 8888.6415s
Epoch: 7 cost time: 00h:02m:43.27s
Epoch: 7 | Train Loss: 0.0211709 Vali Loss: 0.0328852 Test Loss: 0.0476506
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0257308
	speed: 0.4564s/iter; left time: 26335.0466s
	iters: 200, epoch: 8 | loss: 0.0251673
	speed: 0.1442s/iter; left time: 8308.5205s
	iters: 300, epoch: 8 | loss: 0.0193176
	speed: 0.1653s/iter; left time: 9502.6545s
	iters: 400, epoch: 8 | loss: 0.0221599
	speed: 0.1773s/iter; left time: 10176.6146s
	iters: 500, epoch: 8 | loss: 0.0184828
	speed: 0.1404s/iter; left time: 8044.8569s
	iters: 600, epoch: 8 | loss: 0.0199777
	speed: 0.1533s/iter; left time: 8770.9536s
	iters: 700, epoch: 8 | loss: 0.0262421
	speed: 0.1444s/iter; left time: 8247.0761s
	iters: 800, epoch: 8 | loss: 0.0216570
	speed: 0.1474s/iter; left time: 8401.0695s
	iters: 900, epoch: 8 | loss: 0.0189521
	speed: 0.1461s/iter; left time: 8313.5330s
	iters: 1000, epoch: 8 | loss: 0.0170493
	speed: 0.1487s/iter; left time: 8444.5690s
	iters: 1100, epoch: 8 | loss: 0.0247314
	speed: 0.1461s/iter; left time: 8284.4914s
Epoch: 8 cost time: 00h:02m:47.85s
Epoch: 8 | Train Loss: 0.0200113 Vali Loss: 0.0340090 Test Loss: 0.0489460
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512
Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512
Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512
Scaled mse:0.043486617505550385, rmse:0.20853444933891296, mae:0.14944525063037872, rse:0.7227435111999512
Intermediate time for GB and pred_len 168: 00h:27m:36.47s

Intermediate time for GB: 01h:41m:48.68s


=== Starting experiments for country: ES ===


=== Starting experiments for pred_len: 24 ===

[2024-11-13 13:14:37,168] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:14:37,200] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:14:37,201] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:14:37,204] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:14:38,221] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:14:38,221] [INFO] [comm.py:637:init_distributed] cdb=None
train 86331
val 18651
test 18651
train 86331
train 86331
train 86331
val 18651
val 18651
val 18651
test 18651
test 18651
test 18651
[2024-11-13 13:14:40,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 13:14:41,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 13:14:41,002] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 13:14:41,002] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 13:14:41,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 13:14:41,003] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 13:14:41,003] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 13:14:41,003] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 13:14:41,373] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 13:14:41,374] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 13:14:41,374] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.6 GB, percent = 3.0%
[2024-11-13 13:14:41,518] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 13:14:41,519] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 13:14:41,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 30.9 GB, percent = 3.1%
[2024-11-13 13:14:41,519] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 13:14:41,642] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 13:14:41,643] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 13:14:41,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 31.16 GB, percent = 3.1%
[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 13:14:41,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 13:14:41,644] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f05605bf990>
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 13:14:41,645] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 13:14:41,646] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 13:14:41,646] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0585907
	speed: 0.1656s/iter; left time: 8918.4506s
	iters: 200, epoch: 1 | loss: 0.0585956
	speed: 0.1400s/iter; left time: 7521.9832s
	iters: 300, epoch: 1 | loss: 0.0362778
	speed: 0.1364s/iter; left time: 7317.1248s
	iters: 400, epoch: 1 | loss: 0.0259126
	speed: 0.1335s/iter; left time: 7146.9061s
	iters: 500, epoch: 1 | loss: 0.0213988
	speed: 0.1639s/iter; left time: 8758.2332s
	iters: 600, epoch: 1 | loss: 0.0159848
	speed: 0.1621s/iter; left time: 8648.6353s
Epoch: 1 cost time: 00h:01m:38.55s
Epoch: 1 | Train Loss: 0.0414548 Vali Loss: 0.0136225 Test Loss: 0.0177220
Validation loss decreased (inf --> 0.013622).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0234336
	speed: 0.5833s/iter; left time: 29830.8631s
	iters: 200, epoch: 2 | loss: 0.0188118
	speed: 0.1344s/iter; left time: 6857.9279s
	iters: 300, epoch: 2 | loss: 0.0143802
	speed: 0.1353s/iter; left time: 6893.1007s
	iters: 400, epoch: 2 | loss: 0.0143195
	speed: 0.1390s/iter; left time: 7066.3500s
	iters: 500, epoch: 2 | loss: 0.0167430
	speed: 0.1370s/iter; left time: 6951.9470s
	iters: 600, epoch: 2 | loss: 0.0185735
	speed: 0.1372s/iter; left time: 6947.7519s
Epoch: 2 cost time: 00h:01m:32.80s
Epoch: 2 | Train Loss: 0.0162483 Vali Loss: 0.0110432 Test Loss: 0.0145780
Validation loss decreased (0.013622 --> 0.011043).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0135493
	speed: 0.4978s/iter; left time: 24117.7371s
	iters: 200, epoch: 3 | loss: 0.0160991
	speed: 0.1346s/iter; left time: 6505.6790s
	iters: 300, epoch: 3 | loss: 0.0124542
	speed: 0.1422s/iter; left time: 6862.6207s
	iters: 400, epoch: 3 | loss: 0.0136194
	speed: 0.1538s/iter; left time: 7403.0051s
	iters: 500, epoch: 3 | loss: 0.0154468
	speed: 0.1635s/iter; left time: 7856.1242s
	iters: 600, epoch: 3 | loss: 0.0114750
	speed: 0.1435s/iter; left time: 6878.8406s
Epoch: 3 cost time: 00h:01m:38.26s
Epoch: 3 | Train Loss: 0.0148898 Vali Loss: 0.0102945 Test Loss: 0.0132678
Validation loss decreased (0.011043 --> 0.010294).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0159320
	speed: 0.5488s/iter; left time: 25106.9163s
	iters: 200, epoch: 4 | loss: 0.0105587
	speed: 0.1502s/iter; left time: 6854.8949s
	iters: 300, epoch: 4 | loss: 0.0124891
	speed: 0.1534s/iter; left time: 6985.9092s
	iters: 400, epoch: 4 | loss: 0.0100903
	speed: 0.1475s/iter; left time: 6701.9853s
	iters: 500, epoch: 4 | loss: 0.0174520
	speed: 0.1625s/iter; left time: 7369.0156s
	iters: 600, epoch: 4 | loss: 0.0138244
	speed: 0.1497s/iter; left time: 6775.8155s
Epoch: 4 cost time: 00h:01m:40.94s
Epoch: 4 | Train Loss: 0.0141273 Vali Loss: 0.0100167 Test Loss: 0.0129411
Validation loss decreased (0.010294 --> 0.010017).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0148488
	speed: 0.4785s/iter; left time: 20601.5128s
	iters: 200, epoch: 5 | loss: 0.0141623
	speed: 0.1244s/iter; left time: 5342.6730s
	iters: 300, epoch: 5 | loss: 0.0118647
	speed: 0.1236s/iter; left time: 5296.3334s
	iters: 400, epoch: 5 | loss: 0.0145806
	speed: 0.1236s/iter; left time: 5284.5546s
	iters: 500, epoch: 5 | loss: 0.0123270
	speed: 0.1258s/iter; left time: 5366.4768s
	iters: 600, epoch: 5 | loss: 0.0115003
	speed: 0.1266s/iter; left time: 5387.6036s
Epoch: 5 cost time: 00h:01m:25.14s
Epoch: 5 | Train Loss: 0.0135997 Vali Loss: 0.0102100 Test Loss: 0.0134052
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0105686
	speed: 0.3814s/iter; left time: 15390.1491s
	iters: 200, epoch: 6 | loss: 0.0137064
	speed: 0.1242s/iter; left time: 4999.5486s
	iters: 300, epoch: 6 | loss: 0.0133618
	speed: 0.1238s/iter; left time: 4972.7704s
	iters: 400, epoch: 6 | loss: 0.0140145
	speed: 0.1240s/iter; left time: 4965.0977s
	iters: 500, epoch: 6 | loss: 0.0130147
	speed: 0.1229s/iter; left time: 4912.5422s
	iters: 600, epoch: 6 | loss: 0.0116438
	speed: 0.1231s/iter; left time: 4906.5062s
Epoch: 6 cost time: 00h:01m:23.95s
Epoch: 6 | Train Loss: 0.0134458 Vali Loss: 0.0094027 Test Loss: 0.0120869
Validation loss decreased (0.010017 --> 0.009403).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0137705
	speed: 0.4651s/iter; left time: 17513.8505s
	iters: 200, epoch: 7 | loss: 0.0118949
	speed: 0.1246s/iter; left time: 4677.9814s
	iters: 300, epoch: 7 | loss: 0.0193670
	speed: 0.1248s/iter; left time: 4673.2386s
	iters: 400, epoch: 7 | loss: 0.0122732
	speed: 0.1270s/iter; left time: 4744.8331s
	iters: 500, epoch: 7 | loss: 0.0111094
	speed: 0.1238s/iter; left time: 4611.5533s
	iters: 600, epoch: 7 | loss: 0.0107920
	speed: 0.1242s/iter; left time: 4615.9904s
Epoch: 7 cost time: 00h:01m:24.75s
Epoch: 7 | Train Loss: 0.0130861 Vali Loss: 0.0091752 Test Loss: 0.0117255
Validation loss decreased (0.009403 --> 0.009175).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0113322
	speed: 0.4539s/iter; left time: 15870.3355s
	iters: 200, epoch: 8 | loss: 0.0130333
	speed: 0.1229s/iter; left time: 4284.8534s
	iters: 300, epoch: 8 | loss: 0.0108141
	speed: 0.1244s/iter; left time: 4324.9072s
	iters: 400, epoch: 8 | loss: 0.0103628
	speed: 0.1236s/iter; left time: 4283.4558s
	iters: 500, epoch: 8 | loss: 0.0118539
	speed: 0.1225s/iter; left time: 4232.7593s
	iters: 600, epoch: 8 | loss: 0.0142968
	speed: 0.1232s/iter; left time: 4244.6338s
Epoch: 8 cost time: 00h:01m:23.68s
Epoch: 8 | Train Loss: 0.0127573 Vali Loss: 0.0092726 Test Loss: 0.0122493
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0123131
	speed: 0.3818s/iter; left time: 12319.2802s
	iters: 200, epoch: 9 | loss: 0.0127209
	speed: 0.1222s/iter; left time: 3931.3831s
	iters: 300, epoch: 9 | loss: 0.0072432
	speed: 0.1232s/iter; left time: 3951.0173s
	iters: 400, epoch: 9 | loss: 0.0095887
	speed: 0.1253s/iter; left time: 4004.0000s
	iters: 500, epoch: 9 | loss: 0.0158286
	speed: 0.1229s/iter; left time: 3914.8901s
	iters: 600, epoch: 9 | loss: 0.0081707
	speed: 0.1216s/iter; left time: 3863.5983s
Epoch: 9 cost time: 00h:01m:24.02s
Epoch: 9 | Train Loss: 0.0124855 Vali Loss: 0.0090592 Test Loss: 0.0120632
Validation loss decreased (0.009175 --> 0.009059).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0132127
	speed: 0.4664s/iter; left time: 13790.5333s
	iters: 200, epoch: 10 | loss: 0.0110980
	speed: 0.1346s/iter; left time: 3966.3410s
	iters: 300, epoch: 10 | loss: 0.0123905
	speed: 0.1217s/iter; left time: 3573.3920s
	iters: 400, epoch: 10 | loss: 0.0147953
	speed: 0.1245s/iter; left time: 3644.1321s
	iters: 500, epoch: 10 | loss: 0.0115478
	speed: 0.1238s/iter; left time: 3609.6607s
	iters: 600, epoch: 10 | loss: 0.0128286
	speed: 0.1228s/iter; left time: 3570.9932s
Epoch: 10 cost time: 00h:01m:25.35s
Epoch: 10 | Train Loss: 0.0122391 Vali Loss: 0.0091130 Test Loss: 0.0120960
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0174535
	speed: 0.3766s/iter; left time: 10120.7733s
	iters: 200, epoch: 11 | loss: 0.0126983
	speed: 0.1236s/iter; left time: 3308.6341s
	iters: 300, epoch: 11 | loss: 0.0130372
	speed: 0.1277s/iter; left time: 3406.5130s
	iters: 400, epoch: 11 | loss: 0.0176958
	speed: 0.1239s/iter; left time: 3291.3225s
	iters: 500, epoch: 11 | loss: 0.0106839
	speed: 0.1219s/iter; left time: 3227.3351s
	iters: 600, epoch: 11 | loss: 0.0114549
	speed: 0.1232s/iter; left time: 3248.1451s
Epoch: 11 cost time: 00h:01m:25.34s
Epoch: 11 | Train Loss: 0.0121536 Vali Loss: 0.0088604 Test Loss: 0.0117209
Validation loss decreased (0.009059 --> 0.008860).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0116588
	speed: 0.4860s/iter; left time: 11749.6513s
	iters: 200, epoch: 12 | loss: 0.0145723
	speed: 0.1232s/iter; left time: 2966.3678s
	iters: 300, epoch: 12 | loss: 0.0116074
	speed: 0.1251s/iter; left time: 2999.1238s
	iters: 400, epoch: 12 | loss: 0.0114902
	speed: 0.1279s/iter; left time: 3052.3157s
	iters: 500, epoch: 12 | loss: 0.0144032
	speed: 0.1235s/iter; left time: 2937.0960s
	iters: 600, epoch: 12 | loss: 0.0147044
	speed: 0.1237s/iter; left time: 2928.9708s
Epoch: 12 cost time: 00h:01m:24.76s
Epoch: 12 | Train Loss: 0.0119597 Vali Loss: 0.0086890 Test Loss: 0.0113151
Validation loss decreased (0.008860 --> 0.008689).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0143123
	speed: 0.4718s/iter; left time: 10132.6443s
	iters: 200, epoch: 13 | loss: 0.0108766
	speed: 0.1222s/iter; left time: 2612.6563s
	iters: 300, epoch: 13 | loss: 0.0097793
	speed: 0.1241s/iter; left time: 2639.4163s
	iters: 400, epoch: 13 | loss: 0.0138142
	speed: 0.1240s/iter; left time: 2625.9096s
	iters: 500, epoch: 13 | loss: 0.0085804
	speed: 0.1227s/iter; left time: 2585.1142s
	iters: 600, epoch: 13 | loss: 0.0130738
	speed: 0.1230s/iter; left time: 2579.7988s
Epoch: 13 cost time: 00h:01m:23.75s
Epoch: 13 | Train Loss: 0.0120229 Vali Loss: 0.0088181 Test Loss: 0.0115324
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0109734
	speed: 0.3768s/iter; left time: 7076.8174s
	iters: 200, epoch: 14 | loss: 0.0097976
	speed: 0.1241s/iter; left time: 2318.7731s
	iters: 300, epoch: 14 | loss: 0.0141105
	speed: 0.1230s/iter; left time: 2285.3795s
	iters: 400, epoch: 14 | loss: 0.0139851
	speed: 0.1231s/iter; left time: 2274.5216s
	iters: 500, epoch: 14 | loss: 0.0111326
	speed: 0.1383s/iter; left time: 2542.5687s
	iters: 600, epoch: 14 | loss: 0.0157568
	speed: 0.1342s/iter; left time: 2453.0623s
Epoch: 14 cost time: 00h:01m:26.16s
Epoch: 14 | Train Loss: 0.0117440 Vali Loss: 0.0087840 Test Loss: 0.0116497
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0096404
	speed: 0.3657s/iter; left time: 5881.2191s
	iters: 200, epoch: 15 | loss: 0.0107696
	speed: 0.1215s/iter; left time: 1942.6471s
	iters: 300, epoch: 15 | loss: 0.0091478
	speed: 0.1226s/iter; left time: 1947.2254s
	iters: 400, epoch: 15 | loss: 0.0130447
	speed: 0.1236s/iter; left time: 1951.4498s
	iters: 500, epoch: 15 | loss: 0.0125198
	speed: 0.1230s/iter; left time: 1929.2286s
	iters: 600, epoch: 15 | loss: 0.0120653
	speed: 0.1224s/iter; left time: 1907.0463s
Epoch: 15 cost time: 00h:01m:23.14s
Epoch: 15 | Train Loss: 0.0116359 Vali Loss: 0.0085023 Test Loss: 0.0108946
Validation loss decreased (0.008689 --> 0.008502).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0109141
	speed: 0.4585s/iter; left time: 6137.9785s
	iters: 200, epoch: 16 | loss: 0.0124269
	speed: 0.1223s/iter; left time: 1624.6643s
	iters: 300, epoch: 16 | loss: 0.0105661
	speed: 0.1223s/iter; left time: 1612.9687s
	iters: 400, epoch: 16 | loss: 0.0130263
	speed: 0.1249s/iter; left time: 1633.9415s
	iters: 500, epoch: 16 | loss: 0.0099179
	speed: 0.1221s/iter; left time: 1585.0623s
	iters: 600, epoch: 16 | loss: 0.0106084
	speed: 0.1233s/iter; left time: 1588.5283s
Epoch: 16 cost time: 00h:01m:23.43s
Epoch: 16 | Train Loss: 0.0116072 Vali Loss: 0.0090153 Test Loss: 0.0119307
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0144753
	speed: 0.3820s/iter; left time: 4083.6862s
	iters: 200, epoch: 17 | loss: 0.0077437
	speed: 0.1230s/iter; left time: 1302.7615s
	iters: 300, epoch: 17 | loss: 0.0111782
	speed: 0.1232s/iter; left time: 1291.9337s
	iters: 400, epoch: 17 | loss: 0.0086899
	speed: 0.1242s/iter; left time: 1290.5495s
	iters: 500, epoch: 17 | loss: 0.0117684
	speed: 0.1221s/iter; left time: 1256.5225s
	iters: 600, epoch: 17 | loss: 0.0110266
	speed: 0.1256s/iter; left time: 1280.1298s
Epoch: 17 cost time: 00h:01m:23.91s
Epoch: 17 | Train Loss: 0.0115094 Vali Loss: 0.0088644 Test Loss: 0.0113639
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0082685
	speed: 0.3730s/iter; left time: 2980.6251s
	iters: 200, epoch: 18 | loss: 0.0090307
	speed: 0.1224s/iter; left time: 966.0165s
	iters: 300, epoch: 18 | loss: 0.0130933
	speed: 0.1243s/iter; left time: 968.7339s
	iters: 400, epoch: 18 | loss: 0.0109216
	speed: 0.1236s/iter; left time: 950.4307s
	iters: 500, epoch: 18 | loss: 0.0152812
	speed: 0.1232s/iter; left time: 935.0758s
	iters: 600, epoch: 18 | loss: 0.0119109
	speed: 0.1225s/iter; left time: 917.7594s
Epoch: 18 cost time: 00h:01m:23.39s
Epoch: 18 | Train Loss: 0.0116320 Vali Loss: 0.0088605 Test Loss: 0.0113738
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 19 | loss: 0.0093845
	speed: 0.3765s/iter; left time: 1993.6031s
	iters: 200, epoch: 19 | loss: 0.0103307
	speed: 0.1378s/iter; left time: 716.1294s
	iters: 300, epoch: 19 | loss: 0.0093786
	speed: 0.1349s/iter; left time: 687.4181s
	iters: 400, epoch: 19 | loss: 0.0089233
	speed: 0.1224s/iter; left time: 611.2726s
	iters: 500, epoch: 19 | loss: 0.0120973
	speed: 0.1218s/iter; left time: 596.2324s
	iters: 600, epoch: 19 | loss: 0.0120883
	speed: 0.1226s/iter; left time: 587.7565s
Epoch: 19 cost time: 00h:01m:26.00s
Epoch: 19 | Train Loss: 0.0112558 Vali Loss: 0.0087352 Test Loss: 0.0110916
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 20 | loss: 0.0130322
	speed: 0.3735s/iter; left time: 970.4306s
	iters: 200, epoch: 20 | loss: 0.0099038
	speed: 0.1248s/iter; left time: 311.8140s
	iters: 300, epoch: 20 | loss: 0.0115065
	speed: 0.1231s/iter; left time: 295.2821s
	iters: 400, epoch: 20 | loss: 0.0149006
	speed: 0.1253s/iter; left time: 287.9809s
	iters: 500, epoch: 20 | loss: 0.0128831
	speed: 0.1233s/iter; left time: 271.0752s
	iters: 600, epoch: 20 | loss: 0.0122716
	speed: 0.1240s/iter; left time: 260.0797s
Epoch: 20 cost time: 00h:01m:24.43s
Epoch: 20 | Train Loss: 0.0112232 Vali Loss: 0.0089175 Test Loss: 0.0112712
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506
Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506
Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506
Scaled mse:0.010894566774368286, rmse:0.10437703877687454, mae:0.0666496753692627, rse:0.3061654567718506
Intermediate time for ES and pred_len 24: 00h:36m:26.31s


=== Starting experiments for pred_len: 96 ===

[2024-11-13 13:51:04,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:51:04,321] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:51:04,322] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:51:04,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:51:05,637] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 13:51:05,637] [INFO] [comm.py:637:init_distributed] cdb=None
train 86115
val 18435
test 18435
train 86115
train 86115
val 18435
train 86115
val 18435
test 18435
val 18435
test 18435
test 18435
[2024-11-13 13:51:07,713] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 13:51:08,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 13:51:08,677] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 13:51:08,677] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 13:51:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 13:51:08,678] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 13:51:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 13:51:08,678] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 13:51:08,679] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 13:51:09,018] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 13:51:09,019] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB 
[2024-11-13 13:51:09,019] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.37 GB, percent = 3.7%
[2024-11-13 13:51:09,191] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 13:51:09,191] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 13:51:09,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.75 GB, percent = 3.7%
[2024-11-13 13:51:09,192] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 13:51:09,376] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 13:51:09,377] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 13:51:09,377] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 38.18 GB, percent = 3.8%
[2024-11-13 13:51:09,377] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 13:51:09,377] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 13:51:09,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 13:51:09,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 13:51:09,378] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd05348e750>
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 13:51:09,379] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 13:51:09,380] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 13:51:09,381] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 13:51:09,381] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0596709
	speed: 0.1541s/iter; left time: 8280.2504s
	iters: 200, epoch: 1 | loss: 0.0532119
	speed: 0.1262s/iter; left time: 6768.7826s
	iters: 300, epoch: 1 | loss: 0.0370430
	speed: 0.1273s/iter; left time: 6813.1354s
	iters: 400, epoch: 1 | loss: 0.0336645
	speed: 0.1272s/iter; left time: 6794.8625s
	iters: 500, epoch: 1 | loss: 0.0248052
	speed: 0.1275s/iter; left time: 6799.8293s
	iters: 600, epoch: 1 | loss: 0.0247847
	speed: 0.1287s/iter; left time: 6848.0630s
Epoch: 1 cost time: 00h:01m:26.73s
Epoch: 1 | Train Loss: 0.0483457 Vali Loss: 0.0212816 Test Loss: 0.0269613
Validation loss decreased (inf --> 0.021282).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0258570
	speed: 0.5548s/iter; left time: 28312.8267s
	iters: 200, epoch: 2 | loss: 0.0201012
	speed: 0.1282s/iter; left time: 6529.3688s
	iters: 300, epoch: 2 | loss: 0.0219315
	speed: 0.1265s/iter; left time: 6432.2708s
	iters: 400, epoch: 2 | loss: 0.0248280
	speed: 0.1256s/iter; left time: 6374.2098s
	iters: 500, epoch: 2 | loss: 0.0258012
	speed: 0.1278s/iter; left time: 6468.3951s
	iters: 600, epoch: 2 | loss: 0.0192499
	speed: 0.1472s/iter; left time: 7436.4860s
Epoch: 2 cost time: 00h:01m:27.72s
Epoch: 2 | Train Loss: 0.0219829 Vali Loss: 0.0185177 Test Loss: 0.0236853
Validation loss decreased (0.021282 --> 0.018518).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0183666
	speed: 0.4213s/iter; left time: 20366.3054s
	iters: 200, epoch: 3 | loss: 0.0207339
	speed: 0.1249s/iter; left time: 6026.3829s
	iters: 300, epoch: 3 | loss: 0.0203469
	speed: 0.1244s/iter; left time: 5987.8589s
	iters: 400, epoch: 3 | loss: 0.0147172
	speed: 0.1248s/iter; left time: 5995.2718s
	iters: 500, epoch: 3 | loss: 0.0166696
	speed: 0.1237s/iter; left time: 5927.8786s
	iters: 600, epoch: 3 | loss: 0.0183942
	speed: 0.1234s/iter; left time: 5904.6205s
Epoch: 3 cost time: 00h:01m:24.15s
Epoch: 3 | Train Loss: 0.0206732 Vali Loss: 0.0176023 Test Loss: 0.0222772
Validation loss decreased (0.018518 --> 0.017602).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0219100
	speed: 0.4192s/iter; left time: 19137.3536s
	iters: 200, epoch: 4 | loss: 0.0203517
	speed: 0.1238s/iter; left time: 5636.6998s
	iters: 300, epoch: 4 | loss: 0.0225772
	speed: 0.1226s/iter; left time: 5570.7821s
	iters: 400, epoch: 4 | loss: 0.0159180
	speed: 0.1231s/iter; left time: 5582.2138s
	iters: 500, epoch: 4 | loss: 0.0181224
	speed: 0.1256s/iter; left time: 5683.1568s
	iters: 600, epoch: 4 | loss: 0.0191104
	speed: 0.1297s/iter; left time: 5857.7328s
Epoch: 4 cost time: 00h:01m:24.77s
Epoch: 4 | Train Loss: 0.0200406 Vali Loss: 0.0169479 Test Loss: 0.0212023
Validation loss decreased (0.017602 --> 0.016948).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0224293
	speed: 0.4625s/iter; left time: 19867.5847s
	iters: 200, epoch: 5 | loss: 0.0173440
	speed: 0.1222s/iter; left time: 5238.2678s
	iters: 300, epoch: 5 | loss: 0.0235391
	speed: 0.1227s/iter; left time: 5246.0820s
	iters: 400, epoch: 5 | loss: 0.0183559
	speed: 0.1228s/iter; left time: 5237.8337s
	iters: 500, epoch: 5 | loss: 0.0137428
	speed: 0.1229s/iter; left time: 5230.3099s
	iters: 600, epoch: 5 | loss: 0.0179286
	speed: 0.1219s/iter; left time: 5177.0057s
Epoch: 5 cost time: 00h:01m:22.92s
Epoch: 5 | Train Loss: 0.0192499 Vali Loss: 0.0166892 Test Loss: 0.0214334
Validation loss decreased (0.016948 --> 0.016689).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0143708
	speed: 0.4215s/iter; left time: 16972.8677s
	iters: 200, epoch: 6 | loss: 0.0234077
	speed: 0.1219s/iter; left time: 4896.6734s
	iters: 300, epoch: 6 | loss: 0.0196592
	speed: 0.1218s/iter; left time: 4880.0618s
	iters: 400, epoch: 6 | loss: 0.0198589
	speed: 0.1225s/iter; left time: 4895.4348s
	iters: 500, epoch: 6 | loss: 0.0156133
	speed: 0.1243s/iter; left time: 4956.4937s
	iters: 600, epoch: 6 | loss: 0.0178505
	speed: 0.1229s/iter; left time: 4886.8690s
Epoch: 6 cost time: 00h:01m:22.93s
Epoch: 6 | Train Loss: 0.0190729 Vali Loss: 0.0161652 Test Loss: 0.0208077
Validation loss decreased (0.016689 --> 0.016165).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0223626
	speed: 0.4239s/iter; left time: 15927.1970s
	iters: 200, epoch: 7 | loss: 0.0202940
	speed: 0.1220s/iter; left time: 4570.4599s
	iters: 300, epoch: 7 | loss: 0.0214871
	speed: 0.1224s/iter; left time: 4575.3221s
	iters: 400, epoch: 7 | loss: 0.0202324
	speed: 0.1214s/iter; left time: 4523.7880s
	iters: 500, epoch: 7 | loss: 0.0148672
	speed: 0.1216s/iter; left time: 4521.0482s
	iters: 600, epoch: 7 | loss: 0.0179687
	speed: 0.1231s/iter; left time: 4565.2045s
Epoch: 7 cost time: 00h:01m:22.81s
Epoch: 7 | Train Loss: 0.0186758 Vali Loss: 0.0160311 Test Loss: 0.0205422
Validation loss decreased (0.016165 --> 0.016031).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0157311
	speed: 0.4318s/iter; left time: 15064.4128s
	iters: 200, epoch: 8 | loss: 0.0234816
	speed: 0.1213s/iter; left time: 4219.9651s
	iters: 300, epoch: 8 | loss: 0.0252436
	speed: 0.1216s/iter; left time: 4218.3211s
	iters: 400, epoch: 8 | loss: 0.0179009
	speed: 0.1224s/iter; left time: 4234.7011s
	iters: 500, epoch: 8 | loss: 0.0176396
	speed: 0.1212s/iter; left time: 4179.5099s
	iters: 600, epoch: 8 | loss: 0.0130606
	speed: 0.1211s/iter; left time: 4165.0731s
Epoch: 8 cost time: 00h:01m:22.51s
Epoch: 8 | Train Loss: 0.0181763 Vali Loss: 0.0157263 Test Loss: 0.0200718
Validation loss decreased (0.016031 --> 0.015726).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0179632
	speed: 0.4888s/iter; left time: 15737.2908s
	iters: 200, epoch: 9 | loss: 0.0205819
	speed: 0.1211s/iter; left time: 3885.2754s
	iters: 300, epoch: 9 | loss: 0.0164489
	speed: 0.1227s/iter; left time: 3924.7723s
	iters: 400, epoch: 9 | loss: 0.0201637
	speed: 0.1210s/iter; left time: 3857.4777s
	iters: 500, epoch: 9 | loss: 0.0176181
	speed: 0.1211s/iter; left time: 3850.4494s
	iters: 600, epoch: 9 | loss: 0.0184517
	speed: 0.1220s/iter; left time: 3867.1648s
Epoch: 9 cost time: 00h:01m:22.48s
Epoch: 9 | Train Loss: 0.0178964 Vali Loss: 0.0159344 Test Loss: 0.0200433
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0136520
	speed: 0.3705s/iter; left time: 10930.7898s
	iters: 200, epoch: 10 | loss: 0.0210689
	speed: 0.1205s/iter; left time: 3541.9777s
	iters: 300, epoch: 10 | loss: 0.0225531
	speed: 0.1217s/iter; left time: 3565.3884s
	iters: 400, epoch: 10 | loss: 0.0213916
	speed: 0.1214s/iter; left time: 3545.6285s
	iters: 500, epoch: 10 | loss: 0.0140378
	speed: 0.1213s/iter; left time: 3531.0026s
	iters: 600, epoch: 10 | loss: 0.0144618
	speed: 0.1215s/iter; left time: 3524.6228s
Epoch: 10 cost time: 00h:01m:22.03s
Epoch: 10 | Train Loss: 0.0174420 Vali Loss: 0.0157547 Test Loss: 0.0199326
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0239945
	speed: 0.3676s/iter; left time: 9855.5267s
	iters: 200, epoch: 11 | loss: 0.0163100
	speed: 0.1208s/iter; left time: 3227.4399s
	iters: 300, epoch: 11 | loss: 0.0167650
	speed: 0.1219s/iter; left time: 3243.5680s
	iters: 400, epoch: 11 | loss: 0.0160354
	speed: 0.1214s/iter; left time: 3218.6524s
	iters: 500, epoch: 11 | loss: 0.0200000
	speed: 0.1205s/iter; left time: 3182.0253s
	iters: 600, epoch: 11 | loss: 0.0175237
	speed: 0.1203s/iter; left time: 3163.9138s
Epoch: 11 cost time: 00h:01m:21.82s
Epoch: 11 | Train Loss: 0.0173280 Vali Loss: 0.0163085 Test Loss: 0.0206065
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0190451
	speed: 0.3650s/iter; left time: 8802.7618s
	iters: 200, epoch: 12 | loss: 0.0176930
	speed: 0.1219s/iter; left time: 2928.7405s
	iters: 300, epoch: 12 | loss: 0.0149796
	speed: 0.1228s/iter; left time: 2937.8192s
	iters: 400, epoch: 12 | loss: 0.0171055
	speed: 0.1229s/iter; left time: 2927.3107s
	iters: 500, epoch: 12 | loss: 0.0161820
	speed: 0.1205s/iter; left time: 2857.3275s
	iters: 600, epoch: 12 | loss: 0.0151442
	speed: 0.1205s/iter; left time: 2847.2264s
Epoch: 12 cost time: 00h:01m:22.16s
Epoch: 12 | Train Loss: 0.0169954 Vali Loss: 0.0162357 Test Loss: 0.0206839
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0163781
	speed: 0.3688s/iter; left time: 7903.8693s
	iters: 200, epoch: 13 | loss: 0.0216128
	speed: 0.1207s/iter; left time: 2573.4739s
	iters: 300, epoch: 13 | loss: 0.0111176
	speed: 0.1201s/iter; left time: 2550.2163s
	iters: 400, epoch: 13 | loss: 0.0153951
	speed: 0.1202s/iter; left time: 2539.4739s
	iters: 500, epoch: 13 | loss: 0.0192523
	speed: 0.1212s/iter; left time: 2547.8362s
	iters: 600, epoch: 13 | loss: 0.0164807
	speed: 0.1207s/iter; left time: 2526.6495s
Epoch: 13 cost time: 00h:01m:21.59s
Epoch: 13 | Train Loss: 0.0166521 Vali Loss: 0.0167950 Test Loss: 0.0205306
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086
Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086
Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086
Scaled mse:0.02007182687520981, rmse:0.14167506992816925, mae:0.09592236578464508, rse:0.4161672592163086
Intermediate time for ES and pred_len 96: 00h:23m:00.60s


=== Starting experiments for pred_len: 168 ===

[2024-11-13 14:14:04,733] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 14:14:04,736] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 14:14:04,736] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 14:14:04,745] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 14:14:05,810] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 14:14:05,810] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 14:14:05,811] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 14:14:05,811] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 14:14:05,811] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 85899
val 18219
test 18219
train 85899
train 85899
train 85899
val 18219
val 18219
val 18219
test 18219
test 18219
test 18219
[2024-11-13 14:14:08,233] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 14:14:09,266] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 14:14:09,267] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 14:14:09,267] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 14:14:09,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 14:14:09,268] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 14:14:09,268] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 14:14:09,268] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 14:14:09,584] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 14:14:09,584] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 14:14:09,585] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.54 GB, percent = 3.3%
[2024-11-13 14:14:09,749] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 14:14:09,750] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 14:14:09,750] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.77 GB, percent = 3.4%
[2024-11-13 14:14:09,750] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 14:14:09,907] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 14:14:09,907] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 14:14:09,908] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 34.14 GB, percent = 3.4%
[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 14:14:09,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 14:14:09,909] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe038c7e810>
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 14:14:09,909] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 14:14:09,910] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 14:14:09,910] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0565824
	speed: 0.1600s/iter; left time: 8572.6883s
	iters: 200, epoch: 1 | loss: 0.0569034
	speed: 0.1269s/iter; left time: 6786.0773s
	iters: 300, epoch: 1 | loss: 0.0518587
	speed: 0.1271s/iter; left time: 6783.3071s
	iters: 400, epoch: 1 | loss: 0.0307872
	speed: 0.1271s/iter; left time: 6769.3660s
	iters: 500, epoch: 1 | loss: 0.0314419
	speed: 0.1273s/iter; left time: 6768.2172s
	iters: 600, epoch: 1 | loss: 0.0260363
	speed: 0.1280s/iter; left time: 6793.3712s
Epoch: 1 cost time: 00h:01m:27.22s
Epoch: 1 | Train Loss: 0.0476036 Vali Loss: 0.0233658 Test Loss: 0.0288830
Validation loss decreased (inf --> 0.023366).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0213778
	speed: 0.5946s/iter; left time: 30265.1573s
	iters: 200, epoch: 2 | loss: 0.0244658
	speed: 0.1267s/iter; left time: 6433.4810s
	iters: 300, epoch: 2 | loss: 0.0254603
	speed: 0.1270s/iter; left time: 6440.6542s
	iters: 400, epoch: 2 | loss: 0.0223233
	speed: 0.1289s/iter; left time: 6521.2317s
	iters: 500, epoch: 2 | loss: 0.0193527
	speed: 0.1262s/iter; left time: 6374.5446s
	iters: 600, epoch: 2 | loss: 0.0209664
	speed: 0.1261s/iter; left time: 6357.2339s
Epoch: 2 cost time: 00h:01m:25.67s
Epoch: 2 | Train Loss: 0.0233597 Vali Loss: 0.0200944 Test Loss: 0.0255333
Validation loss decreased (0.023366 --> 0.020094).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0208655
	speed: 0.4640s/iter; left time: 22371.6611s
	iters: 200, epoch: 3 | loss: 0.0178275
	speed: 0.1246s/iter; left time: 5993.6641s
	iters: 300, epoch: 3 | loss: 0.0186255
	speed: 0.1271s/iter; left time: 6101.4731s
	iters: 400, epoch: 3 | loss: 0.0170459
	speed: 0.1262s/iter; left time: 6044.6347s
	iters: 500, epoch: 3 | loss: 0.0153410
	speed: 0.1263s/iter; left time: 6037.7861s
	iters: 600, epoch: 3 | loss: 0.0206425
	speed: 0.1267s/iter; left time: 6043.2833s
Epoch: 3 cost time: 00h:01m:25.22s
Epoch: 3 | Train Loss: 0.0217546 Vali Loss: 0.0193492 Test Loss: 0.0245727
Validation loss decreased (0.020094 --> 0.019349).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0185548
	speed: 0.4429s/iter; left time: 20164.6581s
	iters: 200, epoch: 4 | loss: 0.0180719
	speed: 0.1259s/iter; left time: 5720.7953s
	iters: 300, epoch: 4 | loss: 0.0205472
	speed: 0.1270s/iter; left time: 5757.3097s
	iters: 400, epoch: 4 | loss: 0.0181765
	speed: 0.1276s/iter; left time: 5772.6649s
	iters: 500, epoch: 4 | loss: 0.0236331
	speed: 0.1252s/iter; left time: 5649.7645s
	iters: 600, epoch: 4 | loss: 0.0187238
	speed: 0.1287s/iter; left time: 5794.1486s
Epoch: 4 cost time: 00h:01m:26.87s
Epoch: 4 | Train Loss: 0.0211093 Vali Loss: 0.0182891 Test Loss: 0.0227510
Validation loss decreased (0.019349 --> 0.018289).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0187627
	speed: 0.4851s/iter; left time: 20783.1460s
	iters: 200, epoch: 5 | loss: 0.0220212
	speed: 0.1276s/iter; left time: 5454.4736s
	iters: 300, epoch: 5 | loss: 0.0210257
	speed: 0.1266s/iter; left time: 5399.9272s
	iters: 400, epoch: 5 | loss: 0.0186015
	speed: 0.1258s/iter; left time: 5352.2610s
	iters: 500, epoch: 5 | loss: 0.0257642
	speed: 0.1249s/iter; left time: 5301.4490s
	iters: 600, epoch: 5 | loss: 0.0190592
	speed: 0.1299s/iter; left time: 5501.5710s
Epoch: 5 cost time: 00h:01m:27.13s
Epoch: 5 | Train Loss: 0.0204508 Vali Loss: 0.0182497 Test Loss: 0.0227887
Validation loss decreased (0.018289 --> 0.018250).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0195714
	speed: 0.4645s/iter; left time: 18656.2162s
	iters: 200, epoch: 6 | loss: 0.0196226
	speed: 0.1262s/iter; left time: 5056.3637s
	iters: 300, epoch: 6 | loss: 0.0216896
	speed: 0.1310s/iter; left time: 5234.3878s
	iters: 400, epoch: 6 | loss: 0.0158579
	speed: 0.1255s/iter; left time: 5001.6455s
	iters: 500, epoch: 6 | loss: 0.0186769
	speed: 0.1407s/iter; left time: 5595.3024s
	iters: 600, epoch: 6 | loss: 0.0187091
	speed: 0.1530s/iter; left time: 6067.1431s
Epoch: 6 cost time: 00h:01m:29.94s
Epoch: 6 | Train Loss: 0.0200203 Vali Loss: 0.0177962 Test Loss: 0.0221645
Validation loss decreased (0.018250 --> 0.017796).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0158030
	speed: 0.4879s/iter; left time: 18285.1390s
	iters: 200, epoch: 7 | loss: 0.0222547
	speed: 0.1297s/iter; left time: 4846.0219s
	iters: 300, epoch: 7 | loss: 0.0215090
	speed: 0.1568s/iter; left time: 5845.7176s
	iters: 400, epoch: 7 | loss: 0.0224705
	speed: 0.1688s/iter; left time: 6276.3726s
	iters: 500, epoch: 7 | loss: 0.0232703
	speed: 0.1624s/iter; left time: 6021.7367s
	iters: 600, epoch: 7 | loss: 0.0177696
	speed: 0.1647s/iter; left time: 6088.3996s
Epoch: 7 cost time: 00h:01m:44.05s
Epoch: 7 | Train Loss: 0.0197670 Vali Loss: 0.0177549 Test Loss: 0.0221883
Validation loss decreased (0.017796 --> 0.017755).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0203262
	speed: 0.6675s/iter; left time: 23222.8095s
	iters: 200, epoch: 8 | loss: 0.0188753
	speed: 0.1815s/iter; left time: 6295.9654s
	iters: 300, epoch: 8 | loss: 0.0204109
	speed: 0.1672s/iter; left time: 5785.3996s
	iters: 400, epoch: 8 | loss: 0.0213176
	speed: 0.1696s/iter; left time: 5850.0454s
	iters: 500, epoch: 8 | loss: 0.0165740
	speed: 0.1688s/iter; left time: 5804.8748s
	iters: 600, epoch: 8 | loss: 0.0212293
	speed: 0.1639s/iter; left time: 5620.0228s
Epoch: 8 cost time: 00h:01m:56.30s
Epoch: 8 | Train Loss: 0.0193864 Vali Loss: 0.0175740 Test Loss: 0.0218846
Validation loss decreased (0.017755 --> 0.017574).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0163664
	speed: 0.5237s/iter; left time: 16813.9164s
	iters: 200, epoch: 9 | loss: 0.0184284
	speed: 0.1447s/iter; left time: 4630.3970s
	iters: 300, epoch: 9 | loss: 0.0206715
	speed: 0.1305s/iter; left time: 4163.1917s
	iters: 400, epoch: 9 | loss: 0.0168874
	speed: 0.1275s/iter; left time: 4054.7804s
	iters: 500, epoch: 9 | loss: 0.0197874
	speed: 0.1272s/iter; left time: 4032.6580s
	iters: 600, epoch: 9 | loss: 0.0207158
	speed: 0.1253s/iter; left time: 3959.2846s
Epoch: 9 cost time: 00h:01m:27.87s
Epoch: 9 | Train Loss: 0.0189410 Vali Loss: 0.0176340 Test Loss: 0.0218443
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0205027
	speed: 0.3829s/iter; left time: 11266.9392s
	iters: 200, epoch: 10 | loss: 0.0160529
	speed: 0.1255s/iter; left time: 3681.1416s
	iters: 300, epoch: 10 | loss: 0.0160090
	speed: 0.1230s/iter; left time: 3594.8396s
	iters: 400, epoch: 10 | loss: 0.0175601
	speed: 0.1260s/iter; left time: 3668.7326s
	iters: 500, epoch: 10 | loss: 0.0185336
	speed: 0.1253s/iter; left time: 3635.8107s
	iters: 600, epoch: 10 | loss: 0.0158149
	speed: 0.1265s/iter; left time: 3659.0470s
Epoch: 10 cost time: 00h:01m:24.55s
Epoch: 10 | Train Loss: 0.0186634 Vali Loss: 0.0173984 Test Loss: 0.0219072
Validation loss decreased (0.017574 --> 0.017398).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0155978
	speed: 0.5511s/iter; left time: 14736.5266s
	iters: 200, epoch: 11 | loss: 0.0173789
	speed: 0.1247s/iter; left time: 3321.7816s
	iters: 300, epoch: 11 | loss: 0.0213597
	speed: 0.1263s/iter; left time: 3351.7796s
	iters: 400, epoch: 11 | loss: 0.0158266
	speed: 0.1277s/iter; left time: 3376.2096s
	iters: 500, epoch: 11 | loss: 0.0126218
	speed: 0.1295s/iter; left time: 3410.4362s
	iters: 600, epoch: 11 | loss: 0.0165448
	speed: 0.1395s/iter; left time: 3659.7786s
Epoch: 11 cost time: 00h:01m:29.19s
Epoch: 11 | Train Loss: 0.0183404 Vali Loss: 0.0178120 Test Loss: 0.0219151
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0241187
	speed: 0.4348s/iter; left time: 10459.6825s
	iters: 200, epoch: 12 | loss: 0.0165954
	speed: 0.1371s/iter; left time: 3284.9798s
	iters: 300, epoch: 12 | loss: 0.0191694
	speed: 0.1234s/iter; left time: 2944.6971s
	iters: 400, epoch: 12 | loss: 0.0125043
	speed: 0.1230s/iter; left time: 2923.2223s
	iters: 500, epoch: 12 | loss: 0.0127750
	speed: 0.1223s/iter; left time: 2892.7088s
	iters: 600, epoch: 12 | loss: 0.0202185
	speed: 0.1221s/iter; left time: 2875.4930s
Epoch: 12 cost time: 00h:01m:26.24s
Epoch: 12 | Train Loss: 0.0179019 Vali Loss: 0.0179225 Test Loss: 0.0220747
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0203301
	speed: 0.3734s/iter; left time: 7981.7241s
	iters: 200, epoch: 13 | loss: 0.0183116
	speed: 0.1223s/iter; left time: 2602.3052s
	iters: 300, epoch: 13 | loss: 0.0150865
	speed: 0.1215s/iter; left time: 2571.5067s
	iters: 400, epoch: 13 | loss: 0.0208507
	speed: 0.1221s/iter; left time: 2573.5021s
	iters: 500, epoch: 13 | loss: 0.0178677
	speed: 0.1220s/iter; left time: 2558.1077s
	iters: 600, epoch: 13 | loss: 0.0164449
	speed: 0.1219s/iter; left time: 2544.2793s
Epoch: 13 cost time: 00h:01m:22.57s
Epoch: 13 | Train Loss: 0.0176932 Vali Loss: 0.0184704 Test Loss: 0.0224477
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0207964
	speed: 0.3702s/iter; left time: 6919.2874s
	iters: 200, epoch: 14 | loss: 0.0177327
	speed: 0.1217s/iter; left time: 2261.6883s
	iters: 300, epoch: 14 | loss: 0.0148072
	speed: 0.1212s/iter; left time: 2241.0804s
	iters: 400, epoch: 14 | loss: 0.0167184
	speed: 0.1302s/iter; left time: 2394.1710s
	iters: 500, epoch: 14 | loss: 0.0167975
	speed: 0.1513s/iter; left time: 2767.8354s
	iters: 600, epoch: 14 | loss: 0.0154496
	speed: 0.1469s/iter; left time: 2672.7491s
Epoch: 14 cost time: 00h:01m:28.90s
Epoch: 14 | Train Loss: 0.0171912 Vali Loss: 0.0198765 Test Loss: 0.0227924
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0150788
	speed: 0.3746s/iter; left time: 5996.2363s
	iters: 200, epoch: 15 | loss: 0.0178858
	speed: 0.1244s/iter; left time: 1978.2785s
	iters: 300, epoch: 15 | loss: 0.0212907
	speed: 0.1231s/iter; left time: 1945.0361s
	iters: 400, epoch: 15 | loss: 0.0173432
	speed: 0.1245s/iter; left time: 1955.4916s
	iters: 500, epoch: 15 | loss: 0.0160355
	speed: 0.1244s/iter; left time: 1940.7078s
	iters: 600, epoch: 15 | loss: 0.0175801
	speed: 0.1255s/iter; left time: 1945.6707s
Epoch: 15 cost time: 00h:01m:23.88s
Epoch: 15 | Train Loss: 0.0169270 Vali Loss: 0.0197246 Test Loss: 0.0226869
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958
Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958
Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958
Scaled mse:0.021907182410359383, rmse:0.1480107456445694, mae:0.10050459951162338, rse:0.4344274699687958
Intermediate time for ES and pred_len 168: 00h:28m:46.51s

Intermediate time for ES: 01h:28m:13.42s



=== Starting experiments for country: FR ===


=== Starting experiments for pred_len: 24 ===

[2024-11-14 13:57:59,560] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,571] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
train 86835
val 18651
test 18651
train 86835
train 86835
train 86835
val 18651
val 18651
test 18651
val 18651
test 18651
test 18651
[2024-11-14 13:58:08,639] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-14 13:58:10,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-14 13:58:10,414] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-14 13:58:10,798] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-14 13:58:10,799] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-14 13:58:10,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.35 GB, percent = 2.6%
[2024-11-14 13:58:11,154] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-14 13:58:11,154] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-14 13:58:11,155] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%
[2024-11-14 13:58:11,155] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-14 13:58:11,284] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-14 13:58:11,285] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-14 13:58:11,285] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-14 13:58:11,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-14 13:58:11,286] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32cbb78450>
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-14 13:58:11,288] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0239559
	speed: 0.2802s/iter; left time: 22775.5551s
	iters: 200, epoch: 1 | loss: 0.0201448
	speed: 0.1183s/iter; left time: 9601.5377s
	iters: 300, epoch: 1 | loss: 0.0162691
	speed: 0.1169s/iter; left time: 9480.7446s
	iters: 400, epoch: 1 | loss: 0.0126257
	speed: 0.1165s/iter; left time: 9435.1100s
	iters: 500, epoch: 1 | loss: 0.0144120
	speed: 0.1172s/iter; left time: 9483.2932s
	iters: 600, epoch: 1 | loss: 0.0185334
	speed: 0.1176s/iter; left time: 9497.9162s
Epoch: 1 cost time: 00h:01m:33.36s
Epoch: 1 | Train Loss: 0.0225287 Vali Loss: 0.0118601 Test Loss: 0.0136884
Validation loss decreased (inf --> 0.011860).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0131000
	speed: 0.6516s/iter; left time: 51201.0050s
	iters: 200, epoch: 2 | loss: 0.0220157
	speed: 0.1155s/iter; left time: 9064.4683s
	iters: 300, epoch: 2 | loss: 0.0084849
	speed: 0.1168s/iter; left time: 9154.1646s
	iters: 400, epoch: 2 | loss: 0.0084919
	speed: 0.1152s/iter; left time: 9015.7136s
	iters: 500, epoch: 2 | loss: 0.0147144
	speed: 0.1156s/iter; left time: 9038.7933s
	iters: 600, epoch: 2 | loss: 0.0093752
	speed: 0.1190s/iter; left time: 9288.0978s
Epoch: 2 cost time: 00h:01m:19.40s
Epoch: 2 | Train Loss: 0.0104779 Vali Loss: 0.0106168 Test Loss: 0.0121037
Validation loss decreased (0.011860 --> 0.010617).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0076656
	speed: 0.4371s/iter; left time: 33163.9484s
	iters: 200, epoch: 3 | loss: 0.0123325
	speed: 0.1146s/iter; left time: 8682.3240s
	iters: 300, epoch: 3 | loss: 0.0073657
	speed: 0.1148s/iter; left time: 8682.7302s
	iters: 400, epoch: 3 | loss: 0.0088692
	speed: 0.1151s/iter; left time: 8696.7682s
	iters: 500, epoch: 3 | loss: 0.0086650
	speed: 0.1156s/iter; left time: 8727.1908s
	iters: 600, epoch: 3 | loss: 0.0082646
	speed: 0.1178s/iter; left time: 8874.5341s
Epoch: 3 cost time: 00h:01m:20.57s
Epoch: 3 | Train Loss: 0.0094833 Vali Loss: 0.0101822 Test Loss: 0.0114591
Validation loss decreased (0.010617 --> 0.010182).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0109312
	speed: 0.5266s/iter; left time: 38524.0249s
	iters: 200, epoch: 4 | loss: 0.0118849
	speed: 0.1167s/iter; left time: 8526.1306s
	iters: 300, epoch: 4 | loss: 0.0099334
	speed: 0.1147s/iter; left time: 8367.2881s
	iters: 400, epoch: 4 | loss: 0.0101027
	speed: 0.1146s/iter; left time: 8352.2250s
	iters: 500, epoch: 4 | loss: 0.0118740
	speed: 0.1145s/iter; left time: 8326.7028s
	iters: 600, epoch: 4 | loss: 0.0064906
	speed: 0.1145s/iter; left time: 8318.2984s
Epoch: 4 cost time: 00h:01m:18.59s
Epoch: 4 | Train Loss: 0.0094642 Vali Loss: 0.0098645 Test Loss: 0.0111941
Validation loss decreased (0.010182 --> 0.009865).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0145120
	speed: 0.4577s/iter; left time: 32241.6792s
	iters: 200, epoch: 5 | loss: 0.0107904
	speed: 0.1139s/iter; left time: 8014.7542s
	iters: 300, epoch: 5 | loss: 0.0115122
	speed: 0.1143s/iter; left time: 8031.6079s
	iters: 400, epoch: 5 | loss: 0.0064738
	speed: 0.1144s/iter; left time: 8023.9122s
	iters: 500, epoch: 5 | loss: 0.0065169
	speed: 0.1136s/iter; left time: 7959.0587s
	iters: 600, epoch: 5 | loss: 0.0121063
	speed: 0.1137s/iter; left time: 7949.9830s
Epoch: 5 cost time: 00h:01m:18.09s
Epoch: 5 | Train Loss: 0.0089185 Vali Loss: 0.0098854 Test Loss: 0.0113841
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0107378
	speed: 0.3610s/iter; left time: 24448.0126s
	iters: 200, epoch: 6 | loss: 0.0071157
	speed: 0.1182s/iter; left time: 7996.0440s
	iters: 300, epoch: 6 | loss: 0.0085659
	speed: 0.1181s/iter; left time: 7973.6379s
	iters: 400, epoch: 6 | loss: 0.0074021
	speed: 0.1223s/iter; left time: 8243.7480s
	iters: 500, epoch: 6 | loss: 0.0067580
	speed: 0.1229s/iter; left time: 8272.0650s
	iters: 600, epoch: 6 | loss: 0.0085170
	speed: 0.2517s/iter; left time: 16919.5556s
Epoch: 6 cost time: 00h:01m:45.53s
Epoch: 6 | Train Loss: 0.0088672 Vali Loss: 0.0098212 Test Loss: 0.0112326
Validation loss decreased (0.009865 --> 0.009821).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0133668
	speed: 1.0716s/iter; left time: 69670.8480s
	iters: 200, epoch: 7 | loss: 0.0055773
	speed: 0.4422s/iter; left time: 28703.5360s
	iters: 300, epoch: 7 | loss: 0.0110484
	speed: 0.4404s/iter; left time: 28541.6124s
	iters: 400, epoch: 7 | loss: 0.0098669
	speed: 0.4496s/iter; left time: 29095.6542s
	iters: 500, epoch: 7 | loss: 0.0105809
	speed: 0.4466s/iter; left time: 28856.8058s
	iters: 600, epoch: 7 | loss: 0.0083511
	speed: 0.4468s/iter; left time: 28825.8824s
Epoch: 7 cost time: 00h:04m:51.21s
Epoch: 7 | Train Loss: 0.0086675 Vali Loss: 0.0096691 Test Loss: 0.0111444
Validation loss decreased (0.009821 --> 0.009669).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0081424
	speed: 1.4640s/iter; left time: 91206.3695s
	iters: 200, epoch: 8 | loss: 0.0119270
	speed: 0.4520s/iter; left time: 28111.3485s
	iters: 300, epoch: 8 | loss: 0.0090215
	speed: 0.4443s/iter; left time: 27593.2379s
	iters: 400, epoch: 8 | loss: 0.0059014
	speed: 0.1566s/iter; left time: 9707.4069s
	iters: 500, epoch: 8 | loss: 0.0100736
	speed: 0.2694s/iter; left time: 16678.4595s
	iters: 600, epoch: 8 | loss: 0.0117893
	speed: 0.2278s/iter; left time: 14078.8536s
Epoch: 8 cost time: 00h:03m:29.50s
Epoch: 8 | Train Loss: 0.0086480 Vali Loss: 0.0095132 Test Loss: 0.0111062
Validation loss decreased (0.009669 --> 0.009513).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0090044
	speed: 0.4103s/iter; left time: 24446.3345s
	iters: 200, epoch: 9 | loss: 0.0094946
	speed: 0.1114s/iter; left time: 6625.0114s
	iters: 300, epoch: 9 | loss: 0.0079705
	speed: 0.1121s/iter; left time: 6658.8228s
	iters: 400, epoch: 9 | loss: 0.0099703
	speed: 0.1128s/iter; left time: 6689.6377s
	iters: 500, epoch: 9 | loss: 0.0080995
	speed: 0.1125s/iter; left time: 6657.6125s
	iters: 600, epoch: 9 | loss: 0.0063729
	speed: 0.1120s/iter; left time: 6617.9407s
Epoch: 9 cost time: 00h:01m:16.71s
Epoch: 9 | Train Loss: 0.0085195 Vali Loss: 0.0092639 Test Loss: 0.0106769
Validation loss decreased (0.009513 --> 0.009264).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0053214
	speed: 0.4978s/iter; left time: 28311.7073s
	iters: 200, epoch: 10 | loss: 0.0139130
	speed: 0.1256s/iter; left time: 7132.7705s
	iters: 300, epoch: 10 | loss: 0.0081261
	speed: 0.2800s/iter; left time: 15868.6923s
	iters: 400, epoch: 10 | loss: 0.0048974
	speed: 0.4382s/iter; left time: 24790.6404s
	iters: 500, epoch: 10 | loss: 0.0076337
	speed: 0.4319s/iter; left time: 24392.6828s
	iters: 600, epoch: 10 | loss: 0.0100944
	speed: 0.4225s/iter; left time: 23815.2874s
Epoch: 10 cost time: 00h:03m:34.94s
Epoch: 10 | Train Loss: 0.0086789 Vali Loss: 0.0094028 Test Loss: 0.0108369
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0053404
	speed: 1.3813s/iter; left time: 74810.0582s
	iters: 200, epoch: 11 | loss: 0.0082792
	speed: 0.4443s/iter; left time: 24020.2399s
	iters: 300, epoch: 11 | loss: 0.0070582
	speed: 0.4496s/iter; left time: 24259.9557s
	iters: 400, epoch: 11 | loss: 0.0067524
	speed: 0.4511s/iter; left time: 24294.5859s
	iters: 500, epoch: 11 | loss: 0.0078302
	speed: 0.4544s/iter; left time: 24430.0277s
	iters: 600, epoch: 11 | loss: 0.0079525
	speed: 0.3612s/iter; left time: 19382.8251s
Epoch: 11 cost time: 00h:04m:42.41s
Epoch: 11 | Train Loss: 0.0083401 Vali Loss: 0.0091377 Test Loss: 0.0106622
Validation loss decreased (0.009264 --> 0.009138).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0096092
	speed: 0.8587s/iter; left time: 44180.8874s
	iters: 200, epoch: 12 | loss: 0.0080460
	speed: 0.1527s/iter; left time: 7841.6917s
	iters: 300, epoch: 12 | loss: 0.0127255
	speed: 0.1116s/iter; left time: 5721.7644s
	iters: 400, epoch: 12 | loss: 0.0106730
	speed: 0.1136s/iter; left time: 5808.5216s
	iters: 500, epoch: 12 | loss: 0.0083658
	speed: 0.1125s/iter; left time: 5740.9163s
	iters: 600, epoch: 12 | loss: 0.0132466
	speed: 0.1126s/iter; left time: 5735.2817s
Epoch: 12 cost time: 00h:01m:34.02s
Epoch: 12 | Train Loss: 0.0082366 Vali Loss: 0.0090408 Test Loss: 0.0104260
Validation loss decreased (0.009138 --> 0.009041).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0112591
	speed: 0.6866s/iter; left time: 33461.1953s
	iters: 200, epoch: 13 | loss: 0.0129779
	speed: 0.2682s/iter; left time: 13043.7910s
	iters: 300, epoch: 13 | loss: 0.0067636
	speed: 0.2726s/iter; left time: 13232.8373s
	iters: 400, epoch: 13 | loss: 0.0061778
	speed: 0.2815s/iter; left time: 13636.8595s
	iters: 500, epoch: 13 | loss: 0.0104312
	speed: 0.2945s/iter; left time: 14233.7605s
	iters: 600, epoch: 13 | loss: 0.0091502
	speed: 0.3002s/iter; left time: 14482.2562s
Epoch: 13 cost time: 00h:03m:13.26s
Epoch: 13 | Train Loss: 0.0080974 Vali Loss: 0.0090926 Test Loss: 0.0107355
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0067687
	speed: 0.9535s/iter; left time: 43883.0498s
	iters: 200, epoch: 14 | loss: 0.0091137
	speed: 0.3083s/iter; left time: 14156.9275s
	iters: 300, epoch: 14 | loss: 0.0098937
	speed: 0.3212s/iter; left time: 14718.1476s
	iters: 400, epoch: 14 | loss: 0.0086841
	speed: 0.3156s/iter; left time: 14431.2787s
	iters: 500, epoch: 14 | loss: 0.0082142
	speed: 0.3186s/iter; left time: 14534.1315s
	iters: 600, epoch: 14 | loss: 0.0087004
	speed: 0.2979s/iter; left time: 13560.4861s
Epoch: 14 cost time: 00h:03m:21.18s
Epoch: 14 | Train Loss: 0.0081368 Vali Loss: 0.0089570 Test Loss: 0.0104242
Validation loss decreased (0.009041 --> 0.008957).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0167940
	speed: 0.4613s/iter; left time: 19978.9698s
	iters: 200, epoch: 15 | loss: 0.0050574
	speed: 0.1128s/iter; left time: 4875.5426s
	iters: 300, epoch: 15 | loss: 0.0104902
	speed: 0.1127s/iter; left time: 4856.3856s
	iters: 400, epoch: 15 | loss: 0.0089468
	speed: 0.1135s/iter; left time: 4881.1831s
	iters: 500, epoch: 15 | loss: 0.0126251
	speed: 0.1118s/iter; left time: 4797.1002s
	iters: 600, epoch: 15 | loss: 0.0059486
	speed: 0.1117s/iter; left time: 4781.2579s
Epoch: 15 cost time: 00h:01m:17.22s
Epoch: 15 | Train Loss: 0.0081152 Vali Loss: 0.0090319 Test Loss: 0.0105617
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0057051
	speed: 0.3483s/iter; left time: 14137.5610s
	iters: 200, epoch: 16 | loss: 0.0066385
	speed: 0.1124s/iter; left time: 4551.3529s
	iters: 300, epoch: 16 | loss: 0.0073851
	speed: 0.1130s/iter; left time: 4564.1924s
	iters: 400, epoch: 16 | loss: 0.0081082
	speed: 0.1127s/iter; left time: 4541.3136s
	iters: 500, epoch: 16 | loss: 0.0123262
	speed: 0.1123s/iter; left time: 4514.9041s
	iters: 600, epoch: 16 | loss: 0.0072221
	speed: 0.1126s/iter; left time: 4515.8427s
Epoch: 16 cost time: 00h:01m:16.88s
Epoch: 16 | Train Loss: 0.0078252 Vali Loss: 0.0091145 Test Loss: 0.0108264
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0097102
	speed: 0.4192s/iter; left time: 15879.5557s
	iters: 200, epoch: 17 | loss: 0.0064988
	speed: 0.2752s/iter; left time: 10399.4114s
	iters: 300, epoch: 17 | loss: 0.0057568
	speed: 0.2698s/iter; left time: 10168.3240s
	iters: 400, epoch: 17 | loss: 0.0065034
	speed: 0.2709s/iter; left time: 10182.9524s
	iters: 500, epoch: 17 | loss: 0.0095674
	speed: 0.2763s/iter; left time: 10355.6380s
	iters: 600, epoch: 17 | loss: 0.0062516
	speed: 0.2860s/iter; left time: 10689.9752s
Epoch: 17 cost time: 00h:02m:59.26s
Epoch: 17 | Train Loss: 0.0078117 Vali Loss: 0.0089328 Test Loss: 0.0105026
Validation loss decreased (0.008957 --> 0.008933).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0078043
	speed: 1.0131s/iter; left time: 35629.1468s
	iters: 200, epoch: 18 | loss: 0.0063590
	speed: 0.3082s/iter; left time: 10810.2302s
	iters: 300, epoch: 18 | loss: 0.0094975
	speed: 0.3022s/iter; left time: 10569.3144s
	iters: 400, epoch: 18 | loss: 0.0071088
	speed: 0.3130s/iter; left time: 10913.1535s
	iters: 500, epoch: 18 | loss: 0.0055368
	speed: 0.3153s/iter; left time: 10961.2643s
	iters: 600, epoch: 18 | loss: 0.0132828
	speed: 0.3191s/iter; left time: 11063.1123s
Epoch: 18 cost time: 00h:03m:32.03s
Epoch: 18 | Train Loss: 0.0078160 Vali Loss: 0.0089081 Test Loss: 0.0103755
Validation loss decreased (0.008933 --> 0.008908).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 19 | loss: 0.0115771
	speed: 1.0496s/iter; left time: 34066.2837s
	iters: 200, epoch: 19 | loss: 0.0104875
	speed: 0.3142s/iter; left time: 10165.5884s
	iters: 300, epoch: 19 | loss: 0.0096097
	speed: 0.3253s/iter; left time: 10492.0188s
	iters: 400, epoch: 19 | loss: 0.0031140
	speed: 0.3250s/iter; left time: 10452.3471s
	iters: 500, epoch: 19 | loss: 0.0102776
	speed: 0.3221s/iter; left time: 10325.3806s
	iters: 600, epoch: 19 | loss: 0.0063839
	speed: 0.3233s/iter; left time: 10331.0277s
Epoch: 19 cost time: 00h:03m:37.71s
Epoch: 19 | Train Loss: 0.0077374 Vali Loss: 0.0088936 Test Loss: 0.0104487
Validation loss decreased (0.008908 --> 0.008894).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 20 | loss: 0.0030808
	speed: 1.0573s/iter; left time: 31449.6509s
	iters: 200, epoch: 20 | loss: 0.0082016
	speed: 0.3248s/iter; left time: 9628.7199s
	iters: 300, epoch: 20 | loss: 0.0043443
	speed: 0.3291s/iter; left time: 9723.1929s
	iters: 400, epoch: 20 | loss: 0.0143065
	speed: 0.3298s/iter; left time: 9711.0027s
	iters: 500, epoch: 20 | loss: 0.0083869
	speed: 0.3263s/iter; left time: 9575.7146s
	iters: 600, epoch: 20 | loss: 0.0069334
	speed: 0.3203s/iter; left time: 9366.3274s
Epoch: 20 cost time: 00h:03m:41.86s
Epoch: 20 | Train Loss: 0.0076499 Vali Loss: 0.0088768 Test Loss: 0.0103445
Validation loss decreased (0.008894 --> 0.008877).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 21 | loss: 0.0050495
	speed: 1.0619s/iter; left time: 28704.9816s
	iters: 200, epoch: 21 | loss: 0.0056376
	speed: 0.3320s/iter; left time: 8940.7912s
	iters: 300, epoch: 21 | loss: 0.0080506
	speed: 0.3343s/iter; left time: 8970.4401s
	iters: 400, epoch: 21 | loss: 0.0066239
	speed: 0.3225s/iter; left time: 8621.1311s
	iters: 500, epoch: 21 | loss: 0.0070929
	speed: 0.3161s/iter; left time: 8418.6437s
	iters: 600, epoch: 21 | loss: 0.0058815
	speed: 0.3361s/iter; left time: 8916.4134s
Epoch: 21 cost time: 00h:03m:43.81s
Epoch: 21 | Train Loss: 0.0076399 Vali Loss: 0.0088993 Test Loss: 0.0104224
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 22 | loss: 0.0054120
	speed: 1.0256s/iter; left time: 24939.4306s
	iters: 200, epoch: 22 | loss: 0.0076757
	speed: 0.3348s/iter; left time: 8108.1047s
	iters: 300, epoch: 22 | loss: 0.0048412
	speed: 0.3362s/iter; left time: 8107.4460s
	iters: 400, epoch: 22 | loss: 0.0044447
	speed: 0.3327s/iter; left time: 7989.7958s
	iters: 500, epoch: 22 | loss: 0.0083622
	speed: 0.3392s/iter; left time: 8113.5795s
	iters: 600, epoch: 22 | loss: 0.0153576
	speed: 0.3308s/iter; left time: 7879.0503s
Epoch: 22 cost time: 00h:03m:47.24s
Epoch: 22 | Train Loss: 0.0075452 Vali Loss: 0.0089097 Test Loss: 0.0103669
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 23 | loss: 0.0055570
	speed: 1.0418s/iter; left time: 22508.6243s
	iters: 200, epoch: 23 | loss: 0.0043441
	speed: 0.3345s/iter; left time: 7193.3601s
	iters: 300, epoch: 23 | loss: 0.0085570
	speed: 0.3377s/iter; left time: 7228.8754s
	iters: 400, epoch: 23 | loss: 0.0088893
	speed: 0.3361s/iter; left time: 7161.4683s
	iters: 500, epoch: 23 | loss: 0.0037170
	speed: 0.3218s/iter; left time: 6824.1497s
	iters: 600, epoch: 23 | loss: 0.0061619
	speed: 0.3421s/iter; left time: 7219.1257s
Epoch: 23 cost time: 00h:03m:48.29s
Epoch: 23 | Train Loss: 0.0075131 Vali Loss: 0.0089658 Test Loss: 0.0102591
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 24 | loss: 0.0099053
	speed: 1.0770s/iter; left time: 20346.9385s
	iters: 200, epoch: 24 | loss: 0.0091285
	speed: 0.3492s/iter; left time: 6562.2366s
	iters: 300, epoch: 24 | loss: 0.0093744
	speed: 0.3467s/iter; left time: 6480.0304s
	iters: 400, epoch: 24 | loss: 0.0068118
	speed: 0.3448s/iter; left time: 6410.8172s
	iters: 500, epoch: 24 | loss: 0.0037521
	speed: 0.3279s/iter; left time: 6064.4418s
	iters: 600, epoch: 24 | loss: 0.0060791
	speed: 0.3475s/iter; left time: 6390.4704s
Epoch: 24 cost time: 00h:03m:54.15s
Epoch: 24 | Train Loss: 0.0075297 Vali Loss: 0.0088701 Test Loss: 0.0103922
Validation loss decreased (0.008877 --> 0.008870).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 25 | loss: 0.0065410
	speed: 1.1906s/iter; left time: 19261.9734s
	iters: 200, epoch: 25 | loss: 0.0056375
	speed: 0.3496s/iter; left time: 5621.5463s
	iters: 300, epoch: 25 | loss: 0.0102988
	speed: 0.3466s/iter; left time: 5538.5983s
	iters: 400, epoch: 25 | loss: 0.0065078
	speed: 0.3504s/iter; left time: 5563.6305s
	iters: 500, epoch: 25 | loss: 0.0056647
	speed: 0.3474s/iter; left time: 5482.1096s
	iters: 600, epoch: 25 | loss: 0.0058788
	speed: 0.3510s/iter; left time: 5504.1013s
Epoch: 25 cost time: 00h:03m:57.45s
Epoch: 25 | Train Loss: 0.0074501 Vali Loss: 0.0090055 Test Loss: 0.0105855
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 26 | loss: 0.0120122
	speed: 1.0667s/iter; left time: 14364.3894s
	iters: 200, epoch: 26 | loss: 0.0055594
	speed: 0.3323s/iter; left time: 4441.2763s
	iters: 300, epoch: 26 | loss: 0.0064205
	speed: 0.3485s/iter; left time: 4622.8483s
	iters: 400, epoch: 26 | loss: 0.0117745
	speed: 0.3476s/iter; left time: 4576.4656s
	iters: 500, epoch: 26 | loss: 0.0076545
	speed: 0.3508s/iter; left time: 4583.1005s
	iters: 600, epoch: 26 | loss: 0.0078711
	speed: 0.3509s/iter; left time: 4550.3585s
Epoch: 26 cost time: 00h:03m:54.73s
Epoch: 26 | Train Loss: 0.0073374 Vali Loss: 0.0090989 Test Loss: 0.0103317
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 27 | loss: 0.0077098
	speed: 1.0886s/iter; left time: 11705.9631s
	iters: 200, epoch: 27 | loss: 0.0055892
	speed: 0.3521s/iter; left time: 3751.0327s
	iters: 300, epoch: 27 | loss: 0.0084353
	speed: 0.3504s/iter; left time: 3697.2570s
	iters: 400, epoch: 27 | loss: 0.0111528
	speed: 0.3529s/iter; left time: 3689.3583s
	iters: 500, epoch: 27 | loss: 0.0081662
	speed: 0.3508s/iter; left time: 3632.0384s
	iters: 600, epoch: 27 | loss: 0.0053349
	speed: 0.3516s/iter; left time: 3605.1329s
Epoch: 27 cost time: 00h:03m:58.54s
Epoch: 27 | Train Loss: 0.0073602 Vali Loss: 0.0089851 Test Loss: 0.0103161
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 28 | loss: 0.0063998
	speed: 1.0859s/iter; left time: 8730.6862s
	iters: 200, epoch: 28 | loss: 0.0083144
	speed: 0.3520s/iter; left time: 2795.0274s
	iters: 300, epoch: 28 | loss: 0.0065535
	speed: 0.3532s/iter; left time: 2769.0085s
	iters: 400, epoch: 28 | loss: 0.0060916
	speed: 0.3519s/iter; left time: 2723.8142s
	iters: 500, epoch: 28 | loss: 0.0064265
	speed: 0.3533s/iter; left time: 2698.9254s
	iters: 600, epoch: 28 | loss: 0.0053284
	speed: 0.3489s/iter; left time: 2630.6304s
Epoch: 28 cost time: 00h:03m:59.29s
Epoch: 28 | Train Loss: 0.0073103 Vali Loss: 0.0090719 Test Loss: 0.0104575
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 29 | loss: 0.0062723
	speed: 1.0915s/iter; left time: 5814.3815s
	iters: 200, epoch: 29 | loss: 0.0058362
	speed: 0.3520s/iter; left time: 1839.7125s
	iters: 300, epoch: 29 | loss: 0.0083566
	speed: 0.3357s/iter; left time: 1721.2573s
	iters: 400, epoch: 29 | loss: 0.0075264
	speed: 0.3597s/iter; left time: 1807.9777s
	iters: 500, epoch: 29 | loss: 0.0039099
	speed: 0.3620s/iter; left time: 1783.6610s
	iters: 600, epoch: 29 | loss: 0.0062451
	speed: 0.3572s/iter; left time: 1724.2323s
Epoch: 29 cost time: 00h:04m:00.51s
Epoch: 29 | Train Loss: 0.0072024 Vali Loss: 0.0089768 Test Loss: 0.0106518
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Intermediate time for FR and pred_len 24: 01h:45m:51.65s

Intermediate time for FR: 01h:45m:51.65s


=== Starting experiments for pred_len: 96 ===

[2024-11-13 15:18:46,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:18:46,865] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:18:46,872] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:18:46,876] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:18:47,868] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:18:47,869] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:18:47,869] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 86619
val 18435
test 18435
train 86619
val 18435
train 86619
train 86619
test 18435
valval  1843518435

test 18435
test 18435
[2024-11-13 15:18:50,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 15:18:50,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 15:18:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 15:18:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 15:18:50,874] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 15:18:50,875] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 15:18:50,875] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 15:18:50,875] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 15:18:51,208] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 15:18:51,209] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB 
[2024-11-13 15:18:51,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.61 GB, percent = 20.3%
[2024-11-13 15:18:51,392] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 15:18:51,392] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 15:18:51,393] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 204.79 GB, percent = 20.3%
[2024-11-13 15:18:51,393] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 15:18:51,553] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 15:18:51,553] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 15:18:51,553] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 205.03 GB, percent = 20.4%
[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 15:18:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 15:18:51,554] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4554b6ebd0>
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 15:18:51,555] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 15:18:51,556] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 15:18:51,556] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0365720
	speed: 0.1419s/iter; left time: 7664.2947s
	iters: 200, epoch: 1 | loss: 0.0280516
	speed: 0.1165s/iter; left time: 6280.5259s
	iters: 300, epoch: 1 | loss: 0.0237334
	speed: 0.1152s/iter; left time: 6199.3223s
	iters: 400, epoch: 1 | loss: 0.0220072
	speed: 0.1150s/iter; left time: 6176.2841s
	iters: 500, epoch: 1 | loss: 0.0152992
	speed: 0.1148s/iter; left time: 6153.5580s
	iters: 600, epoch: 1 | loss: 0.0149078
	speed: 0.1153s/iter; left time: 6173.1032s
Epoch: 1 cost time: 00h:01m:19.30s
Epoch: 1 | Train Loss: 0.0296274 Vali Loss: 0.0188544 Test Loss: 0.0241413
Validation loss decreased (inf --> 0.018854).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0257344
	speed: 0.4362s/iter; left time: 22381.5116s
	iters: 200, epoch: 2 | loss: 0.0131987
	speed: 0.1145s/iter; left time: 5861.7240s
	iters: 300, epoch: 2 | loss: 0.0125734
	speed: 0.1177s/iter; left time: 6017.1740s
	iters: 400, epoch: 2 | loss: 0.0112410
	speed: 0.1131s/iter; left time: 5768.9233s
	iters: 500, epoch: 2 | loss: 0.0113286
	speed: 0.1142s/iter; left time: 5814.1264s
	iters: 600, epoch: 2 | loss: 0.0112728
	speed: 0.1139s/iter; left time: 5789.5294s
Epoch: 2 cost time: 00h:01m:17.98s
Epoch: 2 | Train Loss: 0.0150897 Vali Loss: 0.0162923 Test Loss: 0.0207778
Validation loss decreased (0.018854 --> 0.016292).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0193557
	speed: 0.3832s/iter; left time: 18627.1257s
	iters: 200, epoch: 3 | loss: 0.0122473
	speed: 0.1138s/iter; left time: 5522.5575s
	iters: 300, epoch: 3 | loss: 0.0140332
	speed: 0.1137s/iter; left time: 5502.2013s
	iters: 400, epoch: 3 | loss: 0.0137614
	speed: 0.1128s/iter; left time: 5446.8939s
	iters: 500, epoch: 3 | loss: 0.0180832
	speed: 0.1153s/iter; left time: 5556.6085s
	iters: 600, epoch: 3 | loss: 0.0061270
	speed: 0.1132s/iter; left time: 5448.0168s
Epoch: 3 cost time: 00h:01m:17.49s
Epoch: 3 | Train Loss: 0.0144389 Vali Loss: 0.0159258 Test Loss: 0.0202510
Validation loss decreased (0.016292 --> 0.015926).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0191393
	speed: 0.3971s/iter; left time: 18229.9854s
	iters: 200, epoch: 4 | loss: 0.0209338
	speed: 0.1128s/iter; left time: 5165.3610s
	iters: 300, epoch: 4 | loss: 0.0178645
	speed: 0.1127s/iter; left time: 5151.2119s
	iters: 400, epoch: 4 | loss: 0.0173927
	speed: 0.1135s/iter; left time: 5176.2412s
	iters: 500, epoch: 4 | loss: 0.0158787
	speed: 0.1125s/iter; left time: 5121.1046s
	iters: 600, epoch: 4 | loss: 0.0139707
	speed: 0.1122s/iter; left time: 5094.4243s
Epoch: 4 cost time: 00h:01m:16.74s
Epoch: 4 | Train Loss: 0.0139130 Vali Loss: 0.0156960 Test Loss: 0.0202585
Validation loss decreased (0.015926 --> 0.015696).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0164406
	speed: 0.4045s/iter; left time: 17474.8781s
	iters: 200, epoch: 5 | loss: 0.0133676
	speed: 0.1116s/iter; left time: 4808.6903s
	iters: 300, epoch: 5 | loss: 0.0133471
	speed: 0.1123s/iter; left time: 4828.8202s
	iters: 400, epoch: 5 | loss: 0.0130934
	speed: 0.1122s/iter; left time: 4811.6602s
	iters: 500, epoch: 5 | loss: 0.0143422
	speed: 0.1123s/iter; left time: 4806.3726s
	iters: 600, epoch: 5 | loss: 0.0112816
	speed: 0.1114s/iter; left time: 4758.3289s
Epoch: 5 cost time: 00h:01m:16.45s
Epoch: 5 | Train Loss: 0.0133617 Vali Loss: 0.0154382 Test Loss: 0.0200492
Validation loss decreased (0.015696 --> 0.015438).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0113203
	speed: 0.4274s/iter; left time: 17304.8190s
	iters: 200, epoch: 6 | loss: 0.0115995
	speed: 0.1112s/iter; left time: 4491.5802s
	iters: 300, epoch: 6 | loss: 0.0073734
	speed: 0.1113s/iter; left time: 4485.9441s
	iters: 400, epoch: 6 | loss: 0.0149999
	speed: 0.1113s/iter; left time: 4472.1406s
	iters: 500, epoch: 6 | loss: 0.0097439
	speed: 0.1119s/iter; left time: 4486.1774s
	iters: 600, epoch: 6 | loss: 0.0117388
	speed: 0.1116s/iter; left time: 4462.6731s
Epoch: 6 cost time: 00h:01m:15.96s
Epoch: 6 | Train Loss: 0.0132906 Vali Loss: 0.0152655 Test Loss: 0.0199258
Validation loss decreased (0.015438 --> 0.015265).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0131203
	speed: 0.4346s/iter; left time: 16422.5427s
	iters: 200, epoch: 7 | loss: 0.0149721
	speed: 0.1110s/iter; left time: 4184.6685s
	iters: 300, epoch: 7 | loss: 0.0097917
	speed: 0.1107s/iter; left time: 4159.5746s
	iters: 400, epoch: 7 | loss: 0.0154822
	speed: 0.1111s/iter; left time: 4165.3162s
	iters: 500, epoch: 7 | loss: 0.0144257
	speed: 0.1110s/iter; left time: 4151.0080s
	iters: 600, epoch: 7 | loss: 0.0235436
	speed: 0.1113s/iter; left time: 4150.6749s
Epoch: 7 cost time: 00h:01m:15.79s
Epoch: 7 | Train Loss: 0.0129925 Vali Loss: 0.0151174 Test Loss: 0.0199530
Validation loss decreased (0.015265 --> 0.015117).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0148050
	speed: 0.4010s/iter; left time: 14067.4559s
	iters: 200, epoch: 8 | loss: 0.0146219
	speed: 0.1114s/iter; left time: 3898.3741s
	iters: 300, epoch: 8 | loss: 0.0177321
	speed: 0.1105s/iter; left time: 3855.7388s
	iters: 400, epoch: 8 | loss: 0.0141665
	speed: 0.1105s/iter; left time: 3843.9874s
	iters: 500, epoch: 8 | loss: 0.0116750
	speed: 0.1106s/iter; left time: 3833.8648s
	iters: 600, epoch: 8 | loss: 0.0111578
	speed: 0.1112s/iter; left time: 3845.3055s
Epoch: 8 cost time: 00h:01m:15.63s
Epoch: 8 | Train Loss: 0.0128574 Vali Loss: 0.0150844 Test Loss: 0.0198784
Validation loss decreased (0.015117 --> 0.015084).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0125546
	speed: 0.3980s/iter; left time: 12885.7508s
	iters: 200, epoch: 9 | loss: 0.0176416
	speed: 0.1111s/iter; left time: 3586.3694s
	iters: 300, epoch: 9 | loss: 0.0117473
	speed: 0.1105s/iter; left time: 3555.9730s
	iters: 400, epoch: 9 | loss: 0.0085340
	speed: 0.1106s/iter; left time: 3547.2687s
	iters: 500, epoch: 9 | loss: 0.0147927
	speed: 0.1102s/iter; left time: 3524.2646s
	iters: 600, epoch: 9 | loss: 0.0098768
	speed: 0.1103s/iter; left time: 3516.0856s
Epoch: 9 cost time: 00h:01m:15.41s
Epoch: 9 | Train Loss: 0.0128993 Vali Loss: 0.0149726 Test Loss: 0.0199329
Validation loss decreased (0.015084 --> 0.014973).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0176372
	speed: 0.4246s/iter; left time: 12596.2343s
	iters: 200, epoch: 10 | loss: 0.0126642
	speed: 0.1105s/iter; left time: 3266.2789s
	iters: 300, epoch: 10 | loss: 0.0110052
	speed: 0.1106s/iter; left time: 3258.6639s
	iters: 400, epoch: 10 | loss: 0.0197459
	speed: 0.1106s/iter; left time: 3246.9784s
	iters: 500, epoch: 10 | loss: 0.0204530
	speed: 0.1105s/iter; left time: 3233.4938s
	iters: 600, epoch: 10 | loss: 0.0130680
	speed: 0.1099s/iter; left time: 3205.7374s
Epoch: 10 cost time: 00h:01m:15.28s
Epoch: 10 | Train Loss: 0.0128206 Vali Loss: 0.0151203 Test Loss: 0.0199767
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0164479
	speed: 0.3447s/iter; left time: 9293.0614s
	iters: 200, epoch: 11 | loss: 0.0108230
	speed: 0.1108s/iter; left time: 2977.2863s
	iters: 300, epoch: 11 | loss: 0.0085999
	speed: 0.1118s/iter; left time: 2993.1143s
	iters: 400, epoch: 11 | loss: 0.0096984
	speed: 0.1106s/iter; left time: 2947.9467s
	iters: 500, epoch: 11 | loss: 0.0173206
	speed: 0.1112s/iter; left time: 2954.8101s
	iters: 600, epoch: 11 | loss: 0.0124708
	speed: 0.1106s/iter; left time: 2927.1686s
Epoch: 11 cost time: 00h:01m:15.78s
Epoch: 11 | Train Loss: 0.0124435 Vali Loss: 0.0150630 Test Loss: 0.0202035
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0131188
	speed: 0.3425s/iter; left time: 8307.4692s
	iters: 200, epoch: 12 | loss: 0.0120664
	speed: 0.1111s/iter; left time: 2683.0771s
	iters: 300, epoch: 12 | loss: 0.0137616
	speed: 0.1113s/iter; left time: 2677.7270s
	iters: 400, epoch: 12 | loss: 0.0140155
	speed: 0.1115s/iter; left time: 2669.8734s
	iters: 500, epoch: 12 | loss: 0.0105363
	speed: 0.1110s/iter; left time: 2648.3874s
	iters: 600, epoch: 12 | loss: 0.0159831
	speed: 0.1106s/iter; left time: 2626.8158s
Epoch: 12 cost time: 00h:01m:15.51s
Epoch: 12 | Train Loss: 0.0123154 Vali Loss: 0.0148068 Test Loss: 0.0198685
Validation loss decreased (0.014973 --> 0.014807).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0107450
	speed: 0.3894s/iter; left time: 8392.2184s
	iters: 200, epoch: 13 | loss: 0.0130737
	speed: 0.1120s/iter; left time: 2402.1775s
	iters: 300, epoch: 13 | loss: 0.0093372
	speed: 0.1115s/iter; left time: 2379.8026s
	iters: 400, epoch: 13 | loss: 0.0096826
	speed: 0.1099s/iter; left time: 2335.0702s
	iters: 500, epoch: 13 | loss: 0.0082334
	speed: 0.1100s/iter; left time: 2325.8892s
	iters: 600, epoch: 13 | loss: 0.0136555
	speed: 0.1105s/iter; left time: 2325.4441s
Epoch: 13 cost time: 00h:01m:15.36s
Epoch: 13 | Train Loss: 0.0123478 Vali Loss: 0.0147222 Test Loss: 0.0195867
Validation loss decreased (0.014807 --> 0.014722).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0138494
	speed: 0.3884s/iter; left time: 7318.9826s
	iters: 200, epoch: 14 | loss: 0.0141361
	speed: 0.1105s/iter; left time: 2071.1033s
	iters: 300, epoch: 14 | loss: 0.0092835
	speed: 0.1116s/iter; left time: 2079.8785s
	iters: 400, epoch: 14 | loss: 0.0081837
	speed: 0.1170s/iter; left time: 2169.1301s
	iters: 500, epoch: 14 | loss: 0.0109799
	speed: 0.1158s/iter; left time: 2136.2294s
	iters: 600, epoch: 14 | loss: 0.0090381
	speed: 0.1116s/iter; left time: 2047.5919s
Epoch: 14 cost time: 00h:01m:17.05s
Epoch: 14 | Train Loss: 0.0121958 Vali Loss: 0.0146016 Test Loss: 0.0196531
Validation loss decreased (0.014722 --> 0.014602).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0129044
	speed: 0.3849s/iter; left time: 6211.5884s
	iters: 200, epoch: 15 | loss: 0.0082274
	speed: 0.1107s/iter; left time: 1774.5290s
	iters: 300, epoch: 15 | loss: 0.0082835
	speed: 0.1101s/iter; left time: 1754.2830s
	iters: 400, epoch: 15 | loss: 0.0094719
	speed: 0.1150s/iter; left time: 1820.6878s
	iters: 500, epoch: 15 | loss: 0.0099078
	speed: 0.1112s/iter; left time: 1750.1903s
	iters: 600, epoch: 15 | loss: 0.0100165
	speed: 0.1101s/iter; left time: 1721.8566s
Epoch: 15 cost time: 00h:01m:15.74s
Epoch: 15 | Train Loss: 0.0118769 Vali Loss: 0.0149706 Test Loss: 0.0198224
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0098050
	speed: 0.3467s/iter; left time: 4656.7881s
	iters: 200, epoch: 16 | loss: 0.0165337
	speed: 0.1117s/iter; left time: 1488.5019s
	iters: 300, epoch: 16 | loss: 0.0093741
	speed: 0.1112s/iter; left time: 1470.6937s
	iters: 400, epoch: 16 | loss: 0.0085768
	speed: 0.1132s/iter; left time: 1485.8487s
	iters: 500, epoch: 16 | loss: 0.0100504
	speed: 0.1122s/iter; left time: 1462.5196s
	iters: 600, epoch: 16 | loss: 0.0101264
	speed: 0.1111s/iter; left time: 1436.1961s
Epoch: 16 cost time: 00h:01m:15.93s
Epoch: 16 | Train Loss: 0.0117034 Vali Loss: 0.0150434 Test Loss: 0.0202259
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0177830
	speed: 0.3434s/iter; left time: 3682.5295s
	iters: 200, epoch: 17 | loss: 0.0133350
	speed: 0.1122s/iter; left time: 1192.1194s
	iters: 300, epoch: 17 | loss: 0.0065481
	speed: 0.1107s/iter; left time: 1164.8550s
	iters: 400, epoch: 17 | loss: 0.0142507
	speed: 0.1113s/iter; left time: 1160.4833s
	iters: 500, epoch: 17 | loss: 0.0093826
	speed: 0.1112s/iter; left time: 1147.9576s
	iters: 600, epoch: 17 | loss: 0.0085390
	speed: 0.1114s/iter; left time: 1138.6955s
Epoch: 17 cost time: 00h:01m:15.84s
Epoch: 17 | Train Loss: 0.0115625 Vali Loss: 0.0149463 Test Loss: 0.0203532
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0068310
	speed: 0.3423s/iter; left time: 2744.6538s
	iters: 200, epoch: 18 | loss: 0.0092751
	speed: 0.1104s/iter; left time: 873.9001s
	iters: 300, epoch: 18 | loss: 0.0090604
	speed: 0.1106s/iter; left time: 864.9826s
	iters: 400, epoch: 18 | loss: 0.0113523
	speed: 0.1116s/iter; left time: 861.5789s
	iters: 500, epoch: 18 | loss: 0.0100290
	speed: 0.1106s/iter; left time: 842.9753s
	iters: 600, epoch: 18 | loss: 0.0088446
	speed: 0.1101s/iter; left time: 827.7318s
Epoch: 18 cost time: 00h:01m:15.23s
Epoch: 18 | Train Loss: 0.0114536 Vali Loss: 0.0154895 Test Loss: 0.0203540
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 19 | loss: 0.0120453
	speed: 0.3382s/iter; left time: 1797.1140s
	iters: 200, epoch: 19 | loss: 0.0099022
	speed: 0.1101s/iter; left time: 573.9307s
	iters: 300, epoch: 19 | loss: 0.0156211
	speed: 0.1124s/iter; left time: 574.5642s
	iters: 400, epoch: 19 | loss: 0.0092177
	speed: 0.1123s/iter; left time: 562.8975s
	iters: 500, epoch: 19 | loss: 0.0075598
	speed: 0.1114s/iter; left time: 547.0750s
	iters: 600, epoch: 19 | loss: 0.0066154
	speed: 0.1104s/iter; left time: 531.3639s
Epoch: 19 cost time: 00h:01m:15.56s
Epoch: 19 | Train Loss: 0.0113176 Vali Loss: 0.0156360 Test Loss: 0.0205360
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391
Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391
Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391
Scaled mse:0.0196530781686306, rmse:0.14018943905830383, mae:0.08574584871530533, rse:0.5423334240913391
Intermediate time for FR and pred_len 96: 00h:30m:21.47s


=== Starting experiments for pred_len: 168 ===

[2024-11-13 15:49:08,824] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:49:08,836] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:49:08,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:49:08,864] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:49:09,868] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 15:49:09,868] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 15:49:09,869] [INFO] [comm.py:637:init_distributed] cdb=None
train 86403
val 18219
test 18219
train 86403
train 86403
val 18219
train 86403
val 18219
test 18219
val 18219
test 18219
test 18219
[2024-11-13 15:49:12,125] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 15:49:13,171] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 15:49:13,172] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 15:49:13,172] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 15:49:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 15:49:13,173] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 15:49:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 15:49:13,173] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 15:49:13,562] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 15:49:13,562] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB 
[2024-11-13 15:49:13,563] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 201.53 GB, percent = 20.0%
[2024-11-13 15:49:13,761] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 15:49:13,762] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 15:49:13,762] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 201.91 GB, percent = 20.0%
[2024-11-13 15:49:13,762] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 15:49:13,949] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 15:49:13,949] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 15:49:13,949] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.39 GB, percent = 20.1%
[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 15:49:13,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 15:49:13,951] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4d03a04910>
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 15:49:13,951] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 15:49:13,952] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 15:49:13,952] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0417766
	speed: 0.1477s/iter; left time: 7959.7124s
	iters: 200, epoch: 1 | loss: 0.0256541
	speed: 0.1169s/iter; left time: 6289.3909s
	iters: 300, epoch: 1 | loss: 0.0370757
	speed: 0.1172s/iter; left time: 6296.3554s
	iters: 400, epoch: 1 | loss: 0.0266412
	speed: 0.1174s/iter; left time: 6291.7026s
	iters: 500, epoch: 1 | loss: 0.0169627
	speed: 0.1172s/iter; left time: 6271.3629s
	iters: 600, epoch: 1 | loss: 0.0200696
	speed: 0.1175s/iter; left time: 6274.2301s
Epoch: 1 cost time: 00h:01m:20.98s
Epoch: 1 | Train Loss: 0.0314196 Vali Loss: 0.0201541 Test Loss: 0.0246362
Validation loss decreased (inf --> 0.020154).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0140743
	speed: 0.4194s/iter; left time: 21474.6910s
	iters: 200, epoch: 2 | loss: 0.0158384
	speed: 0.1160s/iter; left time: 5930.2192s
	iters: 300, epoch: 2 | loss: 0.0162745
	speed: 0.1157s/iter; left time: 5900.6336s
	iters: 400, epoch: 2 | loss: 0.0154659
	speed: 0.1155s/iter; left time: 5878.6740s
	iters: 500, epoch: 2 | loss: 0.0126034
	speed: 0.1156s/iter; left time: 5872.2664s
	iters: 600, epoch: 2 | loss: 0.0152980
	speed: 0.1165s/iter; left time: 5904.5133s
Epoch: 2 cost time: 00h:01m:19.24s
Epoch: 2 | Train Loss: 0.0164696 Vali Loss: 0.0180134 Test Loss: 0.0222745
Validation loss decreased (0.020154 --> 0.018013).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0205479
	speed: 0.3963s/iter; left time: 19220.1896s
	iters: 200, epoch: 3 | loss: 0.0187195
	speed: 0.1169s/iter; left time: 5659.5534s
	iters: 300, epoch: 3 | loss: 0.0161316
	speed: 0.1163s/iter; left time: 5616.5458s
	iters: 400, epoch: 3 | loss: 0.0095569
	speed: 0.1162s/iter; left time: 5598.8585s
	iters: 500, epoch: 3 | loss: 0.0130455
	speed: 0.1171s/iter; left time: 5632.8513s
	iters: 600, epoch: 3 | loss: 0.0106364
	speed: 0.1152s/iter; left time: 5530.9617s
Epoch: 3 cost time: 00h:01m:19.02s
Epoch: 3 | Train Loss: 0.0156743 Vali Loss: 0.0172826 Test Loss: 0.0217130
Validation loss decreased (0.018013 --> 0.017283).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0156313
	speed: 0.4117s/iter; left time: 18857.1162s
	iters: 200, epoch: 4 | loss: 0.0166580
	speed: 0.1146s/iter; left time: 5238.3839s
	iters: 300, epoch: 4 | loss: 0.0135303
	speed: 0.1141s/iter; left time: 5201.0737s
	iters: 400, epoch: 4 | loss: 0.0096387
	speed: 0.1160s/iter; left time: 5277.0169s
	iters: 500, epoch: 4 | loss: 0.0092750
	speed: 0.1157s/iter; left time: 5251.9154s
	iters: 600, epoch: 4 | loss: 0.0137595
	speed: 0.1148s/iter; left time: 5199.4185s
Epoch: 4 cost time: 00h:01m:18.57s
Epoch: 4 | Train Loss: 0.0152146 Vali Loss: 0.0170755 Test Loss: 0.0217249
Validation loss decreased (0.017283 --> 0.017076).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0197449
	speed: 0.3953s/iter; left time: 17039.2556s
	iters: 200, epoch: 5 | loss: 0.0156107
	speed: 0.1151s/iter; left time: 4951.2011s
	iters: 300, epoch: 5 | loss: 0.0155113
	speed: 0.1142s/iter; left time: 4898.3954s
	iters: 400, epoch: 5 | loss: 0.0121896
	speed: 0.1157s/iter; left time: 4953.3144s
	iters: 500, epoch: 5 | loss: 0.0110335
	speed: 0.1164s/iter; left time: 4970.5123s
	iters: 600, epoch: 5 | loss: 0.0154606
	speed: 0.1152s/iter; left time: 4906.7805s
Epoch: 5 cost time: 00h:01m:18.37s
Epoch: 5 | Train Loss: 0.0150166 Vali Loss: 0.0169449 Test Loss: 0.0217564
Validation loss decreased (0.017076 --> 0.016945).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0126255
	speed: 0.3971s/iter; left time: 16042.8409s
	iters: 200, epoch: 6 | loss: 0.0132552
	speed: 0.1149s/iter; left time: 4630.1364s
	iters: 300, epoch: 6 | loss: 0.0162678
	speed: 0.1147s/iter; left time: 4610.6448s
	iters: 400, epoch: 6 | loss: 0.0198398
	speed: 0.1162s/iter; left time: 4660.8443s
	iters: 500, epoch: 6 | loss: 0.0115809
	speed: 0.1138s/iter; left time: 4551.9183s
	iters: 600, epoch: 6 | loss: 0.0190240
	speed: 0.1137s/iter; left time: 4537.5913s
Epoch: 6 cost time: 00h:01m:18.17s
Epoch: 6 | Train Loss: 0.0145469 Vali Loss: 0.0170547 Test Loss: 0.0219276
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0134221
	speed: 0.3479s/iter; left time: 13114.4315s
	iters: 200, epoch: 7 | loss: 0.0113875
	speed: 0.1129s/iter; left time: 4245.2983s
	iters: 300, epoch: 7 | loss: 0.0183335
	speed: 0.1137s/iter; left time: 4262.7963s
	iters: 400, epoch: 7 | loss: 0.0140515
	speed: 0.1222s/iter; left time: 4571.8369s
	iters: 500, epoch: 7 | loss: 0.0185097
	speed: 0.1970s/iter; left time: 7348.2177s
	iters: 600, epoch: 7 | loss: 0.0128493
	speed: 0.1129s/iter; left time: 4200.4297s
Epoch: 7 cost time: 00h:01m:26.35s
Epoch: 7 | Train Loss: 0.0144284 Vali Loss: 0.0167750 Test Loss: 0.0216554
Validation loss decreased (0.016945 --> 0.016775).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0152631
	speed: 0.4829s/iter; left time: 16901.5566s
	iters: 200, epoch: 8 | loss: 0.0107777
	speed: 0.1519s/iter; left time: 5300.6262s
	iters: 300, epoch: 8 | loss: 0.0169159
	speed: 0.1147s/iter; left time: 3992.6808s
	iters: 400, epoch: 8 | loss: 0.0165618
	speed: 0.1147s/iter; left time: 3981.8822s
	iters: 500, epoch: 8 | loss: 0.0104190
	speed: 0.1140s/iter; left time: 3945.8500s
	iters: 600, epoch: 8 | loss: 0.0168079
	speed: 0.1134s/iter; left time: 3913.1608s
Epoch: 8 cost time: 00h:01m:26.72s
Epoch: 8 | Train Loss: 0.0142601 Vali Loss: 0.0163743 Test Loss: 0.0212844
Validation loss decreased (0.016775 --> 0.016374).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0102134
	speed: 0.4321s/iter; left time: 13957.6054s
	iters: 200, epoch: 9 | loss: 0.0134811
	speed: 0.1125s/iter; left time: 3624.2015s
	iters: 300, epoch: 9 | loss: 0.0126132
	speed: 0.1130s/iter; left time: 3626.0318s
	iters: 400, epoch: 9 | loss: 0.0204920
	speed: 0.1134s/iter; left time: 3627.6755s
	iters: 500, epoch: 9 | loss: 0.0125607
	speed: 0.1125s/iter; left time: 3588.3175s
	iters: 600, epoch: 9 | loss: 0.0123675
	speed: 0.1128s/iter; left time: 3586.3467s
Epoch: 9 cost time: 00h:01m:17.57s
Epoch: 9 | Train Loss: 0.0142351 Vali Loss: 0.0164664 Test Loss: 0.0217097
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0138933
	speed: 0.3439s/iter; left time: 10180.2120s
	iters: 200, epoch: 10 | loss: 0.0144352
	speed: 0.1128s/iter; left time: 3326.5281s
	iters: 300, epoch: 10 | loss: 0.0188366
	speed: 0.1124s/iter; left time: 3303.6556s
	iters: 400, epoch: 10 | loss: 0.0131616
	speed: 0.1130s/iter; left time: 3309.6236s
	iters: 500, epoch: 10 | loss: 0.0119591
	speed: 0.1137s/iter; left time: 3320.8010s
	iters: 600, epoch: 10 | loss: 0.0107009
	speed: 0.1134s/iter; left time: 3298.8945s
Epoch: 10 cost time: 00h:01m:16.65s
Epoch: 10 | Train Loss: 0.0140122 Vali Loss: 0.0163399 Test Loss: 0.0212699
Validation loss decreased (0.016374 --> 0.016340).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0127098
	speed: 0.4626s/iter; left time: 12445.2411s
	iters: 200, epoch: 11 | loss: 0.0168296
	speed: 0.1129s/iter; left time: 3027.0891s
	iters: 300, epoch: 11 | loss: 0.0170023
	speed: 0.1144s/iter; left time: 3055.4112s
	iters: 400, epoch: 11 | loss: 0.0155827
	speed: 0.1139s/iter; left time: 3031.0551s
	iters: 500, epoch: 11 | loss: 0.0107336
	speed: 0.1140s/iter; left time: 3020.7316s
	iters: 600, epoch: 11 | loss: 0.0104780
	speed: 0.1165s/iter; left time: 3076.7567s
Epoch: 11 cost time: 00h:01m:17.59s
Epoch: 11 | Train Loss: 0.0137326 Vali Loss: 0.0162093 Test Loss: 0.0212543
Validation loss decreased (0.016340 --> 0.016209).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0166218
	speed: 0.4469s/iter; left time: 10815.4111s
	iters: 200, epoch: 12 | loss: 0.0157183
	speed: 0.1151s/iter; left time: 2774.3302s
	iters: 300, epoch: 12 | loss: 0.0122932
	speed: 0.1124s/iter; left time: 2697.0745s
	iters: 400, epoch: 12 | loss: 0.0200626
	speed: 0.1126s/iter; left time: 2690.6896s
	iters: 500, epoch: 12 | loss: 0.0111632
	speed: 0.1124s/iter; left time: 2674.5361s
	iters: 600, epoch: 12 | loss: 0.0163757
	speed: 0.1125s/iter; left time: 2666.8778s
Epoch: 12 cost time: 00h:01m:17.04s
Epoch: 12 | Train Loss: 0.0135342 Vali Loss: 0.0164481 Test Loss: 0.0215607
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0125801
	speed: 0.3429s/iter; left time: 7372.9554s
	iters: 200, epoch: 13 | loss: 0.0092519
	speed: 0.1122s/iter; left time: 2402.1986s
	iters: 300, epoch: 13 | loss: 0.0114003
	speed: 0.1126s/iter; left time: 2399.3992s
	iters: 400, epoch: 13 | loss: 0.0143267
	speed: 0.1133s/iter; left time: 2402.5757s
	iters: 500, epoch: 13 | loss: 0.0193831
	speed: 0.1134s/iter; left time: 2393.4618s
	iters: 600, epoch: 13 | loss: 0.0142914
	speed: 0.1137s/iter; left time: 2387.4374s
Epoch: 13 cost time: 00h:01m:16.74s
Epoch: 13 | Train Loss: 0.0134906 Vali Loss: 0.0167998 Test Loss: 0.0217321
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0120610
	speed: 0.3434s/iter; left time: 6456.8045s
	iters: 200, epoch: 14 | loss: 0.0161036
	speed: 0.1124s/iter; left time: 2101.4046s
	iters: 300, epoch: 14 | loss: 0.0149346
	speed: 0.1121s/iter; left time: 2085.1297s
	iters: 400, epoch: 14 | loss: 0.0147595
	speed: 0.1137s/iter; left time: 2104.3682s
	iters: 500, epoch: 14 | loss: 0.0182757
	speed: 0.1136s/iter; left time: 2089.5769s
	iters: 600, epoch: 14 | loss: 0.0136856
	speed: 0.1143s/iter; left time: 2091.2705s
Epoch: 14 cost time: 00h:01m:16.92s
Epoch: 14 | Train Loss: 0.0134139 Vali Loss: 0.0169548 Test Loss: 0.0227987
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0185989
	speed: 0.3409s/iter; left time: 5489.4018s
	iters: 200, epoch: 15 | loss: 0.0171201
	speed: 0.1121s/iter; left time: 1794.4336s
	iters: 300, epoch: 15 | loss: 0.0171193
	speed: 0.1126s/iter; left time: 1789.9002s
	iters: 400, epoch: 15 | loss: 0.0135243
	speed: 0.1120s/iter; left time: 1769.7305s
	iters: 500, epoch: 15 | loss: 0.0115249
	speed: 0.1119s/iter; left time: 1756.7221s
	iters: 600, epoch: 15 | loss: 0.0136147
	speed: 0.1120s/iter; left time: 1747.7410s
Epoch: 15 cost time: 00h:01m:16.30s
Epoch: 15 | Train Loss: 0.0131047 Vali Loss: 0.0173370 Test Loss: 0.0227311
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0187483
	speed: 0.3391s/iter; left time: 4543.9042s
	iters: 200, epoch: 16 | loss: 0.0144263
	speed: 0.1120s/iter; left time: 1489.2937s
	iters: 300, epoch: 16 | loss: 0.0091274
	speed: 0.1119s/iter; left time: 1477.5226s
	iters: 400, epoch: 16 | loss: 0.0168024
	speed: 0.1122s/iter; left time: 1469.4416s
	iters: 500, epoch: 16 | loss: 0.0127802
	speed: 0.1129s/iter; left time: 1468.3672s
	iters: 600, epoch: 16 | loss: 0.0151057
	speed: 0.1126s/iter; left time: 1452.8706s
Epoch: 16 cost time: 00h:01m:16.33s
Epoch: 16 | Train Loss: 0.0130004 Vali Loss: 0.0173143 Test Loss: 0.0228985
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354
Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354
Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354
Scaled mse:0.02125428430736065, rmse:0.14578849077224731, mae:0.09115074574947357, rse:0.565285325050354
Intermediate time for FR and pred_len 168: 00h:26m:21.56s

Intermediate time for FR: 01h:32m:39.48s


=== Starting experiments for country: IT ===


=== Starting experiments for pred_len: 24 ===

[2024-11-13 16:15:30,654] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:15:30,664] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:15:30,665] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:15:30,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:15:31,727] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:15:31,728] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 86835
val 18651
test 18651
train 86835
train 86835
train 86835
val 18651
val 18651
val 18651
test 18651
test 18651
test 18651
[2024-11-13 16:15:34,079] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 16:15:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 16:15:35,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 16:15:35,004] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 16:15:35,004] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 16:15:35,004] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 16:15:35,335] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 16:15:35,336] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-13 16:15:35,336] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.05 GB, percent = 20.1%
[2024-11-13 16:15:35,508] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 16:15:35,508] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 16:15:35,508] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.23 GB, percent = 20.1%
[2024-11-13 16:15:35,509] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 16:15:35,637] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 16:15:35,638] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 16:15:35,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 202.53 GB, percent = 20.1%
[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 16:15:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 16:15:35,639] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabecaf0850>
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 16:15:35,639] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 16:15:35,640] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 16:15:35,640] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0669913
	speed: 0.1452s/iter; left time: 7865.3973s
	iters: 200, epoch: 1 | loss: 0.0426923
	speed: 0.1172s/iter; left time: 6337.3856s
	iters: 300, epoch: 1 | loss: 0.0284488
	speed: 0.1179s/iter; left time: 6360.4377s
	iters: 400, epoch: 1 | loss: 0.0195312
	speed: 0.1174s/iter; left time: 6322.7274s
	iters: 500, epoch: 1 | loss: 0.0354719
	speed: 0.1174s/iter; left time: 6312.8136s
	iters: 600, epoch: 1 | loss: 0.0156711
	speed: 0.1167s/iter; left time: 6263.0584s
Epoch: 1 cost time: 00h:01m:20.86s
Epoch: 1 | Train Loss: 0.0390933 Vali Loss: 0.0132475 Test Loss: 0.0142686
Validation loss decreased (inf --> 0.013247).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0214472
	speed: 0.4357s/iter; left time: 22414.6633s
	iters: 200, epoch: 2 | loss: 0.0166737
	speed: 0.1891s/iter; left time: 9711.8633s
	iters: 300, epoch: 2 | loss: 0.0151975
	speed: 0.1149s/iter; left time: 5886.9193s
	iters: 400, epoch: 2 | loss: 0.0165503
	speed: 0.1166s/iter; left time: 5966.1397s
	iters: 500, epoch: 2 | loss: 0.0280452
	speed: 0.1155s/iter; left time: 5896.9445s
	iters: 600, epoch: 2 | loss: 0.0204850
	speed: 0.1196s/iter; left time: 6094.7441s
Epoch: 2 cost time: 00h:01m:28.50s
Epoch: 2 | Train Loss: 0.0186876 Vali Loss: 0.0117981 Test Loss: 0.0127852
Validation loss decreased (0.013247 --> 0.011798).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0146356
	speed: 0.4916s/iter; left time: 23957.2551s
	iters: 200, epoch: 3 | loss: 0.0132557
	speed: 0.1145s/iter; left time: 5568.5583s
	iters: 300, epoch: 3 | loss: 0.0179834
	speed: 0.1144s/iter; left time: 5554.6485s
	iters: 400, epoch: 3 | loss: 0.0231912
	speed: 0.1147s/iter; left time: 5554.1902s
	iters: 500, epoch: 3 | loss: 0.0207518
	speed: 0.1145s/iter; left time: 5531.9955s
	iters: 600, epoch: 3 | loss: 0.0146786
	speed: 0.1146s/iter; left time: 5529.7541s
Epoch: 3 cost time: 00h:01m:23.27s
Epoch: 3 | Train Loss: 0.0170714 Vali Loss: 0.0112131 Test Loss: 0.0121951
Validation loss decreased (0.011798 --> 0.011213).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0206654
	speed: 0.4239s/iter; left time: 19509.9904s
	iters: 200, epoch: 4 | loss: 0.0149954
	speed: 0.1157s/iter; left time: 5311.4744s
	iters: 300, epoch: 4 | loss: 0.0181798
	speed: 0.1144s/iter; left time: 5243.0492s
	iters: 400, epoch: 4 | loss: 0.0221356
	speed: 0.1371s/iter; left time: 6267.5608s
	iters: 500, epoch: 4 | loss: 0.0188067
	speed: 0.1227s/iter; left time: 5596.6184s
	iters: 600, epoch: 4 | loss: 0.0086526
	speed: 0.1164s/iter; left time: 5300.9456s
Epoch: 4 cost time: 00h:01m:21.90s
Epoch: 4 | Train Loss: 0.0162319 Vali Loss: 0.0109968 Test Loss: 0.0120540
Validation loss decreased (0.011213 --> 0.010997).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0238934
	speed: 0.4692s/iter; left time: 20321.0175s
	iters: 200, epoch: 5 | loss: 0.0171008
	speed: 0.1147s/iter; left time: 4956.1446s
	iters: 300, epoch: 5 | loss: 0.0146524
	speed: 0.1138s/iter; left time: 4905.4229s
	iters: 400, epoch: 5 | loss: 0.0147643
	speed: 0.1150s/iter; left time: 4946.2218s
	iters: 500, epoch: 5 | loss: 0.0178727
	speed: 0.1144s/iter; left time: 4909.4947s
	iters: 600, epoch: 5 | loss: 0.0203826
	speed: 0.1141s/iter; left time: 4884.5804s
Epoch: 5 cost time: 00h:01m:18.20s
Epoch: 5 | Train Loss: 0.0159699 Vali Loss: 0.0107941 Test Loss: 0.0118797
Validation loss decreased (0.010997 --> 0.010794).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0171005
	speed: 0.4256s/iter; left time: 17275.7073s
	iters: 200, epoch: 6 | loss: 0.0183887
	speed: 0.1152s/iter; left time: 4663.3577s
	iters: 300, epoch: 6 | loss: 0.0152069
	speed: 0.1142s/iter; left time: 4614.1110s
	iters: 400, epoch: 6 | loss: 0.0157148
	speed: 0.1147s/iter; left time: 4621.5525s
	iters: 500, epoch: 6 | loss: 0.0130175
	speed: 0.1247s/iter; left time: 5010.4445s
	iters: 600, epoch: 6 | loss: 0.0195304
	speed: 0.2539s/iter; left time: 10180.1290s
Epoch: 6 cost time: 00h:01m:43.44s
Epoch: 6 | Train Loss: 0.0155751 Vali Loss: 0.0103284 Test Loss: 0.0113590
Validation loss decreased (0.010794 --> 0.010328).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0149800
	speed: 0.8237s/iter; left time: 31205.0641s
	iters: 200, epoch: 7 | loss: 0.0142167
	speed: 0.2453s/iter; left time: 9269.0863s
	iters: 300, epoch: 7 | loss: 0.0094101
	speed: 0.1755s/iter; left time: 6614.5832s
	iters: 400, epoch: 7 | loss: 0.0143765
	speed: 0.2438s/iter; left time: 9162.8789s
	iters: 500, epoch: 7 | loss: 0.0181995
	speed: 0.2442s/iter; left time: 9151.9915s
	iters: 600, epoch: 7 | loss: 0.0154168
	speed: 0.2447s/iter; left time: 9145.8870s
Epoch: 7 cost time: 00h:02m:38.83s
Epoch: 7 | Train Loss: 0.0153713 Vali Loss: 0.0102515 Test Loss: 0.0112287
Validation loss decreased (0.010328 --> 0.010252).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0148691
	speed: 0.7465s/iter; left time: 26255.7157s
	iters: 200, epoch: 8 | loss: 0.0168062
	speed: 0.1944s/iter; left time: 6819.2589s
	iters: 300, epoch: 8 | loss: 0.0113323
	speed: 0.2416s/iter; left time: 8447.5544s
	iters: 400, epoch: 8 | loss: 0.0202741
	speed: 0.2437s/iter; left time: 8496.8439s
	iters: 500, epoch: 8 | loss: 0.0235890
	speed: 0.2553s/iter; left time: 8875.2362s
	iters: 600, epoch: 8 | loss: 0.0162556
	speed: 0.2579s/iter; left time: 8941.4790s
Epoch: 8 cost time: 00h:02m:36.47s
Epoch: 8 | Train Loss: 0.0149047 Vali Loss: 0.0104321 Test Loss: 0.0115182
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0166072
	speed: 0.5086s/iter; left time: 16508.3473s
	iters: 200, epoch: 9 | loss: 0.0144150
	speed: 0.1152s/iter; left time: 3727.8166s
	iters: 300, epoch: 9 | loss: 0.0153691
	speed: 0.1625s/iter; left time: 5240.1736s
	iters: 400, epoch: 9 | loss: 0.0152353
	speed: 0.1550s/iter; left time: 4983.7200s
	iters: 500, epoch: 9 | loss: 0.0203724
	speed: 0.1142s/iter; left time: 3660.8631s
	iters: 600, epoch: 9 | loss: 0.0157388
	speed: 0.1136s/iter; left time: 3630.6724s
Epoch: 9 cost time: 00h:01m:29.43s
Epoch: 9 | Train Loss: 0.0150001 Vali Loss: 0.0099908 Test Loss: 0.0111266
Validation loss decreased (0.010252 --> 0.009991).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0183513
	speed: 0.4843s/iter; left time: 14405.3458s
	iters: 200, epoch: 10 | loss: 0.0142193
	speed: 0.1125s/iter; left time: 3335.5192s
	iters: 300, epoch: 10 | loss: 0.0162596
	speed: 0.1133s/iter; left time: 3347.6672s
	iters: 400, epoch: 10 | loss: 0.0119137
	speed: 0.1137s/iter; left time: 3348.7877s
	iters: 500, epoch: 10 | loss: 0.0134835
	speed: 0.1135s/iter; left time: 3330.9299s
	iters: 600, epoch: 10 | loss: 0.0192499
	speed: 0.1136s/iter; left time: 3320.9394s
Epoch: 10 cost time: 00h:01m:17.41s
Epoch: 10 | Train Loss: 0.0148069 Vali Loss: 0.0100791 Test Loss: 0.0109957
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0120941
	speed: 0.3553s/iter; left time: 9602.7900s
	iters: 200, epoch: 11 | loss: 0.0227418
	speed: 0.1119s/iter; left time: 3013.6185s
	iters: 300, epoch: 11 | loss: 0.0172830
	speed: 0.1118s/iter; left time: 3000.9905s
	iters: 400, epoch: 11 | loss: 0.0157759
	speed: 0.1138s/iter; left time: 3042.4452s
	iters: 500, epoch: 11 | loss: 0.0149913
	speed: 0.1139s/iter; left time: 3032.2947s
	iters: 600, epoch: 11 | loss: 0.0125543
	speed: 0.1130s/iter; left time: 2999.2075s
Epoch: 11 cost time: 00h:01m:17.41s
Epoch: 11 | Train Loss: 0.0144006 Vali Loss: 0.0102262 Test Loss: 0.0111913
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0114660
	speed: 0.3513s/iter; left time: 8542.0242s
	iters: 200, epoch: 12 | loss: 0.0127194
	speed: 0.1131s/iter; left time: 2738.2732s
	iters: 300, epoch: 12 | loss: 0.0106502
	speed: 0.1142s/iter; left time: 2753.1607s
	iters: 400, epoch: 12 | loss: 0.0113894
	speed: 0.1128s/iter; left time: 2710.3290s
	iters: 500, epoch: 12 | loss: 0.0169212
	speed: 0.1142s/iter; left time: 2731.6365s
	iters: 600, epoch: 12 | loss: 0.0151293
	speed: 0.1126s/iter; left time: 2682.9997s
Epoch: 12 cost time: 00h:01m:17.47s
Epoch: 12 | Train Loss: 0.0144180 Vali Loss: 0.0095405 Test Loss: 0.0104502
Validation loss decreased (0.009991 --> 0.009541).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0119666
	speed: 0.4201s/iter; left time: 9076.3618s
	iters: 200, epoch: 13 | loss: 0.0167867
	speed: 0.1122s/iter; left time: 2411.9673s
	iters: 300, epoch: 13 | loss: 0.0118353
	speed: 0.1141s/iter; left time: 2442.2437s
	iters: 400, epoch: 13 | loss: 0.0124032
	speed: 0.1157s/iter; left time: 2466.0145s
	iters: 500, epoch: 13 | loss: 0.0202020
	speed: 0.1440s/iter; left time: 3053.7562s
	iters: 600, epoch: 13 | loss: 0.0089123
	speed: 0.1130s/iter; left time: 2384.9065s
Epoch: 13 cost time: 00h:01m:21.63s
Epoch: 13 | Train Loss: 0.0141540 Vali Loss: 0.0096531 Test Loss: 0.0106672
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0114304
	speed: 0.3491s/iter; left time: 6594.9007s
	iters: 200, epoch: 14 | loss: 0.0137170
	speed: 0.1124s/iter; left time: 2112.7172s
	iters: 300, epoch: 14 | loss: 0.0187478
	speed: 0.1121s/iter; left time: 2094.7089s
	iters: 400, epoch: 14 | loss: 0.0149776
	speed: 0.1129s/iter; left time: 2098.6591s
	iters: 500, epoch: 14 | loss: 0.0118570
	speed: 0.1129s/iter; left time: 2087.9124s
	iters: 600, epoch: 14 | loss: 0.0176578
	speed: 0.1202s/iter; left time: 2210.9154s
Epoch: 14 cost time: 00h:01m:26.06s
Epoch: 14 | Train Loss: 0.0141004 Vali Loss: 0.0095795 Test Loss: 0.0105532
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0244140
	speed: 0.7338s/iter; left time: 11872.0227s
	iters: 200, epoch: 15 | loss: 0.0115425
	speed: 0.2378s/iter; left time: 3822.9631s
	iters: 300, epoch: 15 | loss: 0.0092068
	speed: 0.2277s/iter; left time: 3638.7324s
	iters: 400, epoch: 15 | loss: 0.0166868
	speed: 0.1126s/iter; left time: 1788.1649s
	iters: 500, epoch: 15 | loss: 0.0117777
	speed: 0.1133s/iter; left time: 1787.3895s
	iters: 600, epoch: 15 | loss: 0.0119113
	speed: 0.1391s/iter; left time: 2180.5823s
Epoch: 15 cost time: 00h:02m:05.31s
Epoch: 15 | Train Loss: 0.0141697 Vali Loss: 0.0096087 Test Loss: 0.0106601
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0134954
	speed: 0.7404s/iter; left time: 9970.3791s
	iters: 200, epoch: 16 | loss: 0.0110016
	speed: 0.2370s/iter; left time: 3167.8364s
	iters: 300, epoch: 16 | loss: 0.0136263
	speed: 0.2361s/iter; left time: 3132.1143s
	iters: 400, epoch: 16 | loss: 0.0144925
	speed: 0.2337s/iter; left time: 3076.5303s
	iters: 500, epoch: 16 | loss: 0.0162292
	speed: 0.2369s/iter; left time: 3095.5136s
	iters: 600, epoch: 16 | loss: 0.0148276
	speed: 0.2366s/iter; left time: 3067.8407s
Epoch: 16 cost time: 00h:02m:40.38s
Epoch: 16 | Train Loss: 0.0135790 Vali Loss: 0.0096248 Test Loss: 0.0105937
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0132713
	speed: 0.7458s/iter; left time: 8020.0443s
	iters: 200, epoch: 17 | loss: 0.0123942
	speed: 0.2335s/iter; left time: 2487.1403s
	iters: 300, epoch: 17 | loss: 0.0115208
	speed: 0.2319s/iter; left time: 2447.3100s
	iters: 400, epoch: 17 | loss: 0.0120183
	speed: 0.2297s/iter; left time: 2401.3973s
	iters: 500, epoch: 17 | loss: 0.0133054
	speed: 0.2337s/iter; left time: 2419.7483s
	iters: 600, epoch: 17 | loss: 0.0104209
	speed: 0.2316s/iter; left time: 2374.2674s
Epoch: 17 cost time: 00h:02m:37.98s
Epoch: 17 | Train Loss: 0.0135725 Vali Loss: 0.0096924 Test Loss: 0.0105969
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871
Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871
Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871
Scaled mse:0.01045023463666439, rmse:0.1022263914346695, mae:0.06180146336555481, rse:0.3870426118373871
Intermediate time for IT and pred_len 24: 00h:37m:06.66s


=== Starting experiments for pred_len: 96 ===

[2024-11-13 16:52:55,838] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:52:55,845] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:52:55,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:52:55,864] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 16:52:56,642] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:52:56,643] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:52:56,645] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 16:52:56,645] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 16:52:56,645] [INFO] [comm.py:637:init_distributed] cdb=None
train 86619
val 18435
test 18435
train 86619
train 86619
val 18435
val 18435
test 18435
test 18435
train 86619
val 18435
test 18435
[2024-11-13 16:52:58,918] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 16:53:00,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 16:53:00,360] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 16:53:00,360] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 16:53:00,361] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 16:53:00,361] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 16:53:00,361] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 16:53:00,361] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 16:53:00,685] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 16:53:00,686] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB 
[2024-11-13 16:53:00,686] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.54 GB, percent = 3.7%
[2024-11-13 16:53:00,927] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 16:53:00,927] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 16:53:00,927] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.73 GB, percent = 3.7%
[2024-11-13 16:53:00,928] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 16:53:01,031] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 16:53:01,031] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 16:53:01,031] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 38.05 GB, percent = 3.8%
[2024-11-13 16:53:01,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 16:53:01,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 16:53:01,032] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6b92dfdb50>
[2024-11-13 16:53:01,032] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 16:53:01,033] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 16:53:01,034] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0709942
	speed: 0.2675s/iter; left time: 14451.6049s
	iters: 200, epoch: 1 | loss: 0.0518521
	speed: 0.2367s/iter; left time: 12762.4700s
	iters: 300, epoch: 1 | loss: 0.0491614
	speed: 0.1374s/iter; left time: 7395.6174s
	iters: 400, epoch: 1 | loss: 0.0385732
	speed: 0.1238s/iter; left time: 6651.0406s
	iters: 500, epoch: 1 | loss: 0.0327860
	speed: 0.1286s/iter; left time: 6894.0485s
	iters: 600, epoch: 1 | loss: 0.0372788
	speed: 0.1703s/iter; left time: 9116.7871s
Epoch: 1 cost time: 00h:01m:57.54s
Epoch: 1 | Train Loss: 0.0561526 Vali Loss: 0.0237931 Test Loss: 0.0257605
Validation loss decreased (inf --> 0.023793).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0234595
	speed: 0.7463s/iter; left time: 38298.2729s
	iters: 200, epoch: 2 | loss: 0.0231870
	speed: 0.1229s/iter; left time: 6294.9837s
	iters: 300, epoch: 2 | loss: 0.0352537
	speed: 0.1219s/iter; left time: 6229.9994s
	iters: 400, epoch: 2 | loss: 0.0295735
	speed: 0.1259s/iter; left time: 6423.6052s
	iters: 500, epoch: 2 | loss: 0.0283380
	speed: 0.1235s/iter; left time: 6290.3216s
	iters: 600, epoch: 2 | loss: 0.0225025
	speed: 0.1210s/iter; left time: 6149.3929s
Epoch: 2 cost time: 00h:01m:24.28s
Epoch: 2 | Train Loss: 0.0272431 Vali Loss: 0.0194962 Test Loss: 0.0212060
Validation loss decreased (0.023793 --> 0.019496).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0334998
	speed: 0.4813s/iter; left time: 23396.3363s
	iters: 200, epoch: 3 | loss: 0.0210327
	speed: 0.1165s/iter; left time: 5652.7593s
	iters: 300, epoch: 3 | loss: 0.0221955
	speed: 0.1197s/iter; left time: 5792.7736s
	iters: 400, epoch: 3 | loss: 0.0250691
	speed: 0.1204s/iter; left time: 5818.3800s
	iters: 500, epoch: 3 | loss: 0.0253366
	speed: 0.1229s/iter; left time: 5923.6119s
	iters: 600, epoch: 3 | loss: 0.0158106
	speed: 0.1185s/iter; left time: 5699.8039s
Epoch: 3 cost time: 00h:01m:22.30s
Epoch: 3 | Train Loss: 0.0251084 Vali Loss: 0.0182750 Test Loss: 0.0199267
Validation loss decreased (0.019496 --> 0.018275).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0260743
	speed: 0.4879s/iter; left time: 22398.2073s
	iters: 200, epoch: 4 | loss: 0.0281503
	speed: 0.1219s/iter; left time: 5584.0512s
	iters: 300, epoch: 4 | loss: 0.0267283
	speed: 0.1181s/iter; left time: 5395.4331s
	iters: 400, epoch: 4 | loss: 0.0361682
	speed: 0.1251s/iter; left time: 5706.8554s
	iters: 500, epoch: 4 | loss: 0.0227277
	speed: 0.1187s/iter; left time: 5402.3643s
	iters: 600, epoch: 4 | loss: 0.0195750
	speed: 0.1200s/iter; left time: 5448.5430s
Epoch: 4 cost time: 00h:01m:22.58s
Epoch: 4 | Train Loss: 0.0242923 Vali Loss: 0.0177414 Test Loss: 0.0192689
Validation loss decreased (0.018275 --> 0.017741).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0244805
	speed: 0.4993s/iter; left time: 21567.4749s
	iters: 200, epoch: 5 | loss: 0.0192584
	speed: 0.1186s/iter; left time: 5109.9401s
	iters: 300, epoch: 5 | loss: 0.0236494
	speed: 0.1199s/iter; left time: 5157.2703s
	iters: 400, epoch: 5 | loss: 0.0253007
	speed: 0.1219s/iter; left time: 5230.7889s
	iters: 500, epoch: 5 | loss: 0.0333484
	speed: 0.1171s/iter; left time: 5012.2101s
	iters: 600, epoch: 5 | loss: 0.0208841
	speed: 0.1211s/iter; left time: 5171.9480s
Epoch: 5 cost time: 00h:01m:21.83s
Epoch: 5 | Train Loss: 0.0233276 Vali Loss: 0.0176504 Test Loss: 0.0191216
Validation loss decreased (0.017741 --> 0.017650).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0227093
	speed: 0.4187s/iter; left time: 16954.2441s
	iters: 200, epoch: 6 | loss: 0.0254766
	speed: 0.1250s/iter; left time: 5048.4486s
	iters: 300, epoch: 6 | loss: 0.0200936
	speed: 0.1291s/iter; left time: 5200.1711s
	iters: 400, epoch: 6 | loss: 0.0175824
	speed: 0.1286s/iter; left time: 5170.3003s
	iters: 500, epoch: 6 | loss: 0.0142362
	speed: 0.1200s/iter; left time: 4810.0125s
	iters: 600, epoch: 6 | loss: 0.0185892
	speed: 0.1205s/iter; left time: 4818.6293s
Epoch: 6 cost time: 00h:01m:24.31s
Epoch: 6 | Train Loss: 0.0230509 Vali Loss: 0.0173448 Test Loss: 0.0188160
Validation loss decreased (0.017650 --> 0.017345).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0263479
	speed: 0.4576s/iter; left time: 17290.5216s
	iters: 200, epoch: 7 | loss: 0.0225599
	speed: 0.1180s/iter; left time: 4446.4617s
	iters: 300, epoch: 7 | loss: 0.0173663
	speed: 0.1153s/iter; left time: 4331.9419s
	iters: 400, epoch: 7 | loss: 0.0211402
	speed: 0.1182s/iter; left time: 4429.8356s
	iters: 500, epoch: 7 | loss: 0.0260078
	speed: 0.1189s/iter; left time: 4446.4647s
	iters: 600, epoch: 7 | loss: 0.0225444
	speed: 0.1210s/iter; left time: 4512.1696s
Epoch: 7 cost time: 00h:01m:21.13s
Epoch: 7 | Train Loss: 0.0225729 Vali Loss: 0.0170060 Test Loss: 0.0185562
Validation loss decreased (0.017345 --> 0.017006).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0153552
	speed: 0.4092s/iter; left time: 14354.4146s
	iters: 200, epoch: 8 | loss: 0.0211704
	speed: 0.1724s/iter; left time: 6029.4184s
	iters: 300, epoch: 8 | loss: 0.0244371
	speed: 0.2222s/iter; left time: 7748.8142s
	iters: 400, epoch: 8 | loss: 0.0261077
	speed: 0.2157s/iter; left time: 7501.7075s
	iters: 500, epoch: 8 | loss: 0.0245105
	speed: 0.2178s/iter; left time: 7553.2370s
	iters: 600, epoch: 8 | loss: 0.0276707
	speed: 0.2154s/iter; left time: 7448.6495s
Epoch: 8 cost time: 00h:02m:14.06s
Epoch: 8 | Train Loss: 0.0222899 Vali Loss: 0.0168095 Test Loss: 0.0182955
Validation loss decreased (0.017006 --> 0.016809).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0243195
	speed: 0.7627s/iter; left time: 24691.0131s
	iters: 200, epoch: 9 | loss: 0.0280231
	speed: 0.2126s/iter; left time: 6862.3340s
	iters: 300, epoch: 9 | loss: 0.0226743
	speed: 0.2131s/iter; left time: 6857.4477s
	iters: 400, epoch: 9 | loss: 0.0140769
	speed: 0.2189s/iter; left time: 7022.1264s
	iters: 500, epoch: 9 | loss: 0.0240373
	speed: 0.2192s/iter; left time: 7009.5796s
	iters: 600, epoch: 9 | loss: 0.0273113
	speed: 0.2219s/iter; left time: 7073.5384s
Epoch: 9 cost time: 00h:02m:27.67s
Epoch: 9 | Train Loss: 0.0220465 Vali Loss: 0.0168548 Test Loss: 0.0183864
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0290507
	speed: 0.6796s/iter; left time: 20160.5646s
	iters: 200, epoch: 10 | loss: 0.0204478
	speed: 0.2147s/iter; left time: 6347.7927s
	iters: 300, epoch: 10 | loss: 0.0142628
	speed: 0.2125s/iter; left time: 6262.7020s
	iters: 400, epoch: 10 | loss: 0.0201488
	speed: 0.2188s/iter; left time: 6426.1579s
	iters: 500, epoch: 10 | loss: 0.0202513
	speed: 0.2186s/iter; left time: 6398.3859s
	iters: 600, epoch: 10 | loss: 0.0239269
	speed: 0.2192s/iter; left time: 6394.0182s
Epoch: 10 cost time: 00h:02m:26.21s
Epoch: 10 | Train Loss: 0.0217414 Vali Loss: 0.0165702 Test Loss: 0.0181968
Validation loss decreased (0.016809 --> 0.016570).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0212407
	speed: 0.6567s/iter; left time: 17706.0212s
	iters: 200, epoch: 11 | loss: 0.0194598
	speed: 0.2097s/iter; left time: 5633.4177s
	iters: 300, epoch: 11 | loss: 0.0158257
	speed: 0.2173s/iter; left time: 5815.0829s
	iters: 400, epoch: 11 | loss: 0.0238603
	speed: 0.2041s/iter; left time: 5440.3537s
	iters: 500, epoch: 11 | loss: 0.0193602
	speed: 0.2202s/iter; left time: 5847.5737s
	iters: 600, epoch: 11 | loss: 0.0151740
	speed: 0.2187s/iter; left time: 5786.3479s
Epoch: 11 cost time: 00h:02m:26.18s
Epoch: 11 | Train Loss: 0.0212928 Vali Loss: 0.0166454 Test Loss: 0.0182453
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0265997
	speed: 0.6909s/iter; left time: 16757.9608s
	iters: 200, epoch: 12 | loss: 0.0186985
	speed: 0.2091s/iter; left time: 5051.7166s
	iters: 300, epoch: 12 | loss: 0.0248060
	speed: 0.2113s/iter; left time: 5082.9155s
	iters: 400, epoch: 12 | loss: 0.0258594
	speed: 0.2168s/iter; left time: 5192.7651s
	iters: 500, epoch: 12 | loss: 0.0200538
	speed: 0.2043s/iter; left time: 4874.1620s
	iters: 600, epoch: 12 | loss: 0.0228923
	speed: 0.2180s/iter; left time: 5179.5704s
Epoch: 12 cost time: 00h:02m:25.85s
Epoch: 12 | Train Loss: 0.0211033 Vali Loss: 0.0167095 Test Loss: 0.0181806
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0230677
	speed: 0.6708s/iter; left time: 14454.3561s
	iters: 200, epoch: 13 | loss: 0.0197223
	speed: 0.2213s/iter; left time: 4747.2691s
	iters: 300, epoch: 13 | loss: 0.0230520
	speed: 0.1514s/iter; left time: 3231.8059s
	iters: 400, epoch: 13 | loss: 0.0240007
	speed: 0.2040s/iter; left time: 4335.0122s
	iters: 500, epoch: 13 | loss: 0.0168106
	speed: 0.2147s/iter; left time: 4540.0864s
	iters: 600, epoch: 13 | loss: 0.0218856
	speed: 0.2101s/iter; left time: 4421.9463s
Epoch: 13 cost time: 00h:02m:16.74s
Epoch: 13 | Train Loss: 0.0207625 Vali Loss: 0.0165105 Test Loss: 0.0183249
Validation loss decreased (0.016570 --> 0.016510).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0177562
	speed: 0.7061s/iter; left time: 13305.5459s
	iters: 200, epoch: 14 | loss: 0.0215960
	speed: 0.2118s/iter; left time: 3969.7001s
	iters: 300, epoch: 14 | loss: 0.0218200
	speed: 0.2135s/iter; left time: 3979.7274s
	iters: 400, epoch: 14 | loss: 0.0114039
	speed: 0.2121s/iter; left time: 3933.2258s
	iters: 500, epoch: 14 | loss: 0.0204489
	speed: 0.2079s/iter; left time: 3833.5743s
	iters: 600, epoch: 14 | loss: 0.0184212
	speed: 0.2134s/iter; left time: 3913.6305s
Epoch: 14 cost time: 00h:02m:23.16s
Epoch: 14 | Train Loss: 0.0207159 Vali Loss: 0.0182826 Test Loss: 0.0195461
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0221164
	speed: 0.6640s/iter; left time: 10714.8964s
	iters: 200, epoch: 15 | loss: 0.0162195
	speed: 0.2127s/iter; left time: 3410.8746s
	iters: 300, epoch: 15 | loss: 0.0149570
	speed: 0.2117s/iter; left time: 3373.5503s
	iters: 400, epoch: 15 | loss: 0.0151238
	speed: 0.2130s/iter; left time: 3373.4287s
	iters: 500, epoch: 15 | loss: 0.0271093
	speed: 0.1996s/iter; left time: 3140.7084s
	iters: 600, epoch: 15 | loss: 0.0163410
	speed: 0.2117s/iter; left time: 3310.1664s
Epoch: 15 cost time: 00h:02m:24.32s
Epoch: 15 | Train Loss: 0.0203983 Vali Loss: 0.0172118 Test Loss: 0.0190688
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0171180
	speed: 0.6457s/iter; left time: 8672.5662s
	iters: 200, epoch: 16 | loss: 0.0223030
	speed: 0.2108s/iter; left time: 2809.8969s
	iters: 300, epoch: 16 | loss: 0.0295624
	speed: 0.2022s/iter; left time: 2675.2776s
	iters: 400, epoch: 16 | loss: 0.0213049
	speed: 0.2116s/iter; left time: 2778.1906s
	iters: 500, epoch: 16 | loss: 0.0199282
	speed: 0.2079s/iter; left time: 2709.1071s
	iters: 600, epoch: 16 | loss: 0.0195881
	speed: 0.2087s/iter; left time: 2699.0198s
Epoch: 16 cost time: 00h:02m:20.05s
Epoch: 16 | Train Loss: 0.0203052 Vali Loss: 0.0174881 Test Loss: 0.0194449
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0297713
	speed: 0.7365s/iter; left time: 7898.8086s
	iters: 200, epoch: 17 | loss: 0.0205002
	speed: 0.2717s/iter; left time: 2886.5750s
	iters: 300, epoch: 17 | loss: 0.0108258
	speed: 0.2295s/iter; left time: 2415.2405s
	iters: 400, epoch: 17 | loss: 0.0248199
	speed: 0.2075s/iter; left time: 2163.3793s
	iters: 500, epoch: 17 | loss: 0.0154806
	speed: 0.2051s/iter; left time: 2117.7817s
	iters: 600, epoch: 17 | loss: 0.0166328
	speed: 0.1398s/iter; left time: 1429.2783s
Epoch: 17 cost time: 00h:02m:28.40s
Epoch: 17 | Train Loss: 0.0199837 Vali Loss: 0.0173948 Test Loss: 0.0190550
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0193233
	speed: 0.7304s/iter; left time: 5856.9636s
	iters: 200, epoch: 18 | loss: 0.0147264
	speed: 0.2415s/iter; left time: 1912.2206s
	iters: 300, epoch: 18 | loss: 0.0189813
	speed: 0.2400s/iter; left time: 1876.4391s
	iters: 400, epoch: 18 | loss: 0.0168320
	speed: 0.2412s/iter; left time: 1861.7095s
	iters: 500, epoch: 18 | loss: 0.0130358
	speed: 0.2393s/iter; left time: 1823.5378s
	iters: 600, epoch: 18 | loss: 0.0204194
	speed: 0.2425s/iter; left time: 1823.0238s
Epoch: 18 cost time: 00h:02m:42.58s
Epoch: 18 | Train Loss: 0.0197306 Vali Loss: 0.0180781 Test Loss: 0.0192325
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355
Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355
Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355
Scaled mse:0.01832490973174572, rmse:0.13536952435970306, mae:0.08470133692026138, rse:0.5118632316589355
Intermediate time for IT and pred_len 96: 00h:46m:40.98s


=== Starting experiments for pred_len: 168 ===

[2024-11-13 17:39:17,127] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 17:39:17,127] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 17:39:17,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 17:39:17,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-13 17:39:18,128] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 17:39:18,128] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-13 17:39:18,129] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 17:39:18,129] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 17:39:18,130] [INFO] [comm.py:637:init_distributed] cdb=None
train 86403
val 18219
test 18219
train 86403
train 86403
train 86403
val 18219
val 18219
val 18219
test 18219
test 18219
test 18219
[2024-11-13 17:39:20,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-13 17:39:21,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-13 17:39:21,203] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-13 17:39:21,203] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-13 17:39:21,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-13 17:39:21,204] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-13 17:39:21,204] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-13 17:39:21,204] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-13 17:39:21,658] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-13 17:39:21,659] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.44 GB         Max_CA 0 GB 
[2024-11-13 17:39:21,660] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.58 GB, percent = 3.7%
[2024-11-13 17:39:21,850] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-13 17:39:21,850] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 17:39:21,851] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.75 GB, percent = 3.7%
[2024-11-13 17:39:21,851] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-13 17:39:21,983] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-13 17:39:21,984] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-13 17:39:21,984] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 37.94 GB, percent = 3.8%
[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-13 17:39:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-13 17:39:21,985] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7eff603e6490>
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-13 17:39:21,986] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-13 17:39:21,987] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-13 17:39:21,988] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-13 17:39:21,988] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0616591
	speed: 0.2700s/iter; left time: 14553.1081s
	iters: 200, epoch: 1 | loss: 0.0595522
	speed: 0.2382s/iter; left time: 12813.9867s
	iters: 300, epoch: 1 | loss: 0.0564701
	speed: 0.2408s/iter; left time: 12931.2314s
	iters: 400, epoch: 1 | loss: 0.0572593
	speed: 0.1235s/iter; left time: 6617.4790s
	iters: 500, epoch: 1 | loss: 0.0395582
	speed: 0.1277s/iter; left time: 6830.5201s
	iters: 600, epoch: 1 | loss: 0.0293994
	speed: 0.1255s/iter; left time: 6699.8988s
Epoch: 1 cost time: 00h:02m:00.18s
Epoch: 1 | Train Loss: 0.0605154 Vali Loss: 0.0258984 Test Loss: 0.0277663
Validation loss decreased (inf --> 0.025898).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0331507
	speed: 0.5792s/iter; left time: 29655.7831s
	iters: 200, epoch: 2 | loss: 0.0272233
	speed: 0.1267s/iter; left time: 6476.2187s
	iters: 300, epoch: 2 | loss: 0.0330929
	speed: 0.1267s/iter; left time: 6459.7128s
	iters: 400, epoch: 2 | loss: 0.0377437
	speed: 0.1238s/iter; left time: 6300.5302s
	iters: 500, epoch: 2 | loss: 0.0229581
	speed: 0.1251s/iter; left time: 6353.2610s
	iters: 600, epoch: 2 | loss: 0.0337943
	speed: 0.1267s/iter; left time: 6423.8079s
Epoch: 2 cost time: 00h:01m:25.70s
Epoch: 2 | Train Loss: 0.0292056 Vali Loss: 0.0210110 Test Loss: 0.0223801
Validation loss decreased (0.025898 --> 0.021011).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0387124
	speed: 0.4440s/iter; left time: 21536.7097s
	iters: 200, epoch: 3 | loss: 0.0326666
	speed: 0.1246s/iter; left time: 6032.1983s
	iters: 300, epoch: 3 | loss: 0.0301178
	speed: 0.1256s/iter; left time: 6065.3144s
	iters: 400, epoch: 3 | loss: 0.0209178
	speed: 0.1285s/iter; left time: 6194.7146s
	iters: 500, epoch: 3 | loss: 0.0242972
	speed: 0.2385s/iter; left time: 11474.1226s
	iters: 600, epoch: 3 | loss: 0.0246501
	speed: 0.2455s/iter; left time: 11784.9761s
Epoch: 3 cost time: 00h:01m:55.53s
Epoch: 3 | Train Loss: 0.0270320 Vali Loss: 0.0202293 Test Loss: 0.0216505
Validation loss decreased (0.021011 --> 0.020229).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0288879
	speed: 0.5225s/iter; left time: 23931.2974s
	iters: 200, epoch: 4 | loss: 0.0373396
	speed: 0.1209s/iter; left time: 5523.5512s
	iters: 300, epoch: 4 | loss: 0.0252346
	speed: 0.1254s/iter; left time: 5720.4639s
	iters: 400, epoch: 4 | loss: 0.0202597
	speed: 0.1216s/iter; left time: 5532.5119s
	iters: 500, epoch: 4 | loss: 0.0241824
	speed: 0.1230s/iter; left time: 5584.1586s
	iters: 600, epoch: 4 | loss: 0.0204294
	speed: 0.1244s/iter; left time: 5635.1281s
Epoch: 4 cost time: 00h:01m:23.79s
Epoch: 4 | Train Loss: 0.0258587 Vali Loss: 0.0196277 Test Loss: 0.0208590
Validation loss decreased (0.020229 --> 0.019628).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0271386
	speed: 0.4502s/iter; left time: 19404.8022s
	iters: 200, epoch: 5 | loss: 0.0153049
	speed: 0.1199s/iter; left time: 5155.5849s
	iters: 300, epoch: 5 | loss: 0.0206532
	speed: 0.1258s/iter; left time: 5398.8703s
	iters: 400, epoch: 5 | loss: 0.0219328
	speed: 0.1224s/iter; left time: 5237.7481s
	iters: 500, epoch: 5 | loss: 0.0251162
	speed: 0.1284s/iter; left time: 5482.3793s
	iters: 600, epoch: 5 | loss: 0.0258013
	speed: 0.1208s/iter; left time: 5144.9730s
Epoch: 5 cost time: 00h:01m:24.07s
Epoch: 5 | Train Loss: 0.0249864 Vali Loss: 0.0190997 Test Loss: 0.0204004
Validation loss decreased (0.019628 --> 0.019100).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0198722
	speed: 0.4405s/iter; left time: 17797.1756s
	iters: 200, epoch: 6 | loss: 0.0364575
	speed: 0.1207s/iter; left time: 4865.7212s
	iters: 300, epoch: 6 | loss: 0.0198864
	speed: 0.1240s/iter; left time: 4985.6761s
	iters: 400, epoch: 6 | loss: 0.0321605
	speed: 0.1205s/iter; left time: 4831.8809s
	iters: 500, epoch: 6 | loss: 0.0232292
	speed: 0.1178s/iter; left time: 4711.0001s
	iters: 600, epoch: 6 | loss: 0.0303167
	speed: 0.1210s/iter; left time: 4827.4800s
Epoch: 6 cost time: 00h:01m:22.71s
Epoch: 6 | Train Loss: 0.0245004 Vali Loss: 0.0188114 Test Loss: 0.0201435
Validation loss decreased (0.019100 --> 0.018811).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0226749
	speed: 0.4422s/iter; left time: 16670.1691s
	iters: 200, epoch: 7 | loss: 0.0304455
	speed: 0.1325s/iter; left time: 4980.8824s
	iters: 300, epoch: 7 | loss: 0.0270933
	speed: 0.2501s/iter; left time: 9379.7193s
	iters: 400, epoch: 7 | loss: 0.0280105
	speed: 0.2466s/iter; left time: 9223.4111s
	iters: 500, epoch: 7 | loss: 0.0284047
	speed: 0.2431s/iter; left time: 9066.0577s
	iters: 600, epoch: 7 | loss: 0.0226332
	speed: 0.2450s/iter; left time: 9113.1425s
Epoch: 7 cost time: 00h:02m:23.13s
Epoch: 7 | Train Loss: 0.0242849 Vali Loss: 0.0186533 Test Loss: 0.0199403
Validation loss decreased (0.018811 --> 0.018653).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0290103
	speed: 0.8248s/iter; left time: 28869.1663s
	iters: 200, epoch: 8 | loss: 0.0232801
	speed: 0.2372s/iter; left time: 8279.5518s
	iters: 300, epoch: 8 | loss: 0.0265249
	speed: 0.2419s/iter; left time: 8417.1788s
	iters: 400, epoch: 8 | loss: 0.0232811
	speed: 0.2404s/iter; left time: 8343.2988s
	iters: 500, epoch: 8 | loss: 0.0170798
	speed: 0.2440s/iter; left time: 8443.1287s
	iters: 600, epoch: 8 | loss: 0.0341334
	speed: 0.2494s/iter; left time: 8605.0228s
Epoch: 8 cost time: 00h:02m:45.39s
Epoch: 8 | Train Loss: 0.0239119 Vali Loss: 0.0185142 Test Loss: 0.0197774
Validation loss decreased (0.018653 --> 0.018514).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0226050
	speed: 0.6948s/iter; left time: 22443.8715s
	iters: 200, epoch: 9 | loss: 0.0330355
	speed: 0.1218s/iter; left time: 3922.8665s
	iters: 300, epoch: 9 | loss: 0.0210071
	speed: 0.1257s/iter; left time: 4036.4820s
	iters: 400, epoch: 9 | loss: 0.0231865
	speed: 0.1210s/iter; left time: 3873.0008s
	iters: 500, epoch: 9 | loss: 0.0199373
	speed: 0.1208s/iter; left time: 3854.8738s
	iters: 600, epoch: 9 | loss: 0.0160295
	speed: 0.1241s/iter; left time: 3947.7961s
Epoch: 9 cost time: 00h:01m:23.65s
Epoch: 9 | Train Loss: 0.0235146 Vali Loss: 0.0181856 Test Loss: 0.0194546
Validation loss decreased (0.018514 --> 0.018186).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0233166
	speed: 0.4351s/iter; left time: 12879.2191s
	iters: 200, epoch: 10 | loss: 0.0208000
	speed: 0.1219s/iter; left time: 3597.2194s
	iters: 300, epoch: 10 | loss: 0.0231443
	speed: 0.1233s/iter; left time: 3625.8678s
	iters: 400, epoch: 10 | loss: 0.0267396
	speed: 0.1830s/iter; left time: 5361.3541s
	iters: 500, epoch: 10 | loss: 0.0223235
	speed: 0.1707s/iter; left time: 4984.7580s
	iters: 600, epoch: 10 | loss: 0.0260408
	speed: 0.1262s/iter; left time: 3673.8941s
Epoch: 10 cost time: 00h:01m:34.58s
Epoch: 10 | Train Loss: 0.0231291 Vali Loss: 0.0183109 Test Loss: 0.0195542
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0200581
	speed: 0.3753s/iter; left time: 10094.7683s
	iters: 200, epoch: 11 | loss: 0.0248631
	speed: 0.1192s/iter; left time: 3194.5035s
	iters: 300, epoch: 11 | loss: 0.0240047
	speed: 0.1242s/iter; left time: 3317.1796s
	iters: 400, epoch: 11 | loss: 0.0290795
	speed: 0.1195s/iter; left time: 3178.4241s
	iters: 500, epoch: 11 | loss: 0.0240012
	speed: 0.1180s/iter; left time: 3128.3767s
	iters: 600, epoch: 11 | loss: 0.0178534
	speed: 0.1214s/iter; left time: 3204.8893s
Epoch: 11 cost time: 00h:01m:21.85s
Epoch: 11 | Train Loss: 0.0227248 Vali Loss: 0.0180482 Test Loss: 0.0191756
Validation loss decreased (0.018186 --> 0.018048).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0224432
	speed: 0.4454s/iter; left time: 10780.3132s
	iters: 200, epoch: 12 | loss: 0.0273953
	speed: 0.1234s/iter; left time: 2975.1092s
	iters: 300, epoch: 12 | loss: 0.0173645
	speed: 0.1205s/iter; left time: 2892.8861s
	iters: 400, epoch: 12 | loss: 0.0313485
	speed: 0.1225s/iter; left time: 2929.0562s
	iters: 500, epoch: 12 | loss: 0.0260381
	speed: 0.1227s/iter; left time: 2920.5502s
	iters: 600, epoch: 12 | loss: 0.0249483
	speed: 0.1218s/iter; left time: 2886.9367s
Epoch: 12 cost time: 00h:01m:23.00s
Epoch: 12 | Train Loss: 0.0226154 Vali Loss: 0.0181090 Test Loss: 0.0194397
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0240247
	speed: 0.3829s/iter; left time: 8232.4275s
	iters: 200, epoch: 13 | loss: 0.0206983
	speed: 0.1205s/iter; left time: 2578.1292s
	iters: 300, epoch: 13 | loss: 0.0213481
	speed: 0.1215s/iter; left time: 2587.3213s
	iters: 400, epoch: 13 | loss: 0.0253885
	speed: 0.1225s/iter; left time: 2596.3894s
	iters: 500, epoch: 13 | loss: 0.0272860
	speed: 0.1210s/iter; left time: 2552.6524s
	iters: 600, epoch: 13 | loss: 0.0219219
	speed: 0.1207s/iter; left time: 2535.0586s
Epoch: 13 cost time: 00h:01m:23.19s
Epoch: 13 | Train Loss: 0.0222798 Vali Loss: 0.0179139 Test Loss: 0.0195298
Validation loss decreased (0.018048 --> 0.017914).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0212308
	speed: 0.5432s/iter; left time: 10212.0143s
	iters: 200, epoch: 14 | loss: 0.0247572
	speed: 0.1216s/iter; left time: 2273.5017s
	iters: 300, epoch: 14 | loss: 0.0236004
	speed: 0.1241s/iter; left time: 2309.2169s
	iters: 400, epoch: 14 | loss: 0.0279050
	speed: 0.1209s/iter; left time: 2236.0902s
	iters: 500, epoch: 14 | loss: 0.0250699
	speed: 0.1224s/iter; left time: 2252.2742s
	iters: 600, epoch: 14 | loss: 0.0282776
	speed: 0.1198s/iter; left time: 2192.7360s
Epoch: 14 cost time: 00h:01m:22.78s
Epoch: 14 | Train Loss: 0.0223627 Vali Loss: 0.0181161 Test Loss: 0.0196190
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0230317
	speed: 0.4702s/iter; left time: 7570.3239s
	iters: 200, epoch: 15 | loss: 0.0248076
	speed: 0.1533s/iter; left time: 2452.9333s
	iters: 300, epoch: 15 | loss: 0.0263772
	speed: 0.1542s/iter; left time: 2452.1800s
	iters: 400, epoch: 15 | loss: 0.0261949
	speed: 0.1520s/iter; left time: 2401.2823s
	iters: 500, epoch: 15 | loss: 0.0259247
	speed: 0.1575s/iter; left time: 2473.2455s
	iters: 600, epoch: 15 | loss: 0.0233346
	speed: 0.1540s/iter; left time: 2402.2484s
Epoch: 15 cost time: 00h:01m:45.36s
Epoch: 15 | Train Loss: 0.0219749 Vali Loss: 0.0182892 Test Loss: 0.0197237
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0221001
	speed: 0.4765s/iter; left time: 6385.1486s
	iters: 200, epoch: 16 | loss: 0.0218049
	speed: 0.1535s/iter; left time: 2042.3210s
	iters: 300, epoch: 16 | loss: 0.0204753
	speed: 0.1509s/iter; left time: 1992.1280s
	iters: 400, epoch: 16 | loss: 0.0198632
	speed: 0.1507s/iter; left time: 1974.8548s
	iters: 500, epoch: 16 | loss: 0.0186507
	speed: 0.1335s/iter; left time: 1735.6644s
	iters: 600, epoch: 16 | loss: 0.0270961
	speed: 0.1318s/iter; left time: 1699.9723s
Epoch: 16 cost time: 00h:01m:37.97s
Epoch: 16 | Train Loss: 0.0218581 Vali Loss: 0.0179782 Test Loss: 0.0199283
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0198847
	speed: 0.3962s/iter; left time: 4239.3411s
	iters: 200, epoch: 17 | loss: 0.0139290
	speed: 0.1193s/iter; left time: 1264.2036s
	iters: 300, epoch: 17 | loss: 0.0195014
	speed: 0.1244s/iter; left time: 1306.5843s
	iters: 400, epoch: 17 | loss: 0.0257014
	speed: 0.1209s/iter; left time: 1257.9345s
	iters: 500, epoch: 17 | loss: 0.0204623
	speed: 0.1240s/iter; left time: 1277.8212s
	iters: 600, epoch: 17 | loss: 0.0272740
	speed: 0.1222s/iter; left time: 1246.8299s
Epoch: 17 cost time: 00h:01m:23.40s
Epoch: 17 | Train Loss: 0.0217688 Vali Loss: 0.0184210 Test Loss: 0.0201249
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0292499
	speed: 0.4253s/iter; left time: 3402.6861s
	iters: 200, epoch: 18 | loss: 0.0208086
	speed: 0.1403s/iter; left time: 1108.1858s
	iters: 300, epoch: 18 | loss: 0.0191685
	speed: 0.1488s/iter; left time: 1160.8442s
	iters: 400, epoch: 18 | loss: 0.0194677
	speed: 0.1688s/iter; left time: 1300.1497s
	iters: 500, epoch: 18 | loss: 0.0234823
	speed: 0.1891s/iter; left time: 1437.6629s
	iters: 600, epoch: 18 | loss: 0.0240304
	speed: 0.1357s/iter; left time: 1018.2579s
Epoch: 18 cost time: 00h:01m:43.71s
Epoch: 18 | Train Loss: 0.0211778 Vali Loss: 0.0183267 Test Loss: 0.0202651
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603
Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603
Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603
Scaled mse:0.01952982507646084, rmse:0.13974915444850922, mae:0.08981484919786453, rse:0.5288854837417603
Intermediate time for IT and pred_len 168: 00h:37m:21.16s

Intermediate time for IT: 02h:01m:08.80s

Total time: 08h:27m:12.89s

