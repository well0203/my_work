
=== Starting experiments for country: FR ===

=== Starting experiments for pred_len: 24 ===

--- Running model for FR, pred_len=24 ---
[2024-11-14 13:57:59,560] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:57:59,571] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-14 13:58:03,477] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-14 13:58:03,477] [INFO] [comm.py:637:init_distributed] cdb=None
train 86835
val 18651
test 18651
train 86835
train 86835
train 86835
val 18651
val 18651
test 18651
val 18651
test 18651
test 18651
[2024-11-14 13:58:08,639] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-14 13:58:10,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-14 13:58:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-14 13:58:10,414] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-14 13:58:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-14 13:58:10,414] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-14 13:58:10,798] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-14 13:58:10,799] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-14 13:58:10,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.35 GB, percent = 2.6%
[2024-11-14 13:58:11,154] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-14 13:58:11,154] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-14 13:58:11,155] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%
[2024-11-14 13:58:11,155] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-14 13:58:11,284] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-14 13:58:11,285] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-14 13:58:11,285] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.56 GB, percent = 2.6%
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-14 13:58:11,285] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-14 13:58:11,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-14 13:58:11,286] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-14 13:58:11,286] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32cbb78450>
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-14 13:58:11,287] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-14 13:58:11,288] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-14 13:58:11,288] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0239559
	speed: 0.2802s/iter; left time: 22775.5551s
	iters: 200, epoch: 1 | loss: 0.0201448
	speed: 0.1183s/iter; left time: 9601.5377s
	iters: 300, epoch: 1 | loss: 0.0162691
	speed: 0.1169s/iter; left time: 9480.7446s
	iters: 400, epoch: 1 | loss: 0.0126257
	speed: 0.1165s/iter; left time: 9435.1100s
	iters: 500, epoch: 1 | loss: 0.0144120
	speed: 0.1172s/iter; left time: 9483.2932s
	iters: 600, epoch: 1 | loss: 0.0185334
	speed: 0.1176s/iter; left time: 9497.9162s
Epoch: 1 cost time: 00h:01m:33.36s
Epoch: 1 | Train Loss: 0.0225287 Vali Loss: 0.0118601 Test Loss: 0.0136884
Validation loss decreased (inf --> 0.011860).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0131000
	speed: 0.6516s/iter; left time: 51201.0050s
	iters: 200, epoch: 2 | loss: 0.0220157
	speed: 0.1155s/iter; left time: 9064.4683s
	iters: 300, epoch: 2 | loss: 0.0084849
	speed: 0.1168s/iter; left time: 9154.1646s
	iters: 400, epoch: 2 | loss: 0.0084919
	speed: 0.1152s/iter; left time: 9015.7136s
	iters: 500, epoch: 2 | loss: 0.0147144
	speed: 0.1156s/iter; left time: 9038.7933s
	iters: 600, epoch: 2 | loss: 0.0093752
	speed: 0.1190s/iter; left time: 9288.0978s
Epoch: 2 cost time: 00h:01m:19.40s
Epoch: 2 | Train Loss: 0.0104779 Vali Loss: 0.0106168 Test Loss: 0.0121037
Validation loss decreased (0.011860 --> 0.010617).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0076656
	speed: 0.4371s/iter; left time: 33163.9484s
	iters: 200, epoch: 3 | loss: 0.0123325
	speed: 0.1146s/iter; left time: 8682.3240s
	iters: 300, epoch: 3 | loss: 0.0073657
	speed: 0.1148s/iter; left time: 8682.7302s
	iters: 400, epoch: 3 | loss: 0.0088692
	speed: 0.1151s/iter; left time: 8696.7682s
	iters: 500, epoch: 3 | loss: 0.0086650
	speed: 0.1156s/iter; left time: 8727.1908s
	iters: 600, epoch: 3 | loss: 0.0082646
	speed: 0.1178s/iter; left time: 8874.5341s
Epoch: 3 cost time: 00h:01m:20.57s
Epoch: 3 | Train Loss: 0.0094833 Vali Loss: 0.0101822 Test Loss: 0.0114591
Validation loss decreased (0.010617 --> 0.010182).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0109312
	speed: 0.5266s/iter; left time: 38524.0249s
	iters: 200, epoch: 4 | loss: 0.0118849
	speed: 0.1167s/iter; left time: 8526.1306s
	iters: 300, epoch: 4 | loss: 0.0099334
	speed: 0.1147s/iter; left time: 8367.2881s
	iters: 400, epoch: 4 | loss: 0.0101027
	speed: 0.1146s/iter; left time: 8352.2250s
	iters: 500, epoch: 4 | loss: 0.0118740
	speed: 0.1145s/iter; left time: 8326.7028s
	iters: 600, epoch: 4 | loss: 0.0064906
	speed: 0.1145s/iter; left time: 8318.2984s
Epoch: 4 cost time: 00h:01m:18.59s
Epoch: 4 | Train Loss: 0.0094642 Vali Loss: 0.0098645 Test Loss: 0.0111941
Validation loss decreased (0.010182 --> 0.009865).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0145120
	speed: 0.4577s/iter; left time: 32241.6792s
	iters: 200, epoch: 5 | loss: 0.0107904
	speed: 0.1139s/iter; left time: 8014.7542s
	iters: 300, epoch: 5 | loss: 0.0115122
	speed: 0.1143s/iter; left time: 8031.6079s
	iters: 400, epoch: 5 | loss: 0.0064738
	speed: 0.1144s/iter; left time: 8023.9122s
	iters: 500, epoch: 5 | loss: 0.0065169
	speed: 0.1136s/iter; left time: 7959.0587s
	iters: 600, epoch: 5 | loss: 0.0121063
	speed: 0.1137s/iter; left time: 7949.9830s
Epoch: 5 cost time: 00h:01m:18.09s
Epoch: 5 | Train Loss: 0.0089185 Vali Loss: 0.0098854 Test Loss: 0.0113841
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0107378
	speed: 0.3610s/iter; left time: 24448.0126s
	iters: 200, epoch: 6 | loss: 0.0071157
	speed: 0.1182s/iter; left time: 7996.0440s
	iters: 300, epoch: 6 | loss: 0.0085659
	speed: 0.1181s/iter; left time: 7973.6379s
	iters: 400, epoch: 6 | loss: 0.0074021
	speed: 0.1223s/iter; left time: 8243.7480s
	iters: 500, epoch: 6 | loss: 0.0067580
	speed: 0.1229s/iter; left time: 8272.0650s
	iters: 600, epoch: 6 | loss: 0.0085170
	speed: 0.2517s/iter; left time: 16919.5556s
Epoch: 6 cost time: 00h:01m:45.53s
Epoch: 6 | Train Loss: 0.0088672 Vali Loss: 0.0098212 Test Loss: 0.0112326
Validation loss decreased (0.009865 --> 0.009821).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0133668
	speed: 1.0716s/iter; left time: 69670.8480s
	iters: 200, epoch: 7 | loss: 0.0055773
	speed: 0.4422s/iter; left time: 28703.5360s
	iters: 300, epoch: 7 | loss: 0.0110484
	speed: 0.4404s/iter; left time: 28541.6124s
	iters: 400, epoch: 7 | loss: 0.0098669
	speed: 0.4496s/iter; left time: 29095.6542s
	iters: 500, epoch: 7 | loss: 0.0105809
	speed: 0.4466s/iter; left time: 28856.8058s
	iters: 600, epoch: 7 | loss: 0.0083511
	speed: 0.4468s/iter; left time: 28825.8824s
Epoch: 7 cost time: 00h:04m:51.21s
Epoch: 7 | Train Loss: 0.0086675 Vali Loss: 0.0096691 Test Loss: 0.0111444
Validation loss decreased (0.009821 --> 0.009669).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0081424
	speed: 1.4640s/iter; left time: 91206.3695s
	iters: 200, epoch: 8 | loss: 0.0119270
	speed: 0.4520s/iter; left time: 28111.3485s
	iters: 300, epoch: 8 | loss: 0.0090215
	speed: 0.4443s/iter; left time: 27593.2379s
	iters: 400, epoch: 8 | loss: 0.0059014
	speed: 0.1566s/iter; left time: 9707.4069s
	iters: 500, epoch: 8 | loss: 0.0100736
	speed: 0.2694s/iter; left time: 16678.4595s
	iters: 600, epoch: 8 | loss: 0.0117893
	speed: 0.2278s/iter; left time: 14078.8536s
Epoch: 8 cost time: 00h:03m:29.50s
Epoch: 8 | Train Loss: 0.0086480 Vali Loss: 0.0095132 Test Loss: 0.0111062
Validation loss decreased (0.009669 --> 0.009513).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0090044
	speed: 0.4103s/iter; left time: 24446.3345s
	iters: 200, epoch: 9 | loss: 0.0094946
	speed: 0.1114s/iter; left time: 6625.0114s
	iters: 300, epoch: 9 | loss: 0.0079705
	speed: 0.1121s/iter; left time: 6658.8228s
	iters: 400, epoch: 9 | loss: 0.0099703
	speed: 0.1128s/iter; left time: 6689.6377s
	iters: 500, epoch: 9 | loss: 0.0080995
	speed: 0.1125s/iter; left time: 6657.6125s
	iters: 600, epoch: 9 | loss: 0.0063729
	speed: 0.1120s/iter; left time: 6617.9407s
Epoch: 9 cost time: 00h:01m:16.71s
Epoch: 9 | Train Loss: 0.0085195 Vali Loss: 0.0092639 Test Loss: 0.0106769
Validation loss decreased (0.009513 --> 0.009264).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0053214
	speed: 0.4978s/iter; left time: 28311.7073s
	iters: 200, epoch: 10 | loss: 0.0139130
	speed: 0.1256s/iter; left time: 7132.7705s
	iters: 300, epoch: 10 | loss: 0.0081261
	speed: 0.2800s/iter; left time: 15868.6923s
	iters: 400, epoch: 10 | loss: 0.0048974
	speed: 0.4382s/iter; left time: 24790.6404s
	iters: 500, epoch: 10 | loss: 0.0076337
	speed: 0.4319s/iter; left time: 24392.6828s
	iters: 600, epoch: 10 | loss: 0.0100944
	speed: 0.4225s/iter; left time: 23815.2874s
Epoch: 10 cost time: 00h:03m:34.94s
Epoch: 10 | Train Loss: 0.0086789 Vali Loss: 0.0094028 Test Loss: 0.0108369
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0053404
	speed: 1.3813s/iter; left time: 74810.0582s
	iters: 200, epoch: 11 | loss: 0.0082792
	speed: 0.4443s/iter; left time: 24020.2399s
	iters: 300, epoch: 11 | loss: 0.0070582
	speed: 0.4496s/iter; left time: 24259.9557s
	iters: 400, epoch: 11 | loss: 0.0067524
	speed: 0.4511s/iter; left time: 24294.5859s
	iters: 500, epoch: 11 | loss: 0.0078302
	speed: 0.4544s/iter; left time: 24430.0277s
	iters: 600, epoch: 11 | loss: 0.0079525
	speed: 0.3612s/iter; left time: 19382.8251s
Epoch: 11 cost time: 00h:04m:42.41s
Epoch: 11 | Train Loss: 0.0083401 Vali Loss: 0.0091377 Test Loss: 0.0106622
Validation loss decreased (0.009264 --> 0.009138).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0096092
	speed: 0.8587s/iter; left time: 44180.8874s
	iters: 200, epoch: 12 | loss: 0.0080460
	speed: 0.1527s/iter; left time: 7841.6917s
	iters: 300, epoch: 12 | loss: 0.0127255
	speed: 0.1116s/iter; left time: 5721.7644s
	iters: 400, epoch: 12 | loss: 0.0106730
	speed: 0.1136s/iter; left time: 5808.5216s
	iters: 500, epoch: 12 | loss: 0.0083658
	speed: 0.1125s/iter; left time: 5740.9163s
	iters: 600, epoch: 12 | loss: 0.0132466
	speed: 0.1126s/iter; left time: 5735.2817s
Epoch: 12 cost time: 00h:01m:34.02s
Epoch: 12 | Train Loss: 0.0082366 Vali Loss: 0.0090408 Test Loss: 0.0104260
Validation loss decreased (0.009138 --> 0.009041).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0112591
	speed: 0.6866s/iter; left time: 33461.1953s
	iters: 200, epoch: 13 | loss: 0.0129779
	speed: 0.2682s/iter; left time: 13043.7910s
	iters: 300, epoch: 13 | loss: 0.0067636
	speed: 0.2726s/iter; left time: 13232.8373s
	iters: 400, epoch: 13 | loss: 0.0061778
	speed: 0.2815s/iter; left time: 13636.8595s
	iters: 500, epoch: 13 | loss: 0.0104312
	speed: 0.2945s/iter; left time: 14233.7605s
	iters: 600, epoch: 13 | loss: 0.0091502
	speed: 0.3002s/iter; left time: 14482.2562s
Epoch: 13 cost time: 00h:03m:13.26s
Epoch: 13 | Train Loss: 0.0080974 Vali Loss: 0.0090926 Test Loss: 0.0107355
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0067687
	speed: 0.9535s/iter; left time: 43883.0498s
	iters: 200, epoch: 14 | loss: 0.0091137
	speed: 0.3083s/iter; left time: 14156.9275s
	iters: 300, epoch: 14 | loss: 0.0098937
	speed: 0.3212s/iter; left time: 14718.1476s
	iters: 400, epoch: 14 | loss: 0.0086841
	speed: 0.3156s/iter; left time: 14431.2787s
	iters: 500, epoch: 14 | loss: 0.0082142
	speed: 0.3186s/iter; left time: 14534.1315s
	iters: 600, epoch: 14 | loss: 0.0087004
	speed: 0.2979s/iter; left time: 13560.4861s
Epoch: 14 cost time: 00h:03m:21.18s
Epoch: 14 | Train Loss: 0.0081368 Vali Loss: 0.0089570 Test Loss: 0.0104242
Validation loss decreased (0.009041 --> 0.008957).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 15 | loss: 0.0167940
	speed: 0.4613s/iter; left time: 19978.9698s
	iters: 200, epoch: 15 | loss: 0.0050574
	speed: 0.1128s/iter; left time: 4875.5426s
	iters: 300, epoch: 15 | loss: 0.0104902
	speed: 0.1127s/iter; left time: 4856.3856s
	iters: 400, epoch: 15 | loss: 0.0089468
	speed: 0.1135s/iter; left time: 4881.1831s
	iters: 500, epoch: 15 | loss: 0.0126251
	speed: 0.1118s/iter; left time: 4797.1002s
	iters: 600, epoch: 15 | loss: 0.0059486
	speed: 0.1117s/iter; left time: 4781.2579s
Epoch: 15 cost time: 00h:01m:17.22s
Epoch: 15 | Train Loss: 0.0081152 Vali Loss: 0.0090319 Test Loss: 0.0105617
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 16 | loss: 0.0057051
	speed: 0.3483s/iter; left time: 14137.5610s
	iters: 200, epoch: 16 | loss: 0.0066385
	speed: 0.1124s/iter; left time: 4551.3529s
	iters: 300, epoch: 16 | loss: 0.0073851
	speed: 0.1130s/iter; left time: 4564.1924s
	iters: 400, epoch: 16 | loss: 0.0081082
	speed: 0.1127s/iter; left time: 4541.3136s
	iters: 500, epoch: 16 | loss: 0.0123262
	speed: 0.1123s/iter; left time: 4514.9041s
	iters: 600, epoch: 16 | loss: 0.0072221
	speed: 0.1126s/iter; left time: 4515.8427s
Epoch: 16 cost time: 00h:01m:16.88s
Epoch: 16 | Train Loss: 0.0078252 Vali Loss: 0.0091145 Test Loss: 0.0108264
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 17 | loss: 0.0097102
	speed: 0.4192s/iter; left time: 15879.5557s
	iters: 200, epoch: 17 | loss: 0.0064988
	speed: 0.2752s/iter; left time: 10399.4114s
	iters: 300, epoch: 17 | loss: 0.0057568
	speed: 0.2698s/iter; left time: 10168.3240s
	iters: 400, epoch: 17 | loss: 0.0065034
	speed: 0.2709s/iter; left time: 10182.9524s
	iters: 500, epoch: 17 | loss: 0.0095674
	speed: 0.2763s/iter; left time: 10355.6380s
	iters: 600, epoch: 17 | loss: 0.0062516
	speed: 0.2860s/iter; left time: 10689.9752s
Epoch: 17 cost time: 00h:02m:59.26s
Epoch: 17 | Train Loss: 0.0078117 Vali Loss: 0.0089328 Test Loss: 0.0105026
Validation loss decreased (0.008957 --> 0.008933).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 18 | loss: 0.0078043
	speed: 1.0131s/iter; left time: 35629.1468s
	iters: 200, epoch: 18 | loss: 0.0063590
	speed: 0.3082s/iter; left time: 10810.2302s
	iters: 300, epoch: 18 | loss: 0.0094975
	speed: 0.3022s/iter; left time: 10569.3144s
	iters: 400, epoch: 18 | loss: 0.0071088
	speed: 0.3130s/iter; left time: 10913.1535s
	iters: 500, epoch: 18 | loss: 0.0055368
	speed: 0.3153s/iter; left time: 10961.2643s
	iters: 600, epoch: 18 | loss: 0.0132828
	speed: 0.3191s/iter; left time: 11063.1123s
Epoch: 18 cost time: 00h:03m:32.03s
Epoch: 18 | Train Loss: 0.0078160 Vali Loss: 0.0089081 Test Loss: 0.0103755
Validation loss decreased (0.008933 --> 0.008908).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 19 | loss: 0.0115771
	speed: 1.0496s/iter; left time: 34066.2837s
	iters: 200, epoch: 19 | loss: 0.0104875
	speed: 0.3142s/iter; left time: 10165.5884s
	iters: 300, epoch: 19 | loss: 0.0096097
	speed: 0.3253s/iter; left time: 10492.0188s
	iters: 400, epoch: 19 | loss: 0.0031140
	speed: 0.3250s/iter; left time: 10452.3471s
	iters: 500, epoch: 19 | loss: 0.0102776
	speed: 0.3221s/iter; left time: 10325.3806s
	iters: 600, epoch: 19 | loss: 0.0063839
	speed: 0.3233s/iter; left time: 10331.0277s
Epoch: 19 cost time: 00h:03m:37.71s
Epoch: 19 | Train Loss: 0.0077374 Vali Loss: 0.0088936 Test Loss: 0.0104487
Validation loss decreased (0.008908 --> 0.008894).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 20 | loss: 0.0030808
	speed: 1.0573s/iter; left time: 31449.6509s
	iters: 200, epoch: 20 | loss: 0.0082016
	speed: 0.3248s/iter; left time: 9628.7199s
	iters: 300, epoch: 20 | loss: 0.0043443
	speed: 0.3291s/iter; left time: 9723.1929s
	iters: 400, epoch: 20 | loss: 0.0143065
	speed: 0.3298s/iter; left time: 9711.0027s
	iters: 500, epoch: 20 | loss: 0.0083869
	speed: 0.3263s/iter; left time: 9575.7146s
	iters: 600, epoch: 20 | loss: 0.0069334
	speed: 0.3203s/iter; left time: 9366.3274s
Epoch: 20 cost time: 00h:03m:41.86s
Epoch: 20 | Train Loss: 0.0076499 Vali Loss: 0.0088768 Test Loss: 0.0103445
Validation loss decreased (0.008894 --> 0.008877).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 21 | loss: 0.0050495
	speed: 1.0619s/iter; left time: 28704.9816s
	iters: 200, epoch: 21 | loss: 0.0056376
	speed: 0.3320s/iter; left time: 8940.7912s
	iters: 300, epoch: 21 | loss: 0.0080506
	speed: 0.3343s/iter; left time: 8970.4401s
	iters: 400, epoch: 21 | loss: 0.0066239
	speed: 0.3225s/iter; left time: 8621.1311s
	iters: 500, epoch: 21 | loss: 0.0070929
	speed: 0.3161s/iter; left time: 8418.6437s
	iters: 600, epoch: 21 | loss: 0.0058815
	speed: 0.3361s/iter; left time: 8916.4134s
Epoch: 21 cost time: 00h:03m:43.81s
Epoch: 21 | Train Loss: 0.0076399 Vali Loss: 0.0088993 Test Loss: 0.0104224
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 22 | loss: 0.0054120
	speed: 1.0256s/iter; left time: 24939.4306s
	iters: 200, epoch: 22 | loss: 0.0076757
	speed: 0.3348s/iter; left time: 8108.1047s
	iters: 300, epoch: 22 | loss: 0.0048412
	speed: 0.3362s/iter; left time: 8107.4460s
	iters: 400, epoch: 22 | loss: 0.0044447
	speed: 0.3327s/iter; left time: 7989.7958s
	iters: 500, epoch: 22 | loss: 0.0083622
	speed: 0.3392s/iter; left time: 8113.5795s
	iters: 600, epoch: 22 | loss: 0.0153576
	speed: 0.3308s/iter; left time: 7879.0503s
Epoch: 22 cost time: 00h:03m:47.24s
Epoch: 22 | Train Loss: 0.0075452 Vali Loss: 0.0089097 Test Loss: 0.0103669
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 23 | loss: 0.0055570
	speed: 1.0418s/iter; left time: 22508.6243s
	iters: 200, epoch: 23 | loss: 0.0043441
	speed: 0.3345s/iter; left time: 7193.3601s
	iters: 300, epoch: 23 | loss: 0.0085570
	speed: 0.3377s/iter; left time: 7228.8754s
	iters: 400, epoch: 23 | loss: 0.0088893
	speed: 0.3361s/iter; left time: 7161.4683s
	iters: 500, epoch: 23 | loss: 0.0037170
	speed: 0.3218s/iter; left time: 6824.1497s
	iters: 600, epoch: 23 | loss: 0.0061619
	speed: 0.3421s/iter; left time: 7219.1257s
Epoch: 23 cost time: 00h:03m:48.29s
Epoch: 23 | Train Loss: 0.0075131 Vali Loss: 0.0089658 Test Loss: 0.0102591
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 24 | loss: 0.0099053
	speed: 1.0770s/iter; left time: 20346.9385s
	iters: 200, epoch: 24 | loss: 0.0091285
	speed: 0.3492s/iter; left time: 6562.2366s
	iters: 300, epoch: 24 | loss: 0.0093744
	speed: 0.3467s/iter; left time: 6480.0304s
	iters: 400, epoch: 24 | loss: 0.0068118
	speed: 0.3448s/iter; left time: 6410.8172s
	iters: 500, epoch: 24 | loss: 0.0037521
	speed: 0.3279s/iter; left time: 6064.4418s
	iters: 600, epoch: 24 | loss: 0.0060791
	speed: 0.3475s/iter; left time: 6390.4704s
Epoch: 24 cost time: 00h:03m:54.15s
Epoch: 24 | Train Loss: 0.0075297 Vali Loss: 0.0088701 Test Loss: 0.0103922
Validation loss decreased (0.008877 --> 0.008870).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 25 | loss: 0.0065410
	speed: 1.1906s/iter; left time: 19261.9734s
	iters: 200, epoch: 25 | loss: 0.0056375
	speed: 0.3496s/iter; left time: 5621.5463s
	iters: 300, epoch: 25 | loss: 0.0102988
	speed: 0.3466s/iter; left time: 5538.5983s
	iters: 400, epoch: 25 | loss: 0.0065078
	speed: 0.3504s/iter; left time: 5563.6305s
	iters: 500, epoch: 25 | loss: 0.0056647
	speed: 0.3474s/iter; left time: 5482.1096s
	iters: 600, epoch: 25 | loss: 0.0058788
	speed: 0.3510s/iter; left time: 5504.1013s
Epoch: 25 cost time: 00h:03m:57.45s
Epoch: 25 | Train Loss: 0.0074501 Vali Loss: 0.0090055 Test Loss: 0.0105855
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 26 | loss: 0.0120122
	speed: 1.0667s/iter; left time: 14364.3894s
	iters: 200, epoch: 26 | loss: 0.0055594
	speed: 0.3323s/iter; left time: 4441.2763s
	iters: 300, epoch: 26 | loss: 0.0064205
	speed: 0.3485s/iter; left time: 4622.8483s
	iters: 400, epoch: 26 | loss: 0.0117745
	speed: 0.3476s/iter; left time: 4576.4656s
	iters: 500, epoch: 26 | loss: 0.0076545
	speed: 0.3508s/iter; left time: 4583.1005s
	iters: 600, epoch: 26 | loss: 0.0078711
	speed: 0.3509s/iter; left time: 4550.3585s
Epoch: 26 cost time: 00h:03m:54.73s
Epoch: 26 | Train Loss: 0.0073374 Vali Loss: 0.0090989 Test Loss: 0.0103317
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 27 | loss: 0.0077098
	speed: 1.0886s/iter; left time: 11705.9631s
	iters: 200, epoch: 27 | loss: 0.0055892
	speed: 0.3521s/iter; left time: 3751.0327s
	iters: 300, epoch: 27 | loss: 0.0084353
	speed: 0.3504s/iter; left time: 3697.2570s
	iters: 400, epoch: 27 | loss: 0.0111528
	speed: 0.3529s/iter; left time: 3689.3583s
	iters: 500, epoch: 27 | loss: 0.0081662
	speed: 0.3508s/iter; left time: 3632.0384s
	iters: 600, epoch: 27 | loss: 0.0053349
	speed: 0.3516s/iter; left time: 3605.1329s
Epoch: 27 cost time: 00h:03m:58.54s
Epoch: 27 | Train Loss: 0.0073602 Vali Loss: 0.0089851 Test Loss: 0.0103161
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 28 | loss: 0.0063998
	speed: 1.0859s/iter; left time: 8730.6862s
	iters: 200, epoch: 28 | loss: 0.0083144
	speed: 0.3520s/iter; left time: 2795.0274s
	iters: 300, epoch: 28 | loss: 0.0065535
	speed: 0.3532s/iter; left time: 2769.0085s
	iters: 400, epoch: 28 | loss: 0.0060916
	speed: 0.3519s/iter; left time: 2723.8142s
	iters: 500, epoch: 28 | loss: 0.0064265
	speed: 0.3533s/iter; left time: 2698.9254s
	iters: 600, epoch: 28 | loss: 0.0053284
	speed: 0.3489s/iter; left time: 2630.6304s
Epoch: 28 cost time: 00h:03m:59.29s
Epoch: 28 | Train Loss: 0.0073103 Vali Loss: 0.0090719 Test Loss: 0.0104575
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 29 | loss: 0.0062723
	speed: 1.0915s/iter; left time: 5814.3815s
	iters: 200, epoch: 29 | loss: 0.0058362
	speed: 0.3520s/iter; left time: 1839.7125s
	iters: 300, epoch: 29 | loss: 0.0083566
	speed: 0.3357s/iter; left time: 1721.2573s
	iters: 400, epoch: 29 | loss: 0.0075264
	speed: 0.3597s/iter; left time: 1807.9777s
	iters: 500, epoch: 29 | loss: 0.0039099
	speed: 0.3620s/iter; left time: 1783.6610s
	iters: 600, epoch: 29 | loss: 0.0062451
	speed: 0.3572s/iter; left time: 1724.2323s
Epoch: 29 cost time: 00h:04m:00.51s
Epoch: 29 | Train Loss: 0.0072024 Vali Loss: 0.0089768 Test Loss: 0.0106518
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Scaled mse:0.01039215736091137, rmse:0.10194192826747894, mae:0.05944359302520752, rse:0.3948001563549042
Intermediate time for FR and pred_len 24: 01h:45m:51.65s
Intermediate time for FR: 01h:45m:51.65s
Total time: 01h:45m:51.67s
