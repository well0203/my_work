
=== Starting experiments for country: ES ===

=== Starting experiments for pred_len: 24 ===

--- Running model for ES, pred_len=24 ---
train 85803
val 18651
test 18651
[2024-11-12 16:19:27,434] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 16:19:28,412] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 16:19:28,412] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 16:19:28,412] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 16:19:28,522] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 16:19:28,523] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 16:19:29,170] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 16:19:29,171] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 16:19:29,171] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 16:19:29,172] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 16:19:29,172] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 16:19:29,173] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 16:19:29,173] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 16:19:29,173] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 16:19:29,173] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 16:19:29,173] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 16:19:29,363] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 16:19:29,363] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 16:19:29,418] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.32 GB, percent = 2.7%
[2024-11-12 16:19:29,524] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 16:19:29,525] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 16:19:29,525] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.26 GB, percent = 2.7%
[2024-11-12 16:19:29,525] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 16:19:29,621] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 16:19:29,621] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 16:19:29,621] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.25 GB, percent = 2.7%
[2024-11-12 16:19:29,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 16:19:29,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 16:19:29,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 16:19:29,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 16:19:29,622] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe7b603f5d0>
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 16:19:29,623] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 16:19:29,624] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 16:19:29,624] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0545830
	speed: 0.1616s/iter; left time: 4316.4985s
	iters: 200, epoch: 1 | loss: 0.0524177
	speed: 0.1232s/iter; left time: 3278.4706s
	iters: 300, epoch: 1 | loss: 0.0306460
	speed: 0.1237s/iter; left time: 3279.9896s
	iters: 400, epoch: 1 | loss: 0.0289302
	speed: 0.1242s/iter; left time: 3279.3330s
	iters: 500, epoch: 1 | loss: 0.0238862
	speed: 0.1232s/iter; left time: 3240.5742s
	iters: 600, epoch: 1 | loss: 0.0211351
	speed: 0.1234s/iter; left time: 3233.6568s
	iters: 700, epoch: 1 | loss: 0.0179487
	speed: 0.1237s/iter; left time: 3229.5367s
	iters: 800, epoch: 1 | loss: 0.0147842
	speed: 0.1240s/iter; left time: 3226.3185s
	iters: 900, epoch: 1 | loss: 0.0156694
	speed: 0.1240s/iter; left time: 3212.9988s
	iters: 1000, epoch: 1 | loss: 0.0168096
	speed: 0.1238s/iter; left time: 3194.1992s
	iters: 1100, epoch: 1 | loss: 0.0131386
	speed: 0.1237s/iter; left time: 3181.3627s
	iters: 1200, epoch: 1 | loss: 0.0158540
	speed: 0.1244s/iter; left time: 3185.2983s
	iters: 1300, epoch: 1 | loss: 0.0143935
	speed: 0.1211s/iter; left time: 3090.3891s
	iters: 1400, epoch: 1 | loss: 0.0187579
	speed: 0.1235s/iter; left time: 3138.2874s
	iters: 1500, epoch: 1 | loss: 0.0147546
	speed: 0.1238s/iter; left time: 3132.9402s
	iters: 1600, epoch: 1 | loss: 0.0141143
	speed: 0.1241s/iter; left time: 3129.5655s
	iters: 1700, epoch: 1 | loss: 0.0153522
	speed: 0.1245s/iter; left time: 3125.7567s
	iters: 1800, epoch: 1 | loss: 0.0165686
	speed: 0.1250s/iter; left time: 3126.7399s
	iters: 1900, epoch: 1 | loss: 0.0149150
	speed: 0.1245s/iter; left time: 3101.0133s
	iters: 2000, epoch: 1 | loss: 0.0139012
	speed: 0.1245s/iter; left time: 3090.1747s
	iters: 2100, epoch: 1 | loss: 0.0128072
	speed: 0.1247s/iter; left time: 3082.2526s
	iters: 2200, epoch: 1 | loss: 0.0128131
	speed: 0.1239s/iter; left time: 3048.5152s
	iters: 2300, epoch: 1 | loss: 0.0175276
	speed: 0.1258s/iter; left time: 3083.1036s
	iters: 2400, epoch: 1 | loss: 0.0129875
	speed: 0.1243s/iter; left time: 3033.6402s
	iters: 2500, epoch: 1 | loss: 0.0118990
	speed: 0.1239s/iter; left time: 3011.8072s
	iters: 2600, epoch: 1 | loss: 0.0176947
	speed: 0.1242s/iter; left time: 3006.3918s
Epoch: 1 cost time: 00h:05m:33.61s
Epoch: 1 | Train Loss: 0.0226912 Vali Loss: 0.0103871 Test Loss: 0.0140691
Validation loss decreased (inf --> 0.010387).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0205152
	speed: 0.9334s/iter; left time: 22428.4132s
	iters: 200, epoch: 2 | loss: 0.0203009
	speed: 0.1180s/iter; left time: 2822.7882s
	iters: 300, epoch: 2 | loss: 0.0151824
	speed: 0.1194s/iter; left time: 2844.4418s
	iters: 400, epoch: 2 | loss: 0.0177140
	speed: 0.1194s/iter; left time: 2833.8397s
	iters: 500, epoch: 2 | loss: 0.0142296
	speed: 0.1198s/iter; left time: 2831.6257s
	iters: 600, epoch: 2 | loss: 0.0179271
	speed: 0.1185s/iter; left time: 2787.2891s
	iters: 700, epoch: 2 | loss: 0.0098558
	speed: 0.1194s/iter; left time: 2797.3130s
	iters: 800, epoch: 2 | loss: 0.0136821
	speed: 0.1187s/iter; left time: 2768.5765s
	iters: 900, epoch: 2 | loss: 0.0110614
	speed: 0.1186s/iter; left time: 2754.2433s
	iters: 1000, epoch: 2 | loss: 0.0143228
	speed: 0.1180s/iter; left time: 2728.7448s
	iters: 1100, epoch: 2 | loss: 0.0152571
	speed: 0.1185s/iter; left time: 2729.2009s
	iters: 1200, epoch: 2 | loss: 0.0116525
	speed: 0.1195s/iter; left time: 2740.9362s
	iters: 1300, epoch: 2 | loss: 0.0108315
	speed: 0.1184s/iter; left time: 2701.9565s
	iters: 1400, epoch: 2 | loss: 0.0112984
	speed: 0.1185s/iter; left time: 2693.8155s
	iters: 1500, epoch: 2 | loss: 0.0174003
	speed: 0.1196s/iter; left time: 2705.4401s
	iters: 1600, epoch: 2 | loss: 0.0209794
	speed: 0.1186s/iter; left time: 2672.1285s
	iters: 1700, epoch: 2 | loss: 0.0115302
	speed: 0.1200s/iter; left time: 2691.9011s
	iters: 1800, epoch: 2 | loss: 0.0119710
	speed: 0.1207s/iter; left time: 2695.6083s
	iters: 1900, epoch: 2 | loss: 0.0138560
	speed: 0.1207s/iter; left time: 2683.0841s
	iters: 2000, epoch: 2 | loss: 0.0142983
	speed: 0.1191s/iter; left time: 2635.8028s
	iters: 2100, epoch: 2 | loss: 0.0135131
	speed: 0.1185s/iter; left time: 2611.5795s
	iters: 2200, epoch: 2 | loss: 0.0116738
	speed: 0.1200s/iter; left time: 2631.2689s
	iters: 2300, epoch: 2 | loss: 0.0148960
	speed: 0.1192s/iter; left time: 2602.9964s
	iters: 2400, epoch: 2 | loss: 0.0159042
	speed: 0.1190s/iter; left time: 2586.8703s
	iters: 2500, epoch: 2 | loss: 0.0098822
	speed: 0.1203s/iter; left time: 2602.9451s
	iters: 2600, epoch: 2 | loss: 0.0133203
	speed: 0.1186s/iter; left time: 2553.7292s
Epoch: 2 cost time: 00h:05m:19.87s
Epoch: 2 | Train Loss: 0.0138356 Vali Loss: 0.0099448 Test Loss: 0.0131996
Validation loss decreased (0.010387 --> 0.009945).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0100902
	speed: 0.8714s/iter; left time: 18602.5200s
	iters: 200, epoch: 3 | loss: 0.0134872
	speed: 0.1178s/iter; left time: 2503.1782s
	iters: 300, epoch: 3 | loss: 0.0159289
	speed: 0.1215s/iter; left time: 2569.2366s
	iters: 400, epoch: 3 | loss: 0.0137253
	speed: 0.1197s/iter; left time: 2518.6229s
	iters: 500, epoch: 3 | loss: 0.0111006
	speed: 0.1183s/iter; left time: 2478.4422s
	iters: 600, epoch: 3 | loss: 0.0095436
	speed: 0.1189s/iter; left time: 2478.1832s
	iters: 700, epoch: 3 | loss: 0.0130041
	speed: 0.1192s/iter; left time: 2473.2162s
	iters: 800, epoch: 3 | loss: 0.0167347
	speed: 0.1195s/iter; left time: 2467.7931s
	iters: 900, epoch: 3 | loss: 0.0127512
	speed: 0.1207s/iter; left time: 2480.1491s
	iters: 1000, epoch: 3 | loss: 0.0110394
	speed: 0.1176s/iter; left time: 2404.8211s
	iters: 1100, epoch: 3 | loss: 0.0102398
	speed: 0.1174s/iter; left time: 2388.9892s
	iters: 1200, epoch: 3 | loss: 0.0169201
	speed: 0.1182s/iter; left time: 2393.7030s
	iters: 1300, epoch: 3 | loss: 0.0113823
	speed: 0.1199s/iter; left time: 2414.9841s
	iters: 1400, epoch: 3 | loss: 0.0151350
	speed: 0.1201s/iter; left time: 2407.8636s
	iters: 1500, epoch: 3 | loss: 0.0148839
	speed: 0.1183s/iter; left time: 2360.5041s
	iters: 1600, epoch: 3 | loss: 0.0100247
	speed: 0.1191s/iter; left time: 2363.1162s
	iters: 1700, epoch: 3 | loss: 0.0106432
	speed: 0.1207s/iter; left time: 2383.9432s
	iters: 1800, epoch: 3 | loss: 0.0109489
	speed: 0.1213s/iter; left time: 2384.1478s
	iters: 1900, epoch: 3 | loss: 0.0138631
	speed: 0.1197s/iter; left time: 2339.2285s
	iters: 2000, epoch: 3 | loss: 0.0116591
	speed: 0.1189s/iter; left time: 2312.7173s
	iters: 2100, epoch: 3 | loss: 0.0118185
	speed: 0.1200s/iter; left time: 2321.3218s
	iters: 2200, epoch: 3 | loss: 0.0165041
	speed: 0.1208s/iter; left time: 2325.9834s
	iters: 2300, epoch: 3 | loss: 0.0137983
	speed: 0.1202s/iter; left time: 2301.7010s
	iters: 2400, epoch: 3 | loss: 0.0186460
	speed: 0.1199s/iter; left time: 2284.8347s
	iters: 2500, epoch: 3 | loss: 0.0103766
	speed: 0.1189s/iter; left time: 2253.6668s
	iters: 2600, epoch: 3 | loss: 0.0119668
	speed: 0.1196s/iter; left time: 2254.0494s
Epoch: 3 cost time: 00h:05m:20.84s
Epoch: 3 | Train Loss: 0.0130627 Vali Loss: 0.0089921 Test Loss: 0.0120798
Validation loss decreased (0.009945 --> 0.008992).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0146747
	speed: 0.8691s/iter; left time: 16224.4939s
	iters: 200, epoch: 4 | loss: 0.0153105
	speed: 0.1188s/iter; left time: 2206.7334s
	iters: 300, epoch: 4 | loss: 0.0100770
	speed: 0.1202s/iter; left time: 2219.4635s
	iters: 400, epoch: 4 | loss: 0.0178037
	speed: 0.1186s/iter; left time: 2177.8376s
	iters: 500, epoch: 4 | loss: 0.0127377
	speed: 0.1198s/iter; left time: 2188.6028s
	iters: 600, epoch: 4 | loss: 0.0111179
	speed: 0.1191s/iter; left time: 2164.0672s
	iters: 700, epoch: 4 | loss: 0.0087008
	speed: 0.1185s/iter; left time: 2140.8619s
	iters: 800, epoch: 4 | loss: 0.0141666
	speed: 0.1192s/iter; left time: 2141.9142s
	iters: 900, epoch: 4 | loss: 0.0116692
	speed: 0.1202s/iter; left time: 2148.1007s
	iters: 1000, epoch: 4 | loss: 0.0144764
	speed: 0.1201s/iter; left time: 2133.4238s
	iters: 1100, epoch: 4 | loss: 0.0140510
	speed: 0.1189s/iter; left time: 2101.1521s
	iters: 1200, epoch: 4 | loss: 0.0130727
	speed: 0.1191s/iter; left time: 2091.9280s
	iters: 1300, epoch: 4 | loss: 0.0121976
	speed: 0.1191s/iter; left time: 2081.0868s
	iters: 1400, epoch: 4 | loss: 0.0091759
	speed: 0.1200s/iter; left time: 2084.0963s
	iters: 1500, epoch: 4 | loss: 0.0110257
	speed: 0.1194s/iter; left time: 2061.7547s
	iters: 1600, epoch: 4 | loss: 0.0118888
	speed: 0.1182s/iter; left time: 2029.2229s
	iters: 1700, epoch: 4 | loss: 0.0131193
	speed: 0.1190s/iter; left time: 2031.2273s
	iters: 1800, epoch: 4 | loss: 0.0137166
	speed: 0.1203s/iter; left time: 2041.1375s
	iters: 1900, epoch: 4 | loss: 0.0155586
	speed: 0.1201s/iter; left time: 2025.3767s
	iters: 2000, epoch: 4 | loss: 0.0153784
	speed: 0.1197s/iter; left time: 2007.7582s
	iters: 2100, epoch: 4 | loss: 0.0150797
	speed: 0.1200s/iter; left time: 2000.0128s
	iters: 2200, epoch: 4 | loss: 0.0131411
	speed: 0.1195s/iter; left time: 1980.1827s
	iters: 2300, epoch: 4 | loss: 0.0105560
	speed: 0.1201s/iter; left time: 1977.0541s
	iters: 2400, epoch: 4 | loss: 0.0119651
	speed: 0.1192s/iter; left time: 1950.2923s
	iters: 2500, epoch: 4 | loss: 0.0126801
	speed: 0.1185s/iter; left time: 1927.4271s
	iters: 2600, epoch: 4 | loss: 0.0122480
	speed: 0.1191s/iter; left time: 1925.6175s
Epoch: 4 cost time: 00h:05m:20.61s
Epoch: 4 | Train Loss: 0.0124224 Vali Loss: 0.0087640 Test Loss: 0.0116099
Validation loss decreased (0.008992 --> 0.008764).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0099333
	speed: 0.8647s/iter; left time: 13824.3762s
	iters: 200, epoch: 5 | loss: 0.0109573
	speed: 0.1192s/iter; left time: 1894.3610s
	iters: 300, epoch: 5 | loss: 0.0131027
	speed: 0.1193s/iter; left time: 1883.3690s
	iters: 400, epoch: 5 | loss: 0.0098820
	speed: 0.1195s/iter; left time: 1875.2156s
	iters: 500, epoch: 5 | loss: 0.0135142
	speed: 0.1190s/iter; left time: 1855.3954s
	iters: 600, epoch: 5 | loss: 0.0152880
	speed: 0.1203s/iter; left time: 1862.5248s
	iters: 700, epoch: 5 | loss: 0.0121359
	speed: 0.1199s/iter; left time: 1844.6621s
	iters: 800, epoch: 5 | loss: 0.0106622
	speed: 0.1188s/iter; left time: 1816.8354s
	iters: 900, epoch: 5 | loss: 0.0117534
	speed: 0.1199s/iter; left time: 1820.8394s
	iters: 1000, epoch: 5 | loss: 0.0137955
	speed: 0.1187s/iter; left time: 1790.6354s
	iters: 1100, epoch: 5 | loss: 0.0135695
	speed: 0.1196s/iter; left time: 1792.3449s
	iters: 1200, epoch: 5 | loss: 0.0122368
	speed: 0.1197s/iter; left time: 1782.2673s
	iters: 1300, epoch: 5 | loss: 0.0120961
	speed: 0.1193s/iter; left time: 1764.2227s
	iters: 1400, epoch: 5 | loss: 0.0096146
	speed: 0.1188s/iter; left time: 1745.4933s
	iters: 1500, epoch: 5 | loss: 0.0135736
	speed: 0.1191s/iter; left time: 1737.0268s
	iters: 1600, epoch: 5 | loss: 0.0118207
	speed: 0.1184s/iter; left time: 1715.6756s
	iters: 1700, epoch: 5 | loss: 0.0116144
	speed: 0.1197s/iter; left time: 1721.9163s
	iters: 1800, epoch: 5 | loss: 0.0153641
	speed: 0.1188s/iter; left time: 1696.6883s
	iters: 1900, epoch: 5 | loss: 0.0104821
	speed: 0.1190s/iter; left time: 1688.7215s
	iters: 2000, epoch: 5 | loss: 0.0124981
	speed: 0.1192s/iter; left time: 1679.7131s
	iters: 2100, epoch: 5 | loss: 0.0090783
	speed: 0.1197s/iter; left time: 1673.7470s
	iters: 2200, epoch: 5 | loss: 0.0103755
	speed: 0.1202s/iter; left time: 1669.6869s
	iters: 2300, epoch: 5 | loss: 0.0143221
	speed: 0.1204s/iter; left time: 1659.9116s
	iters: 2400, epoch: 5 | loss: 0.0115179
	speed: 0.1222s/iter; left time: 1672.7690s
	iters: 2500, epoch: 5 | loss: 0.0138639
	speed: 0.1200s/iter; left time: 1630.4015s
	iters: 2600, epoch: 5 | loss: 0.0118041
	speed: 0.1175s/iter; left time: 1584.5738s
Epoch: 5 cost time: 00h:05m:20.51s
Epoch: 5 | Train Loss: 0.0120566 Vali Loss: 0.0087508 Test Loss: 0.0116617
Validation loss decreased (0.008764 --> 0.008751).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0077944
	speed: 0.8574s/iter; left time: 11408.9212s
	iters: 200, epoch: 6 | loss: 0.0136503
	speed: 0.1180s/iter; left time: 1558.2167s
	iters: 300, epoch: 6 | loss: 0.0145998
	speed: 0.1186s/iter; left time: 1553.7207s
	iters: 400, epoch: 6 | loss: 0.0109427
	speed: 0.1190s/iter; left time: 1547.7695s
	iters: 500, epoch: 6 | loss: 0.0103924
	speed: 0.1187s/iter; left time: 1531.4102s
	iters: 600, epoch: 6 | loss: 0.0118544
	speed: 0.1177s/iter; left time: 1507.7028s
	iters: 700, epoch: 6 | loss: 0.0072835
	speed: 0.1183s/iter; left time: 1503.3968s
	iters: 800, epoch: 6 | loss: 0.0094917
	speed: 0.1174s/iter; left time: 1479.7004s
	iters: 900, epoch: 6 | loss: 0.0155689
	speed: 0.1187s/iter; left time: 1484.6866s
	iters: 1000, epoch: 6 | loss: 0.0105897
	speed: 0.1183s/iter; left time: 1467.0329s
	iters: 1100, epoch: 6 | loss: 0.0129377
	speed: 0.1170s/iter; left time: 1440.1100s
	iters: 1200, epoch: 6 | loss: 0.0119803
	speed: 0.1185s/iter; left time: 1446.5803s
	iters: 1300, epoch: 6 | loss: 0.0124778
	speed: 0.1186s/iter; left time: 1435.4083s
	iters: 1400, epoch: 6 | loss: 0.0118653
	speed: 0.1192s/iter; left time: 1431.6987s
	iters: 1500, epoch: 6 | loss: 0.0136589
	speed: 0.1194s/iter; left time: 1422.1186s
	iters: 1600, epoch: 6 | loss: 0.0110335
	speed: 0.1197s/iter; left time: 1413.4791s
	iters: 1700, epoch: 6 | loss: 0.0109264
	speed: 0.1189s/iter; left time: 1391.8562s
	iters: 1800, epoch: 6 | loss: 0.0141481
	speed: 0.1187s/iter; left time: 1377.0768s
	iters: 1900, epoch: 6 | loss: 0.0107703
	speed: 0.1184s/iter; left time: 1362.3616s
	iters: 2000, epoch: 6 | loss: 0.0117767
	speed: 0.1186s/iter; left time: 1352.3381s
	iters: 2100, epoch: 6 | loss: 0.0127800
	speed: 0.1186s/iter; left time: 1340.7195s
	iters: 2200, epoch: 6 | loss: 0.0089541
	speed: 0.1188s/iter; left time: 1331.0873s
	iters: 2300, epoch: 6 | loss: 0.0124707
	speed: 0.1186s/iter; left time: 1316.7185s
	iters: 2400, epoch: 6 | loss: 0.0082773
	speed: 0.1187s/iter; left time: 1306.8215s
	iters: 2500, epoch: 6 | loss: 0.0091436
	speed: 0.1200s/iter; left time: 1308.1771s
	iters: 2600, epoch: 6 | loss: 0.0112877
	speed: 0.1187s/iter; left time: 1282.6506s
Epoch: 6 cost time: 00h:05m:18.32s
Epoch: 6 | Train Loss: 0.0117904 Vali Loss: 0.0084753 Test Loss: 0.0111235
Validation loss decreased (0.008751 --> 0.008475).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0109215
	speed: 0.8517s/iter; left time: 9049.4171s
	iters: 200, epoch: 7 | loss: 0.0126030
	speed: 0.1179s/iter; left time: 1240.5187s
	iters: 300, epoch: 7 | loss: 0.0113944
	speed: 0.1178s/iter; left time: 1228.3050s
	iters: 400, epoch: 7 | loss: 0.0095302
	speed: 0.1181s/iter; left time: 1218.9817s
	iters: 500, epoch: 7 | loss: 0.0166103
	speed: 0.1179s/iter; left time: 1205.5783s
	iters: 600, epoch: 7 | loss: 0.0112385
	speed: 0.1188s/iter; left time: 1202.5796s
	iters: 700, epoch: 7 | loss: 0.0177693
	speed: 0.1179s/iter; left time: 1182.0126s
	iters: 800, epoch: 7 | loss: 0.0132485
	speed: 0.1179s/iter; left time: 1169.6666s
	iters: 900, epoch: 7 | loss: 0.0122021
	speed: 0.1177s/iter; left time: 1156.1017s
	iters: 1000, epoch: 7 | loss: 0.0141721
	speed: 0.1177s/iter; left time: 1144.3241s
	iters: 1100, epoch: 7 | loss: 0.0123870
	speed: 0.1172s/iter; left time: 1127.7412s
	iters: 1200, epoch: 7 | loss: 0.0111623
	speed: 0.1180s/iter; left time: 1123.6591s
	iters: 1300, epoch: 7 | loss: 0.0121417
	speed: 0.1178s/iter; left time: 1110.3975s
	iters: 1400, epoch: 7 | loss: 0.0126506
	speed: 0.1180s/iter; left time: 1100.0527s
	iters: 1500, epoch: 7 | loss: 0.0120803
	speed: 0.1178s/iter; left time: 1086.3105s
	iters: 1600, epoch: 7 | loss: 0.0116117
	speed: 0.1178s/iter; left time: 1074.9168s
	iters: 1700, epoch: 7 | loss: 0.0117655
	speed: 0.1178s/iter; left time: 1062.8966s
	iters: 1800, epoch: 7 | loss: 0.0078291
	speed: 0.1177s/iter; left time: 1050.0442s
	iters: 1900, epoch: 7 | loss: 0.0121750
	speed: 0.1177s/iter; left time: 1039.1318s
	iters: 2000, epoch: 7 | loss: 0.0129528
	speed: 0.1175s/iter; left time: 1025.2399s
	iters: 2100, epoch: 7 | loss: 0.0115843
	speed: 0.1174s/iter; left time: 1012.5316s
	iters: 2200, epoch: 7 | loss: 0.0159705
	speed: 0.1188s/iter; left time: 1012.4937s
	iters: 2300, epoch: 7 | loss: 0.0142493
	speed: 0.1189s/iter; left time: 1001.5653s
	iters: 2400, epoch: 7 | loss: 0.0106964
	speed: 0.1186s/iter; left time: 987.5312s
	iters: 2500, epoch: 7 | loss: 0.0094670
	speed: 0.1179s/iter; left time: 970.0979s
	iters: 2600, epoch: 7 | loss: 0.0131341
	speed: 0.1174s/iter; left time: 953.4874s
Epoch: 7 cost time: 00h:05m:16.45s
Epoch: 7 | Train Loss: 0.0115988 Vali Loss: 0.0084970 Test Loss: 0.0114120
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0191265
	speed: 0.8158s/iter; left time: 6481.0678s
	iters: 200, epoch: 8 | loss: 0.0117542
	speed: 0.1178s/iter; left time: 924.1769s
	iters: 300, epoch: 8 | loss: 0.0118873
	speed: 0.1177s/iter; left time: 911.5064s
	iters: 400, epoch: 8 | loss: 0.0084380
	speed: 0.1180s/iter; left time: 902.3018s
	iters: 500, epoch: 8 | loss: 0.0124220
	speed: 0.1179s/iter; left time: 889.0856s
	iters: 600, epoch: 8 | loss: 0.0109508
	speed: 0.1177s/iter; left time: 875.9866s
	iters: 700, epoch: 8 | loss: 0.0078423
	speed: 0.1179s/iter; left time: 865.8108s
	iters: 800, epoch: 8 | loss: 0.0108486
	speed: 0.1196s/iter; left time: 866.1193s
	iters: 900, epoch: 8 | loss: 0.0082394
	speed: 0.1193s/iter; left time: 852.4319s
	iters: 1000, epoch: 8 | loss: 0.0138884
	speed: 0.1182s/iter; left time: 832.4729s
	iters: 1100, epoch: 8 | loss: 0.0106513
	speed: 0.1182s/iter; left time: 820.9609s
	iters: 1200, epoch: 8 | loss: 0.0116432
	speed: 0.1186s/iter; left time: 811.7206s
	iters: 1300, epoch: 8 | loss: 0.0080303
	speed: 0.1179s/iter; left time: 794.9923s
	iters: 1400, epoch: 8 | loss: 0.0084128
	speed: 0.1180s/iter; left time: 784.3168s
	iters: 1500, epoch: 8 | loss: 0.0101146
	speed: 0.1180s/iter; left time: 772.0312s
	iters: 1600, epoch: 8 | loss: 0.0125243
	speed: 0.1181s/iter; left time: 760.8584s
	iters: 1700, epoch: 8 | loss: 0.0074849
	speed: 0.1188s/iter; left time: 753.7009s
	iters: 1800, epoch: 8 | loss: 0.0094907
	speed: 0.1190s/iter; left time: 743.0184s
	iters: 1900, epoch: 8 | loss: 0.0126217
	speed: 0.1197s/iter; left time: 735.3316s
	iters: 2000, epoch: 8 | loss: 0.0133219
	speed: 0.1178s/iter; left time: 712.1529s
	iters: 2100, epoch: 8 | loss: 0.0104638
	speed: 0.1187s/iter; left time: 705.5664s
	iters: 2200, epoch: 8 | loss: 0.0088677
	speed: 0.1181s/iter; left time: 690.0636s
	iters: 2300, epoch: 8 | loss: 0.0109687
	speed: 0.1188s/iter; left time: 682.2640s
	iters: 2400, epoch: 8 | loss: 0.0094333
	speed: 0.1168s/iter; left time: 659.4962s
	iters: 2500, epoch: 8 | loss: 0.0135891
	speed: 0.1184s/iter; left time: 656.3103s
	iters: 2600, epoch: 8 | loss: 0.0104805
	speed: 0.1186s/iter; left time: 645.6796s
Epoch: 8 cost time: 00h:05m:17.52s
Epoch: 8 | Train Loss: 0.0114108 Vali Loss: 0.0085304 Test Loss: 0.0112045
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0129170
	speed: 0.8173s/iter; left time: 4301.3760s
	iters: 200, epoch: 9 | loss: 0.0134936
	speed: 0.1179s/iter; left time: 608.6839s
	iters: 300, epoch: 9 | loss: 0.0096854
	speed: 0.1186s/iter; left time: 600.3740s
	iters: 400, epoch: 9 | loss: 0.0109904
	speed: 0.1180s/iter; left time: 585.8107s
	iters: 500, epoch: 9 | loss: 0.0082483
	speed: 0.1177s/iter; left time: 572.3777s
	iters: 600, epoch: 9 | loss: 0.0090961
	speed: 0.1168s/iter; left time: 556.4075s
	iters: 700, epoch: 9 | loss: 0.0129327
	speed: 0.1188s/iter; left time: 553.8567s
	iters: 800, epoch: 9 | loss: 0.0103495
	speed: 0.1184s/iter; left time: 540.3189s
	iters: 900, epoch: 9 | loss: 0.0121629
	speed: 0.1188s/iter; left time: 530.2714s
	iters: 1000, epoch: 9 | loss: 0.0082204
	speed: 0.1172s/iter; left time: 511.5463s
	iters: 1100, epoch: 9 | loss: 0.0094257
	speed: 0.1178s/iter; left time: 502.3288s
	iters: 1200, epoch: 9 | loss: 0.0132612
	speed: 0.1190s/iter; left time: 495.5718s
	iters: 1300, epoch: 9 | loss: 0.0077047
	speed: 0.1183s/iter; left time: 480.7140s
	iters: 1400, epoch: 9 | loss: 0.0120918
	speed: 0.1178s/iter; left time: 466.6769s
	iters: 1500, epoch: 9 | loss: 0.0130780
	speed: 0.1193s/iter; left time: 461.0019s
	iters: 1600, epoch: 9 | loss: 0.0103472
	speed: 0.1176s/iter; left time: 442.3447s
	iters: 1700, epoch: 9 | loss: 0.0085749
	speed: 0.1197s/iter; left time: 438.4042s
	iters: 1800, epoch: 9 | loss: 0.0122015
	speed: 0.1189s/iter; left time: 423.7798s
	iters: 1900, epoch: 9 | loss: 0.0145958
	speed: 0.1184s/iter; left time: 409.9510s
	iters: 2000, epoch: 9 | loss: 0.0118772
	speed: 0.1184s/iter; left time: 398.2628s
	iters: 2100, epoch: 9 | loss: 0.0115369
	speed: 0.1193s/iter; left time: 389.4227s
	iters: 2200, epoch: 9 | loss: 0.0083781
	speed: 0.1200s/iter; left time: 379.5846s
	iters: 2300, epoch: 9 | loss: 0.0116395
	speed: 0.1194s/iter; left time: 365.8267s
	iters: 2400, epoch: 9 | loss: 0.0096427
	speed: 0.1185s/iter; left time: 351.1815s
	iters: 2500, epoch: 9 | loss: 0.0095673
	speed: 0.1182s/iter; left time: 338.2745s
	iters: 2600, epoch: 9 | loss: 0.0096083
	speed: 0.1192s/iter; left time: 329.3200s
Epoch: 9 cost time: 00h:05m:17.76s
Epoch: 9 | Train Loss: 0.0113057 Vali Loss: 0.0088317 Test Loss: 0.0119234
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.011123491451144218, rmse:0.10546796768903732, mae:0.06871083378791809, rse:0.3096562623977661
Intermediate time for ES and pred_len 24: 00h:58m:26.67s

=== Starting experiments for pred_len: 96 ===

--- Running model for ES, pred_len=96 ---
train 85587
val 18435
test 18435
[2024-11-12 17:17:54,907] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 17:17:56,300] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 17:17:56,300] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 17:17:56,300] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 17:17:56,431] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 17:17:56,431] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 17:17:57,276] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 17:17:57,277] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 17:17:57,278] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 17:17:57,279] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 17:17:57,279] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 17:17:57,279] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 17:17:57,279] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 17:17:57,279] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 17:17:57,279] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 17:17:57,279] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 17:17:57,499] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 17:17:57,499] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 17:17:57,522] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.27 GB, percent = 2.7%
[2024-11-12 17:17:57,603] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 17:17:57,603] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 17:17:57,603] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.27 GB, percent = 2.7%
[2024-11-12 17:17:57,603] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 17:17:57,678] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 17:17:57,679] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 17:17:57,679] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.27 GB, percent = 2.7%
[2024-11-12 17:17:57,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 17:17:57,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 17:17:57,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 17:17:57,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 17:17:57,680] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 17:17:57,680] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 17:17:57,680] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 17:17:57,680] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 17:17:57,680] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fccc62b0c90>
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 17:17:57,681] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 17:17:57,682] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 17:17:57,682] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0562205
	speed: 0.1666s/iter; left time: 4437.9411s
	iters: 200, epoch: 1 | loss: 0.0615411
	speed: 0.1229s/iter; left time: 3262.7771s
	iters: 300, epoch: 1 | loss: 0.0496566
	speed: 0.1230s/iter; left time: 3252.0225s
	iters: 400, epoch: 1 | loss: 0.0480501
	speed: 0.1231s/iter; left time: 3243.3070s
	iters: 500, epoch: 1 | loss: 0.0273459
	speed: 0.1232s/iter; left time: 3232.6316s
	iters: 600, epoch: 1 | loss: 0.0255595
	speed: 0.1236s/iter; left time: 3230.3057s
	iters: 700, epoch: 1 | loss: 0.0255974
	speed: 0.1234s/iter; left time: 3212.6665s
	iters: 800, epoch: 1 | loss: 0.0210241
	speed: 0.1235s/iter; left time: 3204.0184s
	iters: 900, epoch: 1 | loss: 0.0281095
	speed: 0.1237s/iter; left time: 3197.2116s
	iters: 1000, epoch: 1 | loss: 0.0249985
	speed: 0.1241s/iter; left time: 3195.6060s
	iters: 1100, epoch: 1 | loss: 0.0248241
	speed: 0.1230s/iter; left time: 3154.4955s
	iters: 1200, epoch: 1 | loss: 0.0176695
	speed: 0.1204s/iter; left time: 3074.8345s
	iters: 1300, epoch: 1 | loss: 0.0228380
	speed: 0.1235s/iter; left time: 3141.3046s
	iters: 1400, epoch: 1 | loss: 0.0220132
	speed: 0.1236s/iter; left time: 3130.9032s
	iters: 1500, epoch: 1 | loss: 0.0217820
	speed: 0.1236s/iter; left time: 3118.7233s
	iters: 1600, epoch: 1 | loss: 0.0208520
	speed: 0.1243s/iter; left time: 3126.2422s
	iters: 1700, epoch: 1 | loss: 0.0255393
	speed: 0.1242s/iter; left time: 3110.1159s
	iters: 1800, epoch: 1 | loss: 0.0229215
	speed: 0.1254s/iter; left time: 3126.4015s
	iters: 1900, epoch: 1 | loss: 0.0277365
	speed: 0.1250s/iter; left time: 3105.4852s
	iters: 2000, epoch: 1 | loss: 0.0169817
	speed: 0.1252s/iter; left time: 3096.6532s
	iters: 2100, epoch: 1 | loss: 0.0212673
	speed: 0.1228s/iter; left time: 3026.1554s
	iters: 2200, epoch: 1 | loss: 0.0152157
	speed: 0.1225s/iter; left time: 3006.5885s
	iters: 2300, epoch: 1 | loss: 0.0253563
	speed: 0.1235s/iter; left time: 3017.8597s
	iters: 2400, epoch: 1 | loss: 0.0144782
	speed: 0.1234s/iter; left time: 3002.8863s
	iters: 2500, epoch: 1 | loss: 0.0202419
	speed: 0.1230s/iter; left time: 2980.8270s
	iters: 2600, epoch: 1 | loss: 0.0236767
	speed: 0.1239s/iter; left time: 2992.2338s
Epoch: 1 cost time: 00h:05m:31.31s
Epoch: 1 | Train Loss: 0.0279895 Vali Loss: 0.0162857 Test Loss: 0.0221774
Validation loss decreased (inf --> 0.016286).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0233489
	speed: 0.8849s/iter; left time: 21208.0040s
	iters: 200, epoch: 2 | loss: 0.0238727
	speed: 0.1186s/iter; left time: 2830.0055s
	iters: 300, epoch: 2 | loss: 0.0188106
	speed: 0.1183s/iter; left time: 2811.9231s
	iters: 400, epoch: 2 | loss: 0.0178056
	speed: 0.1168s/iter; left time: 2764.2094s
	iters: 500, epoch: 2 | loss: 0.0179673
	speed: 0.1170s/iter; left time: 2756.7140s
	iters: 600, epoch: 2 | loss: 0.0194389
	speed: 0.1180s/iter; left time: 2769.8411s
	iters: 700, epoch: 2 | loss: 0.0172730
	speed: 0.1180s/iter; left time: 2758.2273s
	iters: 800, epoch: 2 | loss: 0.0206717
	speed: 0.1175s/iter; left time: 2733.9353s
	iters: 900, epoch: 2 | loss: 0.0163230
	speed: 0.1184s/iter; left time: 2743.9020s
	iters: 1000, epoch: 2 | loss: 0.0172873
	speed: 0.1188s/iter; left time: 2741.0415s
	iters: 1100, epoch: 2 | loss: 0.0158728
	speed: 0.1191s/iter; left time: 2736.4587s
	iters: 1200, epoch: 2 | loss: 0.0228843
	speed: 0.1179s/iter; left time: 2696.4611s
	iters: 1300, epoch: 2 | loss: 0.0178757
	speed: 0.1177s/iter; left time: 2680.3184s
	iters: 1400, epoch: 2 | loss: 0.0154823
	speed: 0.1181s/iter; left time: 2677.2383s
	iters: 1500, epoch: 2 | loss: 0.0186933
	speed: 0.1183s/iter; left time: 2669.5409s
	iters: 1600, epoch: 2 | loss: 0.0156614
	speed: 0.1189s/iter; left time: 2670.6355s
	iters: 1700, epoch: 2 | loss: 0.0269292
	speed: 0.1181s/iter; left time: 2640.9991s
	iters: 1800, epoch: 2 | loss: 0.0205998
	speed: 0.1181s/iter; left time: 2629.6470s
	iters: 1900, epoch: 2 | loss: 0.0165224
	speed: 0.1181s/iter; left time: 2617.3967s
	iters: 2000, epoch: 2 | loss: 0.0198831
	speed: 0.1187s/iter; left time: 2619.6575s
	iters: 2100, epoch: 2 | loss: 0.0175092
	speed: 0.1184s/iter; left time: 2601.5009s
	iters: 2200, epoch: 2 | loss: 0.0193105
	speed: 0.1184s/iter; left time: 2589.7392s
	iters: 2300, epoch: 2 | loss: 0.0158157
	speed: 0.1187s/iter; left time: 2584.0218s
	iters: 2400, epoch: 2 | loss: 0.0190078
	speed: 0.1181s/iter; left time: 2558.6817s
	iters: 2500, epoch: 2 | loss: 0.0142906
	speed: 0.1179s/iter; left time: 2543.2930s
	iters: 2600, epoch: 2 | loss: 0.0173382
	speed: 0.1179s/iter; left time: 2530.0321s
Epoch: 2 cost time: 00h:05m:16.32s
Epoch: 2 | Train Loss: 0.0188001 Vali Loss: 0.0155285 Test Loss: 0.0209162
Validation loss decreased (0.016286 --> 0.015528).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0177140
	speed: 0.8416s/iter; left time: 17920.7836s
	iters: 200, epoch: 3 | loss: 0.0196787
	speed: 0.1177s/iter; left time: 2494.9457s
	iters: 300, epoch: 3 | loss: 0.0188081
	speed: 0.1181s/iter; left time: 2490.5147s
	iters: 400, epoch: 3 | loss: 0.0183770
	speed: 0.1180s/iter; left time: 2477.3454s
	iters: 500, epoch: 3 | loss: 0.0175956
	speed: 0.1178s/iter; left time: 2462.0997s
	iters: 600, epoch: 3 | loss: 0.0203164
	speed: 0.1181s/iter; left time: 2455.4090s
	iters: 700, epoch: 3 | loss: 0.0214823
	speed: 0.1168s/iter; left time: 2416.5487s
	iters: 800, epoch: 3 | loss: 0.0185821
	speed: 0.1177s/iter; left time: 2422.8392s
	iters: 900, epoch: 3 | loss: 0.0175205
	speed: 0.1183s/iter; left time: 2423.3668s
	iters: 1000, epoch: 3 | loss: 0.0216272
	speed: 0.1187s/iter; left time: 2421.6122s
	iters: 1100, epoch: 3 | loss: 0.0181549
	speed: 0.1183s/iter; left time: 2400.3742s
	iters: 1200, epoch: 3 | loss: 0.0184777
	speed: 0.1182s/iter; left time: 2387.4054s
	iters: 1300, epoch: 3 | loss: 0.0182787
	speed: 0.1190s/iter; left time: 2391.9697s
	iters: 1400, epoch: 3 | loss: 0.0148450
	speed: 0.1184s/iter; left time: 2366.9604s
	iters: 1500, epoch: 3 | loss: 0.0185719
	speed: 0.1182s/iter; left time: 2352.0284s
	iters: 1600, epoch: 3 | loss: 0.0238936
	speed: 0.1181s/iter; left time: 2336.7643s
	iters: 1700, epoch: 3 | loss: 0.0209816
	speed: 0.1189s/iter; left time: 2341.9524s
	iters: 1800, epoch: 3 | loss: 0.0160317
	speed: 0.1190s/iter; left time: 2332.1289s
	iters: 1900, epoch: 3 | loss: 0.0230464
	speed: 0.1186s/iter; left time: 2311.9086s
	iters: 2000, epoch: 3 | loss: 0.0154415
	speed: 0.1186s/iter; left time: 2300.1311s
	iters: 2100, epoch: 3 | loss: 0.0259799
	speed: 0.1188s/iter; left time: 2291.3574s
	iters: 2200, epoch: 3 | loss: 0.0223718
	speed: 0.1186s/iter; left time: 2276.3903s
	iters: 2300, epoch: 3 | loss: 0.0149282
	speed: 0.1186s/iter; left time: 2264.1633s
	iters: 2400, epoch: 3 | loss: 0.0149720
	speed: 0.1182s/iter; left time: 2245.7500s
	iters: 2500, epoch: 3 | loss: 0.0186536
	speed: 0.1179s/iter; left time: 2227.9261s
	iters: 2600, epoch: 3 | loss: 0.0159037
	speed: 0.1194s/iter; left time: 2243.4022s
Epoch: 3 cost time: 00h:05m:16.77s
Epoch: 3 | Train Loss: 0.0178578 Vali Loss: 0.0147861 Test Loss: 0.0201043
Validation loss decreased (0.015528 --> 0.014786).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0156282
	speed: 0.8238s/iter; left time: 15337.4997s
	iters: 200, epoch: 4 | loss: 0.0179232
	speed: 0.1170s/iter; left time: 2167.1841s
	iters: 300, epoch: 4 | loss: 0.0139722
	speed: 0.1181s/iter; left time: 2175.4024s
	iters: 400, epoch: 4 | loss: 0.0160886
	speed: 0.1186s/iter; left time: 2171.8512s
	iters: 500, epoch: 4 | loss: 0.0219021
	speed: 0.1185s/iter; left time: 2159.5564s
	iters: 600, epoch: 4 | loss: 0.0160224
	speed: 0.1180s/iter; left time: 2137.9910s
	iters: 700, epoch: 4 | loss: 0.0190574
	speed: 0.1178s/iter; left time: 2123.3011s
	iters: 800, epoch: 4 | loss: 0.0162646
	speed: 0.1180s/iter; left time: 2113.9964s
	iters: 900, epoch: 4 | loss: 0.0151715
	speed: 0.1178s/iter; left time: 2098.8193s
	iters: 1000, epoch: 4 | loss: 0.0161191
	speed: 0.1165s/iter; left time: 2065.0858s
	iters: 1100, epoch: 4 | loss: 0.0185219
	speed: 0.1179s/iter; left time: 2077.8344s
	iters: 1200, epoch: 4 | loss: 0.0181180
	speed: 0.1169s/iter; left time: 2047.2558s
	iters: 1300, epoch: 4 | loss: 0.0156713
	speed: 0.1185s/iter; left time: 2064.2798s
	iters: 1400, epoch: 4 | loss: 0.0189912
	speed: 0.1189s/iter; left time: 2058.5864s
	iters: 1500, epoch: 4 | loss: 0.0157091
	speed: 0.1194s/iter; left time: 2056.1028s
	iters: 1600, epoch: 4 | loss: 0.0179429
	speed: 0.1189s/iter; left time: 2034.7169s
	iters: 1700, epoch: 4 | loss: 0.0189502
	speed: 0.1188s/iter; left time: 2021.2511s
	iters: 1800, epoch: 4 | loss: 0.0136407
	speed: 0.1186s/iter; left time: 2006.9146s
	iters: 1900, epoch: 4 | loss: 0.0182259
	speed: 0.1182s/iter; left time: 1987.2078s
	iters: 2000, epoch: 4 | loss: 0.0210587
	speed: 0.1184s/iter; left time: 1978.7604s
	iters: 2100, epoch: 4 | loss: 0.0209161
	speed: 0.1183s/iter; left time: 1965.7442s
	iters: 2200, epoch: 4 | loss: 0.0143668
	speed: 0.1170s/iter; left time: 1932.5544s
	iters: 2300, epoch: 4 | loss: 0.0166738
	speed: 0.1178s/iter; left time: 1933.9576s
	iters: 2400, epoch: 4 | loss: 0.0210513
	speed: 0.1172s/iter; left time: 1912.9111s
	iters: 2500, epoch: 4 | loss: 0.0150934
	speed: 0.1169s/iter; left time: 1896.5542s
	iters: 2600, epoch: 4 | loss: 0.0159758
	speed: 0.1175s/iter; left time: 1893.6409s
Epoch: 4 cost time: 00h:05m:15.83s
Epoch: 4 | Train Loss: 0.0171844 Vali Loss: 0.0148527 Test Loss: 0.0205019
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0164564
	speed: 0.8089s/iter; left time: 12898.3397s
	iters: 200, epoch: 5 | loss: 0.0232456
	speed: 0.1187s/iter; left time: 1880.0776s
	iters: 300, epoch: 5 | loss: 0.0143200
	speed: 0.1186s/iter; left time: 1867.2551s
	iters: 400, epoch: 5 | loss: 0.0119979
	speed: 0.1182s/iter; left time: 1849.9386s
	iters: 500, epoch: 5 | loss: 0.0154182
	speed: 0.1180s/iter; left time: 1834.7602s
	iters: 600, epoch: 5 | loss: 0.0172274
	speed: 0.1168s/iter; left time: 1803.5917s
	iters: 700, epoch: 5 | loss: 0.0155693
	speed: 0.1193s/iter; left time: 1831.0214s
	iters: 800, epoch: 5 | loss: 0.0174366
	speed: 0.1183s/iter; left time: 1803.1188s
	iters: 900, epoch: 5 | loss: 0.0182169
	speed: 0.1190s/iter; left time: 1802.6044s
	iters: 1000, epoch: 5 | loss: 0.0161281
	speed: 0.1180s/iter; left time: 1775.4184s
	iters: 1100, epoch: 5 | loss: 0.0165226
	speed: 0.1180s/iter; left time: 1763.6820s
	iters: 1200, epoch: 5 | loss: 0.0199898
	speed: 0.1176s/iter; left time: 1745.5717s
	iters: 1300, epoch: 5 | loss: 0.0103930
	speed: 0.1178s/iter; left time: 1736.5485s
	iters: 1400, epoch: 5 | loss: 0.0212616
	speed: 0.1186s/iter; left time: 1737.2714s
	iters: 1500, epoch: 5 | loss: 0.0157672
	speed: 0.1188s/iter; left time: 1727.5973s
	iters: 1600, epoch: 5 | loss: 0.0173533
	speed: 0.1189s/iter; left time: 1716.9214s
	iters: 1700, epoch: 5 | loss: 0.0115727
	speed: 0.1200s/iter; left time: 1722.0483s
	iters: 1800, epoch: 5 | loss: 0.0136951
	speed: 0.1182s/iter; left time: 1683.7876s
	iters: 1900, epoch: 5 | loss: 0.0184059
	speed: 0.1185s/iter; left time: 1675.6718s
	iters: 2000, epoch: 5 | loss: 0.0125891
	speed: 0.1184s/iter; left time: 1662.7249s
	iters: 2100, epoch: 5 | loss: 0.0189931
	speed: 0.1179s/iter; left time: 1644.4952s
	iters: 2200, epoch: 5 | loss: 0.0154729
	speed: 0.1178s/iter; left time: 1630.9708s
	iters: 2300, epoch: 5 | loss: 0.0149517
	speed: 0.1177s/iter; left time: 1618.4238s
	iters: 2400, epoch: 5 | loss: 0.0133085
	speed: 0.1181s/iter; left time: 1611.7200s
	iters: 2500, epoch: 5 | loss: 0.0149594
	speed: 0.1188s/iter; left time: 1609.7117s
	iters: 2600, epoch: 5 | loss: 0.0149752
	speed: 0.1174s/iter; left time: 1578.3938s
Epoch: 5 cost time: 00h:05m:16.58s
Epoch: 5 | Train Loss: 0.0166053 Vali Loss: 0.0153127 Test Loss: 0.0214258
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0223485
	speed: 0.8000s/iter; left time: 10616.1517s
	iters: 200, epoch: 6 | loss: 0.0121768
	speed: 0.1178s/iter; left time: 1551.6269s
	iters: 300, epoch: 6 | loss: 0.0126217
	speed: 0.1185s/iter; left time: 1548.7992s
	iters: 400, epoch: 6 | loss: 0.0139260
	speed: 0.1179s/iter; left time: 1529.6608s
	iters: 500, epoch: 6 | loss: 0.0171338
	speed: 0.1182s/iter; left time: 1521.1395s
	iters: 600, epoch: 6 | loss: 0.0149041
	speed: 0.1175s/iter; left time: 1500.2834s
	iters: 700, epoch: 6 | loss: 0.0175755
	speed: 0.1170s/iter; left time: 1482.6186s
	iters: 800, epoch: 6 | loss: 0.0145324
	speed: 0.1172s/iter; left time: 1473.6181s
	iters: 900, epoch: 6 | loss: 0.0171111
	speed: 0.1167s/iter; left time: 1455.8475s
	iters: 1000, epoch: 6 | loss: 0.0146363
	speed: 0.1173s/iter; left time: 1451.3341s
	iters: 1100, epoch: 6 | loss: 0.0150884
	speed: 0.1166s/iter; left time: 1430.5942s
	iters: 1200, epoch: 6 | loss: 0.0143555
	speed: 0.1178s/iter; left time: 1433.9907s
	iters: 1300, epoch: 6 | loss: 0.0188446
	speed: 0.1182s/iter; left time: 1426.8920s
	iters: 1400, epoch: 6 | loss: 0.0166732
	speed: 0.1181s/iter; left time: 1413.5783s
	iters: 1500, epoch: 6 | loss: 0.0156958
	speed: 0.1189s/iter; left time: 1411.6659s
	iters: 1600, epoch: 6 | loss: 0.0139390
	speed: 0.1182s/iter; left time: 1391.4093s
	iters: 1700, epoch: 6 | loss: 0.0142860
	speed: 0.1177s/iter; left time: 1373.9553s
	iters: 1800, epoch: 6 | loss: 0.0155759
	speed: 0.1195s/iter; left time: 1382.7811s
	iters: 1900, epoch: 6 | loss: 0.0164216
	speed: 0.1184s/iter; left time: 1358.3143s
	iters: 2000, epoch: 6 | loss: 0.0164557
	speed: 0.1186s/iter; left time: 1348.1671s
	iters: 2100, epoch: 6 | loss: 0.0135262
	speed: 0.1163s/iter; left time: 1311.1888s
	iters: 2200, epoch: 6 | loss: 0.0157910
	speed: 0.1182s/iter; left time: 1320.4871s
	iters: 2300, epoch: 6 | loss: 0.0160504
	speed: 0.1177s/iter; left time: 1302.9435s
	iters: 2400, epoch: 6 | loss: 0.0174849
	speed: 0.1180s/iter; left time: 1294.2419s
	iters: 2500, epoch: 6 | loss: 0.0164651
	speed: 0.1180s/iter; left time: 1283.1895s
	iters: 2600, epoch: 6 | loss: 0.0133010
	speed: 0.1182s/iter; left time: 1272.9705s
Epoch: 6 cost time: 00h:05m:15.46s
Epoch: 6 | Train Loss: 0.0159895 Vali Loss: 0.0162522 Test Loss: 0.0233178
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.02010430209338665, rmse:0.14178964495658875, mae:0.09643717110157013, rse:0.41650381684303284
Intermediate time for ES and pred_len 96: 00h:38m:52.80s

=== Starting experiments for pred_len: 168 ===

--- Running model for ES, pred_len=168 ---
train 85371
val 18219
test 18219
[2024-11-12 17:56:46,934] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 17:56:47,952] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 17:56:47,952] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 17:56:47,952] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 17:56:48,069] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 17:56:48,069] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 17:56:48,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 17:56:48,990] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 17:56:48,990] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 17:56:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 17:56:48,991] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 17:56:48,991] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 17:56:48,991] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 17:56:48,991] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 17:56:48,991] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 17:56:48,991] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 17:56:49,200] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 17:56:49,201] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 17:56:49,230] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.65 GB, percent = 2.7%
[2024-11-12 17:56:49,312] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 17:56:49,312] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 17:56:49,313] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.65 GB, percent = 2.7%
[2024-11-12 17:56:49,313] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 17:56:49,384] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 17:56:49,384] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 17:56:49,384] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 27.65 GB, percent = 2.7%
[2024-11-12 17:56:49,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 17:56:49,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 17:56:49,385] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 17:56:49,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 17:56:49,385] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3a2c8f3790>
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 17:56:49,385] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 17:56:49,386] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 17:56:49,386] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0667607
	speed: 0.1632s/iter; left time: 4336.6251s
	iters: 200, epoch: 1 | loss: 0.0647801
	speed: 0.1229s/iter; left time: 3254.5416s
	iters: 300, epoch: 1 | loss: 0.0477799
	speed: 0.1220s/iter; left time: 3218.3779s
	iters: 400, epoch: 1 | loss: 0.0545483
	speed: 0.1229s/iter; left time: 3229.1619s
	iters: 500, epoch: 1 | loss: 0.0587163
	speed: 0.1232s/iter; left time: 3225.1612s
	iters: 600, epoch: 1 | loss: 0.0502290
	speed: 0.1233s/iter; left time: 3214.8226s
	iters: 700, epoch: 1 | loss: 0.0262464
	speed: 0.1232s/iter; left time: 3200.2894s
	iters: 800, epoch: 1 | loss: 0.0290788
	speed: 0.1231s/iter; left time: 3184.7234s
	iters: 900, epoch: 1 | loss: 0.0317928
	speed: 0.1246s/iter; left time: 3209.8712s
	iters: 1000, epoch: 1 | loss: 0.0281496
	speed: 0.1239s/iter; left time: 3179.8947s
	iters: 1100, epoch: 1 | loss: 0.0204535
	speed: 0.1218s/iter; left time: 3114.0214s
	iters: 1200, epoch: 1 | loss: 0.0259826
	speed: 0.1231s/iter; left time: 3136.1018s
	iters: 1300, epoch: 1 | loss: 0.0238629
	speed: 0.1235s/iter; left time: 3132.2769s
	iters: 1400, epoch: 1 | loss: 0.0234594
	speed: 0.1229s/iter; left time: 3104.6009s
	iters: 1500, epoch: 1 | loss: 0.0263001
	speed: 0.1233s/iter; left time: 3104.6554s
	iters: 1600, epoch: 1 | loss: 0.0232118
	speed: 0.1240s/iter; left time: 3109.4820s
	iters: 1700, epoch: 1 | loss: 0.0200240
	speed: 0.1246s/iter; left time: 3111.1812s
	iters: 1800, epoch: 1 | loss: 0.0255148
	speed: 0.1230s/iter; left time: 3059.3099s
	iters: 1900, epoch: 1 | loss: 0.0210648
	speed: 0.1234s/iter; left time: 3057.1488s
	iters: 2000, epoch: 1 | loss: 0.0221414
	speed: 0.1245s/iter; left time: 3070.5787s
	iters: 2100, epoch: 1 | loss: 0.0192121
	speed: 0.1236s/iter; left time: 3037.5319s
	iters: 2200, epoch: 1 | loss: 0.0171362
	speed: 0.1236s/iter; left time: 3024.8261s
	iters: 2300, epoch: 1 | loss: 0.0202530
	speed: 0.1231s/iter; left time: 2999.5523s
	iters: 2400, epoch: 1 | loss: 0.0246680
	speed: 0.1219s/iter; left time: 2957.5271s
	iters: 2500, epoch: 1 | loss: 0.0196966
	speed: 0.1236s/iter; left time: 2988.6726s
	iters: 2600, epoch: 1 | loss: 0.0254166
	speed: 0.1247s/iter; left time: 3001.3103s
Epoch: 1 cost time: 00h:05m:30.07s
Epoch: 1 | Train Loss: 0.0319512 Vali Loss: 0.0181582 Test Loss: 0.0240126
Validation loss decreased (inf --> 0.018158).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0194376
	speed: 0.9184s/iter; left time: 21953.2976s
	iters: 200, epoch: 2 | loss: 0.0190713
	speed: 0.1178s/iter; left time: 2805.2188s
	iters: 300, epoch: 2 | loss: 0.0243947
	speed: 0.1177s/iter; left time: 2788.8932s
	iters: 400, epoch: 2 | loss: 0.0185748
	speed: 0.1176s/iter; left time: 2776.0729s
	iters: 500, epoch: 2 | loss: 0.0251139
	speed: 0.1173s/iter; left time: 2757.4459s
	iters: 600, epoch: 2 | loss: 0.0186687
	speed: 0.1172s/iter; left time: 2742.1655s
	iters: 700, epoch: 2 | loss: 0.0184233
	speed: 0.1185s/iter; left time: 2760.7513s
	iters: 800, epoch: 2 | loss: 0.0201159
	speed: 0.1179s/iter; left time: 2734.8896s
	iters: 900, epoch: 2 | loss: 0.0221073
	speed: 0.1183s/iter; left time: 2732.6111s
	iters: 1000, epoch: 2 | loss: 0.0207406
	speed: 0.1177s/iter; left time: 2707.6848s
	iters: 1100, epoch: 2 | loss: 0.0195115
	speed: 0.1178s/iter; left time: 2698.2652s
	iters: 1200, epoch: 2 | loss: 0.0222875
	speed: 0.1188s/iter; left time: 2708.5535s
	iters: 1300, epoch: 2 | loss: 0.0243909
	speed: 0.1178s/iter; left time: 2673.4086s
	iters: 1400, epoch: 2 | loss: 0.0181278
	speed: 0.1174s/iter; left time: 2653.7444s
	iters: 1500, epoch: 2 | loss: 0.0217748
	speed: 0.1180s/iter; left time: 2655.9170s
	iters: 1600, epoch: 2 | loss: 0.0201805
	speed: 0.1174s/iter; left time: 2629.3216s
	iters: 1700, epoch: 2 | loss: 0.0213851
	speed: 0.1190s/iter; left time: 2654.6299s
	iters: 1800, epoch: 2 | loss: 0.0188240
	speed: 0.1184s/iter; left time: 2628.8722s
	iters: 1900, epoch: 2 | loss: 0.0202289
	speed: 0.1182s/iter; left time: 2613.4081s
	iters: 2000, epoch: 2 | loss: 0.0190315
	speed: 0.1193s/iter; left time: 2624.7080s
	iters: 2100, epoch: 2 | loss: 0.0236717
	speed: 0.1186s/iter; left time: 2597.3438s
	iters: 2200, epoch: 2 | loss: 0.0170608
	speed: 0.1183s/iter; left time: 2579.2916s
	iters: 2300, epoch: 2 | loss: 0.0158485
	speed: 0.1179s/iter; left time: 2559.2699s
	iters: 2400, epoch: 2 | loss: 0.0226687
	speed: 0.1178s/iter; left time: 2544.0586s
	iters: 2500, epoch: 2 | loss: 0.0172871
	speed: 0.1185s/iter; left time: 2548.5726s
	iters: 2600, epoch: 2 | loss: 0.0202338
	speed: 0.1184s/iter; left time: 2534.0215s
Epoch: 2 cost time: 00h:05m:15.23s
Epoch: 2 | Train Loss: 0.0203841 Vali Loss: 0.0179391 Test Loss: 0.0229153
Validation loss decreased (0.018158 --> 0.017939).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0193680
	speed: 0.8070s/iter; left time: 17137.8533s
	iters: 200, epoch: 3 | loss: 0.0121227
	speed: 0.1176s/iter; left time: 2485.0639s
	iters: 300, epoch: 3 | loss: 0.0174387
	speed: 0.1186s/iter; left time: 2494.9651s
	iters: 400, epoch: 3 | loss: 0.0227722
	speed: 0.1180s/iter; left time: 2470.0942s
	iters: 500, epoch: 3 | loss: 0.0166070
	speed: 0.1172s/iter; left time: 2442.1831s
	iters: 600, epoch: 3 | loss: 0.0221936
	speed: 0.1178s/iter; left time: 2443.1420s
	iters: 700, epoch: 3 | loss: 0.0160178
	speed: 0.1183s/iter; left time: 2440.4893s
	iters: 800, epoch: 3 | loss: 0.0227847
	speed: 0.1173s/iter; left time: 2408.6429s
	iters: 900, epoch: 3 | loss: 0.0167493
	speed: 0.1179s/iter; left time: 2408.7605s
	iters: 1000, epoch: 3 | loss: 0.0210526
	speed: 0.1182s/iter; left time: 2404.3344s
	iters: 1100, epoch: 3 | loss: 0.0223127
	speed: 0.1179s/iter; left time: 2385.5123s
	iters: 1200, epoch: 3 | loss: 0.0235141
	speed: 0.1185s/iter; left time: 2387.0457s
	iters: 1300, epoch: 3 | loss: 0.0188325
	speed: 0.1179s/iter; left time: 2363.0085s
	iters: 1400, epoch: 3 | loss: 0.0204581
	speed: 0.1182s/iter; left time: 2357.2105s
	iters: 1500, epoch: 3 | loss: 0.0177237
	speed: 0.1178s/iter; left time: 2335.8888s
	iters: 1600, epoch: 3 | loss: 0.0202699
	speed: 0.1182s/iter; left time: 2332.5838s
	iters: 1700, epoch: 3 | loss: 0.0218207
	speed: 0.1184s/iter; left time: 2324.9928s
	iters: 1800, epoch: 3 | loss: 0.0194628
	speed: 0.1182s/iter; left time: 2309.9816s
	iters: 1900, epoch: 3 | loss: 0.0180804
	speed: 0.1181s/iter; left time: 2294.6968s
	iters: 2000, epoch: 3 | loss: 0.0179418
	speed: 0.1188s/iter; left time: 2297.7200s
	iters: 2100, epoch: 3 | loss: 0.0266515
	speed: 0.1190s/iter; left time: 2289.1929s
	iters: 2200, epoch: 3 | loss: 0.0168945
	speed: 0.1187s/iter; left time: 2271.1368s
	iters: 2300, epoch: 3 | loss: 0.0167936
	speed: 0.1182s/iter; left time: 2249.3002s
	iters: 2400, epoch: 3 | loss: 0.0180226
	speed: 0.1183s/iter; left time: 2240.8459s
	iters: 2500, epoch: 3 | loss: 0.0177396
	speed: 0.1170s/iter; left time: 2204.4213s
	iters: 2600, epoch: 3 | loss: 0.0216624
	speed: 0.1171s/iter; left time: 2194.2616s
Epoch: 3 cost time: 00h:05m:15.10s
Epoch: 3 | Train Loss: 0.0191922 Vali Loss: 0.0174912 Test Loss: 0.0217524
Validation loss decreased (0.017939 --> 0.017491).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0197114
	speed: 0.8130s/iter; left time: 15097.8171s
	iters: 200, epoch: 4 | loss: 0.0157281
	speed: 0.1177s/iter; left time: 2173.0645s
	iters: 300, epoch: 4 | loss: 0.0173862
	speed: 0.1181s/iter; left time: 2169.0379s
	iters: 400, epoch: 4 | loss: 0.0214945
	speed: 0.1181s/iter; left time: 2158.2216s
	iters: 500, epoch: 4 | loss: 0.0179972
	speed: 0.1174s/iter; left time: 2134.0199s
	iters: 600, epoch: 4 | loss: 0.0244034
	speed: 0.1182s/iter; left time: 2135.6793s
	iters: 700, epoch: 4 | loss: 0.0205954
	speed: 0.1190s/iter; left time: 2138.6474s
	iters: 800, epoch: 4 | loss: 0.0168051
	speed: 0.1180s/iter; left time: 2109.0248s
	iters: 900, epoch: 4 | loss: 0.0156627
	speed: 0.1191s/iter; left time: 2115.9773s
	iters: 1000, epoch: 4 | loss: 0.0150014
	speed: 0.1185s/iter; left time: 2093.8602s
	iters: 1100, epoch: 4 | loss: 0.0196936
	speed: 0.1187s/iter; left time: 2085.2078s
	iters: 1200, epoch: 4 | loss: 0.0162599
	speed: 0.1181s/iter; left time: 2063.1241s
	iters: 1300, epoch: 4 | loss: 0.0157100
	speed: 0.1637s/iter; left time: 2842.8910s
	iters: 1400, epoch: 4 | loss: 0.0163708
	speed: 0.2175s/iter; left time: 3756.5008s
	iters: 1500, epoch: 4 | loss: 0.0184773
	speed: 0.1171s/iter; left time: 2010.1331s
	iters: 1600, epoch: 4 | loss: 0.0178051
	speed: 0.1179s/iter; left time: 2013.3931s
	iters: 1700, epoch: 4 | loss: 0.0146827
	speed: 0.1186s/iter; left time: 2012.1632s
	iters: 1800, epoch: 4 | loss: 0.0199797
	speed: 0.1177s/iter; left time: 1985.1764s
	iters: 1900, epoch: 4 | loss: 0.0219427
	speed: 0.1181s/iter; left time: 1980.6564s
	iters: 2000, epoch: 4 | loss: 0.0172931
	speed: 0.1182s/iter; left time: 1970.8105s
	iters: 2100, epoch: 4 | loss: 0.0249441
	speed: 0.1187s/iter; left time: 1966.5472s
	iters: 2200, epoch: 4 | loss: 0.0137533
	speed: 0.1173s/iter; left time: 1931.5018s
	iters: 2300, epoch: 4 | loss: 0.0201245
	speed: 0.1172s/iter; left time: 1918.3426s
	iters: 2400, epoch: 4 | loss: 0.0179501
	speed: 0.1188s/iter; left time: 1933.0041s
	iters: 2500, epoch: 4 | loss: 0.0201867
	speed: 0.1173s/iter; left time: 1897.0598s
	iters: 2600, epoch: 4 | loss: 0.0167003
	speed: 0.1171s/iter; left time: 1881.6582s
Epoch: 4 cost time: 00h:05m:29.65s
Epoch: 4 | Train Loss: 0.0182527 Vali Loss: 0.0173162 Test Loss: 0.0225081
Validation loss decreased (0.017491 --> 0.017316).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0200314
	speed: 0.8082s/iter; left time: 12853.3632s
	iters: 200, epoch: 5 | loss: 0.0179361
	speed: 0.1177s/iter; left time: 1860.2239s
	iters: 300, epoch: 5 | loss: 0.0198960
	speed: 0.1185s/iter; left time: 1860.1479s
	iters: 400, epoch: 5 | loss: 0.0163527
	speed: 0.1169s/iter; left time: 1824.2823s
	iters: 500, epoch: 5 | loss: 0.0205075
	speed: 0.1176s/iter; left time: 1823.8173s
	iters: 600, epoch: 5 | loss: 0.0179766
	speed: 0.1174s/iter; left time: 1808.3802s
	iters: 700, epoch: 5 | loss: 0.0176956
	speed: 0.1181s/iter; left time: 1807.5243s
	iters: 800, epoch: 5 | loss: 0.0171347
	speed: 0.1192s/iter; left time: 1811.4759s
	iters: 900, epoch: 5 | loss: 0.0197008
	speed: 0.1183s/iter; left time: 1787.3294s
	iters: 1000, epoch: 5 | loss: 0.0161437
	speed: 0.1182s/iter; left time: 1773.0945s
	iters: 1100, epoch: 5 | loss: 0.0157670
	speed: 0.1181s/iter; left time: 1759.6408s
	iters: 1200, epoch: 5 | loss: 0.0174743
	speed: 0.1177s/iter; left time: 1741.5752s
	iters: 1300, epoch: 5 | loss: 0.0173994
	speed: 0.1180s/iter; left time: 1735.2992s
	iters: 1400, epoch: 5 | loss: 0.0178414
	speed: 0.1182s/iter; left time: 1725.7536s
	iters: 1500, epoch: 5 | loss: 0.0156718
	speed: 0.1183s/iter; left time: 1716.0461s
	iters: 1600, epoch: 5 | loss: 0.0229516
	speed: 0.1181s/iter; left time: 1700.7650s
	iters: 1700, epoch: 5 | loss: 0.0207339
	speed: 0.1189s/iter; left time: 1700.7933s
	iters: 1800, epoch: 5 | loss: 0.0168245
	speed: 0.1186s/iter; left time: 1684.8331s
	iters: 1900, epoch: 5 | loss: 0.0168168
	speed: 0.1192s/iter; left time: 1680.9545s
	iters: 2000, epoch: 5 | loss: 0.0133681
	speed: 0.1174s/iter; left time: 1643.4702s
	iters: 2100, epoch: 5 | loss: 0.0198422
	speed: 0.1169s/iter; left time: 1625.5910s
	iters: 2200, epoch: 5 | loss: 0.0165978
	speed: 0.1180s/iter; left time: 1628.4821s
	iters: 2300, epoch: 5 | loss: 0.0154563
	speed: 0.1182s/iter; left time: 1619.6566s
	iters: 2400, epoch: 5 | loss: 0.0174232
	speed: 0.1188s/iter; left time: 1615.4339s
	iters: 2500, epoch: 5 | loss: 0.0147960
	speed: 0.1192s/iter; left time: 1609.2460s
	iters: 2600, epoch: 5 | loss: 0.0185831
	speed: 0.1182s/iter; left time: 1583.7089s
Epoch: 5 cost time: 00h:05m:15.40s
Epoch: 5 | Train Loss: 0.0175474 Vali Loss: 0.0173207 Test Loss: 0.0230904
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0134064
	speed: 0.7871s/iter; left time: 10418.1306s
	iters: 200, epoch: 6 | loss: 0.0215653
	speed: 0.1184s/iter; left time: 1554.8739s
	iters: 300, epoch: 6 | loss: 0.0164681
	speed: 0.1188s/iter; left time: 1548.9519s
	iters: 400, epoch: 6 | loss: 0.0167478
	speed: 0.1178s/iter; left time: 1524.4756s
	iters: 500, epoch: 6 | loss: 0.0182057
	speed: 0.1189s/iter; left time: 1525.6303s
	iters: 600, epoch: 6 | loss: 0.0186282
	speed: 0.1180s/iter; left time: 1502.3207s
	iters: 700, epoch: 6 | loss: 0.0166893
	speed: 0.1176s/iter; left time: 1485.8864s
	iters: 800, epoch: 6 | loss: 0.0223045
	speed: 0.1177s/iter; left time: 1475.3988s
	iters: 900, epoch: 6 | loss: 0.0178537
	speed: 0.1180s/iter; left time: 1467.1709s
	iters: 1000, epoch: 6 | loss: 0.0236684
	speed: 0.1164s/iter; left time: 1436.1879s
	iters: 1100, epoch: 6 | loss: 0.0190349
	speed: 0.1188s/iter; left time: 1453.6551s
	iters: 1200, epoch: 6 | loss: 0.0180589
	speed: 0.1183s/iter; left time: 1435.4167s
	iters: 1300, epoch: 6 | loss: 0.0161074
	speed: 0.1190s/iter; left time: 1432.4422s
	iters: 1400, epoch: 6 | loss: 0.0120870
	speed: 0.1178s/iter; left time: 1406.1309s
	iters: 1500, epoch: 6 | loss: 0.0148896
	speed: 0.1172s/iter; left time: 1387.7033s
	iters: 1600, epoch: 6 | loss: 0.0173568
	speed: 0.1196s/iter; left time: 1403.2309s
	iters: 1700, epoch: 6 | loss: 0.0191535
	speed: 0.1178s/iter; left time: 1371.2970s
	iters: 1800, epoch: 6 | loss: 0.0163945
	speed: 0.1164s/iter; left time: 1342.4194s
	iters: 1900, epoch: 6 | loss: 0.0169746
	speed: 0.1172s/iter; left time: 1340.2614s
	iters: 2000, epoch: 6 | loss: 0.0157028
	speed: 0.1176s/iter; left time: 1332.5739s
	iters: 2100, epoch: 6 | loss: 0.0154276
	speed: 0.1175s/iter; left time: 1319.8241s
	iters: 2200, epoch: 6 | loss: 0.0156748
	speed: 0.1180s/iter; left time: 1314.2257s
	iters: 2300, epoch: 6 | loss: 0.0185000
	speed: 0.1177s/iter; left time: 1298.6110s
	iters: 2400, epoch: 6 | loss: 0.0123806
	speed: 0.1178s/iter; left time: 1287.8039s
	iters: 2500, epoch: 6 | loss: 0.0169220
	speed: 0.1174s/iter; left time: 1272.5997s
	iters: 2600, epoch: 6 | loss: 0.0163204
	speed: 0.1171s/iter; left time: 1256.6723s
Epoch: 6 cost time: 00h:05m:14.59s
Epoch: 6 | Train Loss: 0.0166633 Vali Loss: 0.0181887 Test Loss: 0.0242044
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0168511
	speed: 0.8037s/iter; left time: 8494.8262s
	iters: 200, epoch: 7 | loss: 0.0155326
	speed: 0.1181s/iter; left time: 1235.9952s
	iters: 300, epoch: 7 | loss: 0.0165381
	speed: 0.1187s/iter; left time: 1230.8126s
	iters: 400, epoch: 7 | loss: 0.0137137
	speed: 0.1171s/iter; left time: 1202.5307s
	iters: 500, epoch: 7 | loss: 0.0194662
	speed: 0.1183s/iter; left time: 1202.8434s
	iters: 600, epoch: 7 | loss: 0.0215308
	speed: 0.1181s/iter; left time: 1189.0884s
	iters: 700, epoch: 7 | loss: 0.0155118
	speed: 0.1181s/iter; left time: 1176.9503s
	iters: 800, epoch: 7 | loss: 0.0160412
	speed: 0.1181s/iter; left time: 1165.0546s
	iters: 900, epoch: 7 | loss: 0.0167252
	speed: 0.1179s/iter; left time: 1152.0667s
	iters: 1000, epoch: 7 | loss: 0.0198276
	speed: 0.1177s/iter; left time: 1137.6924s
	iters: 1100, epoch: 7 | loss: 0.0160068
	speed: 0.1175s/iter; left time: 1124.7975s
	iters: 1200, epoch: 7 | loss: 0.0173708
	speed: 0.1180s/iter; left time: 1117.0463s
	iters: 1300, epoch: 7 | loss: 0.0135569
	speed: 0.1185s/iter; left time: 1110.5485s
	iters: 1400, epoch: 7 | loss: 0.0157510
	speed: 0.1179s/iter; left time: 1092.8768s
	iters: 1500, epoch: 7 | loss: 0.0136087
	speed: 0.1178s/iter; left time: 1079.9605s
	iters: 1600, epoch: 7 | loss: 0.0147208
	speed: 0.1183s/iter; left time: 1072.5617s
	iters: 1700, epoch: 7 | loss: 0.0187485
	speed: 0.1190s/iter; left time: 1066.8704s
	iters: 1800, epoch: 7 | loss: 0.0158146
	speed: 0.1188s/iter; left time: 1053.3040s
	iters: 1900, epoch: 7 | loss: 0.0147903
	speed: 0.1183s/iter; left time: 1037.4941s
	iters: 2000, epoch: 7 | loss: 0.0110969
	speed: 0.1178s/iter; left time: 1021.5648s
	iters: 2100, epoch: 7 | loss: 0.0155053
	speed: 0.1182s/iter; left time: 1012.9945s
	iters: 2200, epoch: 7 | loss: 0.0184283
	speed: 0.1184s/iter; left time: 1002.3877s
	iters: 2300, epoch: 7 | loss: 0.0142109
	speed: 0.1183s/iter; left time: 990.2640s
	iters: 2400, epoch: 7 | loss: 0.0151944
	speed: 0.1177s/iter; left time: 973.5470s
	iters: 2500, epoch: 7 | loss: 0.0184856
	speed: 0.1178s/iter; left time: 962.3483s
	iters: 2600, epoch: 7 | loss: 0.0145762
	speed: 0.1185s/iter; left time: 956.2792s
Epoch: 7 cost time: 00h:05m:15.16s
Epoch: 7 | Train Loss: 0.0158829 Vali Loss: 0.0191553 Test Loss: 0.0257859
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.022508107125759125, rmse:0.15002702176570892, mae:0.10180532187223434, rse:0.440519779920578
Intermediate time for ES and pred_len 168: 00h:45m:15.40s
Intermediate time for ES: 02h:22m:34.88s

=== Starting experiments for country: FR ===

=== Starting experiments for pred_len: 24 ===

--- Running model for FR, pred_len=24 ---
train 85803
val 18651
test 18651
[2024-11-12 18:42:02,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 18:42:03,314] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 18:42:03,314] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 18:42:03,314] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 18:42:03,405] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 18:42:03,406] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 18:42:03,845] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 18:42:03,846] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 18:42:03,846] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 18:42:03,848] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 18:42:03,848] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 18:42:03,848] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 18:42:03,848] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 18:42:03,848] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 18:42:03,848] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 18:42:03,848] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 18:42:04,025] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 18:42:04,026] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 18:42:04,042] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.51 GB, percent = 2.6%
[2024-11-12 18:42:04,127] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 18:42:04,128] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 18:42:04,128] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.51 GB, percent = 2.6%
[2024-11-12 18:42:04,128] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 18:42:04,208] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 18:42:04,209] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 18:42:04,209] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 26.51 GB, percent = 2.6%
[2024-11-12 18:42:04,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 18:42:04,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 18:42:04,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 18:42:04,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 18:42:04,210] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f309d733110>
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 18:42:04,210] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 18:42:04,211] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 18:42:04,211] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0444962
	speed: 0.1573s/iter; left time: 4201.4335s
	iters: 200, epoch: 1 | loss: 0.0322137
	speed: 0.1220s/iter; left time: 3245.2603s
	iters: 300, epoch: 1 | loss: 0.0260007
	speed: 0.1235s/iter; left time: 3274.0954s
	iters: 400, epoch: 1 | loss: 0.0229708
	speed: 0.1232s/iter; left time: 3252.9537s
	iters: 500, epoch: 1 | loss: 0.0172196
	speed: 0.1232s/iter; left time: 3240.3936s
	iters: 600, epoch: 1 | loss: 0.0131601
	speed: 0.1230s/iter; left time: 3224.9327s
	iters: 700, epoch: 1 | loss: 0.0154212
	speed: 0.1231s/iter; left time: 3214.4310s
	iters: 800, epoch: 1 | loss: 0.0082462
	speed: 0.1234s/iter; left time: 3210.8266s
	iters: 900, epoch: 1 | loss: 0.0102866
	speed: 0.1231s/iter; left time: 3190.2669s
	iters: 1000, epoch: 1 | loss: 0.0180272
	speed: 0.1233s/iter; left time: 3182.2283s
	iters: 1100, epoch: 1 | loss: 0.0058429
	speed: 0.1231s/iter; left time: 3164.7459s
	iters: 1200, epoch: 1 | loss: 0.0124263
	speed: 0.1237s/iter; left time: 3167.5803s
	iters: 1300, epoch: 1 | loss: 0.0098113
	speed: 0.1230s/iter; left time: 3136.9992s
	iters: 1400, epoch: 1 | loss: 0.0096488
	speed: 0.1230s/iter; left time: 3125.0103s
	iters: 1500, epoch: 1 | loss: 0.0112721
	speed: 0.1230s/iter; left time: 3113.4618s
	iters: 1600, epoch: 1 | loss: 0.0067157
	speed: 0.1228s/iter; left time: 3095.2286s
	iters: 1700, epoch: 1 | loss: 0.0133950
	speed: 0.1225s/iter; left time: 3077.1308s
	iters: 1800, epoch: 1 | loss: 0.0064615
	speed: 0.1217s/iter; left time: 3043.8273s
	iters: 1900, epoch: 1 | loss: 0.0078250
	speed: 0.1233s/iter; left time: 3071.5778s
	iters: 2000, epoch: 1 | loss: 0.0104470
	speed: 0.1223s/iter; left time: 3033.2443s
	iters: 2100, epoch: 1 | loss: 0.0059813
	speed: 0.1231s/iter; left time: 3042.7230s
	iters: 2200, epoch: 1 | loss: 0.0102676
	speed: 0.1229s/iter; left time: 3025.4167s
	iters: 2300, epoch: 1 | loss: 0.0117478
	speed: 0.1229s/iter; left time: 3012.0458s
	iters: 2400, epoch: 1 | loss: 0.0072785
	speed: 0.1230s/iter; left time: 3002.9440s
	iters: 2500, epoch: 1 | loss: 0.0127181
	speed: 0.1231s/iter; left time: 2993.0053s
	iters: 2600, epoch: 1 | loss: 0.0094498
	speed: 0.1215s/iter; left time: 2940.5839s
Epoch: 1 cost time: 00h:05m:30.61s
Epoch: 1 | Train Loss: 0.0153905 Vali Loss: 0.0103031 Test Loss: 0.0123303
Validation loss decreased (inf --> 0.010303).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0085017
	speed: 0.9419s/iter; left time: 22635.0155s
	iters: 200, epoch: 2 | loss: 0.0186106
	speed: 0.1194s/iter; left time: 2857.3399s
	iters: 300, epoch: 2 | loss: 0.0121583
	speed: 0.1192s/iter; left time: 2839.8626s
	iters: 400, epoch: 2 | loss: 0.0157024
	speed: 0.1204s/iter; left time: 2856.7835s
	iters: 500, epoch: 2 | loss: 0.0091020
	speed: 0.1197s/iter; left time: 2829.0098s
	iters: 600, epoch: 2 | loss: 0.0099834
	speed: 0.1357s/iter; left time: 3194.0647s
	iters: 700, epoch: 2 | loss: 0.0058643
	speed: 0.1191s/iter; left time: 2790.9231s
	iters: 800, epoch: 2 | loss: 0.0085307
	speed: 0.1199s/iter; left time: 2797.1782s
	iters: 900, epoch: 2 | loss: 0.0073949
	speed: 0.1176s/iter; left time: 2732.1667s
	iters: 1000, epoch: 2 | loss: 0.0051499
	speed: 0.1201s/iter; left time: 2777.2088s
	iters: 1100, epoch: 2 | loss: 0.0080156
	speed: 0.1201s/iter; left time: 2766.1863s
	iters: 1200, epoch: 2 | loss: 0.0144322
	speed: 0.1188s/iter; left time: 2724.3725s
	iters: 1300, epoch: 2 | loss: 0.0107730
	speed: 0.1185s/iter; left time: 2704.9540s
	iters: 1400, epoch: 2 | loss: 0.0059757
	speed: 0.1200s/iter; left time: 2728.0709s
	iters: 1500, epoch: 2 | loss: 0.0105960
	speed: 0.1188s/iter; left time: 2687.8788s
	iters: 1600, epoch: 2 | loss: 0.0108737
	speed: 0.1189s/iter; left time: 2677.7066s
	iters: 1700, epoch: 2 | loss: 0.0048201
	speed: 0.1195s/iter; left time: 2681.0910s
	iters: 1800, epoch: 2 | loss: 0.0090435
	speed: 0.1190s/iter; left time: 2657.1134s
	iters: 1900, epoch: 2 | loss: 0.0124431
	speed: 0.1192s/iter; left time: 2650.2937s
	iters: 2000, epoch: 2 | loss: 0.0078362
	speed: 0.1190s/iter; left time: 2633.7970s
	iters: 2100, epoch: 2 | loss: 0.0067583
	speed: 0.1186s/iter; left time: 2613.6855s
	iters: 2200, epoch: 2 | loss: 0.0141461
	speed: 0.1196s/iter; left time: 2623.6617s
	iters: 2300, epoch: 2 | loss: 0.0102704
	speed: 0.1186s/iter; left time: 2588.8704s
	iters: 2400, epoch: 2 | loss: 0.0092546
	speed: 0.1198s/iter; left time: 2602.8768s
	iters: 2500, epoch: 2 | loss: 0.0064699
	speed: 0.1217s/iter; left time: 2632.6361s
	iters: 2600, epoch: 2 | loss: 0.0081176
	speed: 0.1194s/iter; left time: 2569.7127s
Epoch: 2 cost time: 00h:05m:21.97s
Epoch: 2 | Train Loss: 0.0091760 Vali Loss: 0.0096819 Test Loss: 0.0112116
Validation loss decreased (0.010303 --> 0.009682).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0106176
	speed: 0.8651s/iter; left time: 18468.5117s
	iters: 200, epoch: 3 | loss: 0.0104928
	speed: 0.1191s/iter; left time: 2530.0656s
	iters: 300, epoch: 3 | loss: 0.0096982
	speed: 0.1192s/iter; left time: 2521.6983s
	iters: 400, epoch: 3 | loss: 0.0107002
	speed: 0.1198s/iter; left time: 2522.4475s
	iters: 500, epoch: 3 | loss: 0.0050922
	speed: 0.1187s/iter; left time: 2486.2886s
	iters: 600, epoch: 3 | loss: 0.0071053
	speed: 0.1182s/iter; left time: 2464.3875s
	iters: 700, epoch: 3 | loss: 0.0081983
	speed: 0.1189s/iter; left time: 2467.9337s
	iters: 800, epoch: 3 | loss: 0.0078431
	speed: 0.1192s/iter; left time: 2460.4467s
	iters: 900, epoch: 3 | loss: 0.0084672
	speed: 0.1204s/iter; left time: 2474.7421s
	iters: 1000, epoch: 3 | loss: 0.0076965
	speed: 0.1201s/iter; left time: 2456.5279s
	iters: 1100, epoch: 3 | loss: 0.0033199
	speed: 0.1210s/iter; left time: 2463.0454s
	iters: 1200, epoch: 3 | loss: 0.0122402
	speed: 0.1210s/iter; left time: 2449.6674s
	iters: 1300, epoch: 3 | loss: 0.0057300
	speed: 0.1189s/iter; left time: 2395.3961s
	iters: 1400, epoch: 3 | loss: 0.0072302
	speed: 0.1197s/iter; left time: 2400.1009s
	iters: 1500, epoch: 3 | loss: 0.0128537
	speed: 0.1204s/iter; left time: 2401.0207s
	iters: 1600, epoch: 3 | loss: 0.0078604
	speed: 0.1194s/iter; left time: 2370.0790s
	iters: 1700, epoch: 3 | loss: 0.0065746
	speed: 0.1201s/iter; left time: 2371.0148s
	iters: 1800, epoch: 3 | loss: 0.0067097
	speed: 0.1206s/iter; left time: 2370.5239s
	iters: 1900, epoch: 3 | loss: 0.0090133
	speed: 0.1197s/iter; left time: 2340.9100s
	iters: 2000, epoch: 3 | loss: 0.0061281
	speed: 0.1186s/iter; left time: 2306.7584s
	iters: 2100, epoch: 3 | loss: 0.0045120
	speed: 0.1193s/iter; left time: 2309.1822s
	iters: 2200, epoch: 3 | loss: 0.0076447
	speed: 0.1198s/iter; left time: 2305.9860s
	iters: 2300, epoch: 3 | loss: 0.0080694
	speed: 0.1359s/iter; left time: 2601.7501s
	iters: 2400, epoch: 3 | loss: 0.0048762
	speed: 0.1194s/iter; left time: 2274.6516s
	iters: 2500, epoch: 3 | loss: 0.0112840
	speed: 0.1186s/iter; left time: 2247.8425s
	iters: 2600, epoch: 3 | loss: 0.0123261
	speed: 0.1194s/iter; left time: 2250.6599s
Epoch: 3 cost time: 00h:05m:22.63s
Epoch: 3 | Train Loss: 0.0087474 Vali Loss: 0.0099604 Test Loss: 0.0116793
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0085789
	speed: 0.8461s/iter; left time: 15795.8758s
	iters: 200, epoch: 4 | loss: 0.0139824
	speed: 0.1190s/iter; left time: 2210.4174s
	iters: 300, epoch: 4 | loss: 0.0049015
	speed: 0.1203s/iter; left time: 2221.8464s
	iters: 400, epoch: 4 | loss: 0.0097717
	speed: 0.1194s/iter; left time: 2192.3090s
	iters: 500, epoch: 4 | loss: 0.0062868
	speed: 0.1190s/iter; left time: 2174.3231s
	iters: 600, epoch: 4 | loss: 0.0052928
	speed: 0.1199s/iter; left time: 2179.1452s
	iters: 700, epoch: 4 | loss: 0.0079444
	speed: 0.1200s/iter; left time: 2168.1768s
	iters: 800, epoch: 4 | loss: 0.0084604
	speed: 0.1192s/iter; left time: 2141.3376s
	iters: 900, epoch: 4 | loss: 0.0061194
	speed: 0.1226s/iter; left time: 2189.8917s
	iters: 1000, epoch: 4 | loss: 0.0085348
	speed: 0.1193s/iter; left time: 2119.8731s
	iters: 1100, epoch: 4 | loss: 0.0092854
	speed: 0.1204s/iter; left time: 2126.7613s
	iters: 1200, epoch: 4 | loss: 0.0093839
	speed: 0.1192s/iter; left time: 2094.5767s
	iters: 1300, epoch: 4 | loss: 0.0080101
	speed: 0.1201s/iter; left time: 2097.5708s
	iters: 1400, epoch: 4 | loss: 0.0062446
	speed: 0.1191s/iter; left time: 2067.8797s
	iters: 1500, epoch: 4 | loss: 0.0097147
	speed: 0.1179s/iter; left time: 2036.0572s
	iters: 1600, epoch: 4 | loss: 0.0088930
	speed: 0.1197s/iter; left time: 2055.5062s
	iters: 1700, epoch: 4 | loss: 0.0071742
	speed: 0.1206s/iter; left time: 2059.2418s
	iters: 1800, epoch: 4 | loss: 0.0085443
	speed: 0.1220s/iter; left time: 2069.7578s
	iters: 1900, epoch: 4 | loss: 0.0061091
	speed: 0.1205s/iter; left time: 2032.2087s
	iters: 2000, epoch: 4 | loss: 0.0056131
	speed: 0.1194s/iter; left time: 2002.5241s
	iters: 2100, epoch: 4 | loss: 0.0124798
	speed: 0.1196s/iter; left time: 1993.7562s
	iters: 2200, epoch: 4 | loss: 0.0103136
	speed: 0.1199s/iter; left time: 1987.0250s
	iters: 2300, epoch: 4 | loss: 0.0067623
	speed: 0.1195s/iter; left time: 1968.0909s
	iters: 2400, epoch: 4 | loss: 0.0059178
	speed: 0.1193s/iter; left time: 1953.4698s
	iters: 2500, epoch: 4 | loss: 0.0050894
	speed: 0.1214s/iter; left time: 1975.6150s
	iters: 2600, epoch: 4 | loss: 0.0083984
	speed: 0.1205s/iter; left time: 1948.7737s
Epoch: 4 cost time: 00h:05m:21.73s
Epoch: 4 | Train Loss: 0.0084747 Vali Loss: 0.0094833 Test Loss: 0.0110422
Validation loss decreased (0.009682 --> 0.009483).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0082026
	speed: 0.8559s/iter; left time: 13682.8304s
	iters: 200, epoch: 5 | loss: 0.0066821
	speed: 0.1188s/iter; left time: 1887.1593s
	iters: 300, epoch: 5 | loss: 0.0117661
	speed: 0.1211s/iter; left time: 1911.7179s
	iters: 400, epoch: 5 | loss: 0.0055167
	speed: 0.1187s/iter; left time: 1862.1305s
	iters: 500, epoch: 5 | loss: 0.0147786
	speed: 0.1185s/iter; left time: 1846.6278s
	iters: 600, epoch: 5 | loss: 0.0090691
	speed: 0.1195s/iter; left time: 1851.4253s
	iters: 700, epoch: 5 | loss: 0.0085299
	speed: 0.1189s/iter; left time: 1830.1970s
	iters: 800, epoch: 5 | loss: 0.0092835
	speed: 0.1183s/iter; left time: 1808.5057s
	iters: 900, epoch: 5 | loss: 0.0144290
	speed: 0.1186s/iter; left time: 1801.7886s
	iters: 1000, epoch: 5 | loss: 0.0057977
	speed: 0.1187s/iter; left time: 1790.5969s
	iters: 1100, epoch: 5 | loss: 0.0072538
	speed: 0.1189s/iter; left time: 1782.6373s
	iters: 1200, epoch: 5 | loss: 0.0055248
	speed: 0.1185s/iter; left time: 1763.8397s
	iters: 1300, epoch: 5 | loss: 0.0123671
	speed: 0.1174s/iter; left time: 1735.6951s
	iters: 1400, epoch: 5 | loss: 0.0067170
	speed: 0.1153s/iter; left time: 1692.7462s
	iters: 1500, epoch: 5 | loss: 0.0113062
	speed: 0.1174s/iter; left time: 1712.5350s
	iters: 1600, epoch: 5 | loss: 0.0062030
	speed: 0.1183s/iter; left time: 1713.4916s
	iters: 1700, epoch: 5 | loss: 0.0073668
	speed: 0.1207s/iter; left time: 1736.2221s
	iters: 1800, epoch: 5 | loss: 0.0072769
	speed: 0.1213s/iter; left time: 1733.6664s
	iters: 1900, epoch: 5 | loss: 0.0128810
	speed: 0.1191s/iter; left time: 1689.7713s
	iters: 2000, epoch: 5 | loss: 0.0108227
	speed: 0.1186s/iter; left time: 1671.2174s
	iters: 2100, epoch: 5 | loss: 0.0068438
	speed: 0.1189s/iter; left time: 1662.6476s
	iters: 2200, epoch: 5 | loss: 0.0089800
	speed: 0.1190s/iter; left time: 1652.5528s
	iters: 2300, epoch: 5 | loss: 0.0078216
	speed: 0.1176s/iter; left time: 1621.3891s
	iters: 2400, epoch: 5 | loss: 0.0104458
	speed: 0.1185s/iter; left time: 1621.8556s
	iters: 2500, epoch: 5 | loss: 0.0108531
	speed: 0.1195s/iter; left time: 1623.2137s
	iters: 2600, epoch: 5 | loss: 0.0074144
	speed: 0.1185s/iter; left time: 1598.3200s
Epoch: 5 cost time: 00h:05m:18.57s
Epoch: 5 | Train Loss: 0.0081867 Vali Loss: 0.0096661 Test Loss: 0.0112585
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0068791
	speed: 0.8344s/iter; left time: 11102.5269s
	iters: 200, epoch: 6 | loss: 0.0129260
	speed: 0.1185s/iter; left time: 1565.3440s
	iters: 300, epoch: 6 | loss: 0.0127757
	speed: 0.1195s/iter; left time: 1566.2525s
	iters: 400, epoch: 6 | loss: 0.0065614
	speed: 0.1191s/iter; left time: 1549.3013s
	iters: 500, epoch: 6 | loss: 0.0055619
	speed: 0.1183s/iter; left time: 1526.2527s
	iters: 600, epoch: 6 | loss: 0.0065307
	speed: 0.1191s/iter; left time: 1524.8617s
	iters: 700, epoch: 6 | loss: 0.0035814
	speed: 0.1153s/iter; left time: 1465.2353s
	iters: 800, epoch: 6 | loss: 0.0113507
	speed: 0.1168s/iter; left time: 1472.5385s
	iters: 900, epoch: 6 | loss: 0.0102819
	speed: 0.1186s/iter; left time: 1482.9687s
	iters: 1000, epoch: 6 | loss: 0.0124274
	speed: 0.1187s/iter; left time: 1472.2692s
	iters: 1100, epoch: 6 | loss: 0.0075018
	speed: 0.1184s/iter; left time: 1456.8116s
	iters: 1200, epoch: 6 | loss: 0.0114392
	speed: 0.1182s/iter; left time: 1443.2508s
	iters: 1300, epoch: 6 | loss: 0.0065461
	speed: 0.1162s/iter; left time: 1406.4837s
	iters: 1400, epoch: 6 | loss: 0.0055617
	speed: 0.1173s/iter; left time: 1408.8241s
	iters: 1500, epoch: 6 | loss: 0.0098405
	speed: 0.1186s/iter; left time: 1412.2717s
	iters: 1600, epoch: 6 | loss: 0.0102533
	speed: 0.1203s/iter; left time: 1420.3052s
	iters: 1700, epoch: 6 | loss: 0.0054765
	speed: 0.1194s/iter; left time: 1397.7352s
	iters: 1800, epoch: 6 | loss: 0.0106227
	speed: 0.1192s/iter; left time: 1383.4443s
	iters: 1900, epoch: 6 | loss: 0.0062775
	speed: 0.1191s/iter; left time: 1370.6864s
	iters: 2000, epoch: 6 | loss: 0.0068300
	speed: 0.1192s/iter; left time: 1359.5386s
	iters: 2100, epoch: 6 | loss: 0.0078323
	speed: 0.1185s/iter; left time: 1339.1999s
	iters: 2200, epoch: 6 | loss: 0.0088947
	speed: 0.1197s/iter; left time: 1341.3545s
	iters: 2300, epoch: 6 | loss: 0.0041810
	speed: 0.1190s/iter; left time: 1321.3224s
	iters: 2400, epoch: 6 | loss: 0.0048208
	speed: 0.1183s/iter; left time: 1302.0067s
	iters: 2500, epoch: 6 | loss: 0.0056515
	speed: 0.1182s/iter; left time: 1289.5240s
	iters: 2600, epoch: 6 | loss: 0.0076506
	speed: 0.1193s/iter; left time: 1289.1202s
Epoch: 6 cost time: 00h:05m:18.06s
Epoch: 6 | Train Loss: 0.0079494 Vali Loss: 0.0091909 Test Loss: 0.0107529
Validation loss decreased (0.009483 --> 0.009191).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0143468
	speed: 0.8496s/iter; left time: 9026.9684s
	iters: 200, epoch: 7 | loss: 0.0084022
	speed: 0.1189s/iter; left time: 1251.5589s
	iters: 300, epoch: 7 | loss: 0.0066824
	speed: 0.1192s/iter; left time: 1243.0220s
	iters: 400, epoch: 7 | loss: 0.0081792
	speed: 0.1185s/iter; left time: 1223.4369s
	iters: 500, epoch: 7 | loss: 0.0074039
	speed: 0.1192s/iter; left time: 1219.1257s
	iters: 600, epoch: 7 | loss: 0.0060807
	speed: 0.1192s/iter; left time: 1207.0384s
	iters: 700, epoch: 7 | loss: 0.0078752
	speed: 0.1192s/iter; left time: 1195.4552s
	iters: 800, epoch: 7 | loss: 0.0106734
	speed: 0.1194s/iter; left time: 1184.9399s
	iters: 900, epoch: 7 | loss: 0.0053014
	speed: 0.1179s/iter; left time: 1158.7439s
	iters: 1000, epoch: 7 | loss: 0.0093736
	speed: 0.1188s/iter; left time: 1155.7287s
	iters: 1100, epoch: 7 | loss: 0.0074351
	speed: 0.1200s/iter; left time: 1155.3358s
	iters: 1200, epoch: 7 | loss: 0.0103754
	speed: 0.1192s/iter; left time: 1135.5651s
	iters: 1300, epoch: 7 | loss: 0.0050626
	speed: 0.1192s/iter; left time: 1123.3193s
	iters: 1400, epoch: 7 | loss: 0.0063779
	speed: 0.1204s/iter; left time: 1122.4662s
	iters: 1500, epoch: 7 | loss: 0.0081218
	speed: 0.1190s/iter; left time: 1097.3953s
	iters: 1600, epoch: 7 | loss: 0.0084652
	speed: 0.1191s/iter; left time: 1086.8467s
	iters: 1700, epoch: 7 | loss: 0.0063734
	speed: 0.1188s/iter; left time: 1072.5150s
	iters: 1800, epoch: 7 | loss: 0.0070088
	speed: 0.1206s/iter; left time: 1076.3766s
	iters: 1900, epoch: 7 | loss: 0.0081773
	speed: 0.1193s/iter; left time: 1052.6970s
	iters: 2000, epoch: 7 | loss: 0.0104023
	speed: 0.1188s/iter; left time: 1036.7921s
	iters: 2100, epoch: 7 | loss: 0.0082982
	speed: 0.1196s/iter; left time: 1031.4943s
	iters: 2200, epoch: 7 | loss: 0.0080761
	speed: 0.1197s/iter; left time: 1020.2795s
	iters: 2300, epoch: 7 | loss: 0.0103042
	speed: 0.1189s/iter; left time: 1001.7898s
	iters: 2400, epoch: 7 | loss: 0.0109918
	speed: 0.1191s/iter; left time: 991.5219s
	iters: 2500, epoch: 7 | loss: 0.0065413
	speed: 0.1190s/iter; left time: 978.4973s
	iters: 2600, epoch: 7 | loss: 0.0080837
	speed: 0.1189s/iter; left time: 965.9768s
Epoch: 7 cost time: 00h:05m:19.92s
Epoch: 7 | Train Loss: 0.0077676 Vali Loss: 0.0092396 Test Loss: 0.0109264
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0145342
	speed: 0.8430s/iter; left time: 6697.0581s
	iters: 200, epoch: 8 | loss: 0.0050922
	speed: 0.1205s/iter; left time: 944.8156s
	iters: 300, epoch: 8 | loss: 0.0105490
	speed: 0.1194s/iter; left time: 924.6926s
	iters: 400, epoch: 8 | loss: 0.0073536
	speed: 0.1203s/iter; left time: 919.3431s
	iters: 500, epoch: 8 | loss: 0.0075059
	speed: 0.1209s/iter; left time: 912.2485s
	iters: 600, epoch: 8 | loss: 0.0066650
	speed: 0.1198s/iter; left time: 892.0919s
	iters: 700, epoch: 8 | loss: 0.0072620
	speed: 0.1195s/iter; left time: 877.8306s
	iters: 800, epoch: 8 | loss: 0.0089689
	speed: 0.1189s/iter; left time: 861.2522s
	iters: 900, epoch: 8 | loss: 0.0076963
	speed: 0.1198s/iter; left time: 855.7440s
	iters: 1000, epoch: 8 | loss: 0.0081367
	speed: 0.1197s/iter; left time: 843.1442s
	iters: 1100, epoch: 8 | loss: 0.0055714
	speed: 0.1192s/iter; left time: 827.5685s
	iters: 1200, epoch: 8 | loss: 0.0088810
	speed: 0.1194s/iter; left time: 817.4813s
	iters: 1300, epoch: 8 | loss: 0.0076921
	speed: 0.1186s/iter; left time: 799.9099s
	iters: 1400, epoch: 8 | loss: 0.0078052
	speed: 0.1175s/iter; left time: 780.4206s
	iters: 1500, epoch: 8 | loss: 0.0069064
	speed: 0.1194s/iter; left time: 781.4263s
	iters: 1600, epoch: 8 | loss: 0.0089180
	speed: 0.1189s/iter; left time: 766.0942s
	iters: 1700, epoch: 8 | loss: 0.0098277
	speed: 0.1187s/iter; left time: 753.0525s
	iters: 1800, epoch: 8 | loss: 0.0097571
	speed: 0.1201s/iter; left time: 750.0584s
	iters: 1900, epoch: 8 | loss: 0.0076540
	speed: 0.1215s/iter; left time: 746.6747s
	iters: 2000, epoch: 8 | loss: 0.0066878
	speed: 0.1190s/iter; left time: 719.2204s
	iters: 2100, epoch: 8 | loss: 0.0051629
	speed: 0.1197s/iter; left time: 711.5669s
	iters: 2200, epoch: 8 | loss: 0.0060304
	speed: 0.1205s/iter; left time: 704.1403s
	iters: 2300, epoch: 8 | loss: 0.0112157
	speed: 0.1192s/iter; left time: 684.8316s
	iters: 2400, epoch: 8 | loss: 0.0062966
	speed: 0.1190s/iter; left time: 671.8634s
	iters: 2500, epoch: 8 | loss: 0.0067169
	speed: 0.1194s/iter; left time: 661.7387s
	iters: 2600, epoch: 8 | loss: 0.0074308
	speed: 0.1184s/iter; left time: 644.8389s
Epoch: 8 cost time: 00h:05m:20.52s
Epoch: 8 | Train Loss: 0.0076511 Vali Loss: 0.0093917 Test Loss: 0.0110719
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0063627
	speed: 0.8441s/iter; left time: 4442.6898s
	iters: 200, epoch: 9 | loss: 0.0081278
	speed: 0.1177s/iter; left time: 607.8514s
	iters: 300, epoch: 9 | loss: 0.0049066
	speed: 0.1185s/iter; left time: 599.7612s
	iters: 400, epoch: 9 | loss: 0.0069859
	speed: 0.1197s/iter; left time: 594.3020s
	iters: 500, epoch: 9 | loss: 0.0059178
	speed: 0.1193s/iter; left time: 580.1887s
	iters: 600, epoch: 9 | loss: 0.0046414
	speed: 0.1186s/iter; left time: 564.8682s
	iters: 700, epoch: 9 | loss: 0.0070652
	speed: 0.1196s/iter; left time: 557.6605s
	iters: 800, epoch: 9 | loss: 0.0058564
	speed: 0.1193s/iter; left time: 544.3545s
	iters: 900, epoch: 9 | loss: 0.0121706
	speed: 0.1183s/iter; left time: 527.8719s
	iters: 1000, epoch: 9 | loss: 0.0047649
	speed: 0.1185s/iter; left time: 516.9304s
	iters: 1100, epoch: 9 | loss: 0.0070160
	speed: 0.1188s/iter; left time: 506.6098s
	iters: 1200, epoch: 9 | loss: 0.0055626
	speed: 0.1184s/iter; left time: 493.0936s
	iters: 1300, epoch: 9 | loss: 0.0044197
	speed: 0.1185s/iter; left time: 481.4364s
	iters: 1400, epoch: 9 | loss: 0.0083944
	speed: 0.1190s/iter; left time: 471.7193s
	iters: 1500, epoch: 9 | loss: 0.0081034
	speed: 0.1182s/iter; left time: 456.6608s
	iters: 1600, epoch: 9 | loss: 0.0059178
	speed: 0.1175s/iter; left time: 442.1777s
	iters: 1700, epoch: 9 | loss: 0.0043409
	speed: 0.1182s/iter; left time: 433.1121s
	iters: 1800, epoch: 9 | loss: 0.0073731
	speed: 0.1212s/iter; left time: 431.7087s
	iters: 1900, epoch: 9 | loss: 0.0054103
	speed: 0.1190s/iter; left time: 411.9469s
	iters: 2000, epoch: 9 | loss: 0.0077524
	speed: 0.1186s/iter; left time: 398.7655s
	iters: 2100, epoch: 9 | loss: 0.0083913
	speed: 0.1194s/iter; left time: 389.5559s
	iters: 2200, epoch: 9 | loss: 0.0083011
	speed: 0.1179s/iter; left time: 372.8144s
	iters: 2300, epoch: 9 | loss: 0.0051046
	speed: 0.1182s/iter; left time: 362.0299s
	iters: 2400, epoch: 9 | loss: 0.0048293
	speed: 0.1199s/iter; left time: 355.2815s
	iters: 2500, epoch: 9 | loss: 0.0046664
	speed: 0.1185s/iter; left time: 339.2371s
	iters: 2600, epoch: 9 | loss: 0.0062383
	speed: 0.1187s/iter; left time: 327.8890s
Epoch: 9 cost time: 00h:05m:18.70s
Epoch: 9 | Train Loss: 0.0074667 Vali Loss: 0.0092518 Test Loss: 0.0108834
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.010752849280834198, rmse:0.10369594395160675, mae:0.06316586583852768, rse:0.40005412697792053
Intermediate time for FR and pred_len 24: 00h:58m:36.24s

=== Starting experiments for pred_len: 96 ===

--- Running model for FR, pred_len=96 ---
train 85587
val 18435
test 18435
[2024-11-12 19:40:39,903] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 19:40:40,879] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 19:40:40,879] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 19:40:40,879] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 19:40:40,967] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 19:40:40,967] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 19:40:41,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 19:40:41,422] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 19:40:41,423] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 19:40:41,424] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 19:40:41,424] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 19:40:41,424] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 19:40:41,424] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 19:40:41,424] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 19:40:41,424] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 19:40:41,424] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 19:40:42,027] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 19:40:42,028] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 19:40:42,042] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.2 GB, percent = 2.2%
[2024-11-12 19:40:42,124] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 19:40:42,125] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 19:40:42,125] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.2 GB, percent = 2.2%
[2024-11-12 19:40:42,125] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 19:40:42,203] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 19:40:42,203] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 19:40:42,204] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 22.2 GB, percent = 2.2%
[2024-11-12 19:40:42,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 19:40:42,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 19:40:42,204] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 19:40:42,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 19:40:42,205] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd90533e790>
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 19:40:42,205] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 19:40:42,206] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 19:40:42,206] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0450392
	speed: 0.1628s/iter; left time: 4337.3849s
	iters: 200, epoch: 1 | loss: 0.0411741
	speed: 0.1244s/iter; left time: 3300.7929s
	iters: 300, epoch: 1 | loss: 0.0245169
	speed: 0.1258s/iter; left time: 3327.4208s
	iters: 400, epoch: 1 | loss: 0.0257455
	speed: 0.1253s/iter; left time: 3301.4227s
	iters: 500, epoch: 1 | loss: 0.0221113
	speed: 0.1257s/iter; left time: 3297.9165s
	iters: 600, epoch: 1 | loss: 0.0234084
	speed: 0.1245s/iter; left time: 3254.6448s
	iters: 700, epoch: 1 | loss: 0.0310447
	speed: 0.1262s/iter; left time: 3287.1346s
	iters: 800, epoch: 1 | loss: 0.0166334
	speed: 0.1235s/iter; left time: 3203.8028s
	iters: 900, epoch: 1 | loss: 0.0154001
	speed: 0.1240s/iter; left time: 3204.2683s
	iters: 1000, epoch: 1 | loss: 0.0179012
	speed: 0.1247s/iter; left time: 3210.6706s
	iters: 1100, epoch: 1 | loss: 0.0106971
	speed: 0.1242s/iter; left time: 3184.1694s
	iters: 1200, epoch: 1 | loss: 0.0100931
	speed: 0.1238s/iter; left time: 3161.9546s
	iters: 1300, epoch: 1 | loss: 0.0116561
	speed: 0.1245s/iter; left time: 3168.4548s
	iters: 1400, epoch: 1 | loss: 0.0133071
	speed: 0.1255s/iter; left time: 3181.0361s
	iters: 1500, epoch: 1 | loss: 0.0142755
	speed: 0.1239s/iter; left time: 3126.7784s
	iters: 1600, epoch: 1 | loss: 0.0116211
	speed: 0.1232s/iter; left time: 3096.3543s
	iters: 1700, epoch: 1 | loss: 0.0150364
	speed: 0.1234s/iter; left time: 3089.1415s
	iters: 1800, epoch: 1 | loss: 0.0159214
	speed: 0.1254s/iter; left time: 3127.7773s
	iters: 1900, epoch: 1 | loss: 0.0203218
	speed: 0.1258s/iter; left time: 3125.5841s
	iters: 2000, epoch: 1 | loss: 0.0085747
	speed: 0.1253s/iter; left time: 3098.9239s
	iters: 2100, epoch: 1 | loss: 0.0166121
	speed: 0.1238s/iter; left time: 3049.8196s
	iters: 2200, epoch: 1 | loss: 0.0126489
	speed: 0.1253s/iter; left time: 3074.8359s
	iters: 2300, epoch: 1 | loss: 0.0141676
	speed: 0.1229s/iter; left time: 3002.9730s
	iters: 2400, epoch: 1 | loss: 0.0099115
	speed: 0.1246s/iter; left time: 3033.5939s
	iters: 2500, epoch: 1 | loss: 0.0119989
	speed: 0.1251s/iter; left time: 3031.5260s
	iters: 2600, epoch: 1 | loss: 0.0158081
	speed: 0.1256s/iter; left time: 3031.9404s
Epoch: 1 cost time: 00h:05m:34.44s
Epoch: 1 | Train Loss: 0.0189805 Vali Loss: 0.0153633 Test Loss: 0.0191843
Validation loss decreased (inf --> 0.015363).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0158058
	speed: 0.8957s/iter; left time: 21466.6493s
	iters: 200, epoch: 2 | loss: 0.0109375
	speed: 0.1195s/iter; left time: 2851.1903s
	iters: 300, epoch: 2 | loss: 0.0166918
	speed: 0.1208s/iter; left time: 2870.7968s
	iters: 400, epoch: 2 | loss: 0.0167429
	speed: 0.1210s/iter; left time: 2862.8784s
	iters: 500, epoch: 2 | loss: 0.0134452
	speed: 0.1213s/iter; left time: 2857.5218s
	iters: 600, epoch: 2 | loss: 0.0162765
	speed: 0.1209s/iter; left time: 2836.9441s
	iters: 700, epoch: 2 | loss: 0.0107006
	speed: 0.1183s/iter; left time: 2764.1770s
	iters: 800, epoch: 2 | loss: 0.0130153
	speed: 0.1187s/iter; left time: 2762.3340s
	iters: 900, epoch: 2 | loss: 0.0123563
	speed: 0.1195s/iter; left time: 2767.6794s
	iters: 1000, epoch: 2 | loss: 0.0131076
	speed: 0.1212s/iter; left time: 2796.8612s
	iters: 1100, epoch: 2 | loss: 0.0114845
	speed: 0.1203s/iter; left time: 2762.5459s
	iters: 1200, epoch: 2 | loss: 0.0235055
	speed: 0.1201s/iter; left time: 2745.4787s
	iters: 1300, epoch: 2 | loss: 0.0117841
	speed: 0.1203s/iter; left time: 2739.8163s
	iters: 1400, epoch: 2 | loss: 0.0162855
	speed: 0.1206s/iter; left time: 2732.7811s
	iters: 1500, epoch: 2 | loss: 0.0190913
	speed: 0.1190s/iter; left time: 2684.4730s
	iters: 1600, epoch: 2 | loss: 0.0117552
	speed: 0.1207s/iter; left time: 2711.1629s
	iters: 1700, epoch: 2 | loss: 0.0112715
	speed: 0.1199s/iter; left time: 2682.6962s
	iters: 1800, epoch: 2 | loss: 0.0119107
	speed: 0.1213s/iter; left time: 2700.6043s
	iters: 1900, epoch: 2 | loss: 0.0103170
	speed: 0.1187s/iter; left time: 2631.8887s
	iters: 2000, epoch: 2 | loss: 0.0100007
	speed: 0.1182s/iter; left time: 2607.4691s
	iters: 2100, epoch: 2 | loss: 0.0131170
	speed: 0.1200s/iter; left time: 2635.7590s
	iters: 2200, epoch: 2 | loss: 0.0085825
	speed: 0.1189s/iter; left time: 2599.3641s
	iters: 2300, epoch: 2 | loss: 0.0119522
	speed: 0.1175s/iter; left time: 2557.4687s
	iters: 2400, epoch: 2 | loss: 0.0139331
	speed: 0.1212s/iter; left time: 2626.0874s
	iters: 2500, epoch: 2 | loss: 0.0112488
	speed: 0.1196s/iter; left time: 2578.9247s
	iters: 2600, epoch: 2 | loss: 0.0097716
	speed: 0.1191s/iter; left time: 2556.3474s
Epoch: 2 cost time: 00h:05m:20.74s
Epoch: 2 | Train Loss: 0.0127849 Vali Loss: 0.0151459 Test Loss: 0.0192785
Validation loss decreased (0.015363 --> 0.015146).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0118803
	speed: 0.8360s/iter; left time: 17801.9071s
	iters: 200, epoch: 3 | loss: 0.0101367
	speed: 0.1195s/iter; left time: 2532.5289s
	iters: 300, epoch: 3 | loss: 0.0121341
	speed: 0.1211s/iter; left time: 2554.5729s
	iters: 400, epoch: 3 | loss: 0.0125734
	speed: 0.1207s/iter; left time: 2534.8409s
	iters: 500, epoch: 3 | loss: 0.0117910
	speed: 0.1201s/iter; left time: 2509.0806s
	iters: 600, epoch: 3 | loss: 0.0094285
	speed: 0.1207s/iter; left time: 2509.3871s
	iters: 700, epoch: 3 | loss: 0.0180131
	speed: 0.1199s/iter; left time: 2481.9053s
	iters: 800, epoch: 3 | loss: 0.0114065
	speed: 0.1212s/iter; left time: 2495.3422s
	iters: 900, epoch: 3 | loss: 0.0092815
	speed: 0.1209s/iter; left time: 2477.0132s
	iters: 1000, epoch: 3 | loss: 0.0106002
	speed: 0.1188s/iter; left time: 2423.3346s
	iters: 1100, epoch: 3 | loss: 0.0078716
	speed: 0.1182s/iter; left time: 2398.5532s
	iters: 1200, epoch: 3 | loss: 0.0125098
	speed: 0.1181s/iter; left time: 2385.2883s
	iters: 1300, epoch: 3 | loss: 0.0134287
	speed: 0.1183s/iter; left time: 2377.3831s
	iters: 1400, epoch: 3 | loss: 0.0103850
	speed: 0.1179s/iter; left time: 2356.3399s
	iters: 1500, epoch: 3 | loss: 0.0132194
	speed: 0.1173s/iter; left time: 2334.1790s
	iters: 1600, epoch: 3 | loss: 0.0194716
	speed: 0.1182s/iter; left time: 2340.5047s
	iters: 1700, epoch: 3 | loss: 0.0143841
	speed: 0.1184s/iter; left time: 2331.2709s
	iters: 1800, epoch: 3 | loss: 0.0078925
	speed: 0.1182s/iter; left time: 2315.5402s
	iters: 1900, epoch: 3 | loss: 0.0163695
	speed: 0.1187s/iter; left time: 2312.9457s
	iters: 2000, epoch: 3 | loss: 0.0090255
	speed: 0.1179s/iter; left time: 2286.1785s
	iters: 2100, epoch: 3 | loss: 0.0191625
	speed: 0.1171s/iter; left time: 2259.2336s
	iters: 2200, epoch: 3 | loss: 0.0125433
	speed: 0.1187s/iter; left time: 2277.9032s
	iters: 2300, epoch: 3 | loss: 0.0098431
	speed: 0.1177s/iter; left time: 2247.4287s
	iters: 2400, epoch: 3 | loss: 0.0109958
	speed: 0.1177s/iter; left time: 2236.1202s
	iters: 2500, epoch: 3 | loss: 0.0140541
	speed: 0.1177s/iter; left time: 2222.9321s
	iters: 2600, epoch: 3 | loss: 0.0097000
	speed: 0.1176s/iter; left time: 2210.9723s
Epoch: 3 cost time: 00h:05m:18.07s
Epoch: 3 | Train Loss: 0.0120224 Vali Loss: 0.0148805 Test Loss: 0.0193639
Validation loss decreased (0.015146 --> 0.014880).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0104348
	speed: 0.8081s/iter; left time: 15046.3058s
	iters: 200, epoch: 4 | loss: 0.0147447
	speed: 0.1181s/iter; left time: 2186.8513s
	iters: 300, epoch: 4 | loss: 0.0095480
	speed: 0.1179s/iter; left time: 2171.9370s
	iters: 400, epoch: 4 | loss: 0.0170453
	speed: 0.1179s/iter; left time: 2160.2485s
	iters: 500, epoch: 4 | loss: 0.0128944
	speed: 0.1176s/iter; left time: 2142.6745s
	iters: 600, epoch: 4 | loss: 0.0129001
	speed: 0.1180s/iter; left time: 2137.6708s
	iters: 700, epoch: 4 | loss: 0.0104098
	speed: 0.1177s/iter; left time: 2121.1607s
	iters: 800, epoch: 4 | loss: 0.0118125
	speed: 0.1165s/iter; left time: 2087.6058s
	iters: 900, epoch: 4 | loss: 0.0092582
	speed: 0.1174s/iter; left time: 2091.6082s
	iters: 1000, epoch: 4 | loss: 0.0090673
	speed: 0.1178s/iter; left time: 2087.4177s
	iters: 1100, epoch: 4 | loss: 0.0109682
	speed: 0.1176s/iter; left time: 2071.7221s
	iters: 1200, epoch: 4 | loss: 0.0087824
	speed: 0.1177s/iter; left time: 2062.6426s
	iters: 1300, epoch: 4 | loss: 0.0174996
	speed: 0.1181s/iter; left time: 2057.0563s
	iters: 1400, epoch: 4 | loss: 0.0118741
	speed: 0.1180s/iter; left time: 2042.9643s
	iters: 1500, epoch: 4 | loss: 0.0090260
	speed: 0.1178s/iter; left time: 2028.8658s
	iters: 1600, epoch: 4 | loss: 0.0084460
	speed: 0.1180s/iter; left time: 2019.5231s
	iters: 1700, epoch: 4 | loss: 0.0096923
	speed: 0.1156s/iter; left time: 1967.8242s
	iters: 1800, epoch: 4 | loss: 0.0119153
	speed: 0.1165s/iter; left time: 1970.5643s
	iters: 1900, epoch: 4 | loss: 0.0101682
	speed: 0.1158s/iter; left time: 1947.9222s
	iters: 2000, epoch: 4 | loss: 0.0143250
	speed: 0.1175s/iter; left time: 1964.8570s
	iters: 2100, epoch: 4 | loss: 0.0126696
	speed: 0.1188s/iter; left time: 1973.7100s
	iters: 2200, epoch: 4 | loss: 0.0086834
	speed: 0.1182s/iter; left time: 1952.4159s
	iters: 2300, epoch: 4 | loss: 0.0121239
	speed: 0.1185s/iter; left time: 1946.1728s
	iters: 2400, epoch: 4 | loss: 0.0087830
	speed: 0.1201s/iter; left time: 1959.5909s
	iters: 2500, epoch: 4 | loss: 0.0114819
	speed: 0.1184s/iter; left time: 1920.3726s
	iters: 2600, epoch: 4 | loss: 0.0077992
	speed: 0.1179s/iter; left time: 1900.8033s
Epoch: 4 cost time: 00h:05m:15.12s
Epoch: 4 | Train Loss: 0.0113246 Vali Loss: 0.0157741 Test Loss: 0.0208361
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0117146
	speed: 0.7908s/iter; left time: 12609.7681s
	iters: 200, epoch: 5 | loss: 0.0141664
	speed: 0.1180s/iter; left time: 1870.0043s
	iters: 300, epoch: 5 | loss: 0.0105971
	speed: 0.1178s/iter; left time: 1854.9453s
	iters: 400, epoch: 5 | loss: 0.0068634
	speed: 0.1179s/iter; left time: 1843.8859s
	iters: 500, epoch: 5 | loss: 0.0113078
	speed: 0.1178s/iter; left time: 1831.5613s
	iters: 600, epoch: 5 | loss: 0.0122250
	speed: 0.1177s/iter; left time: 1817.7285s
	iters: 700, epoch: 5 | loss: 0.0110457
	speed: 0.1177s/iter; left time: 1806.7614s
	iters: 800, epoch: 5 | loss: 0.0107314
	speed: 0.1177s/iter; left time: 1793.7555s
	iters: 900, epoch: 5 | loss: 0.0124267
	speed: 0.1178s/iter; left time: 1784.0839s
	iters: 1000, epoch: 5 | loss: 0.0073952
	speed: 0.1180s/iter; left time: 1775.4236s
	iters: 1100, epoch: 5 | loss: 0.0145061
	speed: 0.1177s/iter; left time: 1759.5746s
	iters: 1200, epoch: 5 | loss: 0.0089259
	speed: 0.1178s/iter; left time: 1749.2352s
	iters: 1300, epoch: 5 | loss: 0.0098448
	speed: 0.1177s/iter; left time: 1735.5120s
	iters: 1400, epoch: 5 | loss: 0.0111989
	speed: 0.1177s/iter; left time: 1724.3116s
	iters: 1500, epoch: 5 | loss: 0.0060225
	speed: 0.1177s/iter; left time: 1711.8343s
	iters: 1600, epoch: 5 | loss: 0.0105162
	speed: 0.1181s/iter; left time: 1705.8011s
	iters: 1700, epoch: 5 | loss: 0.0075380
	speed: 0.1179s/iter; left time: 1691.7756s
	iters: 1800, epoch: 5 | loss: 0.0110643
	speed: 0.1181s/iter; left time: 1682.7715s
	iters: 1900, epoch: 5 | loss: 0.0108197
	speed: 0.1178s/iter; left time: 1666.7391s
	iters: 2000, epoch: 5 | loss: 0.0077852
	speed: 0.1180s/iter; left time: 1657.2124s
	iters: 2100, epoch: 5 | loss: 0.0108914
	speed: 0.1184s/iter; left time: 1651.3645s
	iters: 2200, epoch: 5 | loss: 0.0142116
	speed: 0.1183s/iter; left time: 1638.1299s
	iters: 2300, epoch: 5 | loss: 0.0143057
	speed: 0.1190s/iter; left time: 1635.7217s
	iters: 2400, epoch: 5 | loss: 0.0085773
	speed: 0.1198s/iter; left time: 1635.0826s
	iters: 2500, epoch: 5 | loss: 0.0131593
	speed: 0.1200s/iter; left time: 1625.4093s
	iters: 2600, epoch: 5 | loss: 0.0088603
	speed: 0.1200s/iter; left time: 1613.7231s
Epoch: 5 cost time: 00h:05m:16.52s
Epoch: 5 | Train Loss: 0.0106526 Vali Loss: 0.0168246 Test Loss: 0.0207549
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0121528
	speed: 0.7919s/iter; left time: 10509.1597s
	iters: 200, epoch: 6 | loss: 0.0100930
	speed: 0.1150s/iter; left time: 1514.1130s
	iters: 300, epoch: 6 | loss: 0.0089551
	speed: 0.1149s/iter; left time: 1502.1871s
	iters: 400, epoch: 6 | loss: 0.0081247
	speed: 0.1150s/iter; left time: 1491.4002s
	iters: 500, epoch: 6 | loss: 0.0075258
	speed: 0.1150s/iter; left time: 1480.1567s
	iters: 600, epoch: 6 | loss: 0.0097040
	speed: 0.1151s/iter; left time: 1469.6264s
	iters: 700, epoch: 6 | loss: 0.0137501
	speed: 0.1164s/iter; left time: 1474.6490s
	iters: 800, epoch: 6 | loss: 0.0117516
	speed: 0.1196s/iter; left time: 1503.2350s
	iters: 900, epoch: 6 | loss: 0.0123034
	speed: 0.1202s/iter; left time: 1498.6262s
	iters: 1000, epoch: 6 | loss: 0.0107576
	speed: 0.1197s/iter; left time: 1480.5264s
	iters: 1100, epoch: 6 | loss: 0.0081077
	speed: 0.1198s/iter; left time: 1469.8049s
	iters: 1200, epoch: 6 | loss: 0.0071426
	speed: 0.1184s/iter; left time: 1440.5441s
	iters: 1300, epoch: 6 | loss: 0.0137052
	speed: 0.1179s/iter; left time: 1423.2973s
	iters: 1400, epoch: 6 | loss: 0.0106240
	speed: 0.1182s/iter; left time: 1414.8075s
	iters: 1500, epoch: 6 | loss: 0.0077702
	speed: 0.1180s/iter; left time: 1401.1017s
	iters: 1600, epoch: 6 | loss: 0.0083429
	speed: 0.1180s/iter; left time: 1389.3397s
	iters: 1700, epoch: 6 | loss: 0.0093484
	speed: 0.1180s/iter; left time: 1377.4317s
	iters: 1800, epoch: 6 | loss: 0.0094698
	speed: 0.1180s/iter; left time: 1365.2938s
	iters: 1900, epoch: 6 | loss: 0.0096716
	speed: 0.1182s/iter; left time: 1356.3881s
	iters: 2000, epoch: 6 | loss: 0.0091432
	speed: 0.1179s/iter; left time: 1340.4306s
	iters: 2100, epoch: 6 | loss: 0.0100183
	speed: 0.1181s/iter; left time: 1331.1194s
	iters: 2200, epoch: 6 | loss: 0.0133788
	speed: 0.1180s/iter; left time: 1317.9794s
	iters: 2300, epoch: 6 | loss: 0.0078084
	speed: 0.1180s/iter; left time: 1305.8324s
	iters: 2400, epoch: 6 | loss: 0.0149702
	speed: 0.1198s/iter; left time: 1313.8929s
	iters: 2500, epoch: 6 | loss: 0.0117440
	speed: 0.1198s/iter; left time: 1302.2538s
	iters: 2600, epoch: 6 | loss: 0.0068400
	speed: 0.1199s/iter; left time: 1291.1852s
Epoch: 6 cost time: 00h:05m:15.28s
Epoch: 6 | Train Loss: 0.0102449 Vali Loss: 0.0165088 Test Loss: 0.0200477
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.019363928586244583, rmse:0.1391543298959732, mae:0.08860348165035248, rse:0.5383290648460388
Intermediate time for FR and pred_len 96: 00h:38m:54.02s

=== Starting experiments for pred_len: 168 ===

--- Running model for FR, pred_len=168 ---
train 85371
val 18219
test 18219
[2024-11-12 20:19:35,003] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 20:19:36,038] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 20:19:36,038] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 20:19:36,038] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 20:19:36,142] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 20:19:36,142] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 20:19:36,991] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 20:19:36,992] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 20:19:36,993] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 20:19:36,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 20:19:36,994] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 20:19:36,994] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 20:19:36,994] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 20:19:36,994] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 20:19:36,994] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 20:19:36,994] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 20:19:37,295] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 20:19:37,295] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 20:19:37,334] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.45 GB, percent = 2.1%
[2024-11-12 20:19:37,469] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 20:19:37,470] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 20:19:37,470] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.46 GB, percent = 2.1%
[2024-11-12 20:19:37,470] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 20:19:37,592] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 20:19:37,593] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 20:19:37,593] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.46 GB, percent = 2.1%
[2024-11-12 20:19:37,594] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 20:19:37,594] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 20:19:37,594] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 20:19:37,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 20:19:37,594] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f20b0ef9010>
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 20:19:37,595] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 20:19:37,596] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 20:19:37,596] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0396952
	speed: 0.1685s/iter; left time: 4477.9704s
	iters: 200, epoch: 1 | loss: 0.0389226
	speed: 0.1247s/iter; left time: 3300.2672s
	iters: 300, epoch: 1 | loss: 0.0310403
	speed: 0.1218s/iter; left time: 3212.3858s
	iters: 400, epoch: 1 | loss: 0.0277982
	speed: 0.1225s/iter; left time: 3218.6011s
	iters: 500, epoch: 1 | loss: 0.0317462
	speed: 0.1225s/iter; left time: 3206.2763s
	iters: 600, epoch: 1 | loss: 0.0256130
	speed: 0.1240s/iter; left time: 3232.8671s
	iters: 700, epoch: 1 | loss: 0.0215947
	speed: 0.1236s/iter; left time: 3210.5235s
	iters: 800, epoch: 1 | loss: 0.0171741
	speed: 0.1232s/iter; left time: 3187.2256s
	iters: 900, epoch: 1 | loss: 0.0206746
	speed: 0.1231s/iter; left time: 3173.2325s
	iters: 1000, epoch: 1 | loss: 0.0213943
	speed: 0.1232s/iter; left time: 3161.8055s
	iters: 1100, epoch: 1 | loss: 0.0193485
	speed: 0.1232s/iter; left time: 3149.6344s
	iters: 1200, epoch: 1 | loss: 0.0158454
	speed: 0.1233s/iter; left time: 3140.2688s
	iters: 1300, epoch: 1 | loss: 0.0171117
	speed: 0.1226s/iter; left time: 3111.1649s
	iters: 1400, epoch: 1 | loss: 0.0130843
	speed: 0.1236s/iter; left time: 3124.2809s
	iters: 1500, epoch: 1 | loss: 0.0180633
	speed: 0.1230s/iter; left time: 3095.2423s
	iters: 1600, epoch: 1 | loss: 0.0168951
	speed: 0.1238s/iter; left time: 3104.2830s
	iters: 1700, epoch: 1 | loss: 0.0139916
	speed: 0.1235s/iter; left time: 3082.7564s
	iters: 1800, epoch: 1 | loss: 0.0174757
	speed: 0.1238s/iter; left time: 3079.1866s
	iters: 1900, epoch: 1 | loss: 0.0154291
	speed: 0.1230s/iter; left time: 3047.2841s
	iters: 2000, epoch: 1 | loss: 0.0142539
	speed: 0.1240s/iter; left time: 3059.4644s
	iters: 2100, epoch: 1 | loss: 0.0161299
	speed: 0.1232s/iter; left time: 3027.4888s
	iters: 2200, epoch: 1 | loss: 0.0133327
	speed: 0.1222s/iter; left time: 2989.6244s
	iters: 2300, epoch: 1 | loss: 0.0161266
	speed: 0.1228s/iter; left time: 2993.8688s
	iters: 2400, epoch: 1 | loss: 0.0131675
	speed: 0.1194s/iter; left time: 2896.9647s
	iters: 2500, epoch: 1 | loss: 0.0135283
	speed: 0.1178s/iter; left time: 2846.9921s
	iters: 2600, epoch: 1 | loss: 0.0187028
	speed: 0.1182s/iter; left time: 2845.4065s
Epoch: 1 cost time: 00h:05m:27.99s
Epoch: 1 | Train Loss: 0.0202631 Vali Loss: 0.0165594 Test Loss: 0.0204801
Validation loss decreased (inf --> 0.016559).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0129171
	speed: 0.8516s/iter; left time: 20355.5284s
	iters: 200, epoch: 2 | loss: 0.0131390
	speed: 0.1177s/iter; left time: 2802.3975s
	iters: 300, epoch: 2 | loss: 0.0138626
	speed: 0.1179s/iter; left time: 2795.7981s
	iters: 400, epoch: 2 | loss: 0.0153124
	speed: 0.1207s/iter; left time: 2848.1697s
	iters: 500, epoch: 2 | loss: 0.0152978
	speed: 0.1202s/iter; left time: 2824.2953s
	iters: 600, epoch: 2 | loss: 0.0140348
	speed: 0.1192s/iter; left time: 2789.7276s
	iters: 700, epoch: 2 | loss: 0.0110302
	speed: 0.1192s/iter; left time: 2776.7331s
	iters: 800, epoch: 2 | loss: 0.0132397
	speed: 0.1185s/iter; left time: 2749.8542s
	iters: 900, epoch: 2 | loss: 0.0165772
	speed: 0.1189s/iter; left time: 2746.6931s
	iters: 1000, epoch: 2 | loss: 0.0107874
	speed: 0.1194s/iter; left time: 2746.9823s
	iters: 1100, epoch: 2 | loss: 0.0108129
	speed: 0.1182s/iter; left time: 2707.6997s
	iters: 1200, epoch: 2 | loss: 0.0124151
	speed: 0.1190s/iter; left time: 2712.8458s
	iters: 1300, epoch: 2 | loss: 0.0126634
	speed: 0.1185s/iter; left time: 2691.1953s
	iters: 1400, epoch: 2 | loss: 0.0145478
	speed: 0.1184s/iter; left time: 2676.6090s
	iters: 1500, epoch: 2 | loss: 0.0183479
	speed: 0.1193s/iter; left time: 2684.9241s
	iters: 1600, epoch: 2 | loss: 0.0120110
	speed: 0.1186s/iter; left time: 2656.5862s
	iters: 1700, epoch: 2 | loss: 0.0155398
	speed: 0.1188s/iter; left time: 2648.9310s
	iters: 1800, epoch: 2 | loss: 0.0138124
	speed: 0.1182s/iter; left time: 2625.1133s
	iters: 1900, epoch: 2 | loss: 0.0101457
	speed: 0.1190s/iter; left time: 2629.5281s
	iters: 2000, epoch: 2 | loss: 0.0139180
	speed: 0.1200s/iter; left time: 2641.1108s
	iters: 2100, epoch: 2 | loss: 0.0154408
	speed: 0.1183s/iter; left time: 2590.7665s
	iters: 2200, epoch: 2 | loss: 0.0114625
	speed: 0.1185s/iter; left time: 2584.0049s
	iters: 2300, epoch: 2 | loss: 0.0090657
	speed: 0.1190s/iter; left time: 2583.8454s
	iters: 2400, epoch: 2 | loss: 0.0156299
	speed: 0.1183s/iter; left time: 2555.5540s
	iters: 2500, epoch: 2 | loss: 0.0100604
	speed: 0.1182s/iter; left time: 2541.2647s
	iters: 2600, epoch: 2 | loss: 0.0139504
	speed: 0.1184s/iter; left time: 2533.5534s
Epoch: 2 cost time: 00h:05m:17.00s
Epoch: 2 | Train Loss: 0.0138196 Vali Loss: 0.0159975 Test Loss: 0.0202182
Validation loss decreased (0.016559 --> 0.015998).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0114765
	speed: 0.8033s/iter; left time: 17058.8026s
	iters: 200, epoch: 3 | loss: 0.0066154
	speed: 0.1165s/iter; left time: 2462.7286s
	iters: 300, epoch: 3 | loss: 0.0107835
	speed: 0.1177s/iter; left time: 2477.0272s
	iters: 400, epoch: 3 | loss: 0.0146891
	speed: 0.1174s/iter; left time: 2459.0399s
	iters: 500, epoch: 3 | loss: 0.0108149
	speed: 0.1184s/iter; left time: 2466.6896s
	iters: 600, epoch: 3 | loss: 0.0132746
	speed: 0.1178s/iter; left time: 2443.2844s
	iters: 700, epoch: 3 | loss: 0.0123814
	speed: 0.1178s/iter; left time: 2430.7580s
	iters: 800, epoch: 3 | loss: 0.0191005
	speed: 0.1166s/iter; left time: 2395.1066s
	iters: 900, epoch: 3 | loss: 0.0097550
	speed: 0.1178s/iter; left time: 2406.7074s
	iters: 1000, epoch: 3 | loss: 0.0135984
	speed: 0.1189s/iter; left time: 2418.1158s
	iters: 1100, epoch: 3 | loss: 0.0107341
	speed: 0.1182s/iter; left time: 2392.1066s
	iters: 1200, epoch: 3 | loss: 0.0200791
	speed: 0.1186s/iter; left time: 2388.2466s
	iters: 1300, epoch: 3 | loss: 0.0127794
	speed: 0.1182s/iter; left time: 2367.5918s
	iters: 1400, epoch: 3 | loss: 0.0146894
	speed: 0.1184s/iter; left time: 2360.9638s
	iters: 1500, epoch: 3 | loss: 0.0103463
	speed: 0.1183s/iter; left time: 2346.0991s
	iters: 1600, epoch: 3 | loss: 0.0168719
	speed: 0.1185s/iter; left time: 2338.2357s
	iters: 1700, epoch: 3 | loss: 0.0171323
	speed: 0.1182s/iter; left time: 2320.2419s
	iters: 1800, epoch: 3 | loss: 0.0145011
	speed: 0.1181s/iter; left time: 2306.5821s
	iters: 1900, epoch: 3 | loss: 0.0188756
	speed: 0.1189s/iter; left time: 2310.8129s
	iters: 2000, epoch: 3 | loss: 0.0091586
	speed: 0.1183s/iter; left time: 2287.9680s
	iters: 2100, epoch: 3 | loss: 0.0192016
	speed: 0.1181s/iter; left time: 2272.7466s
	iters: 2200, epoch: 3 | loss: 0.0112405
	speed: 0.1181s/iter; left time: 2260.4885s
	iters: 2300, epoch: 3 | loss: 0.0101469
	speed: 0.1168s/iter; left time: 2223.0860s
	iters: 2400, epoch: 3 | loss: 0.0168996
	speed: 0.1179s/iter; left time: 2232.1071s
	iters: 2500, epoch: 3 | loss: 0.0111361
	speed: 0.1180s/iter; left time: 2221.9775s
	iters: 2600, epoch: 3 | loss: 0.0105551
	speed: 0.1178s/iter; left time: 2207.1803s
Epoch: 3 cost time: 00h:05m:14.89s
Epoch: 3 | Train Loss: 0.0130641 Vali Loss: 0.0159669 Test Loss: 0.0205408
Validation loss decreased (0.015998 --> 0.015967).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0116718
	speed: 0.7965s/iter; left time: 14791.5457s
	iters: 200, epoch: 4 | loss: 0.0151844
	speed: 0.1156s/iter; left time: 2135.2812s
	iters: 300, epoch: 4 | loss: 0.0079109
	speed: 0.1179s/iter; left time: 2166.5296s
	iters: 400, epoch: 4 | loss: 0.0141269
	speed: 0.1190s/iter; left time: 2174.0490s
	iters: 500, epoch: 4 | loss: 0.0094832
	speed: 0.1177s/iter; left time: 2139.1194s
	iters: 600, epoch: 4 | loss: 0.0137519
	speed: 0.1191s/iter; left time: 2152.7267s
	iters: 700, epoch: 4 | loss: 0.0168084
	speed: 0.1205s/iter; left time: 2165.8159s
	iters: 800, epoch: 4 | loss: 0.0125084
	speed: 0.1180s/iter; left time: 2108.9306s
	iters: 900, epoch: 4 | loss: 0.0086926
	speed: 0.1182s/iter; left time: 2100.3122s
	iters: 1000, epoch: 4 | loss: 0.0132476
	speed: 0.1179s/iter; left time: 2082.4516s
	iters: 1100, epoch: 4 | loss: 0.0159796
	speed: 0.1193s/iter; left time: 2095.5586s
	iters: 1200, epoch: 4 | loss: 0.0104252
	speed: 0.1191s/iter; left time: 2080.2930s
	iters: 1300, epoch: 4 | loss: 0.0091218
	speed: 0.1192s/iter; left time: 2070.1097s
	iters: 1400, epoch: 4 | loss: 0.0086782
	speed: 0.1195s/iter; left time: 2064.1052s
	iters: 1500, epoch: 4 | loss: 0.0108831
	speed: 0.1185s/iter; left time: 2033.9562s
	iters: 1600, epoch: 4 | loss: 0.0112142
	speed: 0.1198s/iter; left time: 2044.2904s
	iters: 1700, epoch: 4 | loss: 0.0103915
	speed: 0.1182s/iter; left time: 2005.0109s
	iters: 1800, epoch: 4 | loss: 0.0119966
	speed: 0.1182s/iter; left time: 1993.8686s
	iters: 1900, epoch: 4 | loss: 0.0121508
	speed: 0.1185s/iter; left time: 1986.5689s
	iters: 2000, epoch: 4 | loss: 0.0122203
	speed: 0.1193s/iter; left time: 1988.6701s
	iters: 2100, epoch: 4 | loss: 0.0186522
	speed: 0.1181s/iter; left time: 1956.3363s
	iters: 2200, epoch: 4 | loss: 0.0069003
	speed: 0.1183s/iter; left time: 1947.8644s
	iters: 2300, epoch: 4 | loss: 0.0162952
	speed: 0.1181s/iter; left time: 1933.7886s
	iters: 2400, epoch: 4 | loss: 0.0083642
	speed: 0.1165s/iter; left time: 1895.7223s
	iters: 2500, epoch: 4 | loss: 0.0148449
	speed: 0.1164s/iter; left time: 1882.9317s
	iters: 2600, epoch: 4 | loss: 0.0146008
	speed: 0.1186s/iter; left time: 1906.0389s
Epoch: 4 cost time: 00h:05m:15.71s
Epoch: 4 | Train Loss: 0.0121474 Vali Loss: 0.0184361 Test Loss: 0.0216450
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0117050
	speed: 0.7876s/iter; left time: 12525.0011s
	iters: 200, epoch: 5 | loss: 0.0124682
	speed: 0.1178s/iter; left time: 1861.8302s
	iters: 300, epoch: 5 | loss: 0.0164941
	speed: 0.1178s/iter; left time: 1850.1476s
	iters: 400, epoch: 5 | loss: 0.0101517
	speed: 0.1181s/iter; left time: 1842.8756s
	iters: 500, epoch: 5 | loss: 0.0105157
	speed: 0.1183s/iter; left time: 1833.2692s
	iters: 600, epoch: 5 | loss: 0.0126912
	speed: 0.1177s/iter; left time: 1813.5357s
	iters: 700, epoch: 5 | loss: 0.0112557
	speed: 0.1177s/iter; left time: 1800.6878s
	iters: 800, epoch: 5 | loss: 0.0098161
	speed: 0.1177s/iter; left time: 1789.8621s
	iters: 900, epoch: 5 | loss: 0.0123347
	speed: 0.1178s/iter; left time: 1778.6737s
	iters: 1000, epoch: 5 | loss: 0.0095558
	speed: 0.1177s/iter; left time: 1765.2888s
	iters: 1100, epoch: 5 | loss: 0.0113308
	speed: 0.1170s/iter; left time: 1744.0548s
	iters: 1200, epoch: 5 | loss: 0.0097510
	speed: 0.1179s/iter; left time: 1745.5381s
	iters: 1300, epoch: 5 | loss: 0.0127526
	speed: 0.1184s/iter; left time: 1741.5237s
	iters: 1400, epoch: 5 | loss: 0.0135294
	speed: 0.1185s/iter; left time: 1730.5548s
	iters: 1500, epoch: 5 | loss: 0.0099966
	speed: 0.1188s/iter; left time: 1723.0880s
	iters: 1600, epoch: 5 | loss: 0.0116613
	speed: 0.1182s/iter; left time: 1701.8966s
	iters: 1700, epoch: 5 | loss: 0.0098514
	speed: 0.1178s/iter; left time: 1685.1197s
	iters: 1800, epoch: 5 | loss: 0.0152843
	speed: 0.1179s/iter; left time: 1674.2915s
	iters: 1900, epoch: 5 | loss: 0.0102675
	speed: 0.1191s/iter; left time: 1679.0493s
	iters: 2000, epoch: 5 | loss: 0.0063129
	speed: 0.1187s/iter; left time: 1661.9780s
	iters: 2100, epoch: 5 | loss: 0.0110215
	speed: 0.1187s/iter; left time: 1649.6010s
	iters: 2200, epoch: 5 | loss: 0.0123013
	speed: 0.1182s/iter; left time: 1631.2583s
	iters: 2300, epoch: 5 | loss: 0.0091697
	speed: 0.1186s/iter; left time: 1624.8205s
	iters: 2400, epoch: 5 | loss: 0.0125893
	speed: 0.1181s/iter; left time: 1606.4311s
	iters: 2500, epoch: 5 | loss: 0.0099446
	speed: 0.1163s/iter; left time: 1570.7085s
	iters: 2600, epoch: 5 | loss: 0.0099037
	speed: 0.1176s/iter; left time: 1576.6022s
Epoch: 5 cost time: 00h:05m:14.95s
Epoch: 5 | Train Loss: 0.0112968 Vali Loss: 0.0192150 Test Loss: 0.0222385
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0083948
	speed: 0.7934s/iter; left time: 10500.7879s
	iters: 200, epoch: 6 | loss: 0.0114102
	speed: 0.1178s/iter; left time: 1547.9484s
	iters: 300, epoch: 6 | loss: 0.0126769
	speed: 0.1183s/iter; left time: 1542.1948s
	iters: 400, epoch: 6 | loss: 0.0070735
	speed: 0.1183s/iter; left time: 1530.3602s
	iters: 500, epoch: 6 | loss: 0.0132286
	speed: 0.1182s/iter; left time: 1516.9562s
	iters: 600, epoch: 6 | loss: 0.0129638
	speed: 0.1181s/iter; left time: 1504.6649s
	iters: 700, epoch: 6 | loss: 0.0114016
	speed: 0.1185s/iter; left time: 1497.4432s
	iters: 800, epoch: 6 | loss: 0.0118810
	speed: 0.1184s/iter; left time: 1484.0799s
	iters: 900, epoch: 6 | loss: 0.0099573
	speed: 0.1183s/iter; left time: 1470.6660s
	iters: 1000, epoch: 6 | loss: 0.0111742
	speed: 0.1184s/iter; left time: 1460.9898s
	iters: 1100, epoch: 6 | loss: 0.0098476
	speed: 0.1197s/iter; left time: 1464.6604s
	iters: 1200, epoch: 6 | loss: 0.0120334
	speed: 0.1181s/iter; left time: 1432.9069s
	iters: 1300, epoch: 6 | loss: 0.0091077
	speed: 0.1157s/iter; left time: 1392.5772s
	iters: 1400, epoch: 6 | loss: 0.0091187
	speed: 0.1174s/iter; left time: 1401.0206s
	iters: 1500, epoch: 6 | loss: 0.0132843
	speed: 0.1196s/iter; left time: 1415.8919s
	iters: 1600, epoch: 6 | loss: 0.0111342
	speed: 0.1188s/iter; left time: 1393.9551s
	iters: 1700, epoch: 6 | loss: 0.0126997
	speed: 0.1187s/iter; left time: 1381.5333s
	iters: 1800, epoch: 6 | loss: 0.0110824
	speed: 0.1188s/iter; left time: 1370.5389s
	iters: 1900, epoch: 6 | loss: 0.0110937
	speed: 0.1182s/iter; left time: 1352.2722s
	iters: 2000, epoch: 6 | loss: 0.0125104
	speed: 0.1193s/iter; left time: 1352.3544s
	iters: 2100, epoch: 6 | loss: 0.0105919
	speed: 0.1187s/iter; left time: 1333.8035s
	iters: 2200, epoch: 6 | loss: 0.0128826
	speed: 0.1189s/iter; left time: 1323.7300s
	iters: 2300, epoch: 6 | loss: 0.0118229
	speed: 0.1187s/iter; left time: 1309.4344s
	iters: 2400, epoch: 6 | loss: 0.0080874
	speed: 0.1182s/iter; left time: 1293.0923s
	iters: 2500, epoch: 6 | loss: 0.0096906
	speed: 0.1186s/iter; left time: 1284.9719s
	iters: 2600, epoch: 6 | loss: 0.0080078
	speed: 0.1190s/iter; left time: 1277.0896s
Epoch: 6 cost time: 00h:05m:16.11s
Epoch: 6 | Train Loss: 0.0105383 Vali Loss: 0.0200892 Test Loss: 0.0229573
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.02054085023701191, rmse:0.14332079887390137, mae:0.092493437230587, rse:0.5552412271499634
Intermediate time for FR and pred_len 168: 00h:38m:38.62s
Intermediate time for FR: 02h:16m:08.88s

=== Starting experiments for country: IT ===

=== Starting experiments for pred_len: 24 ===

--- Running model for IT, pred_len=24 ---
train 85803
val 18651
test 18651
[2024-11-12 20:58:11,723] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 20:58:12,747] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 20:58:12,747] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 20:58:12,747] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 20:58:12,860] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 20:58:12,860] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 20:58:13,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 20:58:13,310] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 20:58:13,310] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 20:58:13,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 20:58:13,311] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 20:58:13,312] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 20:58:13,312] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 20:58:13,312] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 20:58:13,312] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 20:58:13,312] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 20:58:13,490] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 20:58:13,490] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 20:58:13,514] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.23 GB, percent = 2.1%
[2024-11-12 20:58:13,597] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 20:58:13,597] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 20:58:13,597] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.23 GB, percent = 2.1%
[2024-11-12 20:58:13,598] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 20:58:13,675] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 20:58:13,675] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 20:58:13,675] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.23 GB, percent = 2.1%
[2024-11-12 20:58:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 20:58:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 20:58:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 20:58:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 20:58:13,676] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdcb1201290>
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 20:58:13,677] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 20:58:13,678] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 20:58:13,678] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0685159
	speed: 0.1554s/iter; left time: 4151.1054s
	iters: 200, epoch: 1 | loss: 0.0538229
	speed: 0.1230s/iter; left time: 3272.3076s
	iters: 300, epoch: 1 | loss: 0.0390279
	speed: 0.1229s/iter; left time: 3257.0245s
	iters: 400, epoch: 1 | loss: 0.0401875
	speed: 0.1228s/iter; left time: 3242.9719s
	iters: 500, epoch: 1 | loss: 0.0369417
	speed: 0.1229s/iter; left time: 3234.6613s
	iters: 600, epoch: 1 | loss: 0.0201956
	speed: 0.1240s/iter; left time: 3250.2776s
	iters: 700, epoch: 1 | loss: 0.0222709
	speed: 0.1232s/iter; left time: 3215.8722s
	iters: 800, epoch: 1 | loss: 0.0147822
	speed: 0.1227s/iter; left time: 3192.3373s
	iters: 900, epoch: 1 | loss: 0.0152446
	speed: 0.1249s/iter; left time: 3236.0705s
	iters: 1000, epoch: 1 | loss: 0.0305286
	speed: 0.1237s/iter; left time: 3191.7944s
	iters: 1100, epoch: 1 | loss: 0.0163214
	speed: 0.1230s/iter; left time: 3163.1207s
	iters: 1200, epoch: 1 | loss: 0.0225284
	speed: 0.1243s/iter; left time: 3182.9806s
	iters: 1300, epoch: 1 | loss: 0.0239459
	speed: 0.1253s/iter; left time: 3196.5962s
	iters: 1400, epoch: 1 | loss: 0.0129569
	speed: 0.1255s/iter; left time: 3187.8977s
	iters: 1500, epoch: 1 | loss: 0.0161033
	speed: 0.1252s/iter; left time: 3169.9240s
	iters: 1600, epoch: 1 | loss: 0.0124472
	speed: 0.1254s/iter; left time: 3162.2742s
	iters: 1700, epoch: 1 | loss: 0.0221302
	speed: 0.1254s/iter; left time: 3148.7033s
	iters: 1800, epoch: 1 | loss: 0.0147295
	speed: 0.1260s/iter; left time: 3150.3607s
	iters: 1900, epoch: 1 | loss: 0.0138965
	speed: 0.1260s/iter; left time: 3139.0637s
	iters: 2000, epoch: 1 | loss: 0.0210741
	speed: 0.1249s/iter; left time: 3099.6419s
	iters: 2100, epoch: 1 | loss: 0.0109980
	speed: 0.1259s/iter; left time: 3111.0640s
	iters: 2200, epoch: 1 | loss: 0.0166170
	speed: 0.1261s/iter; left time: 3103.2336s
	iters: 2300, epoch: 1 | loss: 0.0213830
	speed: 0.1259s/iter; left time: 3085.5705s
	iters: 2400, epoch: 1 | loss: 0.0159447
	speed: 0.1260s/iter; left time: 3076.9058s
	iters: 2500, epoch: 1 | loss: 0.0190932
	speed: 0.1261s/iter; left time: 3066.1934s
	iters: 2600, epoch: 1 | loss: 0.0130107
	speed: 0.1259s/iter; left time: 3047.2841s
Epoch: 1 cost time: 00h:05m:35.03s
Epoch: 1 | Train Loss: 0.0261947 Vali Loss: 0.0112806 Test Loss: 0.0123892
Validation loss decreased (inf --> 0.011281).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0180760
	speed: 0.8937s/iter; left time: 21474.9994s
	iters: 200, epoch: 2 | loss: 0.0170357
	speed: 0.1204s/iter; left time: 2880.4178s
	iters: 300, epoch: 2 | loss: 0.0192865
	speed: 0.1207s/iter; left time: 2875.8773s
	iters: 400, epoch: 2 | loss: 0.0218281
	speed: 0.1189s/iter; left time: 2821.8017s
	iters: 500, epoch: 2 | loss: 0.0176938
	speed: 0.1196s/iter; left time: 2825.8062s
	iters: 600, epoch: 2 | loss: 0.0105074
	speed: 0.1202s/iter; left time: 2828.2062s
	iters: 700, epoch: 2 | loss: 0.0117768
	speed: 0.1201s/iter; left time: 2813.2084s
	iters: 800, epoch: 2 | loss: 0.0202356
	speed: 0.1203s/iter; left time: 2806.8166s
	iters: 900, epoch: 2 | loss: 0.0135544
	speed: 0.1206s/iter; left time: 2801.3560s
	iters: 1000, epoch: 2 | loss: 0.0144304
	speed: 0.1201s/iter; left time: 2778.4250s
	iters: 1100, epoch: 2 | loss: 0.0137108
	speed: 0.1206s/iter; left time: 2777.7409s
	iters: 1200, epoch: 2 | loss: 0.0173497
	speed: 0.1191s/iter; left time: 2730.4346s
	iters: 1300, epoch: 2 | loss: 0.0125586
	speed: 0.1199s/iter; left time: 2736.9054s
	iters: 1400, epoch: 2 | loss: 0.0098957
	speed: 0.1202s/iter; left time: 2731.1818s
	iters: 1500, epoch: 2 | loss: 0.0119272
	speed: 0.1200s/iter; left time: 2715.4306s
	iters: 1600, epoch: 2 | loss: 0.0197336
	speed: 0.1202s/iter; left time: 2708.1449s
	iters: 1700, epoch: 2 | loss: 0.0113639
	speed: 0.1205s/iter; left time: 2703.2397s
	iters: 1800, epoch: 2 | loss: 0.0173805
	speed: 0.1203s/iter; left time: 2685.3732s
	iters: 1900, epoch: 2 | loss: 0.0161114
	speed: 0.1202s/iter; left time: 2671.7260s
	iters: 2000, epoch: 2 | loss: 0.0148994
	speed: 0.1197s/iter; left time: 2648.2162s
	iters: 2100, epoch: 2 | loss: 0.0123345
	speed: 0.1191s/iter; left time: 2623.7329s
	iters: 2200, epoch: 2 | loss: 0.0184167
	speed: 0.1200s/iter; left time: 2630.6261s
	iters: 2300, epoch: 2 | loss: 0.0131563
	speed: 0.1190s/iter; left time: 2597.7941s
	iters: 2400, epoch: 2 | loss: 0.0118702
	speed: 0.1188s/iter; left time: 2580.9865s
	iters: 2500, epoch: 2 | loss: 0.0114197
	speed: 0.1188s/iter; left time: 2570.1727s
	iters: 2600, epoch: 2 | loss: 0.0149896
	speed: 0.1188s/iter; left time: 2558.1333s
Epoch: 2 cost time: 00h:05m:21.46s
Epoch: 2 | Train Loss: 0.0157138 Vali Loss: 0.0104916 Test Loss: 0.0118753
Validation loss decreased (0.011281 --> 0.010492).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0153372
	speed: 0.8314s/iter; left time: 17749.4456s
	iters: 200, epoch: 3 | loss: 0.0124282
	speed: 0.1179s/iter; left time: 2504.3574s
	iters: 300, epoch: 3 | loss: 0.0157643
	speed: 0.1178s/iter; left time: 2491.3909s
	iters: 400, epoch: 3 | loss: 0.0142025
	speed: 0.1183s/iter; left time: 2489.7729s
	iters: 500, epoch: 3 | loss: 0.0147544
	speed: 0.1187s/iter; left time: 2487.1174s
	iters: 600, epoch: 3 | loss: 0.0149589
	speed: 0.1190s/iter; left time: 2480.6781s
	iters: 700, epoch: 3 | loss: 0.0128942
	speed: 0.1187s/iter; left time: 2463.9090s
	iters: 800, epoch: 3 | loss: 0.0125250
	speed: 0.1179s/iter; left time: 2434.6541s
	iters: 900, epoch: 3 | loss: 0.0136606
	speed: 0.1181s/iter; left time: 2426.5778s
	iters: 1000, epoch: 3 | loss: 0.0145725
	speed: 0.1180s/iter; left time: 2413.5385s
	iters: 1100, epoch: 3 | loss: 0.0148923
	speed: 0.1192s/iter; left time: 2425.4421s
	iters: 1200, epoch: 3 | loss: 0.0150868
	speed: 0.1198s/iter; left time: 2425.8863s
	iters: 1300, epoch: 3 | loss: 0.0126438
	speed: 0.1196s/iter; left time: 2409.2322s
	iters: 1400, epoch: 3 | loss: 0.0156644
	speed: 0.1182s/iter; left time: 2370.7402s
	iters: 1500, epoch: 3 | loss: 0.0119611
	speed: 0.1196s/iter; left time: 2385.4826s
	iters: 1600, epoch: 3 | loss: 0.0131140
	speed: 0.1193s/iter; left time: 2367.4778s
	iters: 1700, epoch: 3 | loss: 0.0125348
	speed: 0.1179s/iter; left time: 2327.5777s
	iters: 1800, epoch: 3 | loss: 0.0107881
	speed: 0.1179s/iter; left time: 2315.9244s
	iters: 1900, epoch: 3 | loss: 0.0143829
	speed: 0.1180s/iter; left time: 2307.5371s
	iters: 2000, epoch: 3 | loss: 0.0107815
	speed: 0.1179s/iter; left time: 2292.1191s
	iters: 2100, epoch: 3 | loss: 0.0135317
	speed: 0.1181s/iter; left time: 2284.4862s
	iters: 2200, epoch: 3 | loss: 0.0147034
	speed: 0.1184s/iter; left time: 2279.9223s
	iters: 2300, epoch: 3 | loss: 0.0107541
	speed: 0.1177s/iter; left time: 2254.4099s
	iters: 2400, epoch: 3 | loss: 0.0142104
	speed: 0.1199s/iter; left time: 2283.8968s
	iters: 2500, epoch: 3 | loss: 0.0132395
	speed: 0.1198s/iter; left time: 2269.6824s
	iters: 2600, epoch: 3 | loss: 0.0184456
	speed: 0.1200s/iter; left time: 2261.0284s
Epoch: 3 cost time: 00h:05m:18.55s
Epoch: 3 | Train Loss: 0.0147667 Vali Loss: 0.0100522 Test Loss: 0.0114825
Validation loss decreased (0.010492 --> 0.010052).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0118942
	speed: 0.8253s/iter; left time: 15406.7826s
	iters: 200, epoch: 4 | loss: 0.0180542
	speed: 0.1200s/iter; left time: 2228.7729s
	iters: 300, epoch: 4 | loss: 0.0142920
	speed: 0.1201s/iter; left time: 2218.4380s
	iters: 400, epoch: 4 | loss: 0.0160521
	speed: 0.1200s/iter; left time: 2204.6821s
	iters: 500, epoch: 4 | loss: 0.0134479
	speed: 0.1202s/iter; left time: 2194.9311s
	iters: 600, epoch: 4 | loss: 0.0134335
	speed: 0.1202s/iter; left time: 2183.9542s
	iters: 700, epoch: 4 | loss: 0.0118842
	speed: 0.1189s/iter; left time: 2147.5954s
	iters: 800, epoch: 4 | loss: 0.0208306
	speed: 0.1200s/iter; left time: 2156.3645s
	iters: 900, epoch: 4 | loss: 0.0112594
	speed: 0.1205s/iter; left time: 2152.4081s
	iters: 1000, epoch: 4 | loss: 0.0140814
	speed: 0.1205s/iter; left time: 2140.4335s
	iters: 1100, epoch: 4 | loss: 0.0124276
	speed: 0.1205s/iter; left time: 2128.1858s
	iters: 1200, epoch: 4 | loss: 0.0146648
	speed: 0.1202s/iter; left time: 2111.1394s
	iters: 1300, epoch: 4 | loss: 0.0163111
	speed: 0.1205s/iter; left time: 2104.1059s
	iters: 1400, epoch: 4 | loss: 0.0085672
	speed: 0.1207s/iter; left time: 2095.9538s
	iters: 1500, epoch: 4 | loss: 0.0122171
	speed: 0.1188s/iter; left time: 2052.2739s
	iters: 1600, epoch: 4 | loss: 0.0151807
	speed: 0.1197s/iter; left time: 2054.4962s
	iters: 1700, epoch: 4 | loss: 0.0155126
	speed: 0.1205s/iter; left time: 2057.1456s
	iters: 1800, epoch: 4 | loss: 0.0173740
	speed: 0.1206s/iter; left time: 2046.1838s
	iters: 1900, epoch: 4 | loss: 0.0195048
	speed: 0.1206s/iter; left time: 2034.5439s
	iters: 2000, epoch: 4 | loss: 0.0105157
	speed: 0.1205s/iter; left time: 2021.3755s
	iters: 2100, epoch: 4 | loss: 0.0181404
	speed: 0.1205s/iter; left time: 2008.0910s
	iters: 2200, epoch: 4 | loss: 0.0115145
	speed: 0.1209s/iter; left time: 2003.7059s
	iters: 2300, epoch: 4 | loss: 0.0101383
	speed: 0.1203s/iter; left time: 1980.7197s
	iters: 2400, epoch: 4 | loss: 0.0154869
	speed: 0.1183s/iter; left time: 1936.8589s
	iters: 2500, epoch: 4 | loss: 0.0147908
	speed: 0.1199s/iter; left time: 1950.6263s
	iters: 2600, epoch: 4 | loss: 0.0160828
	speed: 0.1202s/iter; left time: 1942.8396s
Epoch: 4 cost time: 00h:05m:22.14s
Epoch: 4 | Train Loss: 0.0141236 Vali Loss: 0.0095772 Test Loss: 0.0109636
Validation loss decreased (0.010052 --> 0.009577).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0147402
	speed: 0.8261s/iter; left time: 13206.3729s
	iters: 200, epoch: 5 | loss: 0.0125004
	speed: 0.1203s/iter; left time: 1910.5542s
	iters: 300, epoch: 5 | loss: 0.0193345
	speed: 0.1202s/iter; left time: 1897.8378s
	iters: 400, epoch: 5 | loss: 0.0155532
	speed: 0.1202s/iter; left time: 1886.0482s
	iters: 500, epoch: 5 | loss: 0.0122477
	speed: 0.1200s/iter; left time: 1870.1009s
	iters: 600, epoch: 5 | loss: 0.0131593
	speed: 0.1202s/iter; left time: 1861.4496s
	iters: 700, epoch: 5 | loss: 0.0101924
	speed: 0.1202s/iter; left time: 1848.8866s
	iters: 800, epoch: 5 | loss: 0.0183173
	speed: 0.1210s/iter; left time: 1849.1986s
	iters: 900, epoch: 5 | loss: 0.0157056
	speed: 0.1192s/iter; left time: 1810.6653s
	iters: 1000, epoch: 5 | loss: 0.0098225
	speed: 0.1204s/iter; left time: 1816.7627s
	iters: 1100, epoch: 5 | loss: 0.0163749
	speed: 0.1206s/iter; left time: 1806.7866s
	iters: 1200, epoch: 5 | loss: 0.0121770
	speed: 0.1204s/iter; left time: 1792.2976s
	iters: 1300, epoch: 5 | loss: 0.0124330
	speed: 0.1205s/iter; left time: 1781.2798s
	iters: 1400, epoch: 5 | loss: 0.0112678
	speed: 0.1204s/iter; left time: 1767.8343s
	iters: 1500, epoch: 5 | loss: 0.0169907
	speed: 0.1204s/iter; left time: 1755.5591s
	iters: 1600, epoch: 5 | loss: 0.0119197
	speed: 0.1200s/iter; left time: 1739.0957s
	iters: 1700, epoch: 5 | loss: 0.0173833
	speed: 0.1189s/iter; left time: 1710.4743s
	iters: 1800, epoch: 5 | loss: 0.0114593
	speed: 0.1208s/iter; left time: 1725.9493s
	iters: 1900, epoch: 5 | loss: 0.0092829
	speed: 0.1204s/iter; left time: 1708.1171s
	iters: 2000, epoch: 5 | loss: 0.0180667
	speed: 0.1204s/iter; left time: 1695.6895s
	iters: 2100, epoch: 5 | loss: 0.0138387
	speed: 0.1208s/iter; left time: 1689.1067s
	iters: 2200, epoch: 5 | loss: 0.0123920
	speed: 0.1210s/iter; left time: 1680.3552s
	iters: 2300, epoch: 5 | loss: 0.0184100
	speed: 0.1206s/iter; left time: 1663.3467s
	iters: 2400, epoch: 5 | loss: 0.0186482
	speed: 0.1205s/iter; left time: 1649.7610s
	iters: 2500, epoch: 5 | loss: 0.0142829
	speed: 0.1192s/iter; left time: 1618.8911s
	iters: 2600, epoch: 5 | loss: 0.0095593
	speed: 0.1195s/iter; left time: 1611.4371s
Epoch: 5 cost time: 00h:05m:22.66s
Epoch: 5 | Train Loss: 0.0137007 Vali Loss: 0.0093594 Test Loss: 0.0110088
Validation loss decreased (0.009577 --> 0.009359).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0162855
	speed: 0.8323s/iter; left time: 11074.0988s
	iters: 200, epoch: 6 | loss: 0.0118459
	speed: 0.1193s/iter; left time: 1574.9906s
	iters: 300, epoch: 6 | loss: 0.0183465
	speed: 0.1200s/iter; left time: 1573.2081s
	iters: 400, epoch: 6 | loss: 0.0150153
	speed: 0.1202s/iter; left time: 1563.5565s
	iters: 500, epoch: 6 | loss: 0.0135609
	speed: 0.1201s/iter; left time: 1550.5209s
	iters: 600, epoch: 6 | loss: 0.0151215
	speed: 0.1200s/iter; left time: 1537.0412s
	iters: 700, epoch: 6 | loss: 0.0136610
	speed: 0.1201s/iter; left time: 1525.4068s
	iters: 800, epoch: 6 | loss: 0.0088490
	speed: 0.1200s/iter; left time: 1513.1624s
	iters: 900, epoch: 6 | loss: 0.0130130
	speed: 0.1203s/iter; left time: 1504.0797s
	iters: 1000, epoch: 6 | loss: 0.0120274
	speed: 0.1192s/iter; left time: 1478.2945s
	iters: 1100, epoch: 6 | loss: 0.0119894
	speed: 0.1192s/iter; left time: 1467.1302s
	iters: 1200, epoch: 6 | loss: 0.0142841
	speed: 0.1201s/iter; left time: 1466.4697s
	iters: 1300, epoch: 6 | loss: 0.0106065
	speed: 0.1202s/iter; left time: 1455.6035s
	iters: 1400, epoch: 6 | loss: 0.0108036
	speed: 0.1204s/iter; left time: 1445.1101s
	iters: 1500, epoch: 6 | loss: 0.0158543
	speed: 0.1203s/iter; left time: 1432.1820s
	iters: 1600, epoch: 6 | loss: 0.0113087
	speed: 0.1205s/iter; left time: 1422.1355s
	iters: 1700, epoch: 6 | loss: 0.0104489
	speed: 0.1205s/iter; left time: 1410.7813s
	iters: 1800, epoch: 6 | loss: 0.0163415
	speed: 0.1187s/iter; left time: 1377.9044s
	iters: 1900, epoch: 6 | loss: 0.0127612
	speed: 0.1194s/iter; left time: 1373.6984s
	iters: 2000, epoch: 6 | loss: 0.0125977
	speed: 0.1198s/iter; left time: 1366.8531s
	iters: 2100, epoch: 6 | loss: 0.0094394
	speed: 0.1204s/iter; left time: 1361.6124s
	iters: 2200, epoch: 6 | loss: 0.0116437
	speed: 0.1204s/iter; left time: 1348.6531s
	iters: 2300, epoch: 6 | loss: 0.0175751
	speed: 0.1202s/iter; left time: 1335.4079s
	iters: 2400, epoch: 6 | loss: 0.0131951
	speed: 0.1205s/iter; left time: 1326.5677s
	iters: 2500, epoch: 6 | loss: 0.0133101
	speed: 0.1205s/iter; left time: 1313.6399s
	iters: 2600, epoch: 6 | loss: 0.0134975
	speed: 0.1205s/iter; left time: 1301.7682s
Epoch: 6 cost time: 00h:05m:22.09s
Epoch: 6 | Train Loss: 0.0133536 Vali Loss: 0.0096058 Test Loss: 0.0108095
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0174152
	speed: 0.8111s/iter; left time: 8617.9770s
	iters: 200, epoch: 7 | loss: 0.0187489
	speed: 0.1197s/iter; left time: 1259.9829s
	iters: 300, epoch: 7 | loss: 0.0117891
	speed: 0.1191s/iter; left time: 1241.3385s
	iters: 400, epoch: 7 | loss: 0.0127354
	speed: 0.1191s/iter; left time: 1229.1952s
	iters: 500, epoch: 7 | loss: 0.0156042
	speed: 0.1190s/iter; left time: 1216.8018s
	iters: 600, epoch: 7 | loss: 0.0127855
	speed: 0.1190s/iter; left time: 1205.3399s
	iters: 700, epoch: 7 | loss: 0.0155036
	speed: 0.1198s/iter; left time: 1201.0888s
	iters: 800, epoch: 7 | loss: 0.0158274
	speed: 0.1200s/iter; left time: 1190.8708s
	iters: 900, epoch: 7 | loss: 0.0142470
	speed: 0.1198s/iter; left time: 1177.5186s
	iters: 1000, epoch: 7 | loss: 0.0126780
	speed: 0.1196s/iter; left time: 1162.7707s
	iters: 1100, epoch: 7 | loss: 0.0155426
	speed: 0.1191s/iter; left time: 1146.6305s
	iters: 1200, epoch: 7 | loss: 0.0122411
	speed: 0.1193s/iter; left time: 1135.9645s
	iters: 1300, epoch: 7 | loss: 0.0121622
	speed: 0.1191s/iter; left time: 1122.4522s
	iters: 1400, epoch: 7 | loss: 0.0209989
	speed: 0.1200s/iter; left time: 1118.8953s
	iters: 1500, epoch: 7 | loss: 0.0120442
	speed: 0.1201s/iter; left time: 1107.9887s
	iters: 1600, epoch: 7 | loss: 0.0133398
	speed: 0.1202s/iter; left time: 1096.5887s
	iters: 1700, epoch: 7 | loss: 0.0172512
	speed: 0.1202s/iter; left time: 1085.0974s
	iters: 1800, epoch: 7 | loss: 0.0082703
	speed: 0.1199s/iter; left time: 1070.0857s
	iters: 1900, epoch: 7 | loss: 0.0117215
	speed: 0.1200s/iter; left time: 1058.7095s
	iters: 2000, epoch: 7 | loss: 0.0137244
	speed: 0.1201s/iter; left time: 1048.2518s
	iters: 2100, epoch: 7 | loss: 0.0107342
	speed: 0.1199s/iter; left time: 1034.4944s
	iters: 2200, epoch: 7 | loss: 0.0122395
	speed: 0.1202s/iter; left time: 1024.8229s
	iters: 2300, epoch: 7 | loss: 0.0135718
	speed: 0.1201s/iter; left time: 1012.1244s
	iters: 2400, epoch: 7 | loss: 0.0122959
	speed: 0.1201s/iter; left time: 1000.0535s
	iters: 2500, epoch: 7 | loss: 0.0172985
	speed: 0.1201s/iter; left time: 987.5034s
	iters: 2600, epoch: 7 | loss: 0.0115662
	speed: 0.1201s/iter; left time: 976.0949s
Epoch: 7 cost time: 00h:05m:21.23s
Epoch: 7 | Train Loss: 0.0131364 Vali Loss: 0.0095241 Test Loss: 0.0112181
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0174437
	speed: 0.8085s/iter; left time: 6422.4737s
	iters: 200, epoch: 8 | loss: 0.0134988
	speed: 0.1202s/iter; left time: 942.8455s
	iters: 300, epoch: 8 | loss: 0.0170489
	speed: 0.1200s/iter; left time: 929.5295s
	iters: 400, epoch: 8 | loss: 0.0129379
	speed: 0.1201s/iter; left time: 917.8684s
	iters: 500, epoch: 8 | loss: 0.0102210
	speed: 0.1200s/iter; left time: 905.2302s
	iters: 600, epoch: 8 | loss: 0.0139757
	speed: 0.1206s/iter; left time: 897.4772s
	iters: 700, epoch: 8 | loss: 0.0119929
	speed: 0.1204s/iter; left time: 884.2207s
	iters: 800, epoch: 8 | loss: 0.0110905
	speed: 0.1204s/iter; left time: 872.2214s
	iters: 900, epoch: 8 | loss: 0.0137277
	speed: 0.1206s/iter; left time: 861.3250s
	iters: 1000, epoch: 8 | loss: 0.0219177
	speed: 0.1211s/iter; left time: 853.0112s
	iters: 1100, epoch: 8 | loss: 0.0145128
	speed: 0.1190s/iter; left time: 826.5624s
	iters: 1200, epoch: 8 | loss: 0.0135163
	speed: 0.1182s/iter; left time: 808.7203s
	iters: 1300, epoch: 8 | loss: 0.0157765
	speed: 0.1183s/iter; left time: 797.6816s
	iters: 1400, epoch: 8 | loss: 0.0105938
	speed: 0.1182s/iter; left time: 785.1341s
	iters: 1500, epoch: 8 | loss: 0.0121265
	speed: 0.1183s/iter; left time: 774.3613s
	iters: 1600, epoch: 8 | loss: 0.0109979
	speed: 0.1183s/iter; left time: 762.2344s
	iters: 1700, epoch: 8 | loss: 0.0091685
	speed: 0.1183s/iter; left time: 750.3004s
	iters: 1800, epoch: 8 | loss: 0.0130774
	speed: 0.1184s/iter; left time: 739.0950s
	iters: 1900, epoch: 8 | loss: 0.0130881
	speed: 0.1182s/iter; left time: 726.4977s
	iters: 2000, epoch: 8 | loss: 0.0100635
	speed: 0.1181s/iter; left time: 714.0469s
	iters: 2100, epoch: 8 | loss: 0.0101380
	speed: 0.1181s/iter; left time: 702.2399s
	iters: 2200, epoch: 8 | loss: 0.0156024
	speed: 0.1181s/iter; left time: 690.0025s
	iters: 2300, epoch: 8 | loss: 0.0158889
	speed: 0.1181s/iter; left time: 678.3497s
	iters: 2400, epoch: 8 | loss: 0.0133793
	speed: 0.1180s/iter; left time: 666.1634s
	iters: 2500, epoch: 8 | loss: 0.0136103
	speed: 0.1182s/iter; left time: 655.4105s
	iters: 2600, epoch: 8 | loss: 0.0112722
	speed: 0.1167s/iter; left time: 635.3967s
Epoch: 8 cost time: 00h:05m:19.03s
Epoch: 8 | Train Loss: 0.0127839 Vali Loss: 0.0096197 Test Loss: 0.0115933
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.011008788831532001, rmse:0.10492277890443802, mae:0.06483417749404907, rse:0.3964286744594574
Intermediate time for IT and pred_len 24: 00h:51m:57.23s

=== Starting experiments for pred_len: 96 ===

--- Running model for IT, pred_len=96 ---
train 85587
val 18435
test 18435
[2024-11-12 21:50:08,881] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 21:50:09,837] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 21:50:09,837] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 21:50:09,837] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 21:50:09,928] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 21:50:09,928] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 21:50:10,695] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 21:50:10,696] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 21:50:10,696] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 21:50:10,697] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 21:50:10,697] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 21:50:10,697] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 21:50:10,697] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 21:50:10,697] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 21:50:10,697] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 21:50:10,697] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 21:50:10,879] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 21:50:10,880] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 21:50:10,910] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.95 GB, percent = 2.2%
[2024-11-12 21:50:11,003] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 21:50:11,003] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.73 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 21:50:11,003] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.95 GB, percent = 2.2%
[2024-11-12 21:50:11,003] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 21:50:11,084] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 21:50:11,085] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.84 GB         Max_CA 1 GB 
[2024-11-12 21:50:11,085] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.95 GB, percent = 2.2%
[2024-11-12 21:50:11,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 21:50:11,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 21:50:11,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 21:50:11,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 21:50:11,086] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7feff21a7dd0>
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 21:50:11,086] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 21:50:11,087] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 21:50:11,087] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0694929
	speed: 0.1597s/iter; left time: 4255.8455s
	iters: 200, epoch: 1 | loss: 0.0610348
	speed: 0.1228s/iter; left time: 3258.4021s
	iters: 300, epoch: 1 | loss: 0.0476903
	speed: 0.1228s/iter; left time: 3247.3533s
	iters: 400, epoch: 1 | loss: 0.0505114
	speed: 0.1230s/iter; left time: 3240.1252s
	iters: 500, epoch: 1 | loss: 0.0433032
	speed: 0.1230s/iter; left time: 3227.5404s
	iters: 600, epoch: 1 | loss: 0.0304012
	speed: 0.1228s/iter; left time: 3211.1643s
	iters: 700, epoch: 1 | loss: 0.0364681
	speed: 0.1231s/iter; left time: 3206.1691s
	iters: 800, epoch: 1 | loss: 0.0261162
	speed: 0.1229s/iter; left time: 3189.3040s
	iters: 900, epoch: 1 | loss: 0.0270577
	speed: 0.1232s/iter; left time: 3184.3817s
	iters: 1000, epoch: 1 | loss: 0.0294577
	speed: 0.1230s/iter; left time: 3166.6685s
	iters: 1100, epoch: 1 | loss: 0.0201637
	speed: 0.1231s/iter; left time: 3156.3052s
	iters: 1200, epoch: 1 | loss: 0.0176084
	speed: 0.1228s/iter; left time: 3137.0386s
	iters: 1300, epoch: 1 | loss: 0.0223269
	speed: 0.1228s/iter; left time: 3124.6374s
	iters: 1400, epoch: 1 | loss: 0.0243765
	speed: 0.1230s/iter; left time: 3116.9031s
	iters: 1500, epoch: 1 | loss: 0.0224690
	speed: 0.1230s/iter; left time: 3103.9199s
	iters: 1600, epoch: 1 | loss: 0.0207021
	speed: 0.1231s/iter; left time: 3094.8356s
	iters: 1700, epoch: 1 | loss: 0.0317803
	speed: 0.1232s/iter; left time: 3084.8389s
	iters: 1800, epoch: 1 | loss: 0.0221975
	speed: 0.1229s/iter; left time: 3065.1373s
	iters: 1900, epoch: 1 | loss: 0.0269790
	speed: 0.1230s/iter; left time: 3055.5620s
	iters: 2000, epoch: 1 | loss: 0.0139412
	speed: 0.1227s/iter; left time: 3036.1740s
	iters: 2100, epoch: 1 | loss: 0.0323963
	speed: 0.1228s/iter; left time: 3025.2229s
	iters: 2200, epoch: 1 | loss: 0.0290040
	speed: 0.1230s/iter; left time: 3018.7172s
	iters: 2300, epoch: 1 | loss: 0.0271860
	speed: 0.1228s/iter; left time: 3001.8760s
	iters: 2400, epoch: 1 | loss: 0.0241206
	speed: 0.1230s/iter; left time: 2993.6573s
	iters: 2500, epoch: 1 | loss: 0.0267963
	speed: 0.1230s/iter; left time: 2980.7541s
	iters: 2600, epoch: 1 | loss: 0.0236994
	speed: 0.1230s/iter; left time: 2970.2180s
Epoch: 1 cost time: 00h:05m:29.75s
Epoch: 1 | Train Loss: 0.0329653 Vali Loss: 0.0173980 Test Loss: 0.0185405
Validation loss decreased (inf --> 0.017398).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0272199
	speed: 0.8636s/iter; left time: 20697.1179s
	iters: 200, epoch: 2 | loss: 0.0186267
	speed: 0.1182s/iter; left time: 2820.7139s
	iters: 300, epoch: 2 | loss: 0.0172441
	speed: 0.1183s/iter; left time: 2812.6035s
	iters: 400, epoch: 2 | loss: 0.0219970
	speed: 0.1182s/iter; left time: 2798.4981s
	iters: 500, epoch: 2 | loss: 0.0220297
	speed: 0.1180s/iter; left time: 2780.2725s
	iters: 600, epoch: 2 | loss: 0.0182760
	speed: 0.1179s/iter; left time: 2766.4975s
	iters: 700, epoch: 2 | loss: 0.0163294
	speed: 0.1183s/iter; left time: 2765.0484s
	iters: 800, epoch: 2 | loss: 0.0196641
	speed: 0.1181s/iter; left time: 2747.8187s
	iters: 900, epoch: 2 | loss: 0.0217072
	speed: 0.1185s/iter; left time: 2744.7595s
	iters: 1000, epoch: 2 | loss: 0.0205294
	speed: 0.1182s/iter; left time: 2727.4708s
	iters: 1100, epoch: 2 | loss: 0.0178053
	speed: 0.1186s/iter; left time: 2724.2658s
	iters: 1200, epoch: 2 | loss: 0.0288649
	speed: 0.1184s/iter; left time: 2706.5646s
	iters: 1300, epoch: 2 | loss: 0.0228808
	speed: 0.1182s/iter; left time: 2691.3795s
	iters: 1400, epoch: 2 | loss: 0.0265501
	speed: 0.1183s/iter; left time: 2681.8233s
	iters: 1500, epoch: 2 | loss: 0.0183873
	speed: 0.1187s/iter; left time: 2678.1726s
	iters: 1600, epoch: 2 | loss: 0.0218808
	speed: 0.1183s/iter; left time: 2658.1281s
	iters: 1700, epoch: 2 | loss: 0.0172726
	speed: 0.1184s/iter; left time: 2648.9653s
	iters: 1800, epoch: 2 | loss: 0.0233337
	speed: 0.1183s/iter; left time: 2635.2476s
	iters: 1900, epoch: 2 | loss: 0.0162270
	speed: 0.1187s/iter; left time: 2631.1614s
	iters: 2000, epoch: 2 | loss: 0.0202406
	speed: 0.1184s/iter; left time: 2611.8779s
	iters: 2100, epoch: 2 | loss: 0.0162637
	speed: 0.1187s/iter; left time: 2607.7887s
	iters: 2200, epoch: 2 | loss: 0.0172571
	speed: 0.1183s/iter; left time: 2587.6457s
	iters: 2300, epoch: 2 | loss: 0.0168581
	speed: 0.1189s/iter; left time: 2588.2661s
	iters: 2400, epoch: 2 | loss: 0.0216656
	speed: 0.1184s/iter; left time: 2565.4624s
	iters: 2500, epoch: 2 | loss: 0.0180421
	speed: 0.1183s/iter; left time: 2552.2348s
	iters: 2600, epoch: 2 | loss: 0.0151496
	speed: 0.1183s/iter; left time: 2538.5195s
Epoch: 2 cost time: 00h:05m:16.64s
Epoch: 2 | Train Loss: 0.0215529 Vali Loss: 0.0163842 Test Loss: 0.0182444
Validation loss decreased (0.017398 --> 0.016384).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0224957
	speed: 0.8166s/iter; left time: 17387.7593s
	iters: 200, epoch: 3 | loss: 0.0211753
	speed: 0.1180s/iter; left time: 2500.7697s
	iters: 300, epoch: 3 | loss: 0.0205996
	speed: 0.1183s/iter; left time: 2495.0626s
	iters: 400, epoch: 3 | loss: 0.0235345
	speed: 0.1183s/iter; left time: 2483.9907s
	iters: 500, epoch: 3 | loss: 0.0213881
	speed: 0.1181s/iter; left time: 2467.6234s
	iters: 600, epoch: 3 | loss: 0.0243180
	speed: 0.1198s/iter; left time: 2491.9166s
	iters: 700, epoch: 3 | loss: 0.0170536
	speed: 0.1184s/iter; left time: 2450.2782s
	iters: 800, epoch: 3 | loss: 0.0176462
	speed: 0.1182s/iter; left time: 2434.3755s
	iters: 900, epoch: 3 | loss: 0.0263926
	speed: 0.1183s/iter; left time: 2424.7038s
	iters: 1000, epoch: 3 | loss: 0.0257920
	speed: 0.1183s/iter; left time: 2413.4871s
	iters: 1100, epoch: 3 | loss: 0.0206364
	speed: 0.1183s/iter; left time: 2401.6369s
	iters: 1200, epoch: 3 | loss: 0.0214906
	speed: 0.1182s/iter; left time: 2386.2585s
	iters: 1300, epoch: 3 | loss: 0.0209809
	speed: 0.1184s/iter; left time: 2378.1020s
	iters: 1400, epoch: 3 | loss: 0.0199022
	speed: 0.1183s/iter; left time: 2364.4796s
	iters: 1500, epoch: 3 | loss: 0.0249380
	speed: 0.1182s/iter; left time: 2351.3140s
	iters: 1600, epoch: 3 | loss: 0.0257526
	speed: 0.1188s/iter; left time: 2351.5647s
	iters: 1700, epoch: 3 | loss: 0.0227259
	speed: 0.1182s/iter; left time: 2327.4298s
	iters: 1800, epoch: 3 | loss: 0.0224461
	speed: 0.1183s/iter; left time: 2317.6166s
	iters: 1900, epoch: 3 | loss: 0.0214821
	speed: 0.1187s/iter; left time: 2312.8798s
	iters: 2000, epoch: 3 | loss: 0.0147600
	speed: 0.1189s/iter; left time: 2305.7141s
	iters: 2100, epoch: 3 | loss: 0.0277608
	speed: 0.1190s/iter; left time: 2296.0822s
	iters: 2200, epoch: 3 | loss: 0.0255291
	speed: 0.1184s/iter; left time: 2273.1209s
	iters: 2300, epoch: 3 | loss: 0.0175065
	speed: 0.1189s/iter; left time: 2269.5172s
	iters: 2400, epoch: 3 | loss: 0.0161644
	speed: 0.1186s/iter; left time: 2251.9233s
	iters: 2500, epoch: 3 | loss: 0.0179975
	speed: 0.1183s/iter; left time: 2235.8886s
	iters: 2600, epoch: 3 | loss: 0.0164614
	speed: 0.1184s/iter; left time: 2224.8797s
Epoch: 3 cost time: 00h:05m:17.04s
Epoch: 3 | Train Loss: 0.0201874 Vali Loss: 0.0163050 Test Loss: 0.0187278
Validation loss decreased (0.016384 --> 0.016305).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0149462
	speed: 0.8124s/iter; left time: 15126.3220s
	iters: 200, epoch: 4 | loss: 0.0306469
	speed: 0.1179s/iter; left time: 2184.0401s
	iters: 300, epoch: 4 | loss: 0.0155472
	speed: 0.1187s/iter; left time: 2187.0869s
	iters: 400, epoch: 4 | loss: 0.0248012
	speed: 0.1183s/iter; left time: 2167.4540s
	iters: 500, epoch: 4 | loss: 0.0196272
	speed: 0.1184s/iter; left time: 2157.6122s
	iters: 600, epoch: 4 | loss: 0.0183685
	speed: 0.1191s/iter; left time: 2157.3634s
	iters: 700, epoch: 4 | loss: 0.0231034
	speed: 0.1182s/iter; left time: 2130.7450s
	iters: 800, epoch: 4 | loss: 0.0170192
	speed: 0.1184s/iter; left time: 2122.0650s
	iters: 900, epoch: 4 | loss: 0.0153297
	speed: 0.1176s/iter; left time: 2094.8546s
	iters: 1000, epoch: 4 | loss: 0.0158371
	speed: 0.1171s/iter; left time: 2074.6059s
	iters: 1100, epoch: 4 | loss: 0.0187040
	speed: 0.1170s/iter; left time: 2061.1650s
	iters: 1200, epoch: 4 | loss: 0.0171206
	speed: 0.1169s/iter; left time: 2048.0068s
	iters: 1300, epoch: 4 | loss: 0.0239391
	speed: 0.1170s/iter; left time: 2038.6232s
	iters: 1400, epoch: 4 | loss: 0.0179229
	speed: 0.1170s/iter; left time: 2026.3795s
	iters: 1500, epoch: 4 | loss: 0.0170190
	speed: 0.1169s/iter; left time: 2013.4624s
	iters: 1600, epoch: 4 | loss: 0.0202631
	speed: 0.1181s/iter; left time: 2022.0767s
	iters: 1700, epoch: 4 | loss: 0.0166972
	speed: 0.1183s/iter; left time: 2013.0458s
	iters: 1800, epoch: 4 | loss: 0.0236535
	speed: 0.1180s/iter; left time: 1997.1093s
	iters: 1900, epoch: 4 | loss: 0.0150460
	speed: 0.1185s/iter; left time: 1993.0640s
	iters: 2000, epoch: 4 | loss: 0.0202106
	speed: 0.1186s/iter; left time: 1982.4519s
	iters: 2100, epoch: 4 | loss: 0.0208280
	speed: 0.1185s/iter; left time: 1969.3867s
	iters: 2200, epoch: 4 | loss: 0.0161798
	speed: 0.1183s/iter; left time: 1953.6045s
	iters: 2300, epoch: 4 | loss: 0.0242665
	speed: 0.1182s/iter; left time: 1941.0652s
	iters: 2400, epoch: 4 | loss: 0.0195169
	speed: 0.1183s/iter; left time: 1930.7342s
	iters: 2500, epoch: 4 | loss: 0.0205073
	speed: 0.1185s/iter; left time: 1921.2851s
	iters: 2600, epoch: 4 | loss: 0.0158916
	speed: 0.1182s/iter; left time: 1905.0018s
Epoch: 4 cost time: 00h:05m:15.90s
Epoch: 4 | Train Loss: 0.0189348 Vali Loss: 0.0173067 Test Loss: 0.0214344
EarlyStopping counter: 1 out of 3
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0180995
	speed: 0.7971s/iter; left time: 12709.6100s
	iters: 200, epoch: 5 | loss: 0.0197095
	speed: 0.1193s/iter; left time: 1890.2538s
	iters: 300, epoch: 5 | loss: 0.0229675
	speed: 0.1193s/iter; left time: 1878.7932s
	iters: 400, epoch: 5 | loss: 0.0197423
	speed: 0.1189s/iter; left time: 1859.6279s
	iters: 500, epoch: 5 | loss: 0.0158846
	speed: 0.1184s/iter; left time: 1840.5696s
	iters: 600, epoch: 5 | loss: 0.0230835
	speed: 0.1183s/iter; left time: 1827.1478s
	iters: 700, epoch: 5 | loss: 0.0135149
	speed: 0.1183s/iter; left time: 1814.7125s
	iters: 800, epoch: 5 | loss: 0.0152504
	speed: 0.1185s/iter; left time: 1806.9985s
	iters: 900, epoch: 5 | loss: 0.0175985
	speed: 0.1181s/iter; left time: 1789.2561s
	iters: 1000, epoch: 5 | loss: 0.0131963
	speed: 0.1182s/iter; left time: 1777.9647s
	iters: 1100, epoch: 5 | loss: 0.0131689
	speed: 0.1190s/iter; left time: 1778.3791s
	iters: 1200, epoch: 5 | loss: 0.0162168
	speed: 0.1185s/iter; left time: 1759.5020s
	iters: 1300, epoch: 5 | loss: 0.0161144
	speed: 0.1185s/iter; left time: 1746.6002s
	iters: 1400, epoch: 5 | loss: 0.0195949
	speed: 0.1182s/iter; left time: 1731.6302s
	iters: 1500, epoch: 5 | loss: 0.0137905
	speed: 0.1191s/iter; left time: 1732.1514s
	iters: 1600, epoch: 5 | loss: 0.0177529
	speed: 0.1183s/iter; left time: 1709.0969s
	iters: 1700, epoch: 5 | loss: 0.0135930
	speed: 0.1182s/iter; left time: 1694.8908s
	iters: 1800, epoch: 5 | loss: 0.0197266
	speed: 0.1182s/iter; left time: 1683.2083s
	iters: 1900, epoch: 5 | loss: 0.0125901
	speed: 0.1187s/iter; left time: 1679.6688s
	iters: 2000, epoch: 5 | loss: 0.0140697
	speed: 0.1184s/iter; left time: 1662.6591s
	iters: 2100, epoch: 5 | loss: 0.0179224
	speed: 0.1183s/iter; left time: 1649.1034s
	iters: 2200, epoch: 5 | loss: 0.0175040
	speed: 0.1183s/iter; left time: 1637.7414s
	iters: 2300, epoch: 5 | loss: 0.0234317
	speed: 0.1193s/iter; left time: 1639.2919s
	iters: 2400, epoch: 5 | loss: 0.0175250
	speed: 0.1182s/iter; left time: 1613.1258s
	iters: 2500, epoch: 5 | loss: 0.0212564
	speed: 0.1182s/iter; left time: 1600.9923s
	iters: 2600, epoch: 5 | loss: 0.0128027
	speed: 0.1181s/iter; left time: 1588.4114s
Epoch: 5 cost time: 00h:05m:16.94s
Epoch: 5 | Train Loss: 0.0178580 Vali Loss: 0.0170211 Test Loss: 0.0205465
EarlyStopping counter: 2 out of 3
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0274220
	speed: 0.7991s/iter; left time: 10605.3862s
	iters: 200, epoch: 6 | loss: 0.0163231
	speed: 0.1189s/iter; left time: 1565.9895s
	iters: 300, epoch: 6 | loss: 0.0157869
	speed: 0.1194s/iter; left time: 1560.6201s
	iters: 400, epoch: 6 | loss: 0.0136238
	speed: 0.1184s/iter; left time: 1535.6623s
	iters: 500, epoch: 6 | loss: 0.0148876
	speed: 0.1184s/iter; left time: 1523.4229s
	iters: 600, epoch: 6 | loss: 0.0199551
	speed: 0.1181s/iter; left time: 1507.8738s
	iters: 700, epoch: 6 | loss: 0.0159899
	speed: 0.1180s/iter; left time: 1495.5629s
	iters: 800, epoch: 6 | loss: 0.0183493
	speed: 0.1185s/iter; left time: 1489.6627s
	iters: 900, epoch: 6 | loss: 0.0222189
	speed: 0.1183s/iter; left time: 1475.4458s
	iters: 1000, epoch: 6 | loss: 0.0180203
	speed: 0.1184s/iter; left time: 1464.1826s
	iters: 1100, epoch: 6 | loss: 0.0162416
	speed: 0.1183s/iter; left time: 1451.0638s
	iters: 1200, epoch: 6 | loss: 0.0110515
	speed: 0.1181s/iter; left time: 1436.9564s
	iters: 1300, epoch: 6 | loss: 0.0166524
	speed: 0.1180s/iter; left time: 1423.8569s
	iters: 1400, epoch: 6 | loss: 0.0200620
	speed: 0.1200s/iter; left time: 1435.9932s
	iters: 1500, epoch: 6 | loss: 0.0139170
	speed: 0.1181s/iter; left time: 1401.8972s
	iters: 1600, epoch: 6 | loss: 0.0194305
	speed: 0.1184s/iter; left time: 1393.9447s
	iters: 1700, epoch: 6 | loss: 0.0135993
	speed: 0.1193s/iter; left time: 1391.8636s
	iters: 1800, epoch: 6 | loss: 0.0147370
	speed: 0.1205s/iter; left time: 1394.1119s
	iters: 1900, epoch: 6 | loss: 0.0136333
	speed: 0.1186s/iter; left time: 1360.0710s
	iters: 2000, epoch: 6 | loss: 0.0175431
	speed: 0.1183s/iter; left time: 1345.2756s
	iters: 2100, epoch: 6 | loss: 0.0169017
	speed: 0.1186s/iter; left time: 1336.7683s
	iters: 2200, epoch: 6 | loss: 0.0220713
	speed: 0.1180s/iter; left time: 1318.4192s
	iters: 2300, epoch: 6 | loss: 0.0187113
	speed: 0.1180s/iter; left time: 1306.7620s
	iters: 2400, epoch: 6 | loss: 0.0154803
	speed: 0.1183s/iter; left time: 1297.4570s
	iters: 2500, epoch: 6 | loss: 0.0192516
	speed: 0.1183s/iter; left time: 1285.7295s
	iters: 2600, epoch: 6 | loss: 0.0095078
	speed: 0.1183s/iter; left time: 1274.3960s
Epoch: 6 cost time: 00h:05m:17.02s
Epoch: 6 | Train Loss: 0.0168697 Vali Loss: 0.0163503 Test Loss: 0.0207182
EarlyStopping counter: 3 out of 3
Early stopping
loading model...
Scaled mse:0.018727777525782585, rmse:0.13684946298599243, mae:0.08752837032079697, rse:0.5174592137336731
Intermediate time for IT and pred_len 96: 00h:38m:44.72s

=== Starting experiments for pred_len: 168 ===

--- Running model for IT, pred_len=168 ---
train 85371
val 18219
test 18219
[2024-11-12 22:28:53,484] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-12 22:28:54,435] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-12 22:28:54,436] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-12 22:28:54,436] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2024-11-12 22:28:54,551] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=141.20.21.41, master_port=29500
[2024-11-12 22:28:54,551] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-12 22:28:55,356] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-12 22:28:55,357] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-12 22:28:55,357] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-12 22:28:55,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-12 22:28:55,359] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-12 22:28:55,359] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-12 22:28:55,359] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-12 22:28:55,359] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-12 22:28:55,359] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-12 22:28:55,359] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-12 22:28:55,607] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-12 22:28:55,607] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.64 GB         CA 0.65 GB         Max_CA 1 GB 
[2024-11-12 22:28:55,638] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.43 GB, percent = 2.3%
[2024-11-12 22:28:55,726] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-12 22:28:55,727] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.74 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 22:28:55,727] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.43 GB, percent = 2.3%
[2024-11-12 22:28:55,727] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-12 22:28:55,807] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-12 22:28:55,808] [INFO] [utils.py:801:see_memory_usage] MA 0.54 GB         Max_MA 0.54 GB         CA 0.85 GB         Max_CA 1 GB 
[2024-11-12 22:28:55,808] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 23.43 GB, percent = 2.3%
[2024-11-12 22:28:55,808] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-12 22:28:55,808] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-12 22:28:55,808] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-12 22:28:55,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-12 22:28:55,809] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f05323b1750>
[2024-11-12 22:28:55,809] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   train_batch_size ............. 32
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-12 22:28:55,810] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-12 22:28:55,811] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0760096
	speed: 0.1628s/iter; left time: 4326.5772s
	iters: 200, epoch: 1 | loss: 0.0683317
	speed: 0.1231s/iter; left time: 3257.9985s
	iters: 300, epoch: 1 | loss: 0.0582088
	speed: 0.1232s/iter; left time: 3249.5221s
	iters: 400, epoch: 1 | loss: 0.0557995
	speed: 0.1234s/iter; left time: 3243.0774s
	iters: 500, epoch: 1 | loss: 0.0632301
	speed: 0.1234s/iter; left time: 3228.7350s
	iters: 600, epoch: 1 | loss: 0.0534293
	speed: 0.1238s/iter; left time: 3227.4907s
	iters: 700, epoch: 1 | loss: 0.0340961
	speed: 0.1234s/iter; left time: 3205.8056s
	iters: 800, epoch: 1 | loss: 0.0320352
	speed: 0.1236s/iter; left time: 3196.5368s
	iters: 900, epoch: 1 | loss: 0.0340186
	speed: 0.1238s/iter; left time: 3191.6352s
	iters: 1000, epoch: 1 | loss: 0.0326719
	speed: 0.1235s/iter; left time: 3169.3546s
	iters: 1100, epoch: 1 | loss: 0.0272844
	speed: 0.1235s/iter; left time: 3157.9121s
	iters: 1200, epoch: 1 | loss: 0.0302438
	speed: 0.1239s/iter; left time: 3156.6755s
	iters: 1300, epoch: 1 | loss: 0.0325096
	speed: 0.1234s/iter; left time: 3130.3771s
	iters: 1400, epoch: 1 | loss: 0.0196911
	speed: 0.1234s/iter; left time: 3118.9268s
	iters: 1500, epoch: 1 | loss: 0.0312060
	speed: 0.1234s/iter; left time: 3106.6331s
	iters: 1600, epoch: 1 | loss: 0.0235252
	speed: 0.1233s/iter; left time: 3092.0756s
	iters: 1700, epoch: 1 | loss: 0.0227258
	speed: 0.1235s/iter; left time: 3084.2070s
	iters: 1800, epoch: 1 | loss: 0.0232675
	speed: 0.1246s/iter; left time: 3100.0136s
	iters: 1900, epoch: 1 | loss: 0.0205666
	speed: 0.1238s/iter; left time: 3066.5812s
	iters: 2000, epoch: 1 | loss: 0.0264083
	speed: 0.1235s/iter; left time: 3046.0472s
	iters: 2100, epoch: 1 | loss: 0.0245590
	speed: 0.1239s/iter; left time: 3044.2935s
	iters: 2200, epoch: 1 | loss: 0.0214975
	speed: 0.1239s/iter; left time: 3030.9344s
	iters: 2300, epoch: 1 | loss: 0.0258007
	speed: 0.1237s/iter; left time: 3013.9791s
	iters: 2400, epoch: 1 | loss: 0.0280242
	speed: 0.1234s/iter; left time: 2993.8956s
	iters: 2500, epoch: 1 | loss: 0.0244646
	speed: 0.1234s/iter; left time: 2982.5577s
	iters: 2600, epoch: 1 | loss: 0.0239128
	speed: 0.1235s/iter; left time: 2973.7824s
Epoch: 1 cost time: 00h:05m:30.67s
Epoch: 1 | Train Loss: 0.0353952 Vali Loss: 0.0196393 Test Loss: 0.0204068
Validation loss decreased (inf --> 0.019639).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0263953
	speed: 0.8551s/iter; left time: 20440.0253s
	iters: 200, epoch: 2 | loss: 0.0198965
	speed: 0.1190s/iter; left time: 2832.0463s
	iters: 300, epoch: 2 | loss: 0.0203417
	speed: 0.1193s/iter; left time: 2828.2689s
	iters: 400, epoch: 2 | loss: 0.0276644
	speed: 0.1183s/iter; left time: 2793.4299s
	iters: 500, epoch: 2 | loss: 0.0272038
	speed: 0.1183s/iter; left time: 2781.1242s
	iters: 600, epoch: 2 | loss: 0.0221428
	speed: 0.1184s/iter; left time: 2770.4737s
	iters: 700, epoch: 2 | loss: 0.0161491
	speed: 0.1182s/iter; left time: 2755.4914s
	iters: 800, epoch: 2 | loss: 0.0181733
	speed: 0.1183s/iter; left time: 2743.9207s
	iters: 900, epoch: 2 | loss: 0.0261556
	speed: 0.1183s/iter; left time: 2733.0377s
	iters: 1000, epoch: 2 | loss: 0.0217823
	speed: 0.1182s/iter; left time: 2719.4380s
	iters: 1100, epoch: 2 | loss: 0.0208404
	speed: 0.1182s/iter; left time: 2707.7373s
	iters: 1200, epoch: 2 | loss: 0.0209133
	speed: 0.1182s/iter; left time: 2695.5905s
	iters: 1300, epoch: 2 | loss: 0.0225888
	speed: 0.1184s/iter; left time: 2687.1767s
	iters: 1400, epoch: 2 | loss: 0.0221134
	speed: 0.1184s/iter; left time: 2676.9334s
	iters: 1500, epoch: 2 | loss: 0.0255033
	speed: 0.1185s/iter; left time: 2667.4127s
	iters: 1600, epoch: 2 | loss: 0.0263712
	speed: 0.1187s/iter; left time: 2658.2578s
	iters: 1700, epoch: 2 | loss: 0.0352424
	speed: 0.1183s/iter; left time: 2638.5782s
	iters: 1800, epoch: 2 | loss: 0.0185599
	speed: 0.1184s/iter; left time: 2628.3033s
	iters: 1900, epoch: 2 | loss: 0.0221422
	speed: 0.1183s/iter; left time: 2615.2145s
	iters: 2000, epoch: 2 | loss: 0.0187020
	speed: 0.1188s/iter; left time: 2613.8517s
	iters: 2100, epoch: 2 | loss: 0.0245078
	speed: 0.1184s/iter; left time: 2593.3544s
	iters: 2200, epoch: 2 | loss: 0.0227937
	speed: 0.1187s/iter; left time: 2588.1540s
	iters: 2300, epoch: 2 | loss: 0.0164198
	speed: 0.1187s/iter; left time: 2575.2856s
	iters: 2400, epoch: 2 | loss: 0.0204691
	speed: 0.1187s/iter; left time: 2564.7479s
	iters: 2500, epoch: 2 | loss: 0.0170244
	speed: 0.1186s/iter; left time: 2551.3612s
	iters: 2600, epoch: 2 | loss: 0.0233706
	speed: 0.1186s/iter; left time: 2537.5105s
Epoch: 2 cost time: 00h:05m:16.46s
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1067, in <module>
    main()
  File "/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1063, in main
    launch_command(args)
  File "/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1057, in launch_command
    simple_launcher(args)
  File "/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/lib/python3.11/site-packages/accelerate/commands/launch.py", line 673, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/vol/fob-vol3/nebenf24/riabchuv/.conda/envs/val/bin/python', './Time-LLM/run_main.py', '--task_name', 'long_term_forecast', '--is_training', '1', '--root_path', './datasets/', '--data_path', 'IT_data.csv', '--model_id', '3', '--model', 'TimeLLM', '--data', 'IT', '--features', 'M', '--seq_len', '512', '--pred_len', '168', '--factor', '3', '--enc_in', '3', '--c_out', '3', '--des', 'Exp', '--itr', '1', '--d_model', '16', '--d_ff', '64', '--batch_size', '32', '--learning_rate', '0.001', '--llm_model', 'GPT2', '--llm_dim', '768', '--llm_layers', '12', '--train_epochs', '10', '--patience', '3', '--model_comment', 'TimeLLM+IT']' died with <Signals.SIGTERM: 15>.
