
=== Starting experiments for country: DE ===

=== Starting experiments for pred_len: 24 ===

--- Running model for DE, pred_len=24 ---
[2024-11-16 13:35:46,795] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,816] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:46,822] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 13:35:47,677] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 143885
val 31085
test 31085
train 143885
train 143885
val 31085
train 143885
val 31085
test 31085
val 31085
test 31085
test 31085
[2024-11-16 13:35:49,712] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-16 13:35:50,998] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-16 13:35:50,999] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-16 13:35:51,000] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-16 13:35:51,000] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-16 13:35:51,000] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-16 13:35:51,351] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-16 13:35:51,352] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,352] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 171.88 GB, percent = 17.1%
[2024-11-16 13:35:51,671] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-16 13:35:51,672] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,672] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.23 GB, percent = 17.1%
[2024-11-16 13:35:51,672] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-16 13:35:51,876] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-16 13:35:51,877] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 13:35:51,877] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.55 GB, percent = 17.1%
[2024-11-16 13:35:51,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-16 13:35:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-16 13:35:51,878] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-16 13:35:51,878] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f53b3468590>
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-16 13:35:51,879] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-16 13:35:51,880] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-16 13:35:51,880] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0632429
	speed: 0.3523s/iter; left time: 31645.8063s
	iters: 200, epoch: 1 | loss: 0.0507106
	speed: 0.3228s/iter; left time: 28965.5795s
	iters: 300, epoch: 1 | loss: 0.0394340
	speed: 0.3229s/iter; left time: 28935.6049s
	iters: 400, epoch: 1 | loss: 0.0388287
	speed: 0.3229s/iter; left time: 28909.7832s
	iters: 500, epoch: 1 | loss: 0.0251836
	speed: 0.3229s/iter; left time: 28876.4997s
	iters: 600, epoch: 1 | loss: 0.0225685
	speed: 0.3230s/iter; left time: 28851.5746s
	iters: 700, epoch: 1 | loss: 0.0203688
	speed: 0.3229s/iter; left time: 28813.8099s
	iters: 800, epoch: 1 | loss: 0.0230455
	speed: 0.3229s/iter; left time: 28778.3263s
	iters: 900, epoch: 1 | loss: 0.0221143
	speed: 0.3229s/iter; left time: 28746.0771s
	iters: 1000, epoch: 1 | loss: 0.0220149
	speed: 0.3229s/iter; left time: 28708.3550s
	iters: 1100, epoch: 1 | loss: 0.0171128
	speed: 0.3231s/iter; left time: 28698.6023s
Epoch: 1 cost time: 00h:06m:04.06s
Epoch: 1 | Train Loss: 0.0333040 Vali Loss: 0.0229411 Test Loss: 0.0251673
Validation loss decreased (inf --> 0.022941).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0113808
	speed: 1.2685s/iter; left time: 108237.0048s
	iters: 200, epoch: 2 | loss: 0.0146446
	speed: 0.3227s/iter; left time: 27499.5702s
	iters: 300, epoch: 2 | loss: 0.0160543
	speed: 0.3230s/iter; left time: 27499.5493s
	iters: 400, epoch: 2 | loss: 0.0154046
	speed: 0.3232s/iter; left time: 27476.3707s
	iters: 500, epoch: 2 | loss: 0.0173704
	speed: 0.3231s/iter; left time: 27435.1440s
	iters: 600, epoch: 2 | loss: 0.0362225
	speed: 0.3230s/iter; left time: 27397.2340s
	iters: 700, epoch: 2 | loss: 0.0231417
	speed: 0.2851s/iter; left time: 24156.7660s
	iters: 800, epoch: 2 | loss: 0.0158735
	speed: 0.2628s/iter; left time: 22235.2269s
	iters: 900, epoch: 2 | loss: 0.0282127
	speed: 0.3228s/iter; left time: 27280.8594s
	iters: 1000, epoch: 2 | loss: 0.0127809
	speed: 0.3231s/iter; left time: 27274.1887s
	iters: 1100, epoch: 2 | loss: 0.0152286
	speed: 0.3232s/iter; left time: 27251.1327s
Epoch: 2 cost time: 00h:05m:53.62s
Epoch: 2 | Train Loss: 0.0189258 Vali Loss: 0.0213288 Test Loss: 0.0228869
Validation loss decreased (0.022941 --> 0.021329).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0147866
	speed: 1.2393s/iter; left time: 100172.1175s
	iters: 200, epoch: 3 | loss: 0.0251924
	speed: 0.3226s/iter; left time: 26042.7383s
	iters: 300, epoch: 3 | loss: 0.0179662
	speed: 0.3218s/iter; left time: 25948.7631s
	iters: 400, epoch: 3 | loss: 0.0170750
	speed: 0.3227s/iter; left time: 25983.4054s
	iters: 500, epoch: 3 | loss: 0.0222653
	speed: 0.3229s/iter; left time: 25973.3680s
	iters: 600, epoch: 3 | loss: 0.0223925
	speed: 0.3228s/iter; left time: 25928.9061s
	iters: 700, epoch: 3 | loss: 0.0192496
	speed: 0.3228s/iter; left time: 25901.0748s
	iters: 800, epoch: 3 | loss: 0.0108626
	speed: 0.3234s/iter; left time: 25916.4432s
	iters: 900, epoch: 3 | loss: 0.0230040
	speed: 0.3236s/iter; left time: 25894.3792s
	iters: 1000, epoch: 3 | loss: 0.0219360
	speed: 0.3237s/iter; left time: 25870.1387s
	iters: 1100, epoch: 3 | loss: 0.0159164
	speed: 0.3236s/iter; left time: 25828.7384s
Epoch: 3 cost time: 00h:06m:03.50s
Epoch: 3 | Train Loss: 0.0180037 Vali Loss: 0.0207199 Test Loss: 0.0221680
Validation loss decreased (0.021329 --> 0.020720).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0195420
	speed: 1.2593s/iter; left time: 96126.8123s
	iters: 200, epoch: 4 | loss: 0.0174730
	speed: 0.3234s/iter; left time: 24654.0084s
	iters: 300, epoch: 4 | loss: 0.0160964
	speed: 0.3236s/iter; left time: 24636.3340s
	iters: 400, epoch: 4 | loss: 0.0249365
	speed: 0.3238s/iter; left time: 24619.7518s
	iters: 500, epoch: 4 | loss: 0.0150605
	speed: 0.3236s/iter; left time: 24570.3229s
	iters: 600, epoch: 4 | loss: 0.0123712
	speed: 0.3235s/iter; left time: 24532.5339s
	iters: 700, epoch: 4 | loss: 0.0285275
	speed: 0.3234s/iter; left time: 24494.2035s
	iters: 800, epoch: 4 | loss: 0.0258730
	speed: 0.3235s/iter; left time: 24465.4872s
	iters: 900, epoch: 4 | loss: 0.0145841
	speed: 0.3233s/iter; left time: 24420.9361s
	iters: 1000, epoch: 4 | loss: 0.0179337
	speed: 0.3231s/iter; left time: 24372.6274s
	iters: 1100, epoch: 4 | loss: 0.0190074
	speed: 0.3234s/iter; left time: 24362.5242s
Epoch: 4 cost time: 00h:06m:04.01s
Epoch: 4 | Train Loss: 0.0173761 Vali Loss: 0.0201705 Test Loss: 0.0218201
Validation loss decreased (0.020720 --> 0.020170).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0248572
	speed: 1.2281s/iter; left time: 88221.7166s
	iters: 200, epoch: 5 | loss: 0.0181087
	speed: 0.3236s/iter; left time: 23212.2930s
	iters: 300, epoch: 5 | loss: 0.0130525
	speed: 0.3238s/iter; left time: 23198.4974s
	iters: 400, epoch: 5 | loss: 0.0138662
	speed: 0.3239s/iter; left time: 23173.0373s
	iters: 500, epoch: 5 | loss: 0.0155967
	speed: 0.3239s/iter; left time: 23135.2303s
	iters: 600, epoch: 5 | loss: 0.0198725
	speed: 0.3238s/iter; left time: 23095.7626s
	iters: 700, epoch: 5 | loss: 0.0202901
	speed: 0.3207s/iter; left time: 22844.8257s
	iters: 800, epoch: 5 | loss: 0.0220147
	speed: 0.3239s/iter; left time: 23043.2874s
	iters: 900, epoch: 5 | loss: 0.0173096
	speed: 0.3239s/iter; left time: 23006.4260s
	iters: 1000, epoch: 5 | loss: 0.0179298
	speed: 0.3237s/iter; left time: 22962.7114s
	iters: 1100, epoch: 5 | loss: 0.0144313
	speed: 0.3237s/iter; left time: 22930.7100s
Epoch: 5 cost time: 00h:06m:04.12s
Epoch: 5 | Train Loss: 0.0169353 Vali Loss: 0.0197934 Test Loss: 0.0216913
Validation loss decreased (0.020170 --> 0.019793).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0178052
	speed: 1.2141s/iter; left time: 81761.1781s
	iters: 200, epoch: 6 | loss: 0.0166879
	speed: 0.3222s/iter; left time: 21668.1074s
	iters: 300, epoch: 6 | loss: 0.0215266
	speed: 0.3225s/iter; left time: 21656.1161s
	iters: 400, epoch: 6 | loss: 0.0158959
	speed: 0.3230s/iter; left time: 21652.2129s
	iters: 500, epoch: 6 | loss: 0.0225539
	speed: 0.3235s/iter; left time: 21656.3385s
	iters: 600, epoch: 6 | loss: 0.0142451
	speed: 0.3239s/iter; left time: 21647.6084s
	iters: 700, epoch: 6 | loss: 0.0146011
	speed: 0.3237s/iter; left time: 21605.6043s
	iters: 800, epoch: 6 | loss: 0.0156253
	speed: 0.3240s/iter; left time: 21589.3386s
	iters: 900, epoch: 6 | loss: 0.0157494
	speed: 0.3240s/iter; left time: 21556.3283s
	iters: 1000, epoch: 6 | loss: 0.0117435
	speed: 0.3239s/iter; left time: 21519.5069s
	iters: 1100, epoch: 6 | loss: 0.0189988
	speed: 0.3229s/iter; left time: 21419.9093s
Epoch: 6 cost time: 00h:06m:03.76s
Epoch: 6 | Train Loss: 0.0165692 Vali Loss: 0.0196932 Test Loss: 0.0218816
Validation loss decreased (0.019793 --> 0.019693).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0172309
	speed: 1.2322s/iter; left time: 77437.5813s
	iters: 200, epoch: 7 | loss: 0.0123486
	speed: 0.3239s/iter; left time: 20322.1704s
	iters: 300, epoch: 7 | loss: 0.0212684
	speed: 0.3223s/iter; left time: 20192.9327s
	iters: 400, epoch: 7 | loss: 0.0195458
	speed: 0.3229s/iter; left time: 20195.5811s
	iters: 500, epoch: 7 | loss: 0.0102791
	speed: 0.3243s/iter; left time: 20252.8605s
	iters: 600, epoch: 7 | loss: 0.0223481
	speed: 0.3239s/iter; left time: 20191.7938s
	iters: 700, epoch: 7 | loss: 0.0160351
	speed: 0.3206s/iter; left time: 19956.1453s
	iters: 800, epoch: 7 | loss: 0.0161180
	speed: 0.3227s/iter; left time: 20056.8657s
	iters: 900, epoch: 7 | loss: 0.0166668
	speed: 0.3230s/iter; left time: 20038.1523s
	iters: 1000, epoch: 7 | loss: 0.0117904
	speed: 0.3215s/iter; left time: 19912.8472s
	iters: 1100, epoch: 7 | loss: 0.0165729
	speed: 0.3235s/iter; left time: 20004.3691s
Epoch: 7 cost time: 00h:06m:03.53s
Epoch: 7 | Train Loss: 0.0163646 Vali Loss: 0.0194863 Test Loss: 0.0215178
Validation loss decreased (0.019693 --> 0.019486).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0151283
	speed: 1.2214s/iter; left time: 71268.5130s
	iters: 200, epoch: 8 | loss: 0.0133968
	speed: 0.3230s/iter; left time: 18813.9660s
	iters: 300, epoch: 8 | loss: 0.0154707
	speed: 0.3212s/iter; left time: 18676.8243s
	iters: 400, epoch: 8 | loss: 0.0145500
	speed: 0.3225s/iter; left time: 18721.9795s
	iters: 500, epoch: 8 | loss: 0.0161144
	speed: 0.3240s/iter; left time: 18774.5140s
	iters: 600, epoch: 8 | loss: 0.0167301
	speed: 0.3225s/iter; left time: 18654.1833s
	iters: 700, epoch: 8 | loss: 0.0157079
	speed: 0.3231s/iter; left time: 18655.8284s
	iters: 800, epoch: 8 | loss: 0.0133750
	speed: 0.3224s/iter; left time: 18585.1511s
	iters: 900, epoch: 8 | loss: 0.0157046
	speed: 0.3231s/iter; left time: 18591.7227s
	iters: 1000, epoch: 8 | loss: 0.0103165
	speed: 0.3238s/iter; left time: 18602.5103s
	iters: 1100, epoch: 8 | loss: 0.0139662
	speed: 0.3239s/iter; left time: 18576.8395s
Epoch: 8 cost time: 00h:06m:03.11s
Epoch: 8 | Train Loss: 0.0162527 Vali Loss: 0.0196475 Test Loss: 0.0218817
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0137342
	speed: 1.1821s/iter; left time: 63658.6641s
	iters: 200, epoch: 9 | loss: 0.0193019
	speed: 0.3220s/iter; left time: 17308.2726s
	iters: 300, epoch: 9 | loss: 0.0143334
	speed: 0.3204s/iter; left time: 17192.0739s
	iters: 400, epoch: 9 | loss: 0.0120264
	speed: 0.3234s/iter; left time: 17319.3794s
	iters: 500, epoch: 9 | loss: 0.0147567
	speed: 0.3234s/iter; left time: 17284.7771s
	iters: 600, epoch: 9 | loss: 0.0199759
	speed: 0.3231s/iter; left time: 17240.7187s
	iters: 700, epoch: 9 | loss: 0.0136194
	speed: 0.3227s/iter; left time: 17182.8881s
	iters: 800, epoch: 9 | loss: 0.0103802
	speed: 0.3214s/iter; left time: 17082.7926s
	iters: 900, epoch: 9 | loss: 0.0166926
	speed: 0.3182s/iter; left time: 16881.1642s
	iters: 1000, epoch: 9 | loss: 0.0198049
	speed: 0.3226s/iter; left time: 17081.0444s
	iters: 1100, epoch: 9 | loss: 0.0279420
	speed: 0.3218s/iter; left time: 17006.9184s
Epoch: 9 cost time: 00h:06m:02.41s
Epoch: 9 | Train Loss: 0.0158304 Vali Loss: 0.0192229 Test Loss: 0.0215448
Validation loss decreased (0.019486 --> 0.019223).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0203107
	speed: 1.2472s/iter; left time: 61555.7164s
	iters: 200, epoch: 10 | loss: 0.0133896
	speed: 0.3223s/iter; left time: 15875.1190s
	iters: 300, epoch: 10 | loss: 0.0116833
	speed: 0.3217s/iter; left time: 15813.3395s
	iters: 400, epoch: 10 | loss: 0.0166726
	speed: 0.3182s/iter; left time: 15611.8300s
	iters: 500, epoch: 10 | loss: 0.0108126
	speed: 0.3180s/iter; left time: 15567.2747s
	iters: 600, epoch: 10 | loss: 0.0193221
	speed: 0.3178s/iter; left time: 15524.8963s
	iters: 700, epoch: 10 | loss: 0.0144544
	speed: 0.3169s/iter; left time: 15452.7183s
	iters: 800, epoch: 10 | loss: 0.0147057
	speed: 0.3200s/iter; left time: 15569.0658s
	iters: 900, epoch: 10 | loss: 0.0103039
	speed: 0.3205s/iter; left time: 15564.3570s
	iters: 1000, epoch: 10 | loss: 0.0158291
	speed: 0.3228s/iter; left time: 15641.0012s
	iters: 1100, epoch: 10 | loss: 0.0099726
	speed: 0.3181s/iter; left time: 15384.2536s
Epoch: 10 cost time: 00h:06m:00.19s
Epoch: 10 | Train Loss: 0.0154770 Vali Loss: 0.0195692 Test Loss: 0.0220183
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0165156
	speed: 1.1787s/iter; left time: 52878.7841s
	iters: 200, epoch: 11 | loss: 0.0128128
	speed: 0.3211s/iter; left time: 14370.7279s
	iters: 300, epoch: 11 | loss: 0.0199038
	speed: 0.3201s/iter; left time: 14297.0739s
	iters: 400, epoch: 11 | loss: 0.0113736
	speed: 0.3203s/iter; left time: 14271.6793s
	iters: 500, epoch: 11 | loss: 0.0177854
	speed: 0.3196s/iter; left time: 14208.5477s
	iters: 600, epoch: 11 | loss: 0.0116605
	speed: 0.3232s/iter; left time: 14337.9315s
	iters: 700, epoch: 11 | loss: 0.0130346
	speed: 0.3210s/iter; left time: 14208.8342s
	iters: 800, epoch: 11 | loss: 0.0176324
	speed: 0.3143s/iter; left time: 13877.8484s
	iters: 900, epoch: 11 | loss: 0.0118758
	speed: 0.3189s/iter; left time: 14051.7649s
	iters: 1000, epoch: 11 | loss: 0.0107230
	speed: 0.3171s/iter; left time: 13941.7038s
	iters: 1100, epoch: 11 | loss: 0.0178455
	speed: 0.3166s/iter; left time: 13886.0899s
Epoch: 11 cost time: 00h:05m:59.43s
Epoch: 11 | Train Loss: 0.0154121 Vali Loss: 0.0196424 Test Loss: 0.0221752
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 12 | loss: 0.0175937
	speed: 1.1792s/iter; left time: 47596.4800s
	iters: 200, epoch: 12 | loss: 0.0126353
	speed: 0.3231s/iter; left time: 13009.9612s
	iters: 300, epoch: 12 | loss: 0.0206016
	speed: 0.3230s/iter; left time: 12972.9080s
	iters: 400, epoch: 12 | loss: 0.0102996
	speed: 0.3227s/iter; left time: 12928.5097s
	iters: 500, epoch: 12 | loss: 0.0157677
	speed: 0.3226s/iter; left time: 12894.6865s
	iters: 600, epoch: 12 | loss: 0.0146827
	speed: 0.3226s/iter; left time: 12861.4432s
	iters: 700, epoch: 12 | loss: 0.0145983
	speed: 0.3228s/iter; left time: 12835.6238s
	iters: 800, epoch: 12 | loss: 0.0172842
	speed: 0.3222s/iter; left time: 12781.1348s
	iters: 900, epoch: 12 | loss: 0.0198558
	speed: 0.3228s/iter; left time: 12771.4169s
	iters: 1000, epoch: 12 | loss: 0.0173477
	speed: 0.3226s/iter; left time: 12732.5014s
	iters: 1100, epoch: 12 | loss: 0.0088205
	speed: 0.3228s/iter; left time: 12705.5370s
Epoch: 12 cost time: 00h:06m:03.24s
Epoch: 12 | Train Loss: 0.0152097 Vali Loss: 0.0197840 Test Loss: 0.0225309
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 13 | loss: 0.0192575
	speed: 1.1781s/iter; left time: 42257.1033s
	iters: 200, epoch: 13 | loss: 0.0178487
	speed: 0.3215s/iter; left time: 11500.5318s
	iters: 300, epoch: 13 | loss: 0.0284491
	speed: 0.3226s/iter; left time: 11507.3652s
	iters: 400, epoch: 13 | loss: 0.0159005
	speed: 0.3221s/iter; left time: 11456.8257s
	iters: 500, epoch: 13 | loss: 0.0128495
	speed: 0.3220s/iter; left time: 11421.6517s
	iters: 600, epoch: 13 | loss: 0.0117158
	speed: 0.3228s/iter; left time: 11418.6962s
	iters: 700, epoch: 13 | loss: 0.0135272
	speed: 0.3228s/iter; left time: 11383.7663s
	iters: 800, epoch: 13 | loss: 0.0158590
	speed: 0.3227s/iter; left time: 11348.2563s
	iters: 900, epoch: 13 | loss: 0.0106493
	speed: 0.3223s/iter; left time: 11301.7743s
	iters: 1000, epoch: 13 | loss: 0.0102547
	speed: 0.3196s/iter; left time: 11176.2883s
	iters: 1100, epoch: 13 | loss: 0.0108302
	speed: 0.3185s/iter; left time: 11107.3901s
Epoch: 13 cost time: 00h:06m:02.09s
Epoch: 13 | Train Loss: 0.0150879 Vali Loss: 0.0196984 Test Loss: 0.0227838
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 14 | loss: 0.0201575
	speed: 1.1755s/iter; left time: 36877.6420s
	iters: 200, epoch: 14 | loss: 0.0167995
	speed: 0.3194s/iter; left time: 9989.9229s
	iters: 300, epoch: 14 | loss: 0.0204233
	speed: 0.3225s/iter; left time: 10052.4890s
	iters: 400, epoch: 14 | loss: 0.0163709
	speed: 0.3188s/iter; left time: 9906.4489s
	iters: 500, epoch: 14 | loss: 0.0143940
	speed: 0.3171s/iter; left time: 9821.4001s
	iters: 600, epoch: 14 | loss: 0.0164356
	speed: 0.3179s/iter; left time: 9814.6520s
	iters: 700, epoch: 14 | loss: 0.0193533
	speed: 0.3170s/iter; left time: 9756.5390s
	iters: 800, epoch: 14 | loss: 0.0155888
	speed: 0.3165s/iter; left time: 9707.3937s
	iters: 900, epoch: 14 | loss: 0.0123037
	speed: 0.3178s/iter; left time: 9716.9589s
	iters: 1000, epoch: 14 | loss: 0.0190936
	speed: 0.3170s/iter; left time: 9660.8142s
	iters: 1100, epoch: 14 | loss: 0.0183230
	speed: 0.3205s/iter; left time: 9733.8511s
Epoch: 14 cost time: 00h:05m:58.40s
Epoch: 14 | Train Loss: 0.0148230 Vali Loss: 0.0200994 Test Loss: 0.0234704
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Scaled mse:0.021544765681028366, rmse:0.14678135514259338, mae:0.09620458632707596, rse:0.5180174112319946
Intermediate time for DE and pred_len 24: 01h:44m:15.62s

=== Starting experiments for pred_len: 96 ===

--- Running model for DE, pred_len=96 ---
[2024-11-16 15:20:03,475] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 15:20:03,476] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 15:20:03,479] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 15:20:03,498] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 15:20:04,799] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 15:20:04,799] [INFO] [comm.py:637:init_distributed] cdb=None
train 142645
val 30725
test 30725
train 142645
train 142645
val 30725
val 30725
train 142645
test 30725
test 30725
val 30725
test 30725
[2024-11-16 15:20:06,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-16 15:20:08,168] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-16 15:20:08,169] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-16 15:20:08,169] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-16 15:20:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-16 15:20:08,170] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-16 15:20:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-16 15:20:08,170] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-16 15:20:08,524] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-16 15:20:08,525] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-16 15:20:08,525] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.34 GB, percent = 17.1%
[2024-11-16 15:20:08,665] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-16 15:20:08,665] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 15:20:08,665] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.61 GB, percent = 17.1%
[2024-11-16 15:20:08,665] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-16 15:20:08,798] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-16 15:20:08,799] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 15:20:08,799] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 172.77 GB, percent = 17.1%
[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-16 15:20:08,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-16 15:20:08,800] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0350a3a310>
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-16 15:20:08,800] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-16 15:20:08,801] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-16 15:20:08,801] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0446114
	speed: 0.3681s/iter; left time: 32772.2485s
	iters: 200, epoch: 1 | loss: 0.0507687
	speed: 0.3328s/iter; left time: 29601.4851s
	iters: 300, epoch: 1 | loss: 0.0448839
	speed: 0.3338s/iter; left time: 29657.3482s
	iters: 400, epoch: 1 | loss: 0.0454857
	speed: 0.3328s/iter; left time: 29529.0056s
	iters: 500, epoch: 1 | loss: 0.0477529
	speed: 0.3330s/iter; left time: 29521.1311s
	iters: 600, epoch: 1 | loss: 0.0380543
	speed: 0.3334s/iter; left time: 29519.3228s
	iters: 700, epoch: 1 | loss: 0.0391503
	speed: 0.3331s/iter; left time: 29460.7783s
	iters: 800, epoch: 1 | loss: 0.0337921
	speed: 0.3331s/iter; left time: 29430.4367s
	iters: 900, epoch: 1 | loss: 0.0199492
	speed: 0.3335s/iter; left time: 29426.9081s
	iters: 1000, epoch: 1 | loss: 0.0314866
	speed: 0.3332s/iter; left time: 29370.0940s
	iters: 1100, epoch: 1 | loss: 0.0306014
	speed: 0.3330s/iter; left time: 29318.7378s
Epoch: 1 cost time: 00h:06m:12.80s
Epoch: 1 | Train Loss: 0.0429309 Vali Loss: 0.0322865 Test Loss: 0.0373079
Validation loss decreased (inf --> 0.032287).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0302605
	speed: 1.2869s/iter; left time: 108852.5647s
	iters: 200, epoch: 2 | loss: 0.0323583
	speed: 0.3331s/iter; left time: 28140.0208s
	iters: 300, epoch: 2 | loss: 0.0289049
	speed: 0.3331s/iter; left time: 28106.3431s
	iters: 400, epoch: 2 | loss: 0.0225016
	speed: 0.3334s/iter; left time: 28101.6855s
	iters: 500, epoch: 2 | loss: 0.0280473
	speed: 0.3336s/iter; left time: 28080.0178s
	iters: 600, epoch: 2 | loss: 0.0231738
	speed: 0.3332s/iter; left time: 28014.4420s
	iters: 700, epoch: 2 | loss: 0.0285885
	speed: 0.3335s/iter; left time: 28010.8677s
	iters: 800, epoch: 2 | loss: 0.0226014
	speed: 0.3336s/iter; left time: 27987.7576s
	iters: 900, epoch: 2 | loss: 0.0252432
	speed: 0.3333s/iter; left time: 27928.8521s
	iters: 1000, epoch: 2 | loss: 0.0303803
	speed: 0.3335s/iter; left time: 27909.7248s
	iters: 1100, epoch: 2 | loss: 0.0346701
	speed: 0.3333s/iter; left time: 27862.2472s
Epoch: 2 cost time: 00h:06m:11.75s
Epoch: 2 | Train Loss: 0.0267929 Vali Loss: 0.0310637 Test Loss: 0.0355101
Validation loss decreased (0.032287 --> 0.031064).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0272676
	speed: 0.8939s/iter; left time: 71628.1422s
	iters: 200, epoch: 3 | loss: 0.0320771
	speed: 0.1432s/iter; left time: 11458.2320s
	iters: 300, epoch: 3 | loss: 0.0189163
	speed: 0.1378s/iter; left time: 11015.6804s
	iters: 400, epoch: 3 | loss: 0.0244222
	speed: 0.1350s/iter; left time: 10776.1540s
	iters: 500, epoch: 3 | loss: 0.0268812
	speed: 0.1343s/iter; left time: 10705.6543s
	iters: 600, epoch: 3 | loss: 0.0218725
	speed: 0.1341s/iter; left time: 10677.9071s
	iters: 700, epoch: 3 | loss: 0.0201757
	speed: 0.1339s/iter; left time: 10648.6167s
	iters: 800, epoch: 3 | loss: 0.0237758
	speed: 0.1344s/iter; left time: 10677.9335s
	iters: 900, epoch: 3 | loss: 0.0272236
	speed: 0.1353s/iter; left time: 10735.1516s
	iters: 1000, epoch: 3 | loss: 0.0253749
	speed: 0.1352s/iter; left time: 10707.9453s
	iters: 1100, epoch: 3 | loss: 0.0313581
	speed: 0.1352s/iter; left time: 10694.8882s
Epoch: 3 cost time: 00h:02m:52.32s
Epoch: 3 | Train Loss: 0.0256442 Vali Loss: 0.0304424 Test Loss: 0.0349693
Validation loss decreased (0.031064 --> 0.030442).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0234055
	speed: 0.4695s/iter; left time: 35523.3291s
	iters: 200, epoch: 4 | loss: 0.0379499
	speed: 0.1343s/iter; left time: 10147.7702s
	iters: 300, epoch: 4 | loss: 0.0260023
	speed: 0.1388s/iter; left time: 10476.2268s
	iters: 400, epoch: 4 | loss: 0.0309342
	speed: 0.1360s/iter; left time: 10252.8759s
	iters: 500, epoch: 4 | loss: 0.0230162
	speed: 0.1342s/iter; left time: 10103.0281s
	iters: 600, epoch: 4 | loss: 0.0329382
	speed: 0.1362s/iter; left time: 10239.6939s
	iters: 700, epoch: 4 | loss: 0.0270466
	speed: 0.1343s/iter; left time: 10082.3685s
	iters: 800, epoch: 4 | loss: 0.0279899
	speed: 0.1343s/iter; left time: 10069.3088s
	iters: 900, epoch: 4 | loss: 0.0265847
	speed: 0.1335s/iter; left time: 9997.1779s
	iters: 1000, epoch: 4 | loss: 0.0298538
	speed: 0.1339s/iter; left time: 10009.8275s
	iters: 1100, epoch: 4 | loss: 0.0180375
	speed: 0.1338s/iter; left time: 9989.7818s
Epoch: 4 cost time: 00h:02m:31.06s
Epoch: 4 | Train Loss: 0.0250050 Vali Loss: 0.0307257 Test Loss: 0.0347905
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0198935
	speed: 0.4307s/iter; left time: 30671.0513s
	iters: 200, epoch: 5 | loss: 0.0202480
	speed: 0.1330s/iter; left time: 9458.7539s
	iters: 300, epoch: 5 | loss: 0.0214222
	speed: 0.1365s/iter; left time: 9694.8402s
	iters: 400, epoch: 5 | loss: 0.0283736
	speed: 0.1369s/iter; left time: 9704.8714s
	iters: 500, epoch: 5 | loss: 0.0473016
	speed: 0.1341s/iter; left time: 9493.6185s
	iters: 600, epoch: 5 | loss: 0.0202376
	speed: 0.1332s/iter; left time: 9415.4894s
	iters: 700, epoch: 5 | loss: 0.0211350
	speed: 0.1355s/iter; left time: 9571.4410s
	iters: 800, epoch: 5 | loss: 0.0247196
	speed: 0.1343s/iter; left time: 9466.5839s
	iters: 900, epoch: 5 | loss: 0.0205404
	speed: 0.1335s/iter; left time: 9403.2914s
	iters: 1000, epoch: 5 | loss: 0.0239363
	speed: 0.1354s/iter; left time: 9521.1684s
	iters: 1100, epoch: 5 | loss: 0.0213725
	speed: 0.1334s/iter; left time: 9367.9676s
Epoch: 5 cost time: 00h:02m:30.29s
Epoch: 5 | Train Loss: 0.0242875 Vali Loss: 0.0304751 Test Loss: 0.0348097
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0309512
	speed: 0.4276s/iter; left time: 28547.9665s
	iters: 200, epoch: 6 | loss: 0.0205533
	speed: 0.1323s/iter; left time: 8816.1162s
	iters: 300, epoch: 6 | loss: 0.0224231
	speed: 0.1354s/iter; left time: 9014.1818s
	iters: 400, epoch: 6 | loss: 0.0260528
	speed: 0.1337s/iter; left time: 8886.6849s
	iters: 500, epoch: 6 | loss: 0.0180285
	speed: 0.1326s/iter; left time: 8797.5420s
	iters: 600, epoch: 6 | loss: 0.0195526
	speed: 0.1338s/iter; left time: 8861.9728s
	iters: 700, epoch: 6 | loss: 0.0172730
	speed: 0.1364s/iter; left time: 9020.5564s
	iters: 800, epoch: 6 | loss: 0.0267439
	speed: 0.1332s/iter; left time: 8798.6147s
	iters: 900, epoch: 6 | loss: 0.0241353
	speed: 0.1363s/iter; left time: 8990.9680s
	iters: 1000, epoch: 6 | loss: 0.0157862
	speed: 0.1354s/iter; left time: 8914.7369s
	iters: 1100, epoch: 6 | loss: 0.0216071
	speed: 0.1335s/iter; left time: 8777.0935s
Epoch: 6 cost time: 00h:02m:29.93s
Epoch: 6 | Train Loss: 0.0236184 Vali Loss: 0.0298692 Test Loss: 0.0346515
Validation loss decreased (0.030442 --> 0.029869).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0280138
	speed: 0.4666s/iter; left time: 29066.2313s
	iters: 200, epoch: 7 | loss: 0.0166881
	speed: 0.1319s/iter; left time: 8202.0174s
	iters: 300, epoch: 7 | loss: 0.0210482
	speed: 0.1357s/iter; left time: 8426.4395s
	iters: 400, epoch: 7 | loss: 0.0262818
	speed: 0.1350s/iter; left time: 8372.9613s
	iters: 500, epoch: 7 | loss: 0.0175326
	speed: 0.1328s/iter; left time: 8217.8671s
	iters: 600, epoch: 7 | loss: 0.0228772
	speed: 0.1336s/iter; left time: 8253.3771s
	iters: 700, epoch: 7 | loss: 0.0226232
	speed: 0.1354s/iter; left time: 8351.9640s
	iters: 800, epoch: 7 | loss: 0.0138826
	speed: 0.1342s/iter; left time: 8267.4929s
	iters: 900, epoch: 7 | loss: 0.0142537
	speed: 0.1330s/iter; left time: 8179.2649s
	iters: 1000, epoch: 7 | loss: 0.0185196
	speed: 0.1323s/iter; left time: 8123.0079s
	iters: 1100, epoch: 7 | loss: 0.0187493
	speed: 0.1329s/iter; left time: 8146.0415s
Epoch: 7 cost time: 00h:02m:29.48s
Epoch: 7 | Train Loss: 0.0231316 Vali Loss: 0.0302424 Test Loss: 0.0350993
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0193801
	speed: 0.4266s/iter; left time: 24676.6077s
	iters: 200, epoch: 8 | loss: 0.0185916
	speed: 0.1318s/iter; left time: 7612.2115s
	iters: 300, epoch: 8 | loss: 0.0213563
	speed: 0.1351s/iter; left time: 7789.8088s
	iters: 400, epoch: 8 | loss: 0.0218396
	speed: 0.1327s/iter; left time: 7638.3653s
	iters: 500, epoch: 8 | loss: 0.0290579
	speed: 0.1324s/iter; left time: 7604.5596s
	iters: 600, epoch: 8 | loss: 0.0331087
	speed: 0.1333s/iter; left time: 7645.2016s
	iters: 700, epoch: 8 | loss: 0.0227459
	speed: 0.1377s/iter; left time: 7884.3620s
	iters: 800, epoch: 8 | loss: 0.0252364
	speed: 0.1364s/iter; left time: 7792.3467s
	iters: 900, epoch: 8 | loss: 0.0219207
	speed: 0.1332s/iter; left time: 7600.7579s
	iters: 1000, epoch: 8 | loss: 0.0333456
	speed: 0.1346s/iter; left time: 7661.5944s
	iters: 1100, epoch: 8 | loss: 0.0219904
	speed: 0.1332s/iter; left time: 7570.0531s
Epoch: 8 cost time: 00h:02m:29.87s
Epoch: 8 | Train Loss: 0.0225641 Vali Loss: 0.0309571 Test Loss: 0.0370625
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0179517
	speed: 0.4295s/iter; left time: 22927.9580s
	iters: 200, epoch: 9 | loss: 0.0256749
	speed: 0.1323s/iter; left time: 7049.8919s
	iters: 300, epoch: 9 | loss: 0.0194726
	speed: 0.1347s/iter; left time: 7161.5742s
	iters: 400, epoch: 9 | loss: 0.0239984
	speed: 0.1322s/iter; left time: 7020.0342s
	iters: 500, epoch: 9 | loss: 0.0159293
	speed: 0.1324s/iter; left time: 7016.8628s
	iters: 600, epoch: 9 | loss: 0.0192116
	speed: 0.1327s/iter; left time: 7015.8684s
	iters: 700, epoch: 9 | loss: 0.0197610
	speed: 0.1331s/iter; left time: 7023.9503s
	iters: 800, epoch: 9 | loss: 0.0186225
	speed: 0.1312s/iter; left time: 6913.0395s
	iters: 900, epoch: 9 | loss: 0.0202066
	speed: 0.1312s/iter; left time: 6899.7419s
	iters: 1000, epoch: 9 | loss: 0.0174803
	speed: 0.1317s/iter; left time: 6911.6133s
	iters: 1100, epoch: 9 | loss: 0.0254039
	speed: 0.1317s/iter; left time: 6900.9018s
Epoch: 9 cost time: 00h:02m:27.98s
Epoch: 9 | Train Loss: 0.0215361 Vali Loss: 0.0309942 Test Loss: 0.0375069
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 10 | loss: 0.0238908
	speed: 0.4197s/iter; left time: 20535.0974s
	iters: 200, epoch: 10 | loss: 0.0189626
	speed: 0.1311s/iter; left time: 6402.0846s
	iters: 300, epoch: 10 | loss: 0.0185715
	speed: 0.1344s/iter; left time: 6548.8372s
	iters: 400, epoch: 10 | loss: 0.0208660
	speed: 0.1332s/iter; left time: 6479.2278s
	iters: 500, epoch: 10 | loss: 0.0276698
	speed: 0.1312s/iter; left time: 6365.2740s
	iters: 600, epoch: 10 | loss: 0.0254691
	speed: 0.1334s/iter; left time: 6460.8830s
	iters: 700, epoch: 10 | loss: 0.0252488
	speed: 0.1311s/iter; left time: 6336.3487s
	iters: 800, epoch: 10 | loss: 0.0194837
	speed: 0.1322s/iter; left time: 6374.6949s
	iters: 900, epoch: 10 | loss: 0.0210351
	speed: 0.1334s/iter; left time: 6420.7275s
	iters: 1000, epoch: 10 | loss: 0.0163288
	speed: 0.1333s/iter; left time: 6403.1120s
	iters: 1100, epoch: 10 | loss: 0.0178875
	speed: 0.1332s/iter; left time: 6383.8194s
Epoch: 10 cost time: 00h:02m:28.31s
Epoch: 10 | Train Loss: 0.0208566 Vali Loss: 0.0317293 Test Loss: 0.0390418
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 11 | loss: 0.0179506
	speed: 0.4286s/iter; left time: 19061.1895s
	iters: 200, epoch: 11 | loss: 0.0186568
	speed: 0.1309s/iter; left time: 5809.0643s
	iters: 300, epoch: 11 | loss: 0.0261067
	speed: 0.1322s/iter; left time: 5852.8858s
	iters: 400, epoch: 11 | loss: 0.0201972
	speed: 0.1308s/iter; left time: 5779.6304s
	iters: 500, epoch: 11 | loss: 0.0227132
	speed: 0.1325s/iter; left time: 5841.2526s
	iters: 600, epoch: 11 | loss: 0.0168747
	speed: 0.1312s/iter; left time: 5769.6729s
	iters: 700, epoch: 11 | loss: 0.0244474
	speed: 0.1350s/iter; left time: 5921.2363s
	iters: 800, epoch: 11 | loss: 0.0158708
	speed: 0.1315s/iter; left time: 5757.6713s
	iters: 900, epoch: 11 | loss: 0.0233177
	speed: 0.1314s/iter; left time: 5737.2276s
	iters: 1000, epoch: 11 | loss: 0.0231835
	speed: 0.1333s/iter; left time: 5808.7242s
	iters: 1100, epoch: 11 | loss: 0.0227261
	speed: 0.1328s/iter; left time: 5772.7395s
Epoch: 11 cost time: 00h:02m:27.92s
Epoch: 11 | Train Loss: 0.0203122 Vali Loss: 0.0328566 Test Loss: 0.0383958
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Scaled mse:0.0346515066921711, rmse:0.18614915013313293, mae:0.12807267904281616, rse:0.6591846346855164
Intermediate time for DE and pred_len 96: 00h:42m:21.01s

=== Starting experiments for pred_len: 168 ===

--- Running model for DE, pred_len=168 ---
[2024-11-16 16:02:24,986] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 16:02:24,987] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 16:02:24,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 16:02:24,991] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 16:02:25,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-16 16:02:25,922] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
train 142285
val 30365
test 30365
train 142285
train 142285
val 30365
val 30365
test 30365
train 142285
test 30365
val 30365
test 30365
[2024-11-16 16:02:27,722] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-11-16 16:02:28,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-16 16:02:28,664] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-16 16:02:28,664] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-16 16:02:28,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-11-16 16:02:28,665] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-11-16 16:02:28,665] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-11-16 16:02:28,665] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-11-16 16:02:28,932] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-11-16 16:02:28,933] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.42 GB         CA 0.43 GB         Max_CA 0 GB 
[2024-11-16 16:02:28,933] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.42 GB, percent = 16.9%
[2024-11-16 16:02:29,093] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-11-16 16:02:29,094] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.44 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 16:02:29,094] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.69 GB, percent = 16.9%
[2024-11-16 16:02:29,094] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-11-16 16:02:29,235] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-11-16 16:02:29,235] [INFO] [utils.py:801:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.48 GB         Max_CA 0 GB 
[2024-11-16 16:02:29,235] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 170.88 GB, percent = 17.0%
[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-16 16:02:29,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]
[2024-11-16 16:02:29,236] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-16 16:02:29,236] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8c764f7b10>
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-16 16:02:29,237] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-16 16:02:29,238] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-11-16 16:02:29,238] [INFO] [config.py:986:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
	iters: 100, epoch: 1 | loss: 0.0359368
	speed: 0.1644s/iter; left time: 14606.4632s
	iters: 200, epoch: 1 | loss: 0.0448534
	speed: 0.1390s/iter; left time: 12332.1637s
	iters: 300, epoch: 1 | loss: 0.0433941
	speed: 0.1389s/iter; left time: 12309.2884s
	iters: 400, epoch: 1 | loss: 0.0418473
	speed: 0.1392s/iter; left time: 12326.1744s
	iters: 500, epoch: 1 | loss: 0.0473000
	speed: 0.1394s/iter; left time: 12324.8273s
	iters: 600, epoch: 1 | loss: 0.0449542
	speed: 0.1408s/iter; left time: 12439.1721s
	iters: 700, epoch: 1 | loss: 0.0429755
	speed: 0.1403s/iter; left time: 12374.5821s
	iters: 800, epoch: 1 | loss: 0.0494640
	speed: 0.1412s/iter; left time: 12438.3126s
	iters: 900, epoch: 1 | loss: 0.0413226
	speed: 0.1405s/iter; left time: 12365.4032s
	iters: 1000, epoch: 1 | loss: 0.0430699
	speed: 0.1398s/iter; left time: 12289.0584s
	iters: 1100, epoch: 1 | loss: 0.0416704
	speed: 0.1390s/iter; left time: 12204.6348s
Epoch: 1 cost time: 00h:02m:36.39s
Epoch: 1 | Train Loss: 0.0463432 Vali Loss: 0.0367494 Test Loss: 0.0425076
Validation loss decreased (inf --> 0.036749).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 2 | loss: 0.0301424
	speed: 0.4761s/iter; left time: 40174.5548s
	iters: 200, epoch: 2 | loss: 0.0237679
	speed: 0.1378s/iter; left time: 11609.3023s
	iters: 300, epoch: 2 | loss: 0.0273275
	speed: 0.1368s/iter; left time: 11511.4448s
	iters: 400, epoch: 2 | loss: 0.0363315
	speed: 0.1375s/iter; left time: 11561.1388s
	iters: 500, epoch: 2 | loss: 0.0213703
	speed: 0.1371s/iter; left time: 11509.3981s
	iters: 600, epoch: 2 | loss: 0.0212502
	speed: 0.1373s/iter; left time: 11516.1187s
	iters: 700, epoch: 2 | loss: 0.0261255
	speed: 0.1368s/iter; left time: 11459.1919s
	iters: 800, epoch: 2 | loss: 0.0245793
	speed: 0.1385s/iter; left time: 11593.1419s
	iters: 900, epoch: 2 | loss: 0.0246260
	speed: 0.1380s/iter; left time: 11533.8377s
	iters: 1000, epoch: 2 | loss: 0.0303873
	speed: 0.1370s/iter; left time: 11432.4694s
	iters: 1100, epoch: 2 | loss: 0.0278004
	speed: 0.1371s/iter; left time: 11430.1299s
Epoch: 2 cost time: 00h:02m:33.24s
Epoch: 2 | Train Loss: 0.0293619 Vali Loss: 0.0321608 Test Loss: 0.0374062
Validation loss decreased (0.036749 --> 0.032161).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 3 | loss: 0.0327742
	speed: 0.4578s/iter; left time: 36591.1040s
	iters: 200, epoch: 3 | loss: 0.0426615
	speed: 0.1377s/iter; left time: 10993.7534s
	iters: 300, epoch: 3 | loss: 0.0251191
	speed: 0.1379s/iter; left time: 10996.2049s
	iters: 400, epoch: 3 | loss: 0.0265200
	speed: 0.1389s/iter; left time: 11061.0320s
	iters: 500, epoch: 3 | loss: 0.0287092
	speed: 0.1366s/iter; left time: 10864.1342s
	iters: 600, epoch: 3 | loss: 0.0326744
	speed: 0.1368s/iter; left time: 10866.1062s
	iters: 700, epoch: 3 | loss: 0.0272740
	speed: 0.1371s/iter; left time: 10876.5321s
	iters: 800, epoch: 3 | loss: 0.0219435
	speed: 0.1371s/iter; left time: 10865.1690s
	iters: 900, epoch: 3 | loss: 0.0302642
	speed: 0.1367s/iter; left time: 10817.3811s
	iters: 1000, epoch: 3 | loss: 0.0288475
	speed: 0.1369s/iter; left time: 10819.1898s
	iters: 1100, epoch: 3 | loss: 0.0385880
	speed: 0.1369s/iter; left time: 10804.0629s
Epoch: 3 cost time: 00h:02m:33.14s
Epoch: 3 | Train Loss: 0.0275353 Vali Loss: 0.0315600 Test Loss: 0.0373444
Validation loss decreased (0.032161 --> 0.031560).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 4 | loss: 0.0265749
	speed: 0.4598s/iter; left time: 34704.8146s
	iters: 200, epoch: 4 | loss: 0.0307901
	speed: 0.1361s/iter; left time: 10263.2213s
	iters: 300, epoch: 4 | loss: 0.0240945
	speed: 0.1365s/iter; left time: 10274.5138s
	iters: 400, epoch: 4 | loss: 0.0293844
	speed: 0.1369s/iter; left time: 10294.6333s
	iters: 500, epoch: 4 | loss: 0.0243979
	speed: 0.1361s/iter; left time: 10217.3833s
	iters: 600, epoch: 4 | loss: 0.0342317
	speed: 0.1366s/iter; left time: 10240.4723s
	iters: 700, epoch: 4 | loss: 0.0235715
	speed: 0.1365s/iter; left time: 10224.0853s
	iters: 800, epoch: 4 | loss: 0.0298171
	speed: 0.1359s/iter; left time: 10164.4991s
	iters: 900, epoch: 4 | loss: 0.0243449
	speed: 0.1365s/iter; left time: 10196.7584s
	iters: 1000, epoch: 4 | loss: 0.0236856
	speed: 0.1364s/iter; left time: 10173.1389s
	iters: 1100, epoch: 4 | loss: 0.0263085
	speed: 0.1408s/iter; left time: 10489.8108s
Epoch: 4 cost time: 00h:02m:32.63s
Epoch: 4 | Train Loss: 0.0266994 Vali Loss: 0.0314476 Test Loss: 0.0374681
Validation loss decreased (0.031560 --> 0.031448).  Saving model ...
lr = 0.0000400000
	iters: 100, epoch: 5 | loss: 0.0282643
	speed: 0.4769s/iter; left time: 33879.0590s
	iters: 200, epoch: 5 | loss: 0.0272768
	speed: 0.1355s/iter; left time: 9609.0031s
	iters: 300, epoch: 5 | loss: 0.0217751
	speed: 0.1348s/iter; left time: 9547.8683s
	iters: 400, epoch: 5 | loss: 0.0190624
	speed: 0.1345s/iter; left time: 9517.2029s
	iters: 500, epoch: 5 | loss: 0.0376097
	speed: 0.1345s/iter; left time: 9502.6648s
	iters: 600, epoch: 5 | loss: 0.0226067
	speed: 0.1347s/iter; left time: 9502.2118s
	iters: 700, epoch: 5 | loss: 0.0193413
	speed: 0.1347s/iter; left time: 9490.0790s
	iters: 800, epoch: 5 | loss: 0.0192956
	speed: 0.1354s/iter; left time: 9526.9124s
	iters: 900, epoch: 5 | loss: 0.0285773
	speed: 0.1352s/iter; left time: 9493.8396s
	iters: 1000, epoch: 5 | loss: 0.0276020
	speed: 0.1357s/iter; left time: 9518.0815s
	iters: 1100, epoch: 5 | loss: 0.0244862
	speed: 0.1348s/iter; left time: 9439.1368s
Epoch: 5 cost time: 00h:02m:30.71s
Epoch: 5 | Train Loss: 0.0259062 Vali Loss: 0.0320041 Test Loss: 0.0369165
EarlyStopping counter: 1 out of 5
lr = 0.0000400000
	iters: 100, epoch: 6 | loss: 0.0214580
	speed: 0.4152s/iter; left time: 27645.2994s
	iters: 200, epoch: 6 | loss: 0.0234301
	speed: 0.1349s/iter; left time: 8971.6242s
	iters: 300, epoch: 6 | loss: 0.0200381
	speed: 0.1347s/iter; left time: 8945.3894s
	iters: 400, epoch: 6 | loss: 0.0311940
	speed: 0.1353s/iter; left time: 8970.1261s
	iters: 500, epoch: 6 | loss: 0.0301950
	speed: 0.1358s/iter; left time: 8989.8726s
	iters: 600, epoch: 6 | loss: 0.0203328
	speed: 0.1352s/iter; left time: 8936.7431s
	iters: 700, epoch: 6 | loss: 0.0225864
	speed: 0.1346s/iter; left time: 8879.4130s
	iters: 800, epoch: 6 | loss: 0.0181251
	speed: 0.1347s/iter; left time: 8872.4661s
	iters: 900, epoch: 6 | loss: 0.0198194
	speed: 0.1345s/iter; left time: 8851.4498s
	iters: 1000, epoch: 6 | loss: 0.0240496
	speed: 0.1350s/iter; left time: 8865.7421s
	iters: 1100, epoch: 6 | loss: 0.0215907
	speed: 0.1349s/iter; left time: 8846.1939s
Epoch: 6 cost time: 00h:02m:30.46s
Epoch: 6 | Train Loss: 0.0249147 Vali Loss: 0.0328208 Test Loss: 0.0379578
EarlyStopping counter: 2 out of 5
lr = 0.0000400000
	iters: 100, epoch: 7 | loss: 0.0212492
	speed: 0.4220s/iter; left time: 26225.8119s
	iters: 200, epoch: 7 | loss: 0.0209108
	speed: 0.1354s/iter; left time: 8399.0267s
	iters: 300, epoch: 7 | loss: 0.0251728
	speed: 0.1360s/iter; left time: 8422.7246s
	iters: 400, epoch: 7 | loss: 0.0205974
	speed: 0.1354s/iter; left time: 8375.9464s
	iters: 500, epoch: 7 | loss: 0.0166121
	speed: 0.1343s/iter; left time: 8292.4816s
	iters: 600, epoch: 7 | loss: 0.0294926
	speed: 0.1360s/iter; left time: 8385.7122s
	iters: 700, epoch: 7 | loss: 0.0198359
	speed: 0.1342s/iter; left time: 8259.4761s
	iters: 800, epoch: 7 | loss: 0.0242572
	speed: 0.1343s/iter; left time: 8249.8904s
	iters: 900, epoch: 7 | loss: 0.0213706
	speed: 0.1351s/iter; left time: 8285.5735s
	iters: 1000, epoch: 7 | loss: 0.0236055
	speed: 0.1348s/iter; left time: 8255.7980s
	iters: 1100, epoch: 7 | loss: 0.0265674
	speed: 0.1359s/iter; left time: 8307.5910s
Epoch: 7 cost time: 00h:02m:30.70s
Epoch: 7 | Train Loss: 0.0237292 Vali Loss: 0.0337015 Test Loss: 0.0404406
EarlyStopping counter: 3 out of 5
lr = 0.0000400000
	iters: 100, epoch: 8 | loss: 0.0256783
	speed: 0.4171s/iter; left time: 24068.0077s
	iters: 200, epoch: 8 | loss: 0.0266715
	speed: 0.1349s/iter; left time: 7768.2357s
	iters: 300, epoch: 8 | loss: 0.0207095
	speed: 0.1348s/iter; left time: 7753.1162s
	iters: 400, epoch: 8 | loss: 0.0248391
	speed: 0.1345s/iter; left time: 7722.0082s
	iters: 500, epoch: 8 | loss: 0.0196725
	speed: 0.1351s/iter; left time: 7739.9736s
	iters: 600, epoch: 8 | loss: 0.0184297
	speed: 0.1347s/iter; left time: 7704.9198s
	iters: 700, epoch: 8 | loss: 0.0277658
	speed: 0.1349s/iter; left time: 7701.3372s
	iters: 800, epoch: 8 | loss: 0.0265737
	speed: 0.1350s/iter; left time: 7694.7326s
	iters: 900, epoch: 8 | loss: 0.0243952
	speed: 0.1351s/iter; left time: 7687.6184s
	iters: 1000, epoch: 8 | loss: 0.0238605
	speed: 0.1346s/iter; left time: 7642.9981s
	iters: 1100, epoch: 8 | loss: 0.0290543
	speed: 0.1338s/iter; left time: 7585.7476s
Epoch: 8 cost time: 00h:02m:30.33s
Epoch: 8 | Train Loss: 0.0225662 Vali Loss: 0.0353154 Test Loss: 0.0423327
EarlyStopping counter: 4 out of 5
lr = 0.0000400000
	iters: 100, epoch: 9 | loss: 0.0227777
	speed: 0.4192s/iter; left time: 22323.0045s
	iters: 200, epoch: 9 | loss: 0.0262419
	speed: 0.1338s/iter; left time: 7112.9049s
	iters: 300, epoch: 9 | loss: 0.0207251
	speed: 0.1340s/iter; left time: 7107.4673s
	iters: 400, epoch: 9 | loss: 0.0197221
	speed: 0.1347s/iter; left time: 7133.5740s
	iters: 500, epoch: 9 | loss: 0.0167211
	speed: 0.1348s/iter; left time: 7123.5262s
	iters: 600, epoch: 9 | loss: 0.0236003
	speed: 0.1346s/iter; left time: 7101.7201s
	iters: 700, epoch: 9 | loss: 0.0170809
	speed: 0.1343s/iter; left time: 7073.7127s
	iters: 800, epoch: 9 | loss: 0.0209369
	speed: 0.1345s/iter; left time: 7068.1689s
	iters: 900, epoch: 9 | loss: 0.0263193
	speed: 0.1346s/iter; left time: 7059.6231s
	iters: 1000, epoch: 9 | loss: 0.0252886
	speed: 0.1341s/iter; left time: 7020.5467s
	iters: 1100, epoch: 9 | loss: 0.0198990
	speed: 0.1343s/iter; left time: 7020.1685s
Epoch: 9 cost time: 00h:02m:29.77s
Epoch: 9 | Train Loss: 0.0214407 Vali Loss: 0.0361602 Test Loss: 0.0435897
EarlyStopping counter: 5 out of 5
Early stopping
loading model...
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Scaled mse:0.03746809437870979, rmse:0.19356676936149597, mae:0.1385875940322876, rse:0.6857706904411316
Intermediate time for DE and pred_len 168: 00h:27m:36.38s
Intermediate time for DE: 02h:54m:13.00s
Total time: 02h:54m:13.01s
