Args in experiment:
Namespace(random_seed=2021, is_training=1, model_id='test_20_10', model='Informer', data='custom', root_path='/vol/cs-hu/riabchuv/my_work/datasets/', data_path='GB_data_small.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, seq_len=20, label_len=5, pred_len=10, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=5, c_out=5, d_model=128, n_heads=16, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=5, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=2, batch_size=2, patience=1, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc5_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 139
val 15
test 39
-------------------------------------------------------------------------------------
Epoch: 1
Cost time: 00h:00m:03.55s
Steps: 69 | Train Loss: 1.3227707 Vali Loss: 1.7644511 Test Loss: 1.9218606
Validation loss decreased (inf --> 1.764451).  Saving model ...
Updating learning rate to 0.0001
-------------------------------------------------------------------------------------
Epoch: 2
Cost time: 00h:00m:02.75s
Steps: 69 | Train Loss: 1.1112937 Vali Loss: 1.1520592 Test Loss: 1.5332925
Validation loss decreased (1.764451 --> 1.152059).  Saving model ...
Updating learning rate to 0.0001
-------------------------------------------------------------------------------------
>>>>>>>testing : test_20_10_Informer_custom_ftM_sl20_ll5_pl10_dm128_nh16_el2_dl1_df256_fc5_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 39
mse:15609031.0, mae:2483.043701171875, rse:0.2312779575586319
