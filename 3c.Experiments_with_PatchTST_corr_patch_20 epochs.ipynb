{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. No RevIN](#1-no-revin-instanse-normalization)\n",
    "- [2. No channel-independence (Channel-Mixing)](#2-no-channel-independence-channel-mixing)\n",
    "- [3. No Patching](#3-no-patching)\n",
    "- [4. Time series decomposition](#4-ts-decomposition)\n",
    "\n",
    "Ablation study on PatchTST components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df, running_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. No RevIN (Instanse Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "log_dir = f\"logs/patchtst/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Dynamic variables\n",
    "pred_lens = [24, 96, 168]\n",
    "countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "num_cols = [5, 5, 3, 3, 3]\n",
    "seq_lens = [512, 512, 336, 168, 168]\n",
    "\n",
    "model = \"PatchTST\"\n",
    "loss = \"MSE\"\n",
    "itr=1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_revin.log\"\n",
    "\n",
    "# Parameters for tuning,but default\n",
    "lr = 0.0001\n",
    "n_heads = 16\n",
    "e_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Capture the output in real-time\u001b[39;00m\n\u001b[1;32m     65\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 66\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Print in the .ipynb cell\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"no_revin_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --revin 0 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">-RevIN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0218</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.1906</td>\n",
       "      <td>0.1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.1814</td>\n",
       "      <td>0.1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>0.0640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.0873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.0929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.0658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.0897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model             -RevIN                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0218  0.1477  0.0952\n",
       "        96        0.0363  0.1906  0.1306\n",
       "        168       0.0390  0.1975  0.1373\n",
       "ES      24        0.0184  0.1355  0.0841\n",
       "        96        0.0310  0.1761  0.1143\n",
       "        168       0.0329  0.1814  0.1212\n",
       "FR      24        0.0110  0.1048  0.0640\n",
       "        96        0.0194  0.1392  0.0873\n",
       "        168       0.0214  0.1465  0.0929\n",
       "GB      24        0.0266  0.1631  0.1095\n",
       "        96        0.0406  0.2014  0.1431\n",
       "        168       0.0482  0.2195  0.1556\n",
       "IT      24        0.0109  0.1042  0.0658\n",
       "        96        0.0179  0.1337  0.0860\n",
       "        168       0.0189  0.1375  0.0897"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['-RevIN'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_no_revin.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. No channel independence (Channel-Mixing)\n",
    "\n",
    "It is a channel mixing model, and therefore it needs more dimension of embeddings to capture complex patterns between features. \n",
    "\n",
    "Therefore, it is not fair to keep same d_model and d_ff as in channel mixing. In this regard, we scale them based on number of input features.\n",
    "\n",
    "In other words, for DE data with 5 columns, d_model = 128 x 5, and d_ff = 256 x 5.\n",
    "\n",
    "For ES: d_model = 128 x 3 and d_ff = 256 x 3, etc. It is adjusted automatically in code.\n",
    "\n",
    "Since it converges fast, we reduced max number of epochs and patience.\n",
    "\n",
    "Complete results are in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_336_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0228087\n",
      "\tspeed: 0.1079s/iter; left time: 2405.8189s\n",
      "\titers: 200, epoch: 1 | loss: 0.0179323\n",
      "\tspeed: 0.0775s/iter; left time: 1719.9694s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:17.96s\n",
      "Steps: 224 | Train Loss: 0.0250762 Vali Loss: 0.0243956 Test Loss: 0.0266887\n",
      "Validation loss decreased (inf --> 0.024396).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0143175\n",
      "\tspeed: 0.1491s/iter; left time: 3291.7626s\n",
      "\titers: 200, epoch: 2 | loss: 0.0121034\n",
      "\tspeed: 0.0787s/iter; left time: 1729.8753s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:17.94s\n",
      "Steps: 224 | Train Loss: 0.0153061 Vali Loss: 0.0225978 Test Loss: 0.0254660\n",
      "Validation loss decreased (0.024396 --> 0.022598).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0110634\n",
      "\tspeed: 0.1487s/iter; left time: 3250.0557s\n",
      "\titers: 200, epoch: 3 | loss: 0.0108149\n",
      "\tspeed: 0.0796s/iter; left time: 1731.9148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:18.02s\n",
      "Steps: 224 | Train Loss: 0.0116806 Vali Loss: 0.0242847 Test Loss: 0.0281949\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0086492\n",
      "\tspeed: 0.1372s/iter; left time: 2966.4333s\n",
      "\titers: 200, epoch: 4 | loss: 0.0074749\n",
      "\tspeed: 0.0809s/iter; left time: 1740.8705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:18.26s\n",
      "Steps: 224 | Train Loss: 0.0083592 Vali Loss: 0.0257715 Test Loss: 0.0307205\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059109\n",
      "\tspeed: 0.1393s/iter; left time: 2982.1964s\n",
      "\titers: 200, epoch: 5 | loss: 0.0057949\n",
      "\tspeed: 0.0816s/iter; left time: 1737.7424s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:18.52s\n",
      "Steps: 224 | Train Loss: 0.0061490 Vali Loss: 0.0265567 Test Loss: 0.0322603\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0054164\n",
      "\tspeed: 0.1403s/iter; left time: 2972.1342s\n",
      "\titers: 200, epoch: 6 | loss: 0.0045454\n",
      "\tspeed: 0.0823s/iter; left time: 1734.4703s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:18.53s\n",
      "Steps: 224 | Train Loss: 0.0048884 Vali Loss: 0.0265694 Test Loss: 0.0316106\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0043331\n",
      "\tspeed: 0.1394s/iter; left time: 2921.8639s\n",
      "\titers: 200, epoch: 7 | loss: 0.0042686\n",
      "\tspeed: 0.0807s/iter; left time: 1682.9731s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:18.40s\n",
      "Steps: 224 | Train Loss: 0.0041620 Vali Loss: 0.0267623 Test Loss: 0.0317451\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0035710\n",
      "\tspeed: 0.1412s/iter; left time: 2926.6554s\n",
      "\titers: 200, epoch: 8 | loss: 0.0034310\n",
      "\tspeed: 0.0807s/iter; left time: 1665.8266s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:18.32s\n",
      "Steps: 224 | Train Loss: 0.0036270 Vali Loss: 0.0266479 Test Loss: 0.0319233\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0033539\n",
      "\tspeed: 0.1379s/iter; left time: 2828.0550s\n",
      "\titers: 200, epoch: 9 | loss: 0.0033634\n",
      "\tspeed: 0.0790s/iter; left time: 1612.9094s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:18.09s\n",
      "Steps: 224 | Train Loss: 0.0032561 Vali Loss: 0.0268548 Test Loss: 0.0317931\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0028689\n",
      "\tspeed: 0.1404s/iter; left time: 2848.5519s\n",
      "\titers: 200, epoch: 10 | loss: 0.0029293\n",
      "\tspeed: 0.0817s/iter; left time: 1649.8352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:18.65s\n",
      "Steps: 224 | Train Loss: 0.0029481 Vali Loss: 0.0266907 Test Loss: 0.0310390\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0026734\n",
      "\tspeed: 0.1423s/iter; left time: 2854.2505s\n",
      "\titers: 200, epoch: 11 | loss: 0.0027685\n",
      "\tspeed: 0.0831s/iter; left time: 1659.1757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:18.60s\n",
      "Steps: 224 | Train Loss: 0.0027446 Vali Loss: 0.0267117 Test Loss: 0.0316906\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0026027\n",
      "\tspeed: 0.1404s/iter; left time: 2785.3602s\n",
      "\titers: 200, epoch: 12 | loss: 0.0023858\n",
      "\tspeed: 0.0815s/iter; left time: 1607.7385s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:18.52s\n",
      "Steps: 224 | Train Loss: 0.0025758 Vali Loss: 0.0268825 Test Loss: 0.0318096\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.025466015562415123, rmse:0.1595807522535324, mae:0.10604768246412277, rse:0.5631825923919678\n",
      "Intermediate time for DE and pred_len 24: 00h:04m:38.28s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_512_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0267612\n",
      "\tspeed: 0.1345s/iter; left time: 2973.3365s\n",
      "\titers: 200, epoch: 1 | loss: 0.0241627\n",
      "\tspeed: 0.1159s/iter; left time: 2549.6671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.17s\n",
      "Steps: 222 | Train Loss: 0.0298788 Vali Loss: 0.0330326 Test Loss: 0.0381434\n",
      "Validation loss decreased (inf --> 0.033033).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0213286\n",
      "\tspeed: 0.2111s/iter; left time: 4618.2832s\n",
      "\titers: 200, epoch: 2 | loss: 0.0150444\n",
      "\tspeed: 0.1157s/iter; left time: 2520.3333s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.02s\n",
      "Steps: 222 | Train Loss: 0.0205023 Vali Loss: 0.0394690 Test Loss: 0.0567440\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0106942\n",
      "\tspeed: 0.1993s/iter; left time: 4316.6006s\n",
      "\titers: 200, epoch: 3 | loss: 0.0080675\n",
      "\tspeed: 0.1165s/iter; left time: 2511.3193s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.06s\n",
      "Steps: 222 | Train Loss: 0.0105057 Vali Loss: 0.0410978 Test Loss: 0.0570585\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0070927\n",
      "\tspeed: 0.1976s/iter; left time: 4236.2235s\n",
      "\titers: 200, epoch: 4 | loss: 0.0059473\n",
      "\tspeed: 0.1179s/iter; left time: 2514.4882s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.36s\n",
      "Steps: 222 | Train Loss: 0.0069310 Vali Loss: 0.0419279 Test Loss: 0.0561629\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0055561\n",
      "\tspeed: 0.1992s/iter; left time: 4225.1680s\n",
      "\titers: 200, epoch: 5 | loss: 0.0049814\n",
      "\tspeed: 0.1168s/iter; left time: 2465.3382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.20s\n",
      "Steps: 222 | Train Loss: 0.0053010 Vali Loss: 0.0415496 Test Loss: 0.0554823\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0042187\n",
      "\tspeed: 0.1983s/iter; left time: 4163.4699s\n",
      "\titers: 200, epoch: 6 | loss: 0.0039673\n",
      "\tspeed: 0.1167s/iter; left time: 2438.6321s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.09s\n",
      "Steps: 222 | Train Loss: 0.0043732 Vali Loss: 0.0405251 Test Loss: 0.0546589\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0037001\n",
      "\tspeed: 0.2002s/iter; left time: 4158.9581s\n",
      "\titers: 200, epoch: 7 | loss: 0.0036983\n",
      "\tspeed: 0.1165s/iter; left time: 2407.0645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.20s\n",
      "Steps: 222 | Train Loss: 0.0037926 Vali Loss: 0.0397968 Test Loss: 0.0534103\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0033180\n",
      "\tspeed: 0.1978s/iter; left time: 4064.8042s\n",
      "\titers: 200, epoch: 8 | loss: 0.0033665\n",
      "\tspeed: 0.1168s/iter; left time: 2388.4869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.22s\n",
      "Steps: 222 | Train Loss: 0.0033597 Vali Loss: 0.0391120 Test Loss: 0.0523807\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0031007\n",
      "\tspeed: 0.1994s/iter; left time: 4052.9757s\n",
      "\titers: 200, epoch: 9 | loss: 0.0029520\n",
      "\tspeed: 0.1165s/iter; left time: 2355.4508s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.23s\n",
      "Steps: 222 | Train Loss: 0.0030491 Vali Loss: 0.0393237 Test Loss: 0.0516142\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0027767\n",
      "\tspeed: 0.1985s/iter; left time: 3991.0370s\n",
      "\titers: 200, epoch: 10 | loss: 0.0026486\n",
      "\tspeed: 0.1173s/iter; left time: 2345.4190s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.19s\n",
      "Steps: 222 | Train Loss: 0.0027984 Vali Loss: 0.0385833 Test Loss: 0.0513738\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0025453\n",
      "\tspeed: 0.2015s/iter; left time: 4006.8533s\n",
      "\titers: 200, epoch: 11 | loss: 0.0026200\n",
      "\tspeed: 0.1161s/iter; left time: 2295.9504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.17s\n",
      "Steps: 222 | Train Loss: 0.0025790 Vali Loss: 0.0392718 Test Loss: 0.0512391\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.03814338520169258, rmse:0.1953033208847046, mae:0.13842742145061493, rse:0.6916085481643677\n",
      "Intermediate time for DE and pred_len 96: 00h:05m:59.31s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_DE_512_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0303482\n",
      "\tspeed: 0.1472s/iter; left time: 3252.9327s\n",
      "\titers: 200, epoch: 1 | loss: 0.0259837\n",
      "\tspeed: 0.1169s/iter; left time: 2572.6714s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.66s\n",
      "Steps: 222 | Train Loss: 0.0312894 Vali Loss: 0.0338812 Test Loss: 0.0400263\n",
      "Validation loss decreased (inf --> 0.033881).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0223873\n",
      "\tspeed: 0.2150s/iter; left time: 4704.1597s\n",
      "\titers: 200, epoch: 2 | loss: 0.0144032\n",
      "\tspeed: 0.1179s/iter; left time: 2567.6302s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0203739 Vali Loss: 0.0428706 Test Loss: 0.0591011\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0095180\n",
      "\tspeed: 0.2053s/iter; left time: 4445.1696s\n",
      "\titers: 200, epoch: 3 | loss: 0.0086363\n",
      "\tspeed: 0.1180s/iter; left time: 2542.7405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.46s\n",
      "Steps: 222 | Train Loss: 0.0102204 Vali Loss: 0.0409299 Test Loss: 0.0589293\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0070051\n",
      "\tspeed: 0.2010s/iter; left time: 4308.7348s\n",
      "\titers: 200, epoch: 4 | loss: 0.0060716\n",
      "\tspeed: 0.1186s/iter; left time: 2531.1249s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.55s\n",
      "Steps: 222 | Train Loss: 0.0068285 Vali Loss: 0.0409141 Test Loss: 0.0554637\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0055226\n",
      "\tspeed: 0.1990s/iter; left time: 4222.2798s\n",
      "\titers: 200, epoch: 5 | loss: 0.0048452\n",
      "\tspeed: 0.1192s/iter; left time: 2515.6343s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.60s\n",
      "Steps: 222 | Train Loss: 0.0052085 Vali Loss: 0.0405766 Test Loss: 0.0553284\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0042709\n",
      "\tspeed: 0.1996s/iter; left time: 4189.3609s\n",
      "\titers: 200, epoch: 6 | loss: 0.0039860\n",
      "\tspeed: 0.1194s/iter; left time: 2494.5140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.66s\n",
      "Steps: 222 | Train Loss: 0.0042853 Vali Loss: 0.0407914 Test Loss: 0.0561981\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0036946\n",
      "\tspeed: 0.2039s/iter; left time: 4235.5437s\n",
      "\titers: 200, epoch: 7 | loss: 0.0036571\n",
      "\tspeed: 0.1187s/iter; left time: 2453.9209s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.61s\n",
      "Steps: 222 | Train Loss: 0.0037219 Vali Loss: 0.0396792 Test Loss: 0.0527121\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0033250\n",
      "\tspeed: 0.2032s/iter; left time: 4175.0595s\n",
      "\titers: 200, epoch: 8 | loss: 0.0031910\n",
      "\tspeed: 0.1188s/iter; left time: 2429.5516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.76s\n",
      "Steps: 222 | Train Loss: 0.0033059 Vali Loss: 0.0397238 Test Loss: 0.0522578\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0029823\n",
      "\tspeed: 0.2009s/iter; left time: 4082.3163s\n",
      "\titers: 200, epoch: 9 | loss: 0.0030154\n",
      "\tspeed: 0.1187s/iter; left time: 2401.1134s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.69s\n",
      "Steps: 222 | Train Loss: 0.0030103 Vali Loss: 0.0397284 Test Loss: 0.0528058\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0027213\n",
      "\tspeed: 0.1985s/iter; left time: 3990.9351s\n",
      "\titers: 200, epoch: 10 | loss: 0.0026023\n",
      "\tspeed: 0.1180s/iter; left time: 2360.8793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.40s\n",
      "Steps: 222 | Train Loss: 0.0027673 Vali Loss: 0.0398414 Test Loss: 0.0522234\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0025320\n",
      "\tspeed: 0.2009s/iter; left time: 3994.8234s\n",
      "\titers: 200, epoch: 11 | loss: 0.0024260\n",
      "\tspeed: 0.1184s/iter; left time: 2341.6970s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.48s\n",
      "Steps: 222 | Train Loss: 0.0025736 Vali Loss: 0.0395068 Test Loss: 0.0516529\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04002630338072777, rmse:0.2000657469034195, mae:0.14279745519161224, rse:0.7086489796638489\n",
      "Intermediate time for DE and pred_len 168: 00h:06m:08.44s\n",
      "Intermediate time for DE: 00h:16m:46.03s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0204524\n",
      "\tspeed: 0.1395s/iter; left time: 3097.9385s\n",
      "\titers: 200, epoch: 1 | loss: 0.0172346\n",
      "\tspeed: 0.1120s/iter; left time: 2476.2464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:25.60s\n",
      "Steps: 223 | Train Loss: 0.0216741 Vali Loss: 0.0224694 Test Loss: 0.0285811\n",
      "Validation loss decreased (inf --> 0.022469).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0165505\n",
      "\tspeed: 0.2103s/iter; left time: 4621.6269s\n",
      "\titers: 200, epoch: 2 | loss: 0.0137054\n",
      "\tspeed: 0.1127s/iter; left time: 2465.9090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.50s\n",
      "Steps: 223 | Train Loss: 0.0156341 Vali Loss: 0.0228663 Test Loss: 0.0294817\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0129808\n",
      "\tspeed: 0.1919s/iter; left time: 4175.8264s\n",
      "\titers: 200, epoch: 3 | loss: 0.0097045\n",
      "\tspeed: 0.1133s/iter; left time: 2454.3169s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:25.45s\n",
      "Steps: 223 | Train Loss: 0.0124850 Vali Loss: 0.0272916 Test Loss: 0.0339417\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0103794\n",
      "\tspeed: 0.1931s/iter; left time: 4158.7084s\n",
      "\titers: 200, epoch: 4 | loss: 0.0078741\n",
      "\tspeed: 0.1144s/iter; left time: 2452.7297s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:25.64s\n",
      "Steps: 223 | Train Loss: 0.0089107 Vali Loss: 0.0299794 Test Loss: 0.0360209\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0060216\n",
      "\tspeed: 0.1969s/iter; left time: 4196.3589s\n",
      "\titers: 200, epoch: 5 | loss: 0.0063107\n",
      "\tspeed: 0.1135s/iter; left time: 2408.2542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.73s\n",
      "Steps: 223 | Train Loss: 0.0065563 Vali Loss: 0.0298951 Test Loss: 0.0369057\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0048502\n",
      "\tspeed: 0.1935s/iter; left time: 4079.1300s\n",
      "\titers: 200, epoch: 6 | loss: 0.0046325\n",
      "\tspeed: 0.1152s/iter; left time: 2417.1862s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:25.76s\n",
      "Steps: 223 | Train Loss: 0.0052399 Vali Loss: 0.0298870 Test Loss: 0.0377665\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0045492\n",
      "\tspeed: 0.1954s/iter; left time: 4075.5954s\n",
      "\titers: 200, epoch: 7 | loss: 0.0052110\n",
      "\tspeed: 0.1155s/iter; left time: 2397.1756s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:25.80s\n",
      "Steps: 223 | Train Loss: 0.0045226 Vali Loss: 0.0304545 Test Loss: 0.0381390\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0040665\n",
      "\tspeed: 0.1941s/iter; left time: 4005.9379s\n",
      "\titers: 200, epoch: 8 | loss: 0.0039435\n",
      "\tspeed: 0.1138s/iter; left time: 2336.8719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:25.57s\n",
      "Steps: 223 | Train Loss: 0.0040010 Vali Loss: 0.0297942 Test Loss: 0.0374554\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034011\n",
      "\tspeed: 0.1943s/iter; left time: 3966.7927s\n",
      "\titers: 200, epoch: 9 | loss: 0.0032482\n",
      "\tspeed: 0.1128s/iter; left time: 2291.8405s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:25.60s\n",
      "Steps: 223 | Train Loss: 0.0036401 Vali Loss: 0.0298510 Test Loss: 0.0378472\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031535\n",
      "\tspeed: 0.1959s/iter; left time: 3955.8457s\n",
      "\titers: 200, epoch: 10 | loss: 0.0030436\n",
      "\tspeed: 0.1144s/iter; left time: 2299.7268s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:25.72s\n",
      "Steps: 223 | Train Loss: 0.0033274 Vali Loss: 0.0302574 Test Loss: 0.0372001\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0030076\n",
      "\tspeed: 0.1966s/iter; left time: 3926.1950s\n",
      "\titers: 200, epoch: 11 | loss: 0.0031848\n",
      "\tspeed: 0.1146s/iter; left time: 2277.3600s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:25.87s\n",
      "Steps: 223 | Train Loss: 0.0031077 Vali Loss: 0.0298116 Test Loss: 0.0377274\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.028581099584698677, rmse:0.16905945539474487, mae:0.11745379865169525, rse:0.5832071900367737\n",
      "Intermediate time for GB and pred_len 24: 00h:05m:54.06s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249201\n",
      "\tspeed: 0.1453s/iter; left time: 3210.5184s\n",
      "\titers: 200, epoch: 1 | loss: 0.0207945\n",
      "\tspeed: 0.1171s/iter; left time: 2577.2567s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0265950 Vali Loss: 0.0302113 Test Loss: 0.0427934\n",
      "Validation loss decreased (inf --> 0.030211).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0242963\n",
      "\tspeed: 0.2266s/iter; left time: 4958.5839s\n",
      "\titers: 200, epoch: 2 | loss: 0.0163195\n",
      "\tspeed: 0.1140s/iter; left time: 2483.1500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.74s\n",
      "Steps: 222 | Train Loss: 0.0210160 Vali Loss: 0.0406260 Test Loss: 0.0595929\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0102974\n",
      "\tspeed: 0.1994s/iter; left time: 4317.8766s\n",
      "\titers: 200, epoch: 3 | loss: 0.0083862\n",
      "\tspeed: 0.1161s/iter; left time: 2503.1341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.08s\n",
      "Steps: 222 | Train Loss: 0.0111077 Vali Loss: 0.0442167 Test Loss: 0.0597632\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0075297\n",
      "\tspeed: 0.2042s/iter; left time: 4377.8822s\n",
      "\titers: 200, epoch: 4 | loss: 0.0066034\n",
      "\tspeed: 0.1167s/iter; left time: 2489.0468s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.22s\n",
      "Steps: 222 | Train Loss: 0.0074410 Vali Loss: 0.0445008 Test Loss: 0.0612709\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0057877\n",
      "\tspeed: 0.1990s/iter; left time: 4221.0864s\n",
      "\titers: 200, epoch: 5 | loss: 0.0051656\n",
      "\tspeed: 0.1157s/iter; left time: 2441.9090s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.01s\n",
      "Steps: 222 | Train Loss: 0.0057136 Vali Loss: 0.0430807 Test Loss: 0.0606061\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0046578\n",
      "\tspeed: 0.1965s/iter; left time: 4123.7137s\n",
      "\titers: 200, epoch: 6 | loss: 0.0045613\n",
      "\tspeed: 0.1162s/iter; left time: 2427.8245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.02s\n",
      "Steps: 222 | Train Loss: 0.0047347 Vali Loss: 0.0436480 Test Loss: 0.0575669\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0040779\n",
      "\tspeed: 0.1975s/iter; left time: 4101.0427s\n",
      "\titers: 200, epoch: 7 | loss: 0.0038437\n",
      "\tspeed: 0.1164s/iter; left time: 2405.8765s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.14s\n",
      "Steps: 222 | Train Loss: 0.0041591 Vali Loss: 0.0417144 Test Loss: 0.0575785\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0037653\n",
      "\tspeed: 0.1985s/iter; left time: 4079.4096s\n",
      "\titers: 200, epoch: 8 | loss: 0.0038647\n",
      "\tspeed: 0.1171s/iter; left time: 2394.0861s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:26.08s\n",
      "Steps: 222 | Train Loss: 0.0037220 Vali Loss: 0.0422032 Test Loss: 0.0573435\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034920\n",
      "\tspeed: 0.1997s/iter; left time: 4058.3746s\n",
      "\titers: 200, epoch: 9 | loss: 0.0033256\n",
      "\tspeed: 0.1153s/iter; left time: 2332.6062s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.09s\n",
      "Steps: 222 | Train Loss: 0.0034100 Vali Loss: 0.0411287 Test Loss: 0.0566642\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031117\n",
      "\tspeed: 0.2005s/iter; left time: 4029.8191s\n",
      "\titers: 200, epoch: 10 | loss: 0.0028218\n",
      "\tspeed: 0.1175s/iter; left time: 2351.1313s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:26.40s\n",
      "Steps: 222 | Train Loss: 0.0031732 Vali Loss: 0.0411667 Test Loss: 0.0564241\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0029950\n",
      "\tspeed: 0.1982s/iter; left time: 3940.4835s\n",
      "\titers: 200, epoch: 11 | loss: 0.0029961\n",
      "\tspeed: 0.1178s/iter; left time: 2330.7597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:26.34s\n",
      "Steps: 222 | Train Loss: 0.0029687 Vali Loss: 0.0401659 Test Loss: 0.0557463\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04279341548681259, rmse:0.20686569809913635, mae:0.14713023602962494, rse:0.7153703570365906\n",
      "Intermediate time for GB and pred_len 96: 00h:06m:04.48s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0257415\n",
      "\tspeed: 0.1390s/iter; left time: 3071.1425s\n",
      "\titers: 200, epoch: 1 | loss: 0.0241097\n",
      "\tspeed: 0.1193s/iter; left time: 2624.4066s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.62s\n",
      "Steps: 222 | Train Loss: 0.0276226 Vali Loss: 0.0318170 Test Loss: 0.0450048\n",
      "Validation loss decreased (inf --> 0.031817).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0237916\n",
      "\tspeed: 0.2492s/iter; left time: 5451.7499s\n",
      "\titers: 200, epoch: 2 | loss: 0.0152756\n",
      "\tspeed: 0.1181s/iter; left time: 2571.1995s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:26.35s\n",
      "Steps: 222 | Train Loss: 0.0213474 Vali Loss: 0.0439241 Test Loss: 0.0612773\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0109549\n",
      "\tspeed: 0.1982s/iter; left time: 4293.2388s\n",
      "\titers: 200, epoch: 3 | loss: 0.0101264\n",
      "\tspeed: 0.1178s/iter; left time: 2540.4856s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:26.37s\n",
      "Steps: 222 | Train Loss: 0.0117174 Vali Loss: 0.0464970 Test Loss: 0.0617225\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0073486\n",
      "\tspeed: 0.2047s/iter; left time: 4387.3031s\n",
      "\titers: 200, epoch: 4 | loss: 0.0063245\n",
      "\tspeed: 0.1186s/iter; left time: 2530.4978s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:26.70s\n",
      "Steps: 222 | Train Loss: 0.0073780 Vali Loss: 0.0455833 Test Loss: 0.0603723\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0059219\n",
      "\tspeed: 0.2018s/iter; left time: 4279.8580s\n",
      "\titers: 200, epoch: 5 | loss: 0.0051964\n",
      "\tspeed: 0.1188s/iter; left time: 2508.7083s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:26.54s\n",
      "Steps: 222 | Train Loss: 0.0055432 Vali Loss: 0.0437197 Test Loss: 0.0579041\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0045163\n",
      "\tspeed: 0.2018s/iter; left time: 4235.9388s\n",
      "\titers: 200, epoch: 6 | loss: 0.0042522\n",
      "\tspeed: 0.1176s/iter; left time: 2456.6171s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:26.55s\n",
      "Steps: 222 | Train Loss: 0.0046257 Vali Loss: 0.0429923 Test Loss: 0.0567633\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0039707\n",
      "\tspeed: 0.2008s/iter; left time: 4169.5544s\n",
      "\titers: 200, epoch: 7 | loss: 0.0039244\n",
      "\tspeed: 0.1200s/iter; left time: 2480.8943s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:26.85s\n",
      "Steps: 222 | Train Loss: 0.0040597 Vali Loss: 0.0432213 Test Loss: 0.0562348\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0039215\n",
      "\tspeed: 0.2041s/iter; left time: 4193.5578s\n",
      "\titers: 200, epoch: 8 | loss: 0.0035443\n",
      "\tspeed: 0.1208s/iter; left time: 2470.3162s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:27.05s\n",
      "Steps: 222 | Train Loss: 0.0036566 Vali Loss: 0.0423803 Test Loss: 0.0562973\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0034582\n",
      "\tspeed: 0.2065s/iter; left time: 4197.3482s\n",
      "\titers: 200, epoch: 9 | loss: 0.0031870\n",
      "\tspeed: 0.1212s/iter; left time: 2451.0187s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:27.27s\n",
      "Steps: 222 | Train Loss: 0.0033634 Vali Loss: 0.0417200 Test Loss: 0.0547162\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0029790\n",
      "\tspeed: 0.2052s/iter; left time: 4125.3175s\n",
      "\titers: 200, epoch: 10 | loss: 0.0029285\n",
      "\tspeed: 0.1218s/iter; left time: 2435.4878s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:27.27s\n",
      "Steps: 222 | Train Loss: 0.0031169 Vali Loss: 0.0414041 Test Loss: 0.0538904\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0028094\n",
      "\tspeed: 0.2040s/iter; left time: 4055.2701s\n",
      "\titers: 200, epoch: 11 | loss: 0.0029949\n",
      "\tspeed: 0.1207s/iter; left time: 2387.4828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:27.10s\n",
      "Steps: 222 | Train Loss: 0.0029408 Vali Loss: 0.0405609 Test Loss: 0.0539439\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.045004863291978836, rmse:0.21214349567890167, mae:0.15204809606075287, rse:0.7355319857597351\n",
      "Intermediate time for GB and pred_len 168: 00h:06m:12.93s\n",
      "Intermediate time for GB: 00h:18m:11.46s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0222541\n",
      "\tspeed: 0.0553s/iter; left time: 1233.7852s\n",
      "\titers: 200, epoch: 1 | loss: 0.0173791\n",
      "\tspeed: 0.0300s/iter; left time: 665.6022s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.18s\n",
      "Steps: 224 | Train Loss: 0.0242278 Vali Loss: 0.0146892 Test Loss: 0.0199106\n",
      "Validation loss decreased (inf --> 0.014689).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0093632\n",
      "\tspeed: 0.0665s/iter; left time: 1468.7716s\n",
      "\titers: 200, epoch: 2 | loss: 0.0095968\n",
      "\tspeed: 0.0299s/iter; left time: 657.6159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.05s\n",
      "Steps: 224 | Train Loss: 0.0104013 Vali Loss: 0.0090444 Test Loss: 0.0117678\n",
      "Validation loss decreased (0.014689 --> 0.009044).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0082353\n",
      "\tspeed: 0.0682s/iter; left time: 1490.7586s\n",
      "\titers: 200, epoch: 3 | loss: 0.0082141\n",
      "\tspeed: 0.0285s/iter; left time: 620.8897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.93s\n",
      "Steps: 224 | Train Loss: 0.0086646 Vali Loss: 0.0087289 Test Loss: 0.0114058\n",
      "Validation loss decreased (0.009044 --> 0.008729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0081308\n",
      "\tspeed: 0.0639s/iter; left time: 1382.1534s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084052\n",
      "\tspeed: 0.0299s/iter; left time: 644.3378s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.94s\n",
      "Steps: 224 | Train Loss: 0.0081697 Vali Loss: 0.0094098 Test Loss: 0.0121627\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0081944\n",
      "\tspeed: 0.0635s/iter; left time: 1359.3442s\n",
      "\titers: 200, epoch: 5 | loss: 0.0074900\n",
      "\tspeed: 0.0299s/iter; left time: 636.4195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.02s\n",
      "Steps: 224 | Train Loss: 0.0075967 Vali Loss: 0.0089697 Test Loss: 0.0119432\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0067727\n",
      "\tspeed: 0.0617s/iter; left time: 1307.4678s\n",
      "\titers: 200, epoch: 6 | loss: 0.0068486\n",
      "\tspeed: 0.0294s/iter; left time: 619.8549s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 224 | Train Loss: 0.0070614 Vali Loss: 0.0092476 Test Loss: 0.0119057\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0061740\n",
      "\tspeed: 0.0605s/iter; left time: 1267.1797s\n",
      "\titers: 200, epoch: 7 | loss: 0.0062932\n",
      "\tspeed: 0.0287s/iter; left time: 599.2007s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.86s\n",
      "Steps: 224 | Train Loss: 0.0064601 Vali Loss: 0.0094871 Test Loss: 0.0122043\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0064154\n",
      "\tspeed: 0.0604s/iter; left time: 1252.4978s\n",
      "\titers: 200, epoch: 8 | loss: 0.0066724\n",
      "\tspeed: 0.0295s/iter; left time: 608.8793s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.95s\n",
      "Steps: 224 | Train Loss: 0.0058832 Vali Loss: 0.0096027 Test Loss: 0.0125173\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0051365\n",
      "\tspeed: 0.0594s/iter; left time: 1217.7786s\n",
      "\titers: 200, epoch: 9 | loss: 0.0048415\n",
      "\tspeed: 0.0298s/iter; left time: 609.1436s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.81s\n",
      "Steps: 224 | Train Loss: 0.0053815 Vali Loss: 0.0100015 Test Loss: 0.0128093\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049133\n",
      "\tspeed: 0.0611s/iter; left time: 1238.9837s\n",
      "\titers: 200, epoch: 10 | loss: 0.0047802\n",
      "\tspeed: 0.0295s/iter; left time: 594.5728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 224 | Train Loss: 0.0049502 Vali Loss: 0.0102042 Test Loss: 0.0132874\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0047629\n",
      "\tspeed: 0.0627s/iter; left time: 1258.0624s\n",
      "\titers: 200, epoch: 11 | loss: 0.0041915\n",
      "\tspeed: 0.0317s/iter; left time: 633.0375s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 224 | Train Loss: 0.0046080 Vali Loss: 0.0105283 Test Loss: 0.0133615\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0044058\n",
      "\tspeed: 0.0651s/iter; left time: 1291.0467s\n",
      "\titers: 200, epoch: 12 | loss: 0.0043703\n",
      "\tspeed: 0.0310s/iter; left time: 611.7606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.27s\n",
      "Steps: 224 | Train Loss: 0.0042848 Vali Loss: 0.0104694 Test Loss: 0.0136671\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0038280\n",
      "\tspeed: 0.0630s/iter; left time: 1235.5566s\n",
      "\titers: 200, epoch: 13 | loss: 0.0037476\n",
      "\tspeed: 0.0304s/iter; left time: 594.0538s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 224 | Train Loss: 0.0040548 Vali Loss: 0.0109012 Test Loss: 0.0137471\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011405767872929573, rmse:0.10679779201745987, mae:0.06919430196285248, rse:0.3142929971218109\n",
      "Intermediate time for ES and pred_len 24: 00h:02m:11.87s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0249662\n",
      "\tspeed: 0.0586s/iter; left time: 1307.2795s\n",
      "\titers: 200, epoch: 1 | loss: 0.0213045\n",
      "\tspeed: 0.0304s/iter; left time: 674.4282s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 224 | Train Loss: 0.0275395 Vali Loss: 0.0199210 Test Loss: 0.0259587\n",
      "Validation loss decreased (inf --> 0.019921).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0154199\n",
      "\tspeed: 0.0743s/iter; left time: 1640.5048s\n",
      "\titers: 200, epoch: 2 | loss: 0.0147711\n",
      "\tspeed: 0.0313s/iter; left time: 687.4509s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.28s\n",
      "Steps: 224 | Train Loss: 0.0159194 Vali Loss: 0.0164074 Test Loss: 0.0206553\n",
      "Validation loss decreased (0.019921 --> 0.016407).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0128123\n",
      "\tspeed: 0.0699s/iter; left time: 1527.7572s\n",
      "\titers: 200, epoch: 3 | loss: 0.0120069\n",
      "\tspeed: 0.0280s/iter; left time: 608.6671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.79s\n",
      "Steps: 224 | Train Loss: 0.0133727 Vali Loss: 0.0177141 Test Loss: 0.0220083\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110963\n",
      "\tspeed: 0.0712s/iter; left time: 1539.1342s\n",
      "\titers: 200, epoch: 4 | loss: 0.0101504\n",
      "\tspeed: 0.0312s/iter; left time: 670.7137s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0110040 Vali Loss: 0.0177026 Test Loss: 0.0225919\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0089270\n",
      "\tspeed: 0.0730s/iter; left time: 1562.8129s\n",
      "\titers: 200, epoch: 5 | loss: 0.0081671\n",
      "\tspeed: 0.0320s/iter; left time: 682.5150s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 224 | Train Loss: 0.0087992 Vali Loss: 0.0184645 Test Loss: 0.0240935\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0072723\n",
      "\tspeed: 0.0676s/iter; left time: 1431.4422s\n",
      "\titers: 200, epoch: 6 | loss: 0.0064863\n",
      "\tspeed: 0.0363s/iter; left time: 764.5081s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.65s\n",
      "Steps: 224 | Train Loss: 0.0072933 Vali Loss: 0.0184985 Test Loss: 0.0245565\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064227\n",
      "\tspeed: 0.0715s/iter; left time: 1497.8011s\n",
      "\titers: 200, epoch: 7 | loss: 0.0061511\n",
      "\tspeed: 0.0315s/iter; left time: 656.3951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.89s\n",
      "Steps: 224 | Train Loss: 0.0063212 Vali Loss: 0.0185263 Test Loss: 0.0238103\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0058017\n",
      "\tspeed: 0.0743s/iter; left time: 1541.0346s\n",
      "\titers: 200, epoch: 8 | loss: 0.0057851\n",
      "\tspeed: 0.0317s/iter; left time: 653.1800s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.97s\n",
      "Steps: 224 | Train Loss: 0.0056385 Vali Loss: 0.0192424 Test Loss: 0.0244339\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0049183\n",
      "\tspeed: 0.0704s/iter; left time: 1444.4453s\n",
      "\titers: 200, epoch: 9 | loss: 0.0051946\n",
      "\tspeed: 0.0356s/iter; left time: 726.5123s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.74s\n",
      "Steps: 224 | Train Loss: 0.0051166 Vali Loss: 0.0190300 Test Loss: 0.0246217\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049855\n",
      "\tspeed: 0.0706s/iter; left time: 1432.0759s\n",
      "\titers: 200, epoch: 10 | loss: 0.0045004\n",
      "\tspeed: 0.0307s/iter; left time: 619.9271s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 224 | Train Loss: 0.0047357 Vali Loss: 0.0190064 Test Loss: 0.0247210\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0043923\n",
      "\tspeed: 0.0738s/iter; left time: 1481.1946s\n",
      "\titers: 200, epoch: 11 | loss: 0.0043126\n",
      "\tspeed: 0.0308s/iter; left time: 615.1021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.79s\n",
      "Steps: 224 | Train Loss: 0.0044053 Vali Loss: 0.0192821 Test Loss: 0.0248229\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0039878\n",
      "\tspeed: 0.0681s/iter; left time: 1351.8309s\n",
      "\titers: 200, epoch: 12 | loss: 0.0041882\n",
      "\tspeed: 0.0362s/iter; left time: 713.6469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 224 | Train Loss: 0.0041530 Vali Loss: 0.0190465 Test Loss: 0.0247059\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_96_ES_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020655285567045212, rmse:0.14371946454048157, mae:0.0965302586555481, rse:0.42220455408096313\n",
      "Intermediate time for ES and pred_len 96: 00h:02m:15.77s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_ES_336_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=336, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0244475\n",
      "\tspeed: 0.0552s/iter; left time: 1224.4591s\n",
      "\titers: 200, epoch: 1 | loss: 0.0229005\n",
      "\tspeed: 0.0362s/iter; left time: 799.9027s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.20s\n",
      "Steps: 223 | Train Loss: 0.0284261 Vali Loss: 0.0214357 Test Loss: 0.0273612\n",
      "Validation loss decreased (inf --> 0.021436).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0168547\n",
      "\tspeed: 0.0754s/iter; left time: 1657.8091s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166410\n",
      "\tspeed: 0.0331s/iter; left time: 725.2437s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.89s\n",
      "Steps: 223 | Train Loss: 0.0171577 Vali Loss: 0.0180411 Test Loss: 0.0229071\n",
      "Validation loss decreased (0.021436 --> 0.018041).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0136818\n",
      "\tspeed: 0.0755s/iter; left time: 1641.6418s\n",
      "\titers: 200, epoch: 3 | loss: 0.0134842\n",
      "\tspeed: 0.0317s/iter; left time: 686.7033s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.99s\n",
      "Steps: 223 | Train Loss: 0.0141816 Vali Loss: 0.0183799 Test Loss: 0.0234680\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0115397\n",
      "\tspeed: 0.0722s/iter; left time: 1553.8494s\n",
      "\titers: 200, epoch: 4 | loss: 0.0105017\n",
      "\tspeed: 0.0389s/iter; left time: 833.4224s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 223 | Train Loss: 0.0117780 Vali Loss: 0.0189678 Test Loss: 0.0249497\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0095834\n",
      "\tspeed: 0.0745s/iter; left time: 1587.8296s\n",
      "\titers: 200, epoch: 5 | loss: 0.0090101\n",
      "\tspeed: 0.0330s/iter; left time: 700.8299s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.19s\n",
      "Steps: 223 | Train Loss: 0.0094315 Vali Loss: 0.0192081 Test Loss: 0.0266327\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0077858\n",
      "\tspeed: 0.0788s/iter; left time: 1661.0620s\n",
      "\titers: 200, epoch: 6 | loss: 0.0073812\n",
      "\tspeed: 0.0342s/iter; left time: 717.8340s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 223 | Train Loss: 0.0079274 Vali Loss: 0.0197801 Test Loss: 0.0270143\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0065276\n",
      "\tspeed: 0.0739s/iter; left time: 1542.3686s\n",
      "\titers: 200, epoch: 7 | loss: 0.0063776\n",
      "\tspeed: 0.0381s/iter; left time: 791.7813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0069369 Vali Loss: 0.0198990 Test Loss: 0.0276234\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0062215\n",
      "\tspeed: 0.0746s/iter; left time: 1540.6450s\n",
      "\titers: 200, epoch: 8 | loss: 0.0059965\n",
      "\tspeed: 0.0300s/iter; left time: 616.0683s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.03s\n",
      "Steps: 223 | Train Loss: 0.0062075 Vali Loss: 0.0199308 Test Loss: 0.0275127\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0058798\n",
      "\tspeed: 0.0803s/iter; left time: 1638.4948s\n",
      "\titers: 200, epoch: 9 | loss: 0.0054274\n",
      "\tspeed: 0.0366s/iter; left time: 743.2925s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.77s\n",
      "Steps: 223 | Train Loss: 0.0056564 Vali Loss: 0.0199600 Test Loss: 0.0280890\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0053195\n",
      "\tspeed: 0.0687s/iter; left time: 1386.4426s\n",
      "\titers: 200, epoch: 10 | loss: 0.0049607\n",
      "\tspeed: 0.0339s/iter; left time: 681.2269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.88s\n",
      "Steps: 223 | Train Loss: 0.0052265 Vali Loss: 0.0201242 Test Loss: 0.0278490\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0049777\n",
      "\tspeed: 0.0721s/iter; left time: 1439.7230s\n",
      "\titers: 200, epoch: 11 | loss: 0.0049066\n",
      "\tspeed: 0.0295s/iter; left time: 585.5519s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 223 | Train Loss: 0.0048739 Vali Loss: 0.0201201 Test Loss: 0.0282332\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0045771\n",
      "\tspeed: 0.0727s/iter; left time: 1435.5779s\n",
      "\titers: 200, epoch: 12 | loss: 0.0044787\n",
      "\tspeed: 0.0360s/iter; left time: 707.6159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 223 | Train Loss: 0.0045925 Vali Loss: 0.0199313 Test Loss: 0.0279702\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_ES_336_168_ES_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02290712110698223, rmse:0.1513509899377823, mae:0.10324922204017639, rse:0.4446555972099304\n",
      "Intermediate time for ES and pred_len 168: 00h:02m:19.89s\n",
      "Intermediate time for ES: 00h:06m:47.53s\n",
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0142817\n",
      "\tspeed: 0.0397s/iter; left time: 893.6377s\n",
      "\titers: 200, epoch: 1 | loss: 0.0109015\n",
      "\tspeed: 0.0191s/iter; left time: 427.7089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.50s\n",
      "Steps: 226 | Train Loss: 0.0166739 Vali Loss: 0.0138271 Test Loss: 0.0167391\n",
      "Validation loss decreased (inf --> 0.013827).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0067945\n",
      "\tspeed: 0.0482s/iter; left time: 1072.7639s\n",
      "\titers: 200, epoch: 2 | loss: 0.0061001\n",
      "\tspeed: 0.0219s/iter; left time: 484.7126s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.25s\n",
      "Steps: 226 | Train Loss: 0.0071401 Vali Loss: 0.0095527 Test Loss: 0.0109669\n",
      "Validation loss decreased (0.013827 --> 0.009553).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0051317\n",
      "\tspeed: 0.0465s/iter; left time: 1025.5800s\n",
      "\titers: 200, epoch: 3 | loss: 0.0056985\n",
      "\tspeed: 0.0248s/iter; left time: 544.5277s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0059504 Vali Loss: 0.0092553 Test Loss: 0.0105850\n",
      "Validation loss decreased (0.009553 --> 0.009255).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0054032\n",
      "\tspeed: 0.0473s/iter; left time: 1032.1579s\n",
      "\titers: 200, epoch: 4 | loss: 0.0049541\n",
      "\tspeed: 0.0246s/iter; left time: 534.6705s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0055715 Vali Loss: 0.0093264 Test Loss: 0.0109252\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0057090\n",
      "\tspeed: 0.0438s/iter; left time: 945.6816s\n",
      "\titers: 200, epoch: 5 | loss: 0.0049125\n",
      "\tspeed: 0.0246s/iter; left time: 529.5099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 226 | Train Loss: 0.0051394 Vali Loss: 0.0092770 Test Loss: 0.0106430\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0055766\n",
      "\tspeed: 0.0480s/iter; left time: 1024.9096s\n",
      "\titers: 200, epoch: 6 | loss: 0.0042806\n",
      "\tspeed: 0.0213s/iter; left time: 453.2134s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 226 | Train Loss: 0.0047325 Vali Loss: 0.0091757 Test Loss: 0.0110049\n",
      "Validation loss decreased (0.009255 --> 0.009176).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0041325\n",
      "\tspeed: 0.0575s/iter; left time: 1215.1075s\n",
      "\titers: 200, epoch: 7 | loss: 0.0045434\n",
      "\tspeed: 0.0205s/iter; left time: 431.8695s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.24s\n",
      "Steps: 226 | Train Loss: 0.0042715 Vali Loss: 0.0096620 Test Loss: 0.0119414\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0040274\n",
      "\tspeed: 0.0512s/iter; left time: 1071.2649s\n",
      "\titers: 200, epoch: 8 | loss: 0.0043296\n",
      "\tspeed: 0.0209s/iter; left time: 435.7849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 226 | Train Loss: 0.0038831 Vali Loss: 0.0102222 Test Loss: 0.0123794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0039655\n",
      "\tspeed: 0.0533s/iter; left time: 1102.0029s\n",
      "\titers: 200, epoch: 9 | loss: 0.0035371\n",
      "\tspeed: 0.0199s/iter; left time: 410.6390s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 226 | Train Loss: 0.0035282 Vali Loss: 0.0101897 Test Loss: 0.0123428\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0031067\n",
      "\tspeed: 0.0503s/iter; left time: 1029.5440s\n",
      "\titers: 200, epoch: 10 | loss: 0.0034840\n",
      "\tspeed: 0.0195s/iter; left time: 397.1925s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0032746 Vali Loss: 0.0108053 Test Loss: 0.0126818\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0034887\n",
      "\tspeed: 0.0481s/iter; left time: 973.5595s\n",
      "\titers: 200, epoch: 11 | loss: 0.0030701\n",
      "\tspeed: 0.0165s/iter; left time: 332.9588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0030655 Vali Loss: 0.0105460 Test Loss: 0.0124508\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0027485\n",
      "\tspeed: 0.0453s/iter; left time: 907.6802s\n",
      "\titers: 200, epoch: 12 | loss: 0.0027083\n",
      "\tspeed: 0.0162s/iter; left time: 322.1957s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.85s\n",
      "Steps: 226 | Train Loss: 0.0029029 Vali Loss: 0.0104811 Test Loss: 0.0127481\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0024927\n",
      "\tspeed: 0.0400s/iter; left time: 791.7849s\n",
      "\titers: 200, epoch: 13 | loss: 0.0028051\n",
      "\tspeed: 0.0190s/iter; left time: 373.7724s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0027675 Vali Loss: 0.0105155 Test Loss: 0.0127166\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0026206\n",
      "\tspeed: 0.0500s/iter; left time: 978.9320s\n",
      "\titers: 200, epoch: 14 | loss: 0.0026182\n",
      "\tspeed: 0.0204s/iter; left time: 397.1433s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0026426 Vali Loss: 0.0107463 Test Loss: 0.0129030\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0025154\n",
      "\tspeed: 0.0433s/iter; left time: 837.1179s\n",
      "\titers: 200, epoch: 15 | loss: 0.0025768\n",
      "\tspeed: 0.0240s/iter; left time: 461.1951s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 226 | Train Loss: 0.0025631 Vali Loss: 0.0108644 Test Loss: 0.0129967\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0022232\n",
      "\tspeed: 0.0450s/iter; left time: 859.6571s\n",
      "\titers: 200, epoch: 16 | loss: 0.0022974\n",
      "\tspeed: 0.0249s/iter; left time: 472.9783s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 226 | Train Loss: 0.0024720 Vali Loss: 0.0110153 Test Loss: 0.0128870\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011004881002008915, rmse:0.10490415245294571, mae:0.06105313077569008, rse:0.4047172963619232\n",
      "Intermediate time for FR and pred_len 24: 00h:01m:57.46s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0185340\n",
      "\tspeed: 0.0356s/iter; left time: 796.8574s\n",
      "\titers: 200, epoch: 1 | loss: 0.0147910\n",
      "\tspeed: 0.0194s/iter; left time: 433.2728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.35s\n",
      "Steps: 225 | Train Loss: 0.0200774 Vali Loss: 0.0181967 Test Loss: 0.0233480\n",
      "Validation loss decreased (inf --> 0.018197).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0102094\n",
      "\tspeed: 0.0534s/iter; left time: 1183.3781s\n",
      "\titers: 200, epoch: 2 | loss: 0.0101413\n",
      "\tspeed: 0.0194s/iter; left time: 429.1445s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 225 | Train Loss: 0.0114065 Vali Loss: 0.0150723 Test Loss: 0.0191185\n",
      "Validation loss decreased (0.018197 --> 0.015072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0107937\n",
      "\tspeed: 0.0507s/iter; left time: 1113.9931s\n",
      "\titers: 200, epoch: 3 | loss: 0.0090744\n",
      "\tspeed: 0.0190s/iter; left time: 414.2577s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 225 | Train Loss: 0.0096025 Vali Loss: 0.0159363 Test Loss: 0.0185294\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0082903\n",
      "\tspeed: 0.0508s/iter; left time: 1103.3885s\n",
      "\titers: 200, epoch: 4 | loss: 0.0068811\n",
      "\tspeed: 0.0193s/iter; left time: 417.8840s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.37s\n",
      "Steps: 225 | Train Loss: 0.0081587 Vali Loss: 0.0166399 Test Loss: 0.0192749\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0072266\n",
      "\tspeed: 0.0472s/iter; left time: 1014.5566s\n",
      "\titers: 200, epoch: 5 | loss: 0.0061447\n",
      "\tspeed: 0.0230s/iter; left time: 492.4569s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.42s\n",
      "Steps: 225 | Train Loss: 0.0069639 Vali Loss: 0.0168462 Test Loss: 0.0194454\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0060944\n",
      "\tspeed: 0.0418s/iter; left time: 888.3649s\n",
      "\titers: 200, epoch: 6 | loss: 0.0060795\n",
      "\tspeed: 0.0282s/iter; left time: 597.6238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.56s\n",
      "Steps: 225 | Train Loss: 0.0061028 Vali Loss: 0.0174394 Test Loss: 0.0191661\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0052585\n",
      "\tspeed: 0.0444s/iter; left time: 933.7863s\n",
      "\titers: 200, epoch: 7 | loss: 0.0047111\n",
      "\tspeed: 0.0201s/iter; left time: 420.8287s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.09s\n",
      "Steps: 225 | Train Loss: 0.0054582 Vali Loss: 0.0173450 Test Loss: 0.0193435\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0051130\n",
      "\tspeed: 0.0529s/iter; left time: 1102.6591s\n",
      "\titers: 200, epoch: 8 | loss: 0.0050900\n",
      "\tspeed: 0.0190s/iter; left time: 392.7738s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.83s\n",
      "Steps: 225 | Train Loss: 0.0049748 Vali Loss: 0.0177807 Test Loss: 0.0197438\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0044787\n",
      "\tspeed: 0.0550s/iter; left time: 1133.5448s\n",
      "\titers: 200, epoch: 9 | loss: 0.0054008\n",
      "\tspeed: 0.0195s/iter; left time: 398.8559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0045797 Vali Loss: 0.0180904 Test Loss: 0.0202845\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0043762\n",
      "\tspeed: 0.0532s/iter; left time: 1084.1541s\n",
      "\titers: 200, epoch: 10 | loss: 0.0039011\n",
      "\tspeed: 0.0187s/iter; left time: 378.7681s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.61s\n",
      "Steps: 225 | Train Loss: 0.0042761 Vali Loss: 0.0177933 Test Loss: 0.0203592\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0041078\n",
      "\tspeed: 0.0491s/iter; left time: 988.7858s\n",
      "\titers: 200, epoch: 11 | loss: 0.0040893\n",
      "\tspeed: 0.0255s/iter; left time: 510.8431s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 225 | Train Loss: 0.0040168 Vali Loss: 0.0178445 Test Loss: 0.0206380\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0037558\n",
      "\tspeed: 0.0448s/iter; left time: 892.3402s\n",
      "\titers: 200, epoch: 12 | loss: 0.0037573\n",
      "\tspeed: 0.0285s/iter; left time: 564.5831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.79s\n",
      "Steps: 225 | Train Loss: 0.0038152 Vali Loss: 0.0182746 Test Loss: 0.0207126\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019118474796414375, rmse:0.1382695734500885, mae:0.08493250608444214, rse:0.5348634719848633\n",
      "Intermediate time for FR and pred_len 96: 00h:01m:32.65s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0167841\n",
      "\tspeed: 0.0366s/iter; left time: 819.0665s\n",
      "\titers: 200, epoch: 1 | loss: 0.0179857\n",
      "\tspeed: 0.0272s/iter; left time: 606.9146s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 225 | Train Loss: 0.0214065 Vali Loss: 0.0195938 Test Loss: 0.0242914\n",
      "Validation loss decreased (inf --> 0.019594).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0119704\n",
      "\tspeed: 0.0497s/iter; left time: 1101.0654s\n",
      "\titers: 200, epoch: 2 | loss: 0.0114996\n",
      "\tspeed: 0.0275s/iter; left time: 608.1885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.70s\n",
      "Steps: 225 | Train Loss: 0.0127617 Vali Loss: 0.0161654 Test Loss: 0.0199560\n",
      "Validation loss decreased (0.019594 --> 0.016165).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0097979\n",
      "\tspeed: 0.0490s/iter; left time: 1076.5098s\n",
      "\titers: 200, epoch: 3 | loss: 0.0093169\n",
      "\tspeed: 0.0282s/iter; left time: 615.7201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 225 | Train Loss: 0.0106056 Vali Loss: 0.0170234 Test Loss: 0.0204392\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0089465\n",
      "\tspeed: 0.0467s/iter; left time: 1014.8792s\n",
      "\titers: 200, epoch: 4 | loss: 0.0089064\n",
      "\tspeed: 0.0256s/iter; left time: 553.8914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 225 | Train Loss: 0.0090893 Vali Loss: 0.0174842 Test Loss: 0.0206586\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0076368\n",
      "\tspeed: 0.0454s/iter; left time: 975.6675s\n",
      "\titers: 200, epoch: 5 | loss: 0.0076604\n",
      "\tspeed: 0.0252s/iter; left time: 538.5168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.78s\n",
      "Steps: 225 | Train Loss: 0.0078856 Vali Loss: 0.0187933 Test Loss: 0.0214456\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0073292\n",
      "\tspeed: 0.0495s/iter; left time: 1053.1073s\n",
      "\titers: 200, epoch: 6 | loss: 0.0062847\n",
      "\tspeed: 0.0197s/iter; left time: 416.5636s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.33s\n",
      "Steps: 225 | Train Loss: 0.0069546 Vali Loss: 0.0182947 Test Loss: 0.0217758\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0066456\n",
      "\tspeed: 0.0553s/iter; left time: 1164.5845s\n",
      "\titers: 200, epoch: 7 | loss: 0.0058500\n",
      "\tspeed: 0.0211s/iter; left time: 441.8646s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.15s\n",
      "Steps: 225 | Train Loss: 0.0062480 Vali Loss: 0.0186382 Test Loss: 0.0225218\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0055677\n",
      "\tspeed: 0.0539s/iter; left time: 1123.1297s\n",
      "\titers: 200, epoch: 8 | loss: 0.0053374\n",
      "\tspeed: 0.0210s/iter; left time: 434.5517s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0057114 Vali Loss: 0.0194597 Test Loss: 0.0230711\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0052487\n",
      "\tspeed: 0.0570s/iter; left time: 1173.8884s\n",
      "\titers: 200, epoch: 9 | loss: 0.0048349\n",
      "\tspeed: 0.0210s/iter; left time: 430.1077s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.93s\n",
      "Steps: 225 | Train Loss: 0.0052988 Vali Loss: 0.0192110 Test Loss: 0.0231458\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0050047\n",
      "\tspeed: 0.0575s/iter; left time: 1170.7312s\n",
      "\titers: 200, epoch: 10 | loss: 0.0050389\n",
      "\tspeed: 0.0215s/iter; left time: 435.4071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.04s\n",
      "Steps: 225 | Train Loss: 0.0049720 Vali Loss: 0.0198488 Test Loss: 0.0230945\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0045330\n",
      "\tspeed: 0.0596s/iter; left time: 1200.1550s\n",
      "\titers: 200, epoch: 11 | loss: 0.0046306\n",
      "\tspeed: 0.0208s/iter; left time: 416.1887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 225 | Train Loss: 0.0046707 Vali Loss: 0.0195044 Test Loss: 0.0229342\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0043923\n",
      "\tspeed: 0.0631s/iter; left time: 1256.7383s\n",
      "\titers: 200, epoch: 12 | loss: 0.0038309\n",
      "\tspeed: 0.0208s/iter; left time: 412.1831s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.11s\n",
      "Steps: 225 | Train Loss: 0.0044319 Vali Loss: 0.0197984 Test Loss: 0.0232360\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.01995600387454033, rmse:0.14126572012901306, mae:0.08893339335918427, rse:0.5471355319023132\n",
      "Intermediate time for FR and pred_len 168: 00h:01m:40.54s\n",
      "Intermediate time for FR: 00h:05m:10.65s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0264049\n",
      "\tspeed: 0.0401s/iter; left time: 902.2086s\n",
      "\titers: 200, epoch: 1 | loss: 0.0212160\n",
      "\tspeed: 0.0226s/iter; left time: 505.9598s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.71s\n",
      "Steps: 226 | Train Loss: 0.0303445 Vali Loss: 0.0161344 Test Loss: 0.0174399\n",
      "Validation loss decreased (inf --> 0.016134).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0111341\n",
      "\tspeed: 0.0502s/iter; left time: 1119.0532s\n",
      "\titers: 200, epoch: 2 | loss: 0.0105001\n",
      "\tspeed: 0.0208s/iter; left time: 460.7199s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.08s\n",
      "Steps: 226 | Train Loss: 0.0123827 Vali Loss: 0.0101251 Test Loss: 0.0111429\n",
      "Validation loss decreased (0.016134 --> 0.010125).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0094639\n",
      "\tspeed: 0.0547s/iter; left time: 1205.8333s\n",
      "\titers: 200, epoch: 3 | loss: 0.0118124\n",
      "\tspeed: 0.0194s/iter; left time: 425.4621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0103339 Vali Loss: 0.0096896 Test Loss: 0.0108361\n",
      "Validation loss decreased (0.010125 --> 0.009690).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0101394\n",
      "\tspeed: 0.0546s/iter; left time: 1190.7430s\n",
      "\titers: 200, epoch: 4 | loss: 0.0083410\n",
      "\tspeed: 0.0188s/iter; left time: 407.5656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0097526 Vali Loss: 0.0094186 Test Loss: 0.0106146\n",
      "Validation loss decreased (0.009690 --> 0.009419).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0092185\n",
      "\tspeed: 0.0533s/iter; left time: 1152.1456s\n",
      "\titers: 200, epoch: 5 | loss: 0.0083651\n",
      "\tspeed: 0.0180s/iter; left time: 387.8353s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 226 | Train Loss: 0.0092472 Vali Loss: 0.0092419 Test Loss: 0.0106667\n",
      "Validation loss decreased (0.009419 --> 0.009242).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0088084\n",
      "\tspeed: 0.0502s/iter; left time: 1072.5163s\n",
      "\titers: 200, epoch: 6 | loss: 0.0076559\n",
      "\tspeed: 0.0204s/iter; left time: 432.8715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0086846 Vali Loss: 0.0093900 Test Loss: 0.0106002\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0087312\n",
      "\tspeed: 0.0488s/iter; left time: 1031.6167s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092575\n",
      "\tspeed: 0.0210s/iter; left time: 441.0236s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.13s\n",
      "Steps: 226 | Train Loss: 0.0082112 Vali Loss: 0.0096978 Test Loss: 0.0107965\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0080053\n",
      "\tspeed: 0.0504s/iter; left time: 1053.6779s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081390\n",
      "\tspeed: 0.0182s/iter; left time: 379.3088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.21s\n",
      "Steps: 226 | Train Loss: 0.0076047 Vali Loss: 0.0100151 Test Loss: 0.0111496\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0066461\n",
      "\tspeed: 0.0481s/iter; left time: 994.6268s\n",
      "\titers: 200, epoch: 9 | loss: 0.0070476\n",
      "\tspeed: 0.0233s/iter; left time: 478.9058s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.32s\n",
      "Steps: 226 | Train Loss: 0.0070180 Vali Loss: 0.0101141 Test Loss: 0.0112617\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0057631\n",
      "\tspeed: 0.0453s/iter; left time: 927.9982s\n",
      "\titers: 200, epoch: 10 | loss: 0.0062514\n",
      "\tspeed: 0.0247s/iter; left time: 503.8603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.23s\n",
      "Steps: 226 | Train Loss: 0.0065206 Vali Loss: 0.0103133 Test Loss: 0.0114330\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0062055\n",
      "\tspeed: 0.0424s/iter; left time: 859.1355s\n",
      "\titers: 200, epoch: 11 | loss: 0.0056173\n",
      "\tspeed: 0.0221s/iter; left time: 445.6305s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.15s\n",
      "Steps: 226 | Train Loss: 0.0060727 Vali Loss: 0.0106845 Test Loss: 0.0117234\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0050799\n",
      "\tspeed: 0.0485s/iter; left time: 970.5160s\n",
      "\titers: 200, epoch: 12 | loss: 0.0054049\n",
      "\tspeed: 0.0207s/iter; left time: 411.3019s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.14s\n",
      "Steps: 226 | Train Loss: 0.0057344 Vali Loss: 0.0107858 Test Loss: 0.0115632\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0054517\n",
      "\tspeed: 0.0505s/iter; left time: 999.5847s\n",
      "\titers: 200, epoch: 13 | loss: 0.0050979\n",
      "\tspeed: 0.0186s/iter; left time: 366.9107s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 226 | Train Loss: 0.0054612 Vali Loss: 0.0109315 Test Loss: 0.0117036\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0050719\n",
      "\tspeed: 0.0524s/iter; left time: 1025.0226s\n",
      "\titers: 200, epoch: 14 | loss: 0.0055266\n",
      "\tspeed: 0.0189s/iter; left time: 367.8269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.56s\n",
      "Steps: 226 | Train Loss: 0.0051985 Vali Loss: 0.0107567 Test Loss: 0.0119687\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0050930\n",
      "\tspeed: 0.0540s/iter; left time: 1043.3980s\n",
      "\titers: 200, epoch: 15 | loss: 0.0046835\n",
      "\tspeed: 0.0192s/iter; left time: 369.9461s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:05.36s\n",
      "Steps: 226 | Train Loss: 0.0050102 Vali Loss: 0.0108887 Test Loss: 0.0118751\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010666660964488983, rmse:0.10327953100204468, mae:0.0619974210858345, rse:0.39024245738983154\n",
      "Intermediate time for IT and pred_len 24: 00h:01m:55.42s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0312227\n",
      "\tspeed: 0.0483s/iter; left time: 1080.9080s\n",
      "\titers: 200, epoch: 1 | loss: 0.0266664\n",
      "\tspeed: 0.0282s/iter; left time: 629.2883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.89s\n",
      "Steps: 225 | Train Loss: 0.0356257 Vali Loss: 0.0213617 Test Loss: 0.0232229\n",
      "Validation loss decreased (inf --> 0.021362).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0196660\n",
      "\tspeed: 0.0461s/iter; left time: 1023.0386s\n",
      "\titers: 200, epoch: 2 | loss: 0.0183100\n",
      "\tspeed: 0.0207s/iter; left time: 456.8104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0196914 Vali Loss: 0.0170778 Test Loss: 0.0191014\n",
      "Validation loss decreased (0.021362 --> 0.017078).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0178034\n",
      "\tspeed: 0.0555s/iter; left time: 1219.3081s\n",
      "\titers: 200, epoch: 3 | loss: 0.0181890\n",
      "\tspeed: 0.0198s/iter; left time: 432.1097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.81s\n",
      "Steps: 225 | Train Loss: 0.0174670 Vali Loss: 0.0169888 Test Loss: 0.0188168\n",
      "Validation loss decreased (0.017078 --> 0.016989).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0167031\n",
      "\tspeed: 0.0592s/iter; left time: 1286.5769s\n",
      "\titers: 200, epoch: 4 | loss: 0.0140209\n",
      "\tspeed: 0.0206s/iter; left time: 445.7012s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.22s\n",
      "Steps: 225 | Train Loss: 0.0158114 Vali Loss: 0.0181897 Test Loss: 0.0195597\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0166279\n",
      "\tspeed: 0.0520s/iter; left time: 1118.8445s\n",
      "\titers: 200, epoch: 5 | loss: 0.0126541\n",
      "\tspeed: 0.0208s/iter; left time: 445.7041s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.11s\n",
      "Steps: 225 | Train Loss: 0.0139899 Vali Loss: 0.0198033 Test Loss: 0.0205443\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0126828\n",
      "\tspeed: 0.0552s/iter; left time: 1174.8455s\n",
      "\titers: 200, epoch: 6 | loss: 0.0131766\n",
      "\tspeed: 0.0213s/iter; left time: 451.0288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.98s\n",
      "Steps: 225 | Train Loss: 0.0121957 Vali Loss: 0.0210449 Test Loss: 0.0204667\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0109516\n",
      "\tspeed: 0.0547s/iter; left time: 1150.7185s\n",
      "\titers: 200, epoch: 7 | loss: 0.0098201\n",
      "\tspeed: 0.0210s/iter; left time: 440.7656s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.95s\n",
      "Steps: 225 | Train Loss: 0.0107617 Vali Loss: 0.0208559 Test Loss: 0.0210789\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0108403\n",
      "\tspeed: 0.0446s/iter; left time: 928.2046s\n",
      "\titers: 200, epoch: 8 | loss: 0.0090046\n",
      "\tspeed: 0.0272s/iter; left time: 562.9741s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.72s\n",
      "Steps: 225 | Train Loss: 0.0097365 Vali Loss: 0.0209417 Test Loss: 0.0221892\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0082687\n",
      "\tspeed: 0.0435s/iter; left time: 895.2202s\n",
      "\titers: 200, epoch: 9 | loss: 0.0082735\n",
      "\tspeed: 0.0287s/iter; left time: 589.1983s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0088897 Vali Loss: 0.0214697 Test Loss: 0.0218216\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0087429\n",
      "\tspeed: 0.0459s/iter; left time: 934.2756s\n",
      "\titers: 200, epoch: 10 | loss: 0.0078785\n",
      "\tspeed: 0.0252s/iter; left time: 511.0760s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.97s\n",
      "Steps: 225 | Train Loss: 0.0082368 Vali Loss: 0.0222755 Test Loss: 0.0228619\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0086819\n",
      "\tspeed: 0.0482s/iter; left time: 970.4437s\n",
      "\titers: 200, epoch: 11 | loss: 0.0084118\n",
      "\tspeed: 0.0212s/iter; left time: 425.5440s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.32s\n",
      "Steps: 225 | Train Loss: 0.0077866 Vali Loss: 0.0218232 Test Loss: 0.0222956\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0069241\n",
      "\tspeed: 0.0562s/iter; left time: 1120.4966s\n",
      "\titers: 200, epoch: 12 | loss: 0.0077941\n",
      "\tspeed: 0.0201s/iter; left time: 397.6030s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 225 | Train Loss: 0.0074321 Vali Loss: 0.0221129 Test Loss: 0.0221040\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0076319\n",
      "\tspeed: 0.0500s/iter; left time: 984.5919s\n",
      "\titers: 200, epoch: 13 | loss: 0.0071445\n",
      "\tspeed: 0.0220s/iter; left time: 432.0278s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.64s\n",
      "Steps: 225 | Train Loss: 0.0070701 Vali Loss: 0.0216414 Test Loss: 0.0220283\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.01881682500243187, rmse:0.13717442750930786, mae:0.08598846942186356, rse:0.5186712741851807\n",
      "Intermediate time for IT and pred_len 96: 00h:01m:47.66s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='channel_mixing_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : channel_mixing_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0296239\n",
      "\tspeed: 0.0476s/iter; left time: 1066.8337s\n",
      "\titers: 200, epoch: 1 | loss: 0.0281595\n",
      "\tspeed: 0.0293s/iter; left time: 653.4324s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.90s\n",
      "Steps: 225 | Train Loss: 0.0368548 Vali Loss: 0.0223018 Test Loss: 0.0240904\n",
      "Validation loss decreased (inf --> 0.022302).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0215188\n",
      "\tspeed: 0.0498s/iter; left time: 1103.9927s\n",
      "\titers: 200, epoch: 2 | loss: 0.0191364\n",
      "\tspeed: 0.0280s/iter; left time: 617.8952s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.15s\n",
      "Steps: 225 | Train Loss: 0.0213553 Vali Loss: 0.0186788 Test Loss: 0.0198309\n",
      "Validation loss decreased (0.022302 --> 0.018679).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0185150\n",
      "\tspeed: 0.0511s/iter; left time: 1122.2386s\n",
      "\titers: 200, epoch: 3 | loss: 0.0157815\n",
      "\tspeed: 0.0238s/iter; left time: 521.1293s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.44s\n",
      "Steps: 225 | Train Loss: 0.0186807 Vali Loss: 0.0192694 Test Loss: 0.0201614\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0160066\n",
      "\tspeed: 0.0511s/iter; left time: 1110.7214s\n",
      "\titers: 200, epoch: 4 | loss: 0.0164357\n",
      "\tspeed: 0.0197s/iter; left time: 426.5529s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.96s\n",
      "Steps: 225 | Train Loss: 0.0167670 Vali Loss: 0.0198088 Test Loss: 0.0203017\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0150016\n",
      "\tspeed: 0.0537s/iter; left time: 1155.5778s\n",
      "\titers: 200, epoch: 5 | loss: 0.0145082\n",
      "\tspeed: 0.0210s/iter; left time: 448.7711s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.95s\n",
      "Steps: 225 | Train Loss: 0.0150612 Vali Loss: 0.0214123 Test Loss: 0.0209583\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0135488\n",
      "\tspeed: 0.0539s/iter; left time: 1147.3276s\n",
      "\titers: 200, epoch: 6 | loss: 0.0130257\n",
      "\tspeed: 0.0209s/iter; left time: 442.2027s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.81s\n",
      "Steps: 225 | Train Loss: 0.0135763 Vali Loss: 0.0219544 Test Loss: 0.0217701\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0125295\n",
      "\tspeed: 0.0523s/iter; left time: 1100.0941s\n",
      "\titers: 200, epoch: 7 | loss: 0.0118939\n",
      "\tspeed: 0.0194s/iter; left time: 406.4621s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:05.62s\n",
      "Steps: 225 | Train Loss: 0.0121774 Vali Loss: 0.0224625 Test Loss: 0.0225583\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0113757\n",
      "\tspeed: 0.0457s/iter; left time: 951.1453s\n",
      "\titers: 200, epoch: 8 | loss: 0.0104438\n",
      "\tspeed: 0.0267s/iter; left time: 552.7632s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 225 | Train Loss: 0.0110213 Vali Loss: 0.0227240 Test Loss: 0.0222048\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0101773\n",
      "\tspeed: 0.0443s/iter; left time: 912.6128s\n",
      "\titers: 200, epoch: 9 | loss: 0.0099189\n",
      "\tspeed: 0.0280s/iter; left time: 573.4481s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0101589 Vali Loss: 0.0221913 Test Loss: 0.0226550\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0093672\n",
      "\tspeed: 0.0425s/iter; left time: 866.2472s\n",
      "\titers: 200, epoch: 10 | loss: 0.0089159\n",
      "\tspeed: 0.0202s/iter; left time: 409.3931s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.17s\n",
      "Steps: 225 | Train Loss: 0.0094935 Vali Loss: 0.0220188 Test Loss: 0.0229948\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0092734\n",
      "\tspeed: 0.0562s/iter; left time: 1132.6478s\n",
      "\titers: 200, epoch: 11 | loss: 0.0082421\n",
      "\tspeed: 0.0213s/iter; left time: 427.8571s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:05.06s\n",
      "Steps: 225 | Train Loss: 0.0089367 Vali Loss: 0.0217495 Test Loss: 0.0225141\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0083151\n",
      "\tspeed: 0.0515s/iter; left time: 1026.8916s\n",
      "\titers: 200, epoch: 12 | loss: 0.0083149\n",
      "\tspeed: 0.0184s/iter; left time: 365.0908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:05.01s\n",
      "Steps: 225 | Train Loss: 0.0084946 Vali Loss: 0.0219489 Test Loss: 0.0228725\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : channel_mixing_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019830867648124695, rmse:0.14082211256027222, mae:0.09015556424856186, rse:0.5329583883285522\n",
      "Intermediate time for IT and pred_len 168: 00h:01m:38.59s\n",
      "Intermediate time for IT: 00h:05m:21.67s\n",
      "Total time: 00h:52m:17.35s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_MIX_FEATURES.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            model_id = f\"channel_mixing_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>0.1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.0692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.0611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.0849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.0889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>0.1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>0.1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.0902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0255  0.1596  0.1060\n",
       "        96        0.0381  0.1953  0.1384\n",
       "        168       0.0400  0.2001  0.1428\n",
       "ES      24        0.0114  0.1068  0.0692\n",
       "        96        0.0207  0.1437  0.0965\n",
       "        168       0.0229  0.1514  0.1032\n",
       "FR      24        0.0110  0.1049  0.0611\n",
       "        96        0.0191  0.1383  0.0849\n",
       "        168       0.0200  0.1413  0.0889\n",
       "GB      24        0.0286  0.1691  0.1175\n",
       "        96        0.0428  0.2069  0.1471\n",
       "        168       0.0450  0.2121  0.1520\n",
       "IT      24        0.0107  0.1033  0.0620\n",
       "        96        0.0188  0.1372  0.0860\n",
       "        168       0.0198  0.1408  0.0902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_MIX_FEATURES.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0234579\n",
      "\tspeed: 0.0700s/iter; left time: 1574.1290s\n",
      "\titers: 200, epoch: 1 | loss: 0.0188763\n",
      "\tspeed: 0.0480s/iter; left time: 1074.2645s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 226 | Train Loss: 0.0251614 Vali Loss: 0.0242755 Test Loss: 0.0266506\n",
      "Validation loss decreased (inf --> 0.024276).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0147026\n",
      "\tspeed: 0.0929s/iter; left time: 2069.5066s\n",
      "\titers: 200, epoch: 2 | loss: 0.0127275\n",
      "\tspeed: 0.0476s/iter; left time: 1056.3407s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.16s\n",
      "Steps: 226 | Train Loss: 0.0146741 Vali Loss: 0.0213676 Test Loss: 0.0233677\n",
      "Validation loss decreased (0.024276 --> 0.021368).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0132120\n",
      "\tspeed: 0.1003s/iter; left time: 2210.7943s\n",
      "\titers: 200, epoch: 3 | loss: 0.0103937\n",
      "\tspeed: 0.0483s/iter; left time: 1060.1647s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.26s\n",
      "Steps: 226 | Train Loss: 0.0117886 Vali Loss: 0.0237567 Test Loss: 0.0268710\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0099479\n",
      "\tspeed: 0.0928s/iter; left time: 2025.6468s\n",
      "\titers: 200, epoch: 4 | loss: 0.0083373\n",
      "\tspeed: 0.0488s/iter; left time: 1060.7452s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 226 | Train Loss: 0.0090726 Vali Loss: 0.0256334 Test Loss: 0.0279961\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0073438\n",
      "\tspeed: 0.0861s/iter; left time: 1859.5452s\n",
      "\titers: 200, epoch: 5 | loss: 0.0066576\n",
      "\tspeed: 0.0477s/iter; left time: 1025.7056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.09s\n",
      "Steps: 226 | Train Loss: 0.0069540 Vali Loss: 0.0259422 Test Loss: 0.0284075\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0058564\n",
      "\tspeed: 0.0965s/iter; left time: 2062.8319s\n",
      "\titers: 200, epoch: 6 | loss: 0.0050857\n",
      "\tspeed: 0.0493s/iter; left time: 1048.4800s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.29s\n",
      "Steps: 226 | Train Loss: 0.0056569 Vali Loss: 0.0269273 Test Loss: 0.0289203\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0046096\n",
      "\tspeed: 0.0916s/iter; left time: 1936.9642s\n",
      "\titers: 200, epoch: 7 | loss: 0.0043860\n",
      "\tspeed: 0.0493s/iter; left time: 1037.7382s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 226 | Train Loss: 0.0047801 Vali Loss: 0.0265904 Test Loss: 0.0292324\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0042168\n",
      "\tspeed: 0.0932s/iter; left time: 1949.7598s\n",
      "\titers: 200, epoch: 8 | loss: 0.0038549\n",
      "\tspeed: 0.0497s/iter; left time: 1034.4960s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 226 | Train Loss: 0.0042179 Vali Loss: 0.0266252 Test Loss: 0.0286232\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0042412\n",
      "\tspeed: 0.0944s/iter; left time: 1952.3947s\n",
      "\titers: 200, epoch: 9 | loss: 0.0034975\n",
      "\tspeed: 0.0483s/iter; left time: 994.2371s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.30s\n",
      "Steps: 226 | Train Loss: 0.0037897 Vali Loss: 0.0266672 Test Loss: 0.0288044\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0034196\n",
      "\tspeed: 0.0930s/iter; left time: 1902.4802s\n",
      "\titers: 200, epoch: 10 | loss: 0.0032693\n",
      "\tspeed: 0.0504s/iter; left time: 1026.5745s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.57s\n",
      "Steps: 226 | Train Loss: 0.0034596 Vali Loss: 0.0265314 Test Loss: 0.0287724\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0032838\n",
      "\tspeed: 0.0972s/iter; left time: 1966.9264s\n",
      "\titers: 200, epoch: 11 | loss: 0.0030819\n",
      "\tspeed: 0.0509s/iter; left time: 1025.8333s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.78s\n",
      "Steps: 226 | Train Loss: 0.0032145 Vali Loss: 0.0266764 Test Loss: 0.0286681\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0028341\n",
      "\tspeed: 0.0893s/iter; left time: 1786.6423s\n",
      "\titers: 200, epoch: 12 | loss: 0.0030430\n",
      "\tspeed: 0.0504s/iter; left time: 1003.7997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 226 | Train Loss: 0.0030139 Vali Loss: 0.0263912 Test Loss: 0.0286644\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_24_DE_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02336770109832287, rmse:0.1528649777173996, mae:0.1005844995379448, rse:0.5394816398620605\n",
      "Intermediate time for DE and pred_len 24: 00h:03m:01.06s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0332638\n",
      "\tspeed: 0.0791s/iter; left time: 1771.5424s\n",
      "\titers: 200, epoch: 1 | loss: 0.0300356\n",
      "\tspeed: 0.0502s/iter; left time: 1119.4530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.85s\n",
      "Steps: 225 | Train Loss: 0.0334871 Vali Loss: 0.0348219 Test Loss: 0.0412176\n",
      "Validation loss decreased (inf --> 0.034822).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0224506\n",
      "\tspeed: 0.0969s/iter; left time: 2149.5423s\n",
      "\titers: 200, epoch: 2 | loss: 0.0186753\n",
      "\tspeed: 0.0501s/iter; left time: 1106.3206s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0218892 Vali Loss: 0.0342691 Test Loss: 0.0442524\n",
      "Validation loss decreased (0.034822 --> 0.034269).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0144677\n",
      "\tspeed: 0.1107s/iter; left time: 2430.9855s\n",
      "\titers: 200, epoch: 3 | loss: 0.0123060\n",
      "\tspeed: 0.0490s/iter; left time: 1070.5410s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.55s\n",
      "Steps: 225 | Train Loss: 0.0147549 Vali Loss: 0.0370631 Test Loss: 0.0474851\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0108411\n",
      "\tspeed: 0.0979s/iter; left time: 2127.4352s\n",
      "\titers: 200, epoch: 4 | loss: 0.0097878\n",
      "\tspeed: 0.0483s/iter; left time: 1044.4061s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.39s\n",
      "Steps: 225 | Train Loss: 0.0106510 Vali Loss: 0.0379387 Test Loss: 0.0481010\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0081787\n",
      "\tspeed: 0.0963s/iter; left time: 2070.7255s\n",
      "\titers: 200, epoch: 5 | loss: 0.0077351\n",
      "\tspeed: 0.0461s/iter; left time: 986.1043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.12s\n",
      "Steps: 225 | Train Loss: 0.0083025 Vali Loss: 0.0373596 Test Loss: 0.0464266\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0069760\n",
      "\tspeed: 0.0920s/iter; left time: 1958.2338s\n",
      "\titers: 200, epoch: 6 | loss: 0.0065775\n",
      "\tspeed: 0.0505s/iter; left time: 1068.7359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.56s\n",
      "Steps: 225 | Train Loss: 0.0068311 Vali Loss: 0.0378817 Test Loss: 0.0463117\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0064354\n",
      "\tspeed: 0.0926s/iter; left time: 1949.2758s\n",
      "\titers: 200, epoch: 7 | loss: 0.0056448\n",
      "\tspeed: 0.0511s/iter; left time: 1070.0455s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.55s\n",
      "Steps: 225 | Train Loss: 0.0058802 Vali Loss: 0.0378618 Test Loss: 0.0464821\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0053293\n",
      "\tspeed: 0.1029s/iter; left time: 2143.9633s\n",
      "\titers: 200, epoch: 8 | loss: 0.0054496\n",
      "\tspeed: 0.0506s/iter; left time: 1048.4218s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.68s\n",
      "Steps: 225 | Train Loss: 0.0052071 Vali Loss: 0.0384322 Test Loss: 0.0474449\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0048091\n",
      "\tspeed: 0.0915s/iter; left time: 1884.1641s\n",
      "\titers: 200, epoch: 9 | loss: 0.0047850\n",
      "\tspeed: 0.0495s/iter; left time: 1013.8719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.43s\n",
      "Steps: 225 | Train Loss: 0.0046842 Vali Loss: 0.0378488 Test Loss: 0.0462881\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0044565\n",
      "\tspeed: 0.0980s/iter; left time: 1997.3757s\n",
      "\titers: 200, epoch: 10 | loss: 0.0038844\n",
      "\tspeed: 0.0497s/iter; left time: 1007.4344s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 225 | Train Loss: 0.0043068 Vali Loss: 0.0380081 Test Loss: 0.0465702\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0042491\n",
      "\tspeed: 0.0916s/iter; left time: 1845.9031s\n",
      "\titers: 200, epoch: 11 | loss: 0.0040506\n",
      "\tspeed: 0.0505s/iter; left time: 1013.1239s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.58s\n",
      "Steps: 225 | Train Loss: 0.0040233 Vali Loss: 0.0386904 Test Loss: 0.0469827\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0037805\n",
      "\tspeed: 0.0998s/iter; left time: 1988.0510s\n",
      "\titers: 200, epoch: 12 | loss: 0.0035495\n",
      "\tspeed: 0.0503s/iter; left time: 998.0157s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.54s\n",
      "Steps: 225 | Train Loss: 0.0037756 Vali Loss: 0.0378433 Test Loss: 0.0466710\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_96_DE_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.04425235465168953, rmse:0.21036243438720703, mae:0.14291603863239288, rse:0.744935929775238\n",
      "Intermediate time for DE and pred_len 96: 00h:03m:09.12s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='DE_168_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0361657\n",
      "\tspeed: 0.0668s/iter; left time: 1496.7550s\n",
      "\titers: 200, epoch: 1 | loss: 0.0303956\n",
      "\tspeed: 0.0504s/iter; left time: 1124.5588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.83s\n",
      "Steps: 225 | Train Loss: 0.0362478 Vali Loss: 0.0360635 Test Loss: 0.0436179\n",
      "Validation loss decreased (inf --> 0.036064).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0248308\n",
      "\tspeed: 0.1080s/iter; left time: 2394.2190s\n",
      "\titers: 200, epoch: 2 | loss: 0.0202720\n",
      "\tspeed: 0.0493s/iter; left time: 1088.1554s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:12.10s\n",
      "Steps: 225 | Train Loss: 0.0239077 Vali Loss: 0.0366986 Test Loss: 0.0456352\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0168566\n",
      "\tspeed: 0.0965s/iter; left time: 2119.1420s\n",
      "\titers: 200, epoch: 3 | loss: 0.0140941\n",
      "\tspeed: 0.0500s/iter; left time: 1091.5463s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.64s\n",
      "Steps: 225 | Train Loss: 0.0165090 Vali Loss: 0.0390133 Test Loss: 0.0496497\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0126213\n",
      "\tspeed: 0.1044s/iter; left time: 2268.5671s\n",
      "\titers: 200, epoch: 4 | loss: 0.0113191\n",
      "\tspeed: 0.0512s/iter; left time: 1106.5469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.81s\n",
      "Steps: 225 | Train Loss: 0.0123056 Vali Loss: 0.0389968 Test Loss: 0.0487190\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0098067\n",
      "\tspeed: 0.0983s/iter; left time: 2114.0404s\n",
      "\titers: 200, epoch: 5 | loss: 0.0088960\n",
      "\tspeed: 0.0515s/iter; left time: 1102.7192s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.79s\n",
      "Steps: 225 | Train Loss: 0.0096736 Vali Loss: 0.0396343 Test Loss: 0.0498428\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0080175\n",
      "\tspeed: 0.1043s/iter; left time: 2220.0994s\n",
      "\titers: 200, epoch: 6 | loss: 0.0072905\n",
      "\tspeed: 0.0519s/iter; left time: 1099.7504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.83s\n",
      "Steps: 225 | Train Loss: 0.0079707 Vali Loss: 0.0397890 Test Loss: 0.0493932\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0067940\n",
      "\tspeed: 0.0997s/iter; left time: 2098.0576s\n",
      "\titers: 200, epoch: 7 | loss: 0.0064623\n",
      "\tspeed: 0.0517s/iter; left time: 1084.0475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:12.12s\n",
      "Steps: 225 | Train Loss: 0.0069017 Vali Loss: 0.0401710 Test Loss: 0.0496219\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0063115\n",
      "\tspeed: 0.1011s/iter; left time: 2105.5356s\n",
      "\titers: 200, epoch: 8 | loss: 0.0056388\n",
      "\tspeed: 0.0510s/iter; left time: 1057.4030s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.01s\n",
      "Steps: 225 | Train Loss: 0.0060350 Vali Loss: 0.0401194 Test Loss: 0.0490433\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0054513\n",
      "\tspeed: 0.1060s/iter; left time: 2184.4789s\n",
      "\titers: 200, epoch: 9 | loss: 0.0053721\n",
      "\tspeed: 0.0559s/iter; left time: 1147.0101s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.40s\n",
      "Steps: 225 | Train Loss: 0.0054257 Vali Loss: 0.0398992 Test Loss: 0.0493147\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0049561\n",
      "\tspeed: 0.0967s/iter; left time: 1970.4992s\n",
      "\titers: 200, epoch: 10 | loss: 0.0048473\n",
      "\tspeed: 0.0504s/iter; left time: 1022.6361s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.71s\n",
      "Steps: 225 | Train Loss: 0.0049978 Vali Loss: 0.0399362 Test Loss: 0.0488129\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0048232\n",
      "\tspeed: 0.1024s/iter; left time: 2064.3837s\n",
      "\titers: 200, epoch: 11 | loss: 0.0045335\n",
      "\tspeed: 0.0513s/iter; left time: 1027.8097s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.98s\n",
      "Steps: 225 | Train Loss: 0.0046169 Vali Loss: 0.0395273 Test Loss: 0.0489856\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : DE_168_168_DE_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.0436178483068943, rmse:0.20884886384010315, mae:0.1491432934999466, rse:0.7397594451904297\n",
      "Intermediate time for DE and pred_len 168: 00h:02m:58.36s\n",
      "Intermediate time for DE: 00h:09m:08.55s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0218535\n",
      "\tspeed: 0.0798s/iter; left time: 1796.0078s\n",
      "\titers: 200, epoch: 1 | loss: 0.0183579\n",
      "\tspeed: 0.0503s/iter; left time: 1127.6273s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.70s\n",
      "Steps: 226 | Train Loss: 0.0221939 Vali Loss: 0.0227288 Test Loss: 0.0289717\n",
      "Validation loss decreased (inf --> 0.022729).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0139320\n",
      "\tspeed: 0.1053s/iter; left time: 2345.8819s\n",
      "\titers: 200, epoch: 2 | loss: 0.0140528\n",
      "\tspeed: 0.0499s/iter; left time: 1107.2148s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.49s\n",
      "Steps: 226 | Train Loss: 0.0145776 Vali Loss: 0.0207217 Test Loss: 0.0265528\n",
      "Validation loss decreased (0.022729 --> 0.020722).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0131557\n",
      "\tspeed: 0.0922s/iter; left time: 2033.5530s\n",
      "\titers: 200, epoch: 3 | loss: 0.0105842\n",
      "\tspeed: 0.0506s/iter; left time: 1109.6279s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.60s\n",
      "Steps: 226 | Train Loss: 0.0124672 Vali Loss: 0.0228512 Test Loss: 0.0287703\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0104724\n",
      "\tspeed: 0.0913s/iter; left time: 1992.0171s\n",
      "\titers: 200, epoch: 4 | loss: 0.0084868\n",
      "\tspeed: 0.0460s/iter; left time: 999.2377s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:10.65s\n",
      "Steps: 226 | Train Loss: 0.0099155 Vali Loss: 0.0245583 Test Loss: 0.0308842\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0084019\n",
      "\tspeed: 0.0905s/iter; left time: 1954.3459s\n",
      "\titers: 200, epoch: 5 | loss: 0.0073181\n",
      "\tspeed: 0.0482s/iter; left time: 1036.9286s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.05s\n",
      "Steps: 226 | Train Loss: 0.0077618 Vali Loss: 0.0245623 Test Loss: 0.0308459\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0060664\n",
      "\tspeed: 0.0866s/iter; left time: 1851.6961s\n",
      "\titers: 200, epoch: 6 | loss: 0.0058028\n",
      "\tspeed: 0.0477s/iter; left time: 1015.1849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:10.98s\n",
      "Steps: 226 | Train Loss: 0.0063329 Vali Loss: 0.0250764 Test Loss: 0.0328130\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0056493\n",
      "\tspeed: 0.0916s/iter; left time: 1935.8874s\n",
      "\titers: 200, epoch: 7 | loss: 0.0053948\n",
      "\tspeed: 0.0483s/iter; left time: 1015.6603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.21s\n",
      "Steps: 226 | Train Loss: 0.0054284 Vali Loss: 0.0253184 Test Loss: 0.0329063\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0044056\n",
      "\tspeed: 0.0885s/iter; left time: 1851.6830s\n",
      "\titers: 200, epoch: 8 | loss: 0.0045532\n",
      "\tspeed: 0.0468s/iter; left time: 974.7689s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 226 | Train Loss: 0.0048233 Vali Loss: 0.0254758 Test Loss: 0.0327160\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0040133\n",
      "\tspeed: 0.0913s/iter; left time: 1888.9773s\n",
      "\titers: 200, epoch: 9 | loss: 0.0042028\n",
      "\tspeed: 0.0497s/iter; left time: 1024.3168s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.44s\n",
      "Steps: 226 | Train Loss: 0.0044050 Vali Loss: 0.0256798 Test Loss: 0.0325368\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0039573\n",
      "\tspeed: 0.0908s/iter; left time: 1857.6843s\n",
      "\titers: 200, epoch: 10 | loss: 0.0041755\n",
      "\tspeed: 0.0490s/iter; left time: 997.5784s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.33s\n",
      "Steps: 226 | Train Loss: 0.0040621 Vali Loss: 0.0253594 Test Loss: 0.0324591\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0037444\n",
      "\tspeed: 0.0904s/iter; left time: 1830.0027s\n",
      "\titers: 200, epoch: 11 | loss: 0.0037421\n",
      "\tspeed: 0.0502s/iter; left time: 1011.4231s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.47s\n",
      "Steps: 226 | Train Loss: 0.0037959 Vali Loss: 0.0254270 Test Loss: 0.0329862\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0035340\n",
      "\tspeed: 0.0960s/iter; left time: 1922.0715s\n",
      "\titers: 200, epoch: 12 | loss: 0.0034475\n",
      "\tspeed: 0.0497s/iter; left time: 990.1470s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.53s\n",
      "Steps: 226 | Train Loss: 0.0035778 Vali Loss: 0.0253695 Test Loss: 0.0325746\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_24_GB_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.026552753522992134, rmse:0.16295015811920166, mae:0.10944870114326477, rse:0.5621318817138672\n",
      "Intermediate time for GB and pred_len 24: 00h:03m:02.28s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0288654\n",
      "\tspeed: 0.0655s/iter; left time: 1468.2221s\n",
      "\titers: 200, epoch: 1 | loss: 0.0257712\n",
      "\tspeed: 0.0481s/iter; left time: 1071.9359s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.49s\n",
      "Steps: 225 | Train Loss: 0.0295441 Vali Loss: 0.0322599 Test Loss: 0.0463921\n",
      "Validation loss decreased (inf --> 0.032260).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0204241\n",
      "\tspeed: 0.1087s/iter; left time: 2411.0143s\n",
      "\titers: 200, epoch: 2 | loss: 0.0181788\n",
      "\tspeed: 0.0489s/iter; left time: 1079.3847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.23s\n",
      "Steps: 225 | Train Loss: 0.0211684 Vali Loss: 0.0359143 Test Loss: 0.0490593\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0145193\n",
      "\tspeed: 0.0933s/iter; left time: 2048.7223s\n",
      "\titers: 200, epoch: 3 | loss: 0.0129500\n",
      "\tspeed: 0.0498s/iter; left time: 1089.0880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.41s\n",
      "Steps: 225 | Train Loss: 0.0148745 Vali Loss: 0.0380365 Test Loss: 0.0511157\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0108255\n",
      "\tspeed: 0.0910s/iter; left time: 1977.3957s\n",
      "\titers: 200, epoch: 4 | loss: 0.0103050\n",
      "\tspeed: 0.0481s/iter; left time: 1039.3682s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.17s\n",
      "Steps: 225 | Train Loss: 0.0110534 Vali Loss: 0.0386122 Test Loss: 0.0516633\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0083075\n",
      "\tspeed: 0.0987s/iter; left time: 2121.2651s\n",
      "\titers: 200, epoch: 5 | loss: 0.0079974\n",
      "\tspeed: 0.0550s/iter; left time: 1176.5774s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.17s\n",
      "Steps: 225 | Train Loss: 0.0087882 Vali Loss: 0.0387466 Test Loss: 0.0523526\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0085979\n",
      "\tspeed: 0.0950s/iter; left time: 2020.9800s\n",
      "\titers: 200, epoch: 6 | loss: 0.0070879\n",
      "\tspeed: 0.0497s/iter; left time: 1051.8523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.51s\n",
      "Steps: 225 | Train Loss: 0.0073469 Vali Loss: 0.0378753 Test Loss: 0.0519166\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0068098\n",
      "\tspeed: 0.0964s/iter; left time: 2029.8703s\n",
      "\titers: 200, epoch: 7 | loss: 0.0062420\n",
      "\tspeed: 0.0516s/iter; left time: 1080.4960s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.80s\n",
      "Steps: 225 | Train Loss: 0.0064157 Vali Loss: 0.0382622 Test Loss: 0.0509550\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0059088\n",
      "\tspeed: 0.0940s/iter; left time: 1957.8672s\n",
      "\titers: 200, epoch: 8 | loss: 0.0055143\n",
      "\tspeed: 0.0541s/iter; left time: 1121.3947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:12.05s\n",
      "Steps: 225 | Train Loss: 0.0057242 Vali Loss: 0.0377223 Test Loss: 0.0509155\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0057496\n",
      "\tspeed: 0.0990s/iter; left time: 2039.3611s\n",
      "\titers: 200, epoch: 9 | loss: 0.0049260\n",
      "\tspeed: 0.0509s/iter; left time: 1042.8847s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0052227 Vali Loss: 0.0377632 Test Loss: 0.0501606\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0051351\n",
      "\tspeed: 0.0910s/iter; left time: 1854.3741s\n",
      "\titers: 200, epoch: 10 | loss: 0.0045289\n",
      "\tspeed: 0.0529s/iter; left time: 1072.6055s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.88s\n",
      "Steps: 225 | Train Loss: 0.0048293 Vali Loss: 0.0375795 Test Loss: 0.0502433\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0044681\n",
      "\tspeed: 0.0955s/iter; left time: 1923.7912s\n",
      "\titers: 200, epoch: 11 | loss: 0.0045333\n",
      "\tspeed: 0.0506s/iter; left time: 1015.1115s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.96s\n",
      "Steps: 225 | Train Loss: 0.0044935 Vali Loss: 0.0373866 Test Loss: 0.0508047\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_96_GB_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.0463920496404171, rmse:0.21538813412189484, mae:0.1513613760471344, rse:0.7448421716690063\n",
      "Intermediate time for GB and pred_len 96: 00h:02m:50.20s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='GB_168_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0311655\n",
      "\tspeed: 0.0808s/iter; left time: 1811.0991s\n",
      "\titers: 200, epoch: 1 | loss: 0.0268520\n",
      "\tspeed: 0.0476s/iter; left time: 1062.3274s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:11.50s\n",
      "Steps: 225 | Train Loss: 0.0315050 Vali Loss: 0.0344559 Test Loss: 0.0494408\n",
      "Validation loss decreased (inf --> 0.034456).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0232563\n",
      "\tspeed: 0.1223s/iter; left time: 2712.4060s\n",
      "\titers: 200, epoch: 2 | loss: 0.0201319\n",
      "\tspeed: 0.0510s/iter; left time: 1125.2679s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.63s\n",
      "Steps: 225 | Train Loss: 0.0228723 Vali Loss: 0.0356305 Test Loss: 0.0490330\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0164316\n",
      "\tspeed: 0.0927s/iter; left time: 2035.5572s\n",
      "\titers: 200, epoch: 3 | loss: 0.0143577\n",
      "\tspeed: 0.0530s/iter; left time: 1157.0310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.93s\n",
      "Steps: 225 | Train Loss: 0.0164182 Vali Loss: 0.0385980 Test Loss: 0.0549292\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0119379\n",
      "\tspeed: 0.1062s/iter; left time: 2306.9108s\n",
      "\titers: 200, epoch: 4 | loss: 0.0117706\n",
      "\tspeed: 0.0515s/iter; left time: 1113.7389s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.76s\n",
      "Steps: 225 | Train Loss: 0.0123429 Vali Loss: 0.0391280 Test Loss: 0.0544823\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0101041\n",
      "\tspeed: 0.0958s/iter; left time: 2059.3053s\n",
      "\titers: 200, epoch: 5 | loss: 0.0096695\n",
      "\tspeed: 0.0529s/iter; left time: 1132.9288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:12.33s\n",
      "Steps: 225 | Train Loss: 0.0098903 Vali Loss: 0.0375995 Test Loss: 0.0534631\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0084312\n",
      "\tspeed: 0.1035s/iter; left time: 2202.9476s\n",
      "\titers: 200, epoch: 6 | loss: 0.0077957\n",
      "\tspeed: 0.0535s/iter; left time: 1132.6132s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:12.15s\n",
      "Steps: 225 | Train Loss: 0.0083932 Vali Loss: 0.0387505 Test Loss: 0.0536841\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0073348\n",
      "\tspeed: 0.0960s/iter; left time: 2019.8842s\n",
      "\titers: 200, epoch: 7 | loss: 0.0067890\n",
      "\tspeed: 0.0512s/iter; left time: 1072.1064s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:11.94s\n",
      "Steps: 225 | Train Loss: 0.0073746 Vali Loss: 0.0379722 Test Loss: 0.0532853\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0062798\n",
      "\tspeed: 0.0980s/iter; left time: 2040.6019s\n",
      "\titers: 200, epoch: 8 | loss: 0.0067243\n",
      "\tspeed: 0.0510s/iter; left time: 1057.5884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:11.74s\n",
      "Steps: 225 | Train Loss: 0.0065557 Vali Loss: 0.0379420 Test Loss: 0.0540116\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0063439\n",
      "\tspeed: 0.1052s/iter; left time: 2167.4040s\n",
      "\titers: 200, epoch: 9 | loss: 0.0058767\n",
      "\tspeed: 0.0527s/iter; left time: 1080.9459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:12.14s\n",
      "Steps: 225 | Train Loss: 0.0059587 Vali Loss: 0.0380186 Test Loss: 0.0534061\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0056824\n",
      "\tspeed: 0.0955s/iter; left time: 1945.2567s\n",
      "\titers: 200, epoch: 10 | loss: 0.0056616\n",
      "\tspeed: 0.0567s/iter; left time: 1149.8234s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:12.61s\n",
      "Steps: 225 | Train Loss: 0.0055202 Vali Loss: 0.0375020 Test Loss: 0.0525390\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0051364\n",
      "\tspeed: 0.1039s/iter; left time: 2093.7990s\n",
      "\titers: 200, epoch: 11 | loss: 0.0053312\n",
      "\tspeed: 0.0511s/iter; left time: 1024.1143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.96s\n",
      "Steps: 225 | Train Loss: 0.0051564 Vali Loss: 0.0378486 Test Loss: 0.0529896\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : GB_168_168_GB_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04944076016545296, rmse:0.2223527878522873, mae:0.15743298828601837, rse:0.7709290385246277\n",
      "Intermediate time for GB and pred_len 168: 00h:03m:02.16s\n",
      "Intermediate time for GB: 00h:08m:54.64s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0224124\n",
      "\tspeed: 0.0455s/iter; left time: 1024.1005s\n",
      "\titers: 200, epoch: 1 | loss: 0.0196115\n",
      "\tspeed: 0.0171s/iter; left time: 382.8865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0269212 Vali Loss: 0.0150415 Test Loss: 0.0203113\n",
      "Validation loss decreased (inf --> 0.015041).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0107510\n",
      "\tspeed: 0.0480s/iter; left time: 1068.4218s\n",
      "\titers: 200, epoch: 2 | loss: 0.0087262\n",
      "\tspeed: 0.0191s/iter; left time: 423.5362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0108435 Vali Loss: 0.0091117 Test Loss: 0.0119008\n",
      "Validation loss decreased (0.015041 --> 0.009112).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0088294\n",
      "\tspeed: 0.0500s/iter; left time: 1102.0087s\n",
      "\titers: 200, epoch: 3 | loss: 0.0100748\n",
      "\tspeed: 0.0236s/iter; left time: 518.3261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.86s\n",
      "Steps: 226 | Train Loss: 0.0088790 Vali Loss: 0.0088156 Test Loss: 0.0120870\n",
      "Validation loss decreased (0.009112 --> 0.008816).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0087583\n",
      "\tspeed: 0.0492s/iter; left time: 1073.7082s\n",
      "\titers: 200, epoch: 4 | loss: 0.0078426\n",
      "\tspeed: 0.0201s/iter; left time: 437.1992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.78s\n",
      "Steps: 226 | Train Loss: 0.0082961 Vali Loss: 0.0084341 Test Loss: 0.0115712\n",
      "Validation loss decreased (0.008816 --> 0.008434).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0082415\n",
      "\tspeed: 0.0533s/iter; left time: 1151.3912s\n",
      "\titers: 200, epoch: 5 | loss: 0.0077887\n",
      "\tspeed: 0.0164s/iter; left time: 352.8795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.57s\n",
      "Steps: 226 | Train Loss: 0.0077767 Vali Loss: 0.0086849 Test Loss: 0.0117333\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0078185\n",
      "\tspeed: 0.0463s/iter; left time: 988.9833s\n",
      "\titers: 200, epoch: 6 | loss: 0.0070359\n",
      "\tspeed: 0.0164s/iter; left time: 348.8947s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.35s\n",
      "Steps: 226 | Train Loss: 0.0073781 Vali Loss: 0.0087118 Test Loss: 0.0117897\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0074897\n",
      "\tspeed: 0.0465s/iter; left time: 982.5427s\n",
      "\titers: 200, epoch: 7 | loss: 0.0068850\n",
      "\tspeed: 0.0157s/iter; left time: 331.0021s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.03s\n",
      "Steps: 226 | Train Loss: 0.0069519 Vali Loss: 0.0086680 Test Loss: 0.0120939\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0069666\n",
      "\tspeed: 0.0477s/iter; left time: 997.2415s\n",
      "\titers: 200, epoch: 8 | loss: 0.0063023\n",
      "\tspeed: 0.0164s/iter; left time: 341.8813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.04s\n",
      "Steps: 226 | Train Loss: 0.0065128 Vali Loss: 0.0088269 Test Loss: 0.0124773\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0064918\n",
      "\tspeed: 0.0451s/iter; left time: 933.0429s\n",
      "\titers: 200, epoch: 9 | loss: 0.0061917\n",
      "\tspeed: 0.0164s/iter; left time: 337.4504s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 226 | Train Loss: 0.0061017 Vali Loss: 0.0089658 Test Loss: 0.0123164\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0054921\n",
      "\tspeed: 0.0469s/iter; left time: 959.8101s\n",
      "\titers: 200, epoch: 10 | loss: 0.0062819\n",
      "\tspeed: 0.0219s/iter; left time: 445.3181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 226 | Train Loss: 0.0057397 Vali Loss: 0.0093366 Test Loss: 0.0131734\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0056405\n",
      "\tspeed: 0.0430s/iter; left time: 870.6743s\n",
      "\titers: 200, epoch: 11 | loss: 0.0046495\n",
      "\tspeed: 0.0235s/iter; left time: 472.7532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.80s\n",
      "Steps: 226 | Train Loss: 0.0054258 Vali Loss: 0.0093886 Test Loss: 0.0132575\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0050808\n",
      "\tspeed: 0.0436s/iter; left time: 871.9838s\n",
      "\titers: 200, epoch: 12 | loss: 0.0050846\n",
      "\tspeed: 0.0249s/iter; left time: 495.5559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 226 | Train Loss: 0.0051052 Vali Loss: 0.0094989 Test Loss: 0.0135827\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0045714\n",
      "\tspeed: 0.0455s/iter; left time: 900.1137s\n",
      "\titers: 200, epoch: 13 | loss: 0.0046720\n",
      "\tspeed: 0.0236s/iter; left time: 464.5946s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.50s\n",
      "Steps: 226 | Train Loss: 0.0048812 Vali Loss: 0.0094084 Test Loss: 0.0140043\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0046412\n",
      "\tspeed: 0.0451s/iter; left time: 881.5763s\n",
      "\titers: 200, epoch: 14 | loss: 0.0043698\n",
      "\tspeed: 0.0204s/iter; left time: 396.6475s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.77s\n",
      "Steps: 226 | Train Loss: 0.0046494 Vali Loss: 0.0096403 Test Loss: 0.0141297\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_24_ES_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01157116424292326, rmse:0.10756934434175491, mae:0.06930112838745117, rse:0.31656357645988464\n",
      "Intermediate time for ES and pred_len 24: 00h:01m:44.69s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0274620\n",
      "\tspeed: 0.0403s/iter; left time: 903.6093s\n",
      "\titers: 200, epoch: 1 | loss: 0.0233340\n",
      "\tspeed: 0.0168s/iter; left time: 373.9195s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.16s\n",
      "Steps: 225 | Train Loss: 0.0311444 Vali Loss: 0.0205581 Test Loss: 0.0278780\n",
      "Validation loss decreased (inf --> 0.020558).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0156541\n",
      "\tspeed: 0.0540s/iter; left time: 1198.5014s\n",
      "\titers: 200, epoch: 2 | loss: 0.0156230\n",
      "\tspeed: 0.0168s/iter; left time: 371.3417s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.84s\n",
      "Steps: 225 | Train Loss: 0.0170196 Vali Loss: 0.0159781 Test Loss: 0.0208035\n",
      "Validation loss decreased (0.020558 --> 0.015978).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0142626\n",
      "\tspeed: 0.0513s/iter; left time: 1126.3920s\n",
      "\titers: 200, epoch: 3 | loss: 0.0143844\n",
      "\tspeed: 0.0165s/iter; left time: 360.5415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 225 | Train Loss: 0.0147565 Vali Loss: 0.0153022 Test Loss: 0.0208395\n",
      "Validation loss decreased (0.015978 --> 0.015302).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0129233\n",
      "\tspeed: 0.0488s/iter; left time: 1059.6288s\n",
      "\titers: 200, epoch: 4 | loss: 0.0128198\n",
      "\tspeed: 0.0164s/iter; left time: 354.0143s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 225 | Train Loss: 0.0131599 Vali Loss: 0.0156655 Test Loss: 0.0221003\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0118474\n",
      "\tspeed: 0.0515s/iter; left time: 1107.4777s\n",
      "\titers: 200, epoch: 5 | loss: 0.0098400\n",
      "\tspeed: 0.0174s/iter; left time: 372.8807s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.54s\n",
      "Steps: 225 | Train Loss: 0.0113209 Vali Loss: 0.0163465 Test Loss: 0.0240946\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0092215\n",
      "\tspeed: 0.0470s/iter; left time: 999.8698s\n",
      "\titers: 200, epoch: 6 | loss: 0.0096176\n",
      "\tspeed: 0.0162s/iter; left time: 342.3958s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.46s\n",
      "Steps: 225 | Train Loss: 0.0098619 Vali Loss: 0.0163042 Test Loss: 0.0241483\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0097937\n",
      "\tspeed: 0.0499s/iter; left time: 1050.0762s\n",
      "\titers: 200, epoch: 7 | loss: 0.0080033\n",
      "\tspeed: 0.0164s/iter; left time: 343.5197s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.32s\n",
      "Steps: 225 | Train Loss: 0.0087896 Vali Loss: 0.0167270 Test Loss: 0.0244736\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0076690\n",
      "\tspeed: 0.0485s/iter; left time: 1010.0910s\n",
      "\titers: 200, epoch: 8 | loss: 0.0073171\n",
      "\tspeed: 0.0171s/iter; left time: 355.4400s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0080117 Vali Loss: 0.0169650 Test Loss: 0.0243798\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0070846\n",
      "\tspeed: 0.0461s/iter; left time: 950.3532s\n",
      "\titers: 200, epoch: 9 | loss: 0.0075421\n",
      "\tspeed: 0.0170s/iter; left time: 349.1597s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.58s\n",
      "Steps: 225 | Train Loss: 0.0073967 Vali Loss: 0.0172498 Test Loss: 0.0247350\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0073015\n",
      "\tspeed: 0.0469s/iter; left time: 955.3955s\n",
      "\titers: 200, epoch: 10 | loss: 0.0065840\n",
      "\tspeed: 0.0163s/iter; left time: 330.6696s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.68s\n",
      "Steps: 225 | Train Loss: 0.0069268 Vali Loss: 0.0177244 Test Loss: 0.0248713\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0064723\n",
      "\tspeed: 0.0462s/iter; left time: 931.3291s\n",
      "\titers: 200, epoch: 11 | loss: 0.0066135\n",
      "\tspeed: 0.0164s/iter; left time: 329.7885s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.82s\n",
      "Steps: 225 | Train Loss: 0.0065312 Vali Loss: 0.0173557 Test Loss: 0.0249669\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0063455\n",
      "\tspeed: 0.0480s/iter; left time: 956.3885s\n",
      "\titers: 200, epoch: 12 | loss: 0.0061154\n",
      "\tspeed: 0.0162s/iter; left time: 321.4485s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 225 | Train Loss: 0.0062166 Vali Loss: 0.0179732 Test Loss: 0.0252495\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0060675\n",
      "\tspeed: 0.0479s/iter; left time: 943.5563s\n",
      "\titers: 200, epoch: 13 | loss: 0.0061311\n",
      "\tspeed: 0.0177s/iter; left time: 346.6826s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.90s\n",
      "Steps: 225 | Train Loss: 0.0059431 Vali Loss: 0.0178869 Test Loss: 0.0250740\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_96_ES_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.020839489996433258, rmse:0.14435888826847076, mae:0.09703951328992844, rse:0.42408299446105957\n",
      "Intermediate time for ES and pred_len 96: 00h:01m:33.04s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_168_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0277074\n",
      "\tspeed: 0.0478s/iter; left time: 1070.9026s\n",
      "\titers: 200, epoch: 1 | loss: 0.0247151\n",
      "\tspeed: 0.0247s/iter; left time: 551.4464s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:05.26s\n",
      "Steps: 225 | Train Loss: 0.0327857 Vali Loss: 0.0228405 Test Loss: 0.0301658\n",
      "Validation loss decreased (inf --> 0.022841).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0182268\n",
      "\tspeed: 0.0600s/iter; left time: 1330.2683s\n",
      "\titers: 200, epoch: 2 | loss: 0.0183971\n",
      "\tspeed: 0.0170s/iter; left time: 375.2161s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.79s\n",
      "Steps: 225 | Train Loss: 0.0186799 Vali Loss: 0.0179325 Test Loss: 0.0228984\n",
      "Validation loss decreased (0.022841 --> 0.017933).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0152040\n",
      "\tspeed: 0.0564s/iter; left time: 1237.5159s\n",
      "\titers: 200, epoch: 3 | loss: 0.0145845\n",
      "\tspeed: 0.0168s/iter; left time: 367.2814s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.89s\n",
      "Steps: 225 | Train Loss: 0.0160117 Vali Loss: 0.0180276 Test Loss: 0.0232034\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0137587\n",
      "\tspeed: 0.0492s/iter; left time: 1069.3646s\n",
      "\titers: 200, epoch: 4 | loss: 0.0131766\n",
      "\tspeed: 0.0162s/iter; left time: 350.1691s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.73s\n",
      "Steps: 225 | Train Loss: 0.0140758 Vali Loss: 0.0176767 Test Loss: 0.0228745\n",
      "Validation loss decreased (0.017933 --> 0.017677).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0129491\n",
      "\tspeed: 0.0508s/iter; left time: 1091.4610s\n",
      "\titers: 200, epoch: 5 | loss: 0.0117705\n",
      "\tspeed: 0.0172s/iter; left time: 368.1131s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 225 | Train Loss: 0.0123849 Vali Loss: 0.0186744 Test Loss: 0.0246194\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0110090\n",
      "\tspeed: 0.0491s/iter; left time: 1044.3217s\n",
      "\titers: 200, epoch: 6 | loss: 0.0100024\n",
      "\tspeed: 0.0163s/iter; left time: 345.8416s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.60s\n",
      "Steps: 225 | Train Loss: 0.0109879 Vali Loss: 0.0187169 Test Loss: 0.0243321\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0105492\n",
      "\tspeed: 0.0516s/iter; left time: 1087.0034s\n",
      "\titers: 200, epoch: 7 | loss: 0.0092284\n",
      "\tspeed: 0.0179s/iter; left time: 375.0773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.87s\n",
      "Steps: 225 | Train Loss: 0.0099143 Vali Loss: 0.0193814 Test Loss: 0.0243870\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0092586\n",
      "\tspeed: 0.0525s/iter; left time: 1094.0169s\n",
      "\titers: 200, epoch: 8 | loss: 0.0089153\n",
      "\tspeed: 0.0168s/iter; left time: 349.1782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 225 | Train Loss: 0.0090489 Vali Loss: 0.0190883 Test Loss: 0.0252209\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0082056\n",
      "\tspeed: 0.0545s/iter; left time: 1123.7317s\n",
      "\titers: 200, epoch: 9 | loss: 0.0080735\n",
      "\tspeed: 0.0174s/iter; left time: 356.2632s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 225 | Train Loss: 0.0083603 Vali Loss: 0.0194013 Test Loss: 0.0252273\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0082202\n",
      "\tspeed: 0.0549s/iter; left time: 1119.6085s\n",
      "\titers: 200, epoch: 10 | loss: 0.0073720\n",
      "\tspeed: 0.0169s/iter; left time: 343.1842s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.23s\n",
      "Steps: 225 | Train Loss: 0.0078443 Vali Loss: 0.0196141 Test Loss: 0.0257372\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0069887\n",
      "\tspeed: 0.0570s/iter; left time: 1148.6955s\n",
      "\titers: 200, epoch: 11 | loss: 0.0071465\n",
      "\tspeed: 0.0172s/iter; left time: 343.9165s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.67s\n",
      "Steps: 225 | Train Loss: 0.0073931 Vali Loss: 0.0199717 Test Loss: 0.0256369\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0070513\n",
      "\tspeed: 0.0495s/iter; left time: 986.7180s\n",
      "\titers: 200, epoch: 12 | loss: 0.0067190\n",
      "\tspeed: 0.0182s/iter; left time: 360.8075s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.45s\n",
      "Steps: 225 | Train Loss: 0.0070412 Vali Loss: 0.0200850 Test Loss: 0.0260791\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0065328\n",
      "\tspeed: 0.0513s/iter; left time: 1010.1836s\n",
      "\titers: 200, epoch: 13 | loss: 0.0064198\n",
      "\tspeed: 0.0168s/iter; left time: 329.5841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.41s\n",
      "Steps: 225 | Train Loss: 0.0067512 Vali Loss: 0.0202014 Test Loss: 0.0263437\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0063799\n",
      "\tspeed: 0.0555s/iter; left time: 1080.7838s\n",
      "\titers: 200, epoch: 14 | loss: 0.0066022\n",
      "\tspeed: 0.0211s/iter; left time: 408.7836s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:05.02s\n",
      "Steps: 225 | Train Loss: 0.0064871 Vali Loss: 0.0201594 Test Loss: 0.0262867\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_168_168_ES_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.02287455089390278, rmse:0.15124334394931793, mae:0.10218845307826996, rse:0.4443393349647522\n",
      "Intermediate time for ES and pred_len 168: 00h:01m:52.09s\n",
      "Intermediate time for ES: 00h:05m:09.83s\n",
      "Total time: 00h:23m:13.02s\n"
     ]
    }
   ],
   "source": [
    "countries = ['DE', 'GB', 'ES']\n",
    "num_cols = [5, 5, 3]\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_168.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            seq_len=168\n",
    "\n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.1006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0443</td>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.2088</td>\n",
       "      <td>0.1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.0970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.1574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0234  0.1529  0.1006\n",
       "        96        0.0443  0.2104  0.1429\n",
       "        168       0.0436  0.2088  0.1491\n",
       "ES      24        0.0116  0.1076  0.0693\n",
       "        96        0.0208  0.1444  0.0970\n",
       "        168       0.0229  0.1512  0.1022\n",
       "GB      24        0.0266  0.1630  0.1094\n",
       "        96        0.0464  0.2154  0.1514\n",
       "        168       0.0494  0.2224  0.1574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_168.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_24_ES_PatchTST_custom_ftM_sl96_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0267922\n",
      "\tspeed: 0.0439s/iter; left time: 987.8115s\n",
      "\titers: 200, epoch: 1 | loss: 0.0195768\n",
      "\tspeed: 0.0163s/iter; left time: 364.5006s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.59s\n",
      "Steps: 226 | Train Loss: 0.0295997 Vali Loss: 0.0166912 Test Loss: 0.0213410\n",
      "Validation loss decreased (inf --> 0.016691).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0122195\n",
      "\tspeed: 0.0459s/iter; left time: 1023.3861s\n",
      "\titers: 200, epoch: 2 | loss: 0.0109789\n",
      "\tspeed: 0.0194s/iter; left time: 430.6559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0117965 Vali Loss: 0.0094065 Test Loss: 0.0123087\n",
      "Validation loss decreased (0.016691 --> 0.009406).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0093296\n",
      "\tspeed: 0.0429s/iter; left time: 946.4987s\n",
      "\titers: 200, epoch: 3 | loss: 0.0094486\n",
      "\tspeed: 0.0200s/iter; left time: 439.9887s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:04.65s\n",
      "Steps: 226 | Train Loss: 0.0094586 Vali Loss: 0.0088756 Test Loss: 0.0117917\n",
      "Validation loss decreased (0.009406 --> 0.008876).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0093523\n",
      "\tspeed: 0.0430s/iter; left time: 937.8723s\n",
      "\titers: 200, epoch: 4 | loss: 0.0088712\n",
      "\tspeed: 0.0234s/iter; left time: 508.1298s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:04.63s\n",
      "Steps: 226 | Train Loss: 0.0088098 Vali Loss: 0.0086944 Test Loss: 0.0116282\n",
      "Validation loss decreased (0.008876 --> 0.008694).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0086567\n",
      "\tspeed: 0.0417s/iter; left time: 900.3805s\n",
      "\titers: 200, epoch: 5 | loss: 0.0069733\n",
      "\tspeed: 0.0227s/iter; left time: 486.9472s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:04.51s\n",
      "Steps: 226 | Train Loss: 0.0083599 Vali Loss: 0.0086526 Test Loss: 0.0115494\n",
      "Validation loss decreased (0.008694 --> 0.008653).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0081536\n",
      "\tspeed: 0.0370s/iter; left time: 790.9505s\n",
      "\titers: 200, epoch: 6 | loss: 0.0069282\n",
      "\tspeed: 0.0229s/iter; left time: 487.4311s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:04.51s\n",
      "Steps: 226 | Train Loss: 0.0080306 Vali Loss: 0.0084352 Test Loss: 0.0112584\n",
      "Validation loss decreased (0.008653 --> 0.008435).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0076914\n",
      "\tspeed: 0.0388s/iter; left time: 819.5598s\n",
      "\titers: 200, epoch: 7 | loss: 0.0077376\n",
      "\tspeed: 0.0221s/iter; left time: 466.1116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:04.53s\n",
      "Steps: 226 | Train Loss: 0.0077208 Vali Loss: 0.0084026 Test Loss: 0.0113544\n",
      "Validation loss decreased (0.008435 --> 0.008403).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0075021\n",
      "\tspeed: 0.0387s/iter; left time: 808.9591s\n",
      "\titers: 200, epoch: 8 | loss: 0.0080163\n",
      "\tspeed: 0.0236s/iter; left time: 491.3997s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 226 | Train Loss: 0.0074480 Vali Loss: 0.0085301 Test Loss: 0.0116779\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0070147\n",
      "\tspeed: 0.0357s/iter; left time: 738.4469s\n",
      "\titers: 200, epoch: 9 | loss: 0.0072123\n",
      "\tspeed: 0.0222s/iter; left time: 456.5276s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:04.48s\n",
      "Steps: 226 | Train Loss: 0.0071865 Vali Loss: 0.0083577 Test Loss: 0.0115945\n",
      "Validation loss decreased (0.008403 --> 0.008358).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0069964\n",
      "\tspeed: 0.0389s/iter; left time: 796.8201s\n",
      "\titers: 200, epoch: 10 | loss: 0.0068249\n",
      "\tspeed: 0.0227s/iter; left time: 462.1933s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:04.62s\n",
      "Steps: 226 | Train Loss: 0.0069206 Vali Loss: 0.0087676 Test Loss: 0.0121050\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0071934\n",
      "\tspeed: 0.0353s/iter; left time: 714.0918s\n",
      "\titers: 200, epoch: 11 | loss: 0.0065778\n",
      "\tspeed: 0.0245s/iter; left time: 493.0897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:04.67s\n",
      "Steps: 226 | Train Loss: 0.0066644 Vali Loss: 0.0085964 Test Loss: 0.0120603\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0072954\n",
      "\tspeed: 0.0358s/iter; left time: 717.4733s\n",
      "\titers: 200, epoch: 12 | loss: 0.0067187\n",
      "\tspeed: 0.0238s/iter; left time: 473.7325s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:04.69s\n",
      "Steps: 226 | Train Loss: 0.0064917 Vali Loss: 0.0086457 Test Loss: 0.0120428\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0058575\n",
      "\tspeed: 0.0363s/iter; left time: 717.7930s\n",
      "\titers: 200, epoch: 13 | loss: 0.0061792\n",
      "\tspeed: 0.0240s/iter; left time: 473.4328s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:04.75s\n",
      "Steps: 226 | Train Loss: 0.0062382 Vali Loss: 0.0085987 Test Loss: 0.0123390\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0060048\n",
      "\tspeed: 0.0364s/iter; left time: 712.2198s\n",
      "\titers: 200, epoch: 14 | loss: 0.0065187\n",
      "\tspeed: 0.0227s/iter; left time: 441.4586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:04.61s\n",
      "Steps: 226 | Train Loss: 0.0060652 Vali Loss: 0.0087850 Test Loss: 0.0122426\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0059778\n",
      "\tspeed: 0.0344s/iter; left time: 666.0888s\n",
      "\titers: 200, epoch: 15 | loss: 0.0059392\n",
      "\tspeed: 0.0227s/iter; left time: 435.9288s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:04.52s\n",
      "Steps: 226 | Train Loss: 0.0058822 Vali Loss: 0.0089802 Test Loss: 0.0124355\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0056377\n",
      "\tspeed: 0.0376s/iter; left time: 718.4307s\n",
      "\titers: 200, epoch: 16 | loss: 0.0053625\n",
      "\tspeed: 0.0203s/iter; left time: 385.9553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:04.74s\n",
      "Steps: 226 | Train Loss: 0.0057311 Vali Loss: 0.0089195 Test Loss: 0.0126711\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0050651\n",
      "\tspeed: 0.0386s/iter; left time: 729.7615s\n",
      "\titers: 200, epoch: 17 | loss: 0.0050484\n",
      "\tspeed: 0.0187s/iter; left time: 351.8201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:04.40s\n",
      "Steps: 226 | Train Loss: 0.0055950 Vali Loss: 0.0089465 Test Loss: 0.0125136\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0048361\n",
      "\tspeed: 0.0381s/iter; left time: 711.0792s\n",
      "\titers: 200, epoch: 18 | loss: 0.0054747\n",
      "\tspeed: 0.0163s/iter; left time: 302.5829s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:04.17s\n",
      "Steps: 226 | Train Loss: 0.0054673 Vali Loss: 0.0090529 Test Loss: 0.0126415\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0062222\n",
      "\tspeed: 0.0500s/iter; left time: 921.1105s\n",
      "\titers: 200, epoch: 19 | loss: 0.0052610\n",
      "\tspeed: 0.0245s/iter; left time: 448.9730s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 226 | Train Loss: 0.0053649 Vali Loss: 0.0089901 Test Loss: 0.0127423\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_24_ES_PatchTST_custom_ftM_sl96_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.011594521813094616, rmse:0.10767786204814911, mae:0.06868013739585876, rse:0.3168829381465912\n",
      "Intermediate time for ES and pred_len 24: 00h:02m:06.00s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_96_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_96_ES_PatchTST_custom_ftM_sl96_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0343245\n",
      "\tspeed: 0.0541s/iter; left time: 1218.3482s\n",
      "\titers: 200, epoch: 1 | loss: 0.0278460\n",
      "\tspeed: 0.0238s/iter; left time: 532.3629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 226 | Train Loss: 0.0365318 Vali Loss: 0.0239262 Test Loss: 0.0309951\n",
      "Validation loss decreased (inf --> 0.023926).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0185223\n",
      "\tspeed: 0.0487s/iter; left time: 1085.7438s\n",
      "\titers: 200, epoch: 2 | loss: 0.0166251\n",
      "\tspeed: 0.0245s/iter; left time: 542.2201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.91s\n",
      "Steps: 226 | Train Loss: 0.0192041 Vali Loss: 0.0168355 Test Loss: 0.0217300\n",
      "Validation loss decreased (0.023926 --> 0.016835).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0160262\n",
      "\tspeed: 0.0466s/iter; left time: 1028.2645s\n",
      "\titers: 200, epoch: 3 | loss: 0.0156437\n",
      "\tspeed: 0.0268s/iter; left time: 588.3309s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 226 | Train Loss: 0.0163757 Vali Loss: 0.0161367 Test Loss: 0.0212674\n",
      "Validation loss decreased (0.016835 --> 0.016137).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0143373\n",
      "\tspeed: 0.0502s/iter; left time: 1095.8093s\n",
      "\titers: 200, epoch: 4 | loss: 0.0144804\n",
      "\tspeed: 0.0275s/iter; left time: 596.7106s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0152966 Vali Loss: 0.0164544 Test Loss: 0.0214458\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0147964\n",
      "\tspeed: 0.0493s/iter; left time: 1065.0733s\n",
      "\titers: 200, epoch: 5 | loss: 0.0139811\n",
      "\tspeed: 0.0281s/iter; left time: 604.9082s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.31s\n",
      "Steps: 226 | Train Loss: 0.0141186 Vali Loss: 0.0169926 Test Loss: 0.0220698\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0130579\n",
      "\tspeed: 0.0502s/iter; left time: 1071.9956s\n",
      "\titers: 200, epoch: 6 | loss: 0.0136196\n",
      "\tspeed: 0.0255s/iter; left time: 541.5579s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.27s\n",
      "Steps: 226 | Train Loss: 0.0131513 Vali Loss: 0.0174595 Test Loss: 0.0227895\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0127381\n",
      "\tspeed: 0.0504s/iter; left time: 1065.9463s\n",
      "\titers: 200, epoch: 7 | loss: 0.0121172\n",
      "\tspeed: 0.0274s/iter; left time: 577.3366s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.60s\n",
      "Steps: 226 | Train Loss: 0.0122806 Vali Loss: 0.0182827 Test Loss: 0.0238469\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0118271\n",
      "\tspeed: 0.0505s/iter; left time: 1057.4361s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116580\n",
      "\tspeed: 0.0279s/iter; left time: 580.2713s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.30s\n",
      "Steps: 226 | Train Loss: 0.0115656 Vali Loss: 0.0178909 Test Loss: 0.0232985\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0110704\n",
      "\tspeed: 0.0488s/iter; left time: 1010.4045s\n",
      "\titers: 200, epoch: 9 | loss: 0.0109119\n",
      "\tspeed: 0.0255s/iter; left time: 524.4640s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.38s\n",
      "Steps: 226 | Train Loss: 0.0109450 Vali Loss: 0.0181275 Test Loss: 0.0236892\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0099459\n",
      "\tspeed: 0.0482s/iter; left time: 985.9528s\n",
      "\titers: 200, epoch: 10 | loss: 0.0107087\n",
      "\tspeed: 0.0228s/iter; left time: 465.0159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.83s\n",
      "Steps: 226 | Train Loss: 0.0104373 Vali Loss: 0.0185834 Test Loss: 0.0238976\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0096758\n",
      "\tspeed: 0.0480s/iter; left time: 971.7841s\n",
      "\titers: 200, epoch: 11 | loss: 0.0101523\n",
      "\tspeed: 0.0268s/iter; left time: 540.6212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 226 | Train Loss: 0.0100447 Vali Loss: 0.0185717 Test Loss: 0.0243262\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0096044\n",
      "\tspeed: 0.0490s/iter; left time: 981.5633s\n",
      "\titers: 200, epoch: 12 | loss: 0.0093817\n",
      "\tspeed: 0.0264s/iter; left time: 524.7612s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 226 | Train Loss: 0.0096178 Vali Loss: 0.0189246 Test Loss: 0.0249068\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0095338\n",
      "\tspeed: 0.0471s/iter; left time: 932.7408s\n",
      "\titers: 200, epoch: 13 | loss: 0.0096964\n",
      "\tspeed: 0.0236s/iter; left time: 465.1052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:05.77s\n",
      "Steps: 226 | Train Loss: 0.0093259 Vali Loss: 0.0187492 Test Loss: 0.0248805\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_96_ES_PatchTST_custom_ftM_sl96_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.02126738242805004, rmse:0.14583340287208557, mae:0.09827333688735962, rse:0.4284146726131439\n",
      "Intermediate time for ES and pred_len 96: 00h:01m:48.87s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ES_96_168_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=1, seq_len=96, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ES_96_168_ES_PatchTST_custom_ftM_sl96_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0353431\n",
      "\tspeed: 0.0452s/iter; left time: 1013.0117s\n",
      "\titers: 200, epoch: 1 | loss: 0.0268961\n",
      "\tspeed: 0.0226s/iter; left time: 504.3959s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:04.98s\n",
      "Steps: 225 | Train Loss: 0.0376616 Vali Loss: 0.0256392 Test Loss: 0.0331292\n",
      "Validation loss decreased (inf --> 0.025639).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0218692\n",
      "\tspeed: 0.0434s/iter; left time: 963.5274s\n",
      "\titers: 200, epoch: 2 | loss: 0.0194052\n",
      "\tspeed: 0.0259s/iter; left time: 571.7841s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.80s\n",
      "Steps: 225 | Train Loss: 0.0208068 Vali Loss: 0.0188844 Test Loss: 0.0242439\n",
      "Validation loss decreased (0.025639 --> 0.018884).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0169687\n",
      "\tspeed: 0.0490s/iter; left time: 1076.1243s\n",
      "\titers: 200, epoch: 3 | loss: 0.0190523\n",
      "\tspeed: 0.0219s/iter; left time: 479.3272s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:05.46s\n",
      "Steps: 225 | Train Loss: 0.0179892 Vali Loss: 0.0185537 Test Loss: 0.0235893\n",
      "Validation loss decreased (0.018884 --> 0.018554).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0169186\n",
      "\tspeed: 0.0498s/iter; left time: 1082.8328s\n",
      "\titers: 200, epoch: 4 | loss: 0.0175947\n",
      "\tspeed: 0.0211s/iter; left time: 455.6899s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:05.48s\n",
      "Steps: 225 | Train Loss: 0.0167937 Vali Loss: 0.0184703 Test Loss: 0.0235688\n",
      "Validation loss decreased (0.018554 --> 0.018470).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0160075\n",
      "\tspeed: 0.0458s/iter; left time: 984.4546s\n",
      "\titers: 200, epoch: 5 | loss: 0.0160217\n",
      "\tspeed: 0.0264s/iter; left time: 564.3883s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:05.76s\n",
      "Steps: 225 | Train Loss: 0.0157294 Vali Loss: 0.0188542 Test Loss: 0.0238975\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0145555\n",
      "\tspeed: 0.0511s/iter; left time: 1086.5256s\n",
      "\titers: 200, epoch: 6 | loss: 0.0143680\n",
      "\tspeed: 0.0217s/iter; left time: 458.8588s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:05.58s\n",
      "Steps: 225 | Train Loss: 0.0147236 Vali Loss: 0.0194211 Test Loss: 0.0247843\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0145234\n",
      "\tspeed: 0.0481s/iter; left time: 1011.5290s\n",
      "\titers: 200, epoch: 7 | loss: 0.0138711\n",
      "\tspeed: 0.0291s/iter; left time: 609.5635s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.57s\n",
      "Steps: 225 | Train Loss: 0.0138673 Vali Loss: 0.0195423 Test Loss: 0.0247844\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0137212\n",
      "\tspeed: 0.0466s/iter; left time: 971.0173s\n",
      "\titers: 200, epoch: 8 | loss: 0.0138608\n",
      "\tspeed: 0.0252s/iter; left time: 521.6015s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:05.65s\n",
      "Steps: 225 | Train Loss: 0.0131192 Vali Loss: 0.0199592 Test Loss: 0.0254863\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0122447\n",
      "\tspeed: 0.0462s/iter; left time: 951.0470s\n",
      "\titers: 200, epoch: 9 | loss: 0.0126505\n",
      "\tspeed: 0.0252s/iter; left time: 516.4415s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 225 | Train Loss: 0.0124965 Vali Loss: 0.0200359 Test Loss: 0.0256357\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0119075\n",
      "\tspeed: 0.0474s/iter; left time: 966.0763s\n",
      "\titers: 200, epoch: 10 | loss: 0.0112254\n",
      "\tspeed: 0.0265s/iter; left time: 537.4212s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:05.94s\n",
      "Steps: 225 | Train Loss: 0.0119626 Vali Loss: 0.0205519 Test Loss: 0.0257522\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0117490\n",
      "\tspeed: 0.0452s/iter; left time: 910.8598s\n",
      "\titers: 200, epoch: 11 | loss: 0.0122202\n",
      "\tspeed: 0.0285s/iter; left time: 571.8669s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0114586 Vali Loss: 0.0203659 Test Loss: 0.0264138\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0113394\n",
      "\tspeed: 0.0491s/iter; left time: 977.8944s\n",
      "\titers: 200, epoch: 12 | loss: 0.0116394\n",
      "\tspeed: 0.0282s/iter; left time: 558.4294s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0110902 Vali Loss: 0.0204172 Test Loss: 0.0262498\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0106933\n",
      "\tspeed: 0.0504s/iter; left time: 993.6019s\n",
      "\titers: 200, epoch: 13 | loss: 0.0108677\n",
      "\tspeed: 0.0267s/iter; left time: 523.3469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.36s\n",
      "Steps: 225 | Train Loss: 0.0107315 Vali Loss: 0.0206860 Test Loss: 0.0266960\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0113744\n",
      "\tspeed: 0.0525s/iter; left time: 1023.3682s\n",
      "\titers: 200, epoch: 14 | loss: 0.0098004\n",
      "\tspeed: 0.0235s/iter; left time: 455.5247s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 225 | Train Loss: 0.0104362 Vali Loss: 0.0206958 Test Loss: 0.0264268\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ES_96_168_ES_PatchTST_custom_ftM_sl96_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.023568790405988693, rmse:0.15352129936218262, mae:0.10385473817586899, rse:0.45103174448013306\n",
      "Intermediate time for ES and pred_len 168: 00h:01m:53.63s\n",
      "Intermediate time for ES: 00h:05m:48.49s\n",
      "Total time: 00h:05m:48.50s\n"
     ]
    }
   ],
   "source": [
    "countries = ['ES']\n",
    "num_cols = [3]\n",
    "\n",
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_channel_mixing_96.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "        \n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            seq_len=96\n",
    "\n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --channel_mixing 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.0687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.0983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>0.1039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                 CM                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "ES      24        0.0116  0.1077  0.0687\n",
       "        96        0.0213  0.1458  0.0983\n",
       "        168       0.0236  0.1535  0.1039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['CM'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_channel_mixing_96.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. No patching\n",
    "\n",
    "It runs more than 24 hours on 48GB GPU (1 country around 5-6 hours). Therefore I run it with portions. You can find full results in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: DE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_DE_336_24_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0260697\n",
      "\tspeed: 0.6588s/iter; left time: 2886.1990s\n",
      "\titers: 200, epoch: 1 | loss: 0.0226197\n",
      "\tspeed: 0.6370s/iter; left time: 2726.8262s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:02m:22.87s\n",
      "Steps: 224 | Train Loss: 0.0264336 Vali Loss: 0.0252878 Test Loss: 0.0268633\n",
      "Validation loss decreased (inf --> 0.025288).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0169845\n",
      "\tspeed: 1.0193s/iter; left time: 4237.0523s\n",
      "\titers: 200, epoch: 2 | loss: 0.0164148\n",
      "\tspeed: 0.6412s/iter; left time: 2601.3497s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:02m:23.44s\n",
      "Steps: 224 | Train Loss: 0.0174810 Vali Loss: 0.0230596 Test Loss: 0.0241275\n",
      "Validation loss decreased (0.025288 --> 0.023060).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0151019\n",
      "\tspeed: 1.0193s/iter; left time: 4009.0890s\n",
      "\titers: 200, epoch: 3 | loss: 0.0159202\n",
      "\tspeed: 0.6415s/iter; left time: 2459.0574s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:02m:23.49s\n",
      "Steps: 224 | Train Loss: 0.0157841 Vali Loss: 0.0217520 Test Loss: 0.0237190\n",
      "Validation loss decreased (0.023060 --> 0.021752).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0143405\n",
      "\tspeed: 1.0187s/iter; left time: 3778.3897s\n",
      "\titers: 200, epoch: 4 | loss: 0.0146519\n",
      "\tspeed: 0.6403s/iter; left time: 2311.0133s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:02m:23.35s\n",
      "Steps: 224 | Train Loss: 0.0149145 Vali Loss: 0.0211180 Test Loss: 0.0227146\n",
      "Validation loss decreased (0.021752 --> 0.021118).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0138595\n",
      "\tspeed: 1.0210s/iter; left time: 3558.1533s\n",
      "\titers: 200, epoch: 5 | loss: 0.0137615\n",
      "\tspeed: 0.6416s/iter; left time: 2171.9392s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:02m:23.50s\n",
      "Steps: 224 | Train Loss: 0.0143542 Vali Loss: 0.0204248 Test Loss: 0.0224354\n",
      "Validation loss decreased (0.021118 --> 0.020425).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0139675\n",
      "\tspeed: 1.0177s/iter; left time: 3318.6345s\n",
      "\titers: 200, epoch: 6 | loss: 0.0130971\n",
      "\tspeed: 0.6411s/iter; left time: 2026.5771s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:02m:23.52s\n",
      "Steps: 224 | Train Loss: 0.0141514 Vali Loss: 0.0209303 Test Loss: 0.0226694\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0136732\n",
      "\tspeed: 1.0213s/iter; left time: 3101.5674s\n",
      "\titers: 200, epoch: 7 | loss: 0.0139990\n",
      "\tspeed: 0.6420s/iter; left time: 1885.6039s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:02m:23.61s\n",
      "Steps: 224 | Train Loss: 0.0137967 Vali Loss: 0.0202653 Test Loss: 0.0221296\n",
      "Validation loss decreased (0.020425 --> 0.020265).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0119280\n",
      "\tspeed: 1.0194s/iter; left time: 2867.6396s\n",
      "\titers: 200, epoch: 8 | loss: 0.0139392\n",
      "\tspeed: 0.6416s/iter; left time: 1740.6284s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:02m:23.56s\n",
      "Steps: 224 | Train Loss: 0.0135108 Vali Loss: 0.0204270 Test Loss: 0.0220210\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0132262\n",
      "\tspeed: 1.0188s/iter; left time: 2637.7649s\n",
      "\titers: 200, epoch: 9 | loss: 0.0119706\n",
      "\tspeed: 0.6409s/iter; left time: 1595.1928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:02m:23.66s\n",
      "Steps: 224 | Train Loss: 0.0134475 Vali Loss: 0.0198315 Test Loss: 0.0221825\n",
      "Validation loss decreased (0.020265 --> 0.019832).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0126166\n",
      "\tspeed: 1.0196s/iter; left time: 2411.3822s\n",
      "\titers: 200, epoch: 10 | loss: 0.0121419\n",
      "\tspeed: 0.6417s/iter; left time: 1453.4828s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:02m:23.47s\n",
      "Steps: 224 | Train Loss: 0.0131525 Vali Loss: 0.0197556 Test Loss: 0.0219017\n",
      "Validation loss decreased (0.019832 --> 0.019756).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0105351\n",
      "\tspeed: 1.0181s/iter; left time: 2179.8383s\n",
      "\titers: 200, epoch: 11 | loss: 0.0138581\n",
      "\tspeed: 0.6413s/iter; left time: 1308.9589s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:02m:23.55s\n",
      "Steps: 224 | Train Loss: 0.0130410 Vali Loss: 0.0201910 Test Loss: 0.0224620\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0142836\n",
      "\tspeed: 1.0174s/iter; left time: 1950.4219s\n",
      "\titers: 200, epoch: 12 | loss: 0.0137541\n",
      "\tspeed: 0.6403s/iter; left time: 1163.3357s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:02m:23.30s\n",
      "Steps: 224 | Train Loss: 0.0129711 Vali Loss: 0.0199388 Test Loss: 0.0222619\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0135022\n",
      "\tspeed: 1.0184s/iter; left time: 1724.2212s\n",
      "\titers: 200, epoch: 13 | loss: 0.0121987\n",
      "\tspeed: 0.6409s/iter; left time: 1020.8900s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:02m:23.39s\n",
      "Steps: 224 | Train Loss: 0.0128181 Vali Loss: 0.0200258 Test Loss: 0.0221268\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0135145\n",
      "\tspeed: 1.0158s/iter; left time: 1492.2002s\n",
      "\titers: 200, epoch: 14 | loss: 0.0123531\n",
      "\tspeed: 0.6404s/iter; left time: 876.7295s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:02m:23.33s\n",
      "Steps: 224 | Train Loss: 0.0127739 Vali Loss: 0.0203425 Test Loss: 0.0229596\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0126478\n",
      "\tspeed: 1.0167s/iter; left time: 1265.7383s\n",
      "\titers: 200, epoch: 15 | loss: 0.0129051\n",
      "\tspeed: 0.6406s/iter; left time: 733.5056s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:02m:23.32s\n",
      "Steps: 224 | Train Loss: 0.0126882 Vali Loss: 0.0199901 Test Loss: 0.0218327\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_DE_336_24_DE_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.021901745349168777, rmse:0.14799238741397858, mae:0.09500051289796829, rse:0.5222856402397156\n",
      "Intermediate time for DE and pred_len 24: 00h:41m:49.32s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_DE_512_96_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=64, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0302059\n",
      "\tspeed: 0.7298s/iter; left time: 6422.8837s\n",
      "\titers: 200, epoch: 1 | loss: 0.0298152\n",
      "\tspeed: 0.7214s/iter; left time: 6276.6156s\n",
      "\titers: 300, epoch: 1 | loss: 0.0319153\n",
      "\tspeed: 0.7223s/iter; left time: 6212.4672s\n",
      "\titers: 400, epoch: 1 | loss: 0.0246839\n",
      "\tspeed: 0.7227s/iter; left time: 6143.3740s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:05m:21.48s\n",
      "Steps: 445 | Train Loss: 0.0307763 Vali Loss: 0.0329600 Test Loss: 0.0369852\n",
      "Validation loss decreased (inf --> 0.032960).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0275542\n",
      "\tspeed: 1.5378s/iter; left time: 12850.2656s\n",
      "\titers: 200, epoch: 2 | loss: 0.0238474\n",
      "\tspeed: 0.7231s/iter; left time: 5970.1049s\n",
      "\titers: 300, epoch: 2 | loss: 0.0248203\n",
      "\tspeed: 0.7225s/iter; left time: 5892.5758s\n",
      "\titers: 400, epoch: 2 | loss: 0.0270211\n",
      "\tspeed: 0.7236s/iter; left time: 5829.1719s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:05m:21.51s\n",
      "Steps: 445 | Train Loss: 0.0260601 Vali Loss: 0.0319481 Test Loss: 0.0365307\n",
      "Validation loss decreased (0.032960 --> 0.031948).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0193737\n",
      "\tspeed: 1.5504s/iter; left time: 12264.9685s\n",
      "\titers: 200, epoch: 3 | loss: 0.0231539\n",
      "\tspeed: 0.7247s/iter; left time: 5660.8554s\n",
      "\titers: 300, epoch: 3 | loss: 0.0245521\n",
      "\tspeed: 0.7232s/iter; left time: 5576.6042s\n",
      "\titers: 400, epoch: 3 | loss: 0.0235652\n",
      "\tspeed: 0.7251s/iter; left time: 5518.3615s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:05m:22.20s\n",
      "Steps: 445 | Train Loss: 0.0234424 Vali Loss: 0.0306345 Test Loss: 0.0354226\n",
      "Validation loss decreased (0.031948 --> 0.030634).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0204264\n",
      "\tspeed: 1.5467s/iter; left time: 11547.7607s\n",
      "\titers: 200, epoch: 4 | loss: 0.0231104\n",
      "\tspeed: 0.7237s/iter; left time: 5330.9265s\n",
      "\titers: 300, epoch: 4 | loss: 0.0226001\n",
      "\tspeed: 0.7244s/iter; left time: 5263.2164s\n",
      "\titers: 400, epoch: 4 | loss: 0.0200202\n",
      "\tspeed: 0.7235s/iter; left time: 5184.7825s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:05m:21.81s\n",
      "Steps: 445 | Train Loss: 0.0227452 Vali Loss: 0.0307144 Test Loss: 0.0355179\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0212215\n",
      "\tspeed: 1.5373s/iter; left time: 10793.6920s\n",
      "\titers: 200, epoch: 5 | loss: 0.0210776\n",
      "\tspeed: 0.7230s/iter; left time: 5003.5454s\n",
      "\titers: 300, epoch: 5 | loss: 0.0211146\n",
      "\tspeed: 0.7248s/iter; left time: 4944.0634s\n",
      "\titers: 400, epoch: 5 | loss: 0.0224722\n",
      "\tspeed: 0.7253s/iter; left time: 4874.7811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:05m:22.19s\n",
      "Steps: 445 | Train Loss: 0.0221151 Vali Loss: 0.0308423 Test Loss: 0.0353491\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0235711\n",
      "\tspeed: 1.5481s/iter; left time: 10180.3677s\n",
      "\titers: 200, epoch: 6 | loss: 0.0218183\n",
      "\tspeed: 0.7271s/iter; left time: 4708.8982s\n",
      "\titers: 300, epoch: 6 | loss: 0.0227409\n",
      "\tspeed: 0.7239s/iter; left time: 4615.6164s\n",
      "\titers: 400, epoch: 6 | loss: 0.0197609\n",
      "\tspeed: 0.7252s/iter; left time: 4551.3310s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:05m:22.56s\n",
      "Steps: 445 | Train Loss: 0.0216648 Vali Loss: 0.0307544 Test Loss: 0.0352945\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0218547\n",
      "\tspeed: 1.5446s/iter; left time: 9469.8890s\n",
      "\titers: 200, epoch: 7 | loss: 0.0216096\n",
      "\tspeed: 0.7217s/iter; left time: 4352.7518s\n",
      "\titers: 300, epoch: 7 | loss: 0.0213850\n",
      "\tspeed: 0.7209s/iter; left time: 4275.5458s\n",
      "\titers: 400, epoch: 7 | loss: 0.0228879\n",
      "\tspeed: 0.7209s/iter; left time: 4203.2765s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:05m:21.22s\n",
      "Steps: 445 | Train Loss: 0.0211638 Vali Loss: 0.0311796 Test Loss: 0.0351215\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0246490\n",
      "\tspeed: 1.5320s/iter; left time: 8710.8820s\n",
      "\titers: 200, epoch: 8 | loss: 0.0213583\n",
      "\tspeed: 0.7204s/iter; left time: 4024.3399s\n",
      "\titers: 300, epoch: 8 | loss: 0.0169613\n",
      "\tspeed: 0.7210s/iter; left time: 3955.3070s\n",
      "\titers: 400, epoch: 8 | loss: 0.0203024\n",
      "\tspeed: 0.7208s/iter; left time: 3881.9840s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:05m:20.49s\n",
      "Steps: 445 | Train Loss: 0.0207130 Vali Loss: 0.0311741 Test Loss: 0.0354104\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_DE_512_96_DE_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.03542259708046913, rmse:0.1882089227437973, mae:0.13172055780887604, rse:0.6664858460426331\n",
      "Intermediate time for DE and pred_len 96: 00h:49m:59.51s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_DE_512_168_DE', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='DE_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=64, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0324691\n",
      "\tspeed: 0.7440s/iter; left time: 6532.7714s\n",
      "\titers: 200, epoch: 1 | loss: 0.0306387\n",
      "\tspeed: 0.7225s/iter; left time: 6272.2330s\n",
      "\titers: 300, epoch: 1 | loss: 0.0280163\n",
      "\tspeed: 0.7225s/iter; left time: 6199.3975s\n",
      "\titers: 400, epoch: 1 | loss: 0.0342149\n",
      "\tspeed: 0.7221s/iter; left time: 6123.8226s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:05m:20.89s\n",
      "Steps: 444 | Train Loss: 0.0324338 Vali Loss: 0.0342096 Test Loss: 0.0389256\n",
      "Validation loss decreased (inf --> 0.034210).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0269492\n",
      "\tspeed: 1.5276s/iter; left time: 12735.4590s\n",
      "\titers: 200, epoch: 2 | loss: 0.0258793\n",
      "\tspeed: 0.7218s/iter; left time: 5945.6438s\n",
      "\titers: 300, epoch: 2 | loss: 0.0273381\n",
      "\tspeed: 0.7222s/iter; left time: 5876.4437s\n",
      "\titers: 400, epoch: 2 | loss: 0.0242763\n",
      "\tspeed: 0.7233s/iter; left time: 5813.1601s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:05m:20.72s\n",
      "Steps: 444 | Train Loss: 0.0278843 Vali Loss: 0.0326882 Test Loss: 0.0386646\n",
      "Validation loss decreased (0.034210 --> 0.032688).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0259864\n",
      "\tspeed: 1.5337s/iter; left time: 12105.3527s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251305\n",
      "\tspeed: 0.7225s/iter; left time: 5630.0814s\n",
      "\titers: 300, epoch: 3 | loss: 0.0246578\n",
      "\tspeed: 0.7221s/iter; left time: 5554.8505s\n",
      "\titers: 400, epoch: 3 | loss: 0.0257822\n",
      "\tspeed: 0.7228s/iter; left time: 5488.2795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:05m:20.53s\n",
      "Steps: 444 | Train Loss: 0.0253866 Vali Loss: 0.0320830 Test Loss: 0.0384211\n",
      "Validation loss decreased (0.032688 --> 0.032083).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0251047\n",
      "\tspeed: 1.5417s/iter; left time: 11484.2302s\n",
      "\titers: 200, epoch: 4 | loss: 0.0242919\n",
      "\tspeed: 0.7223s/iter; left time: 5308.2162s\n",
      "\titers: 300, epoch: 4 | loss: 0.0242367\n",
      "\tspeed: 0.7222s/iter; left time: 5235.0003s\n",
      "\titers: 400, epoch: 4 | loss: 0.0245224\n",
      "\tspeed: 0.7220s/iter; left time: 5161.6806s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:05m:20.46s\n",
      "Steps: 444 | Train Loss: 0.0246121 Vali Loss: 0.0318599 Test Loss: 0.0375914\n",
      "Validation loss decreased (0.032083 --> 0.031860).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0264515\n",
      "\tspeed: 1.5376s/iter; left time: 10771.1388s\n",
      "\titers: 200, epoch: 5 | loss: 0.0233539\n",
      "\tspeed: 0.7229s/iter; left time: 4991.9156s\n",
      "\titers: 300, epoch: 5 | loss: 0.0253562\n",
      "\tspeed: 0.7224s/iter; left time: 4916.0267s\n",
      "\titers: 400, epoch: 5 | loss: 0.0226150\n",
      "\tspeed: 0.7221s/iter; left time: 4841.9755s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:05m:20.59s\n",
      "Steps: 444 | Train Loss: 0.0239584 Vali Loss: 0.0322509 Test Loss: 0.0383392\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0239505\n",
      "\tspeed: 1.5238s/iter; left time: 9997.5308s\n",
      "\titers: 200, epoch: 6 | loss: 0.0250145\n",
      "\tspeed: 0.7239s/iter; left time: 4677.0523s\n",
      "\titers: 300, epoch: 6 | loss: 0.0223557\n",
      "\tspeed: 0.7237s/iter; left time: 4603.2522s\n",
      "\titers: 400, epoch: 6 | loss: 0.0271521\n",
      "\tspeed: 0.7234s/iter; left time: 4528.9557s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:05m:21.02s\n",
      "Steps: 444 | Train Loss: 0.0235787 Vali Loss: 0.0321315 Test Loss: 0.0383225\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0222819\n",
      "\tspeed: 1.5223s/iter; left time: 9312.1745s\n",
      "\titers: 200, epoch: 7 | loss: 0.0233028\n",
      "\tspeed: 0.7230s/iter; left time: 4350.1178s\n",
      "\titers: 300, epoch: 7 | loss: 0.0265391\n",
      "\tspeed: 0.7236s/iter; left time: 4281.2955s\n",
      "\titers: 400, epoch: 7 | loss: 0.0239813\n",
      "\tspeed: 0.7231s/iter; left time: 4206.2023s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:05m:20.94s\n",
      "Steps: 444 | Train Loss: 0.0230404 Vali Loss: 0.0324665 Test Loss: 0.0378922\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0231855\n",
      "\tspeed: 1.5267s/iter; left time: 8660.7795s\n",
      "\titers: 200, epoch: 8 | loss: 0.0225013\n",
      "\tspeed: 0.7242s/iter; left time: 4035.8492s\n",
      "\titers: 300, epoch: 8 | loss: 0.0232989\n",
      "\tspeed: 0.7234s/iter; left time: 3959.4316s\n",
      "\titers: 400, epoch: 8 | loss: 0.0216201\n",
      "\tspeed: 0.7240s/iter; left time: 3890.0196s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:05m:21.34s\n",
      "Steps: 444 | Train Loss: 0.0225703 Vali Loss: 0.0328092 Test Loss: 0.0384778\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0221798\n",
      "\tspeed: 1.5256s/iter; left time: 7977.2167s\n",
      "\titers: 200, epoch: 9 | loss: 0.0239219\n",
      "\tspeed: 0.7234s/iter; left time: 3710.1478s\n",
      "\titers: 300, epoch: 9 | loss: 0.0235307\n",
      "\tspeed: 0.7237s/iter; left time: 3639.7235s\n",
      "\titers: 400, epoch: 9 | loss: 0.0219622\n",
      "\tspeed: 0.7234s/iter; left time: 3565.5671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:05m:21.03s\n",
      "Steps: 444 | Train Loss: 0.0222273 Vali Loss: 0.0331927 Test Loss: 0.0387550\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_DE_512_168_DE_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.037591367959976196, rmse:0.193884938955307, mae:0.13648755848407745, rse:0.6867560148239136\n",
      "Intermediate time for DE and pred_len 168: 00h:56m:02.33s\n",
      "Intermediate time for DE: 02h:27m:51.16s\n",
      "\n",
      "=== Starting experiments for country: GB ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_GB_512_24_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=64, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28601\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0289166\n",
      "\tspeed: 0.7436s/iter; left time: 6559.7012s\n",
      "\titers: 200, epoch: 1 | loss: 0.0202293\n",
      "\tspeed: 0.7209s/iter; left time: 6286.9572s\n",
      "\titers: 300, epoch: 1 | loss: 0.0206892\n",
      "\tspeed: 0.7211s/iter; left time: 6216.8743s\n",
      "\titers: 400, epoch: 1 | loss: 0.0226693\n",
      "\tspeed: 0.7200s/iter; left time: 6135.2609s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:05m:21.38s\n",
      "Steps: 446 | Train Loss: 0.0212843 Vali Loss: 0.0216887 Test Loss: 0.0277117\n",
      "Validation loss decreased (inf --> 0.021689).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0158360\n",
      "\tspeed: 1.5439s/iter; left time: 12929.8499s\n",
      "\titers: 200, epoch: 2 | loss: 0.0135614\n",
      "\tspeed: 0.7208s/iter; left time: 5964.3511s\n",
      "\titers: 300, epoch: 2 | loss: 0.0150534\n",
      "\tspeed: 0.7208s/iter; left time: 5892.8308s\n",
      "\titers: 400, epoch: 2 | loss: 0.0144419\n",
      "\tspeed: 0.7207s/iter; left time: 5819.4490s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:05m:21.23s\n",
      "Steps: 446 | Train Loss: 0.0164231 Vali Loss: 0.0205662 Test Loss: 0.0265328\n",
      "Validation loss decreased (0.021689 --> 0.020566).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0148300\n",
      "\tspeed: 1.6666s/iter; left time: 13214.3622s\n",
      "\titers: 200, epoch: 3 | loss: 0.0151173\n",
      "\tspeed: 0.7212s/iter; left time: 5645.9661s\n",
      "\titers: 300, epoch: 3 | loss: 0.0132256\n",
      "\tspeed: 0.7204s/iter; left time: 5567.7178s\n",
      "\titers: 400, epoch: 3 | loss: 0.0163189\n",
      "\tspeed: 0.7207s/iter; left time: 5498.3149s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:05m:33.37s\n",
      "Steps: 446 | Train Loss: 0.0142952 Vali Loss: 0.0201830 Test Loss: 0.0264994\n",
      "Validation loss decreased (0.020566 --> 0.020183).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0111746\n",
      "\tspeed: 1.5462s/iter; left time: 11570.4260s\n",
      "\titers: 200, epoch: 4 | loss: 0.0134375\n",
      "\tspeed: 0.7212s/iter; left time: 5324.3309s\n",
      "\titers: 300, epoch: 4 | loss: 0.0131227\n",
      "\tspeed: 0.7204s/iter; left time: 5246.3204s\n",
      "\titers: 400, epoch: 4 | loss: 0.0098728\n",
      "\tspeed: 0.7203s/iter; left time: 5174.0813s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:05m:21.15s\n",
      "Steps: 446 | Train Loss: 0.0139439 Vali Loss: 0.0203256 Test Loss: 0.0254370\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0136044\n",
      "\tspeed: 1.5432s/iter; left time: 10859.2062s\n",
      "\titers: 200, epoch: 5 | loss: 0.0145468\n",
      "\tspeed: 0.7220s/iter; left time: 5008.5411s\n",
      "\titers: 300, epoch: 5 | loss: 0.0146856\n",
      "\tspeed: 0.7202s/iter; left time: 4924.0024s\n",
      "\titers: 400, epoch: 5 | loss: 0.0142690\n",
      "\tspeed: 0.7362s/iter; left time: 4959.4601s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:05m:22.84s\n",
      "Steps: 446 | Train Loss: 0.0137362 Vali Loss: 0.0201708 Test Loss: 0.0258583\n",
      "Validation loss decreased (0.020183 --> 0.020171).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0127196\n",
      "\tspeed: 1.5433s/iter; left time: 10172.1509s\n",
      "\titers: 200, epoch: 6 | loss: 0.0135240\n",
      "\tspeed: 0.7203s/iter; left time: 4675.6953s\n",
      "\titers: 300, epoch: 6 | loss: 0.0136991\n",
      "\tspeed: 0.7202s/iter; left time: 4602.7816s\n",
      "\titers: 400, epoch: 6 | loss: 0.0113744\n",
      "\tspeed: 0.7204s/iter; left time: 4531.8315s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:05m:21.05s\n",
      "Steps: 446 | Train Loss: 0.0135565 Vali Loss: 0.0196171 Test Loss: 0.0257270\n",
      "Validation loss decreased (0.020171 --> 0.019617).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0126710\n",
      "\tspeed: 1.5451s/iter; left time: 9494.6011s\n",
      "\titers: 200, epoch: 7 | loss: 0.0130240\n",
      "\tspeed: 0.7200s/iter; left time: 4352.1379s\n",
      "\titers: 300, epoch: 7 | loss: 0.0109849\n",
      "\tspeed: 0.7204s/iter; left time: 4282.6498s\n",
      "\titers: 400, epoch: 7 | loss: 0.0159441\n",
      "\tspeed: 0.7204s/iter; left time: 4210.5659s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:05m:20.99s\n",
      "Steps: 446 | Train Loss: 0.0133061 Vali Loss: 0.0198284 Test Loss: 0.0254155\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0134848\n",
      "\tspeed: 1.5634s/iter; left time: 8909.8779s\n",
      "\titers: 200, epoch: 8 | loss: 0.0118706\n",
      "\tspeed: 0.7215s/iter; left time: 4039.5559s\n",
      "\titers: 300, epoch: 8 | loss: 0.0141404\n",
      "\tspeed: 0.7213s/iter; left time: 3966.3329s\n",
      "\titers: 400, epoch: 8 | loss: 0.0131802\n",
      "\tspeed: 0.7227s/iter; left time: 3901.6261s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:05m:23.46s\n",
      "Steps: 446 | Train Loss: 0.0132346 Vali Loss: 0.0200575 Test Loss: 0.0259631\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0118793\n",
      "\tspeed: 1.5455s/iter; left time: 8118.7179s\n",
      "\titers: 200, epoch: 9 | loss: 0.0153380\n",
      "\tspeed: 0.7216s/iter; left time: 3718.1533s\n",
      "\titers: 300, epoch: 9 | loss: 0.0114176\n",
      "\tspeed: 0.7240s/iter; left time: 3658.3251s\n",
      "\titers: 400, epoch: 9 | loss: 0.0105651\n",
      "\tspeed: 0.7404s/iter; left time: 3667.2201s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:05m:23.77s\n",
      "Steps: 446 | Train Loss: 0.0131461 Vali Loss: 0.0199061 Test Loss: 0.0258524\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0160117\n",
      "\tspeed: 1.5474s/iter; left time: 7438.5272s\n",
      "\titers: 200, epoch: 10 | loss: 0.0124742\n",
      "\tspeed: 0.7240s/iter; left time: 3407.8560s\n",
      "\titers: 300, epoch: 10 | loss: 0.0121242\n",
      "\tspeed: 0.7218s/iter; left time: 3325.4090s\n",
      "\titers: 400, epoch: 10 | loss: 0.0108839\n",
      "\tspeed: 0.7218s/iter; left time: 3253.2345s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:05m:21.90s\n",
      "Steps: 446 | Train Loss: 0.0129824 Vali Loss: 0.0199416 Test Loss: 0.0261237\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0101929\n",
      "\tspeed: 1.5477s/iter; left time: 6749.5673s\n",
      "\titers: 200, epoch: 11 | loss: 0.0126501\n",
      "\tspeed: 0.7222s/iter; left time: 3077.1050s\n",
      "\titers: 300, epoch: 11 | loss: 0.0161592\n",
      "\tspeed: 0.7216s/iter; left time: 3002.5468s\n",
      "\titers: 400, epoch: 11 | loss: 0.0145134\n",
      "\tspeed: 0.7215s/iter; left time: 2929.8459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:05m:21.67s\n",
      "Steps: 446 | Train Loss: 0.0128602 Vali Loss: 0.0197561 Test Loss: 0.0259045\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_GB_512_24_GB_PatchTST_custom_ftM_sl512_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.02572702430188656, rmse:0.16039645671844482, mae:0.1057143285870552, rse:0.5530432462692261\n",
      "Intermediate time for GB and pred_len 24: 01h:08m:52.29s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_GB_512_96_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=64, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28529\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0258529\n",
      "\tspeed: 0.7502s/iter; left time: 6602.5246s\n",
      "\titers: 200, epoch: 1 | loss: 0.0296054\n",
      "\tspeed: 0.7227s/iter; left time: 6288.5118s\n",
      "\titers: 300, epoch: 1 | loss: 0.0254370\n",
      "\tspeed: 0.7248s/iter; left time: 6234.0843s\n",
      "\titers: 400, epoch: 1 | loss: 0.0238283\n",
      "\tspeed: 0.7244s/iter; left time: 6158.1998s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:05m:22.24s\n",
      "Steps: 445 | Train Loss: 0.0269416 Vali Loss: 0.0298205 Test Loss: 0.0422618\n",
      "Validation loss decreased (inf --> 0.029821).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0215586\n",
      "\tspeed: 1.5449s/iter; left time: 12908.9473s\n",
      "\titers: 200, epoch: 2 | loss: 0.0228470\n",
      "\tspeed: 0.7232s/iter; left time: 5971.0812s\n",
      "\titers: 300, epoch: 2 | loss: 0.0210400\n",
      "\tspeed: 0.7229s/iter; left time: 5896.3574s\n",
      "\titers: 400, epoch: 2 | loss: 0.0219924\n",
      "\tspeed: 0.7239s/iter; left time: 5831.6937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:05m:21.62s\n",
      "Steps: 445 | Train Loss: 0.0233876 Vali Loss: 0.0290359 Test Loss: 0.0406784\n",
      "Validation loss decreased (0.029821 --> 0.029036).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0199911\n",
      "\tspeed: 1.5437s/iter; left time: 12212.1002s\n",
      "\titers: 200, epoch: 3 | loss: 0.0198404\n",
      "\tspeed: 0.7229s/iter; left time: 5646.7477s\n",
      "\titers: 300, epoch: 3 | loss: 0.0207614\n",
      "\tspeed: 0.7229s/iter; left time: 5574.2629s\n",
      "\titers: 400, epoch: 3 | loss: 0.0233237\n",
      "\tspeed: 0.7242s/iter; left time: 5511.6014s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:05m:21.71s\n",
      "Steps: 445 | Train Loss: 0.0216473 Vali Loss: 0.0290370 Test Loss: 0.0400350\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0204558\n",
      "\tspeed: 1.5390s/iter; left time: 11490.4566s\n",
      "\titers: 200, epoch: 4 | loss: 0.0195280\n",
      "\tspeed: 0.7244s/iter; left time: 5335.8198s\n",
      "\titers: 300, epoch: 4 | loss: 0.0223034\n",
      "\tspeed: 0.7239s/iter; left time: 5259.7417s\n",
      "\titers: 400, epoch: 4 | loss: 0.0215147\n",
      "\tspeed: 0.7265s/iter; left time: 5205.9723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:05m:22.14s\n",
      "Steps: 445 | Train Loss: 0.0212032 Vali Loss: 0.0285863 Test Loss: 0.0402990\n",
      "Validation loss decreased (0.029036 --> 0.028586).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0201620\n",
      "\tspeed: 1.5445s/iter; left time: 10844.0707s\n",
      "\titers: 200, epoch: 5 | loss: 0.0204964\n",
      "\tspeed: 0.7254s/iter; left time: 5020.8222s\n",
      "\titers: 300, epoch: 5 | loss: 0.0218803\n",
      "\tspeed: 0.7253s/iter; left time: 4946.9658s\n",
      "\titers: 400, epoch: 5 | loss: 0.0219664\n",
      "\tspeed: 0.7261s/iter; left time: 4880.2238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:05m:22.69s\n",
      "Steps: 445 | Train Loss: 0.0207783 Vali Loss: 0.0292571 Test Loss: 0.0418711\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0179712\n",
      "\tspeed: 1.5431s/iter; left time: 10147.2277s\n",
      "\titers: 200, epoch: 6 | loss: 0.0215801\n",
      "\tspeed: 0.7254s/iter; left time: 4698.0081s\n",
      "\titers: 300, epoch: 6 | loss: 0.0200419\n",
      "\tspeed: 0.7261s/iter; left time: 4629.5624s\n",
      "\titers: 400, epoch: 6 | loss: 0.0165364\n",
      "\tspeed: 0.7260s/iter; left time: 4556.2920s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:05m:22.78s\n",
      "Steps: 445 | Train Loss: 0.0203742 Vali Loss: 0.0292893 Test Loss: 0.0417165\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0195728\n",
      "\tspeed: 1.5451s/iter; left time: 9472.9812s\n",
      "\titers: 200, epoch: 7 | loss: 0.0203875\n",
      "\tspeed: 0.7257s/iter; left time: 4376.7265s\n",
      "\titers: 300, epoch: 7 | loss: 0.0214673\n",
      "\tspeed: 0.7257s/iter; left time: 4304.3257s\n",
      "\titers: 400, epoch: 7 | loss: 0.0215163\n",
      "\tspeed: 0.7255s/iter; left time: 4230.4574s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:05m:22.54s\n",
      "Steps: 445 | Train Loss: 0.0200647 Vali Loss: 0.0294887 Test Loss: 0.0427642\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0213490\n",
      "\tspeed: 1.5342s/iter; left time: 8723.2094s\n",
      "\titers: 200, epoch: 8 | loss: 0.0187555\n",
      "\tspeed: 0.7205s/iter; left time: 4024.8666s\n",
      "\titers: 300, epoch: 8 | loss: 0.0184164\n",
      "\tspeed: 0.7202s/iter; left time: 3950.8156s\n",
      "\titers: 400, epoch: 8 | loss: 0.0192565\n",
      "\tspeed: 0.7198s/iter; left time: 3876.8855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:05m:20.37s\n",
      "Steps: 445 | Train Loss: 0.0196747 Vali Loss: 0.0301925 Test Loss: 0.0437775\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0174150\n",
      "\tspeed: 1.5324s/iter; left time: 8031.4636s\n",
      "\titers: 200, epoch: 9 | loss: 0.0179154\n",
      "\tspeed: 0.7217s/iter; left time: 3710.0932s\n",
      "\titers: 300, epoch: 9 | loss: 0.0173499\n",
      "\tspeed: 0.7204s/iter; left time: 3631.3070s\n",
      "\titers: 400, epoch: 9 | loss: 0.0182475\n",
      "\tspeed: 0.7198s/iter; left time: 3556.3908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:05m:20.50s\n",
      "Steps: 445 | Train Loss: 0.0193972 Vali Loss: 0.0300176 Test Loss: 0.0431081\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_GB_512_96_GB_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.0402989462018013, rmse:0.20074597001075745, mae:0.14150388538837433, rse:0.6942076086997986\n",
      "Intermediate time for GB and pred_len 96: 00h:56m:15.92s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_GB_512_168_GB', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='GB_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=512, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=5, dec_in=7, c_out=5, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=64, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28457\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0269608\n",
      "\tspeed: 0.7469s/iter; left time: 6558.6765s\n",
      "\titers: 200, epoch: 1 | loss: 0.0274164\n",
      "\tspeed: 0.7241s/iter; left time: 6285.5526s\n",
      "\titers: 300, epoch: 1 | loss: 0.0256406\n",
      "\tspeed: 0.7243s/iter; left time: 6215.2647s\n",
      "\titers: 400, epoch: 1 | loss: 0.0278278\n",
      "\tspeed: 0.7259s/iter; left time: 6156.4625s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:05m:21.81s\n",
      "Steps: 444 | Train Loss: 0.0281285 Vali Loss: 0.0310864 Test Loss: 0.0435887\n",
      "Validation loss decreased (inf --> 0.031086).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0241208\n",
      "\tspeed: 1.5286s/iter; left time: 12744.1454s\n",
      "\titers: 200, epoch: 2 | loss: 0.0249518\n",
      "\tspeed: 0.7239s/iter; left time: 5962.6101s\n",
      "\titers: 300, epoch: 2 | loss: 0.0251283\n",
      "\tspeed: 0.7240s/iter; left time: 5891.1163s\n",
      "\titers: 400, epoch: 2 | loss: 0.0236740\n",
      "\tspeed: 0.7249s/iter; left time: 5826.0042s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:05m:21.37s\n",
      "Steps: 444 | Train Loss: 0.0248076 Vali Loss: 0.0308314 Test Loss: 0.0430559\n",
      "Validation loss decreased (0.031086 --> 0.030831).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0237497\n",
      "\tspeed: 1.5279s/iter; left time: 12060.0242s\n",
      "\titers: 200, epoch: 3 | loss: 0.0228141\n",
      "\tspeed: 0.7230s/iter; left time: 5634.0080s\n",
      "\titers: 300, epoch: 3 | loss: 0.0217565\n",
      "\tspeed: 0.7225s/iter; left time: 5558.0465s\n",
      "\titers: 400, epoch: 3 | loss: 0.0244880\n",
      "\tspeed: 0.7240s/iter; left time: 5497.2140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:05m:20.88s\n",
      "Steps: 444 | Train Loss: 0.0229355 Vali Loss: 0.0306573 Test Loss: 0.0430532\n",
      "Validation loss decreased (0.030831 --> 0.030657).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0244858\n",
      "\tspeed: 1.5291s/iter; left time: 11390.0229s\n",
      "\titers: 200, epoch: 4 | loss: 0.0243690\n",
      "\tspeed: 0.7233s/iter; left time: 5315.1716s\n",
      "\titers: 300, epoch: 4 | loss: 0.0219046\n",
      "\tspeed: 0.7231s/iter; left time: 5241.8854s\n",
      "\titers: 400, epoch: 4 | loss: 0.0218012\n",
      "\tspeed: 0.7234s/iter; left time: 5171.4778s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:05m:21.01s\n",
      "Steps: 444 | Train Loss: 0.0225335 Vali Loss: 0.0309076 Test Loss: 0.0431541\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0233454\n",
      "\tspeed: 1.5234s/iter; left time: 10671.7008s\n",
      "\titers: 200, epoch: 5 | loss: 0.0209921\n",
      "\tspeed: 0.7244s/iter; left time: 5001.8062s\n",
      "\titers: 300, epoch: 5 | loss: 0.0242191\n",
      "\tspeed: 0.7246s/iter; left time: 4930.8705s\n",
      "\titers: 400, epoch: 5 | loss: 0.0218035\n",
      "\tspeed: 0.7235s/iter; left time: 4851.3240s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:05m:21.29s\n",
      "Steps: 444 | Train Loss: 0.0221094 Vali Loss: 0.0308309 Test Loss: 0.0438321\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0201289\n",
      "\tspeed: 1.5211s/iter; left time: 9979.7247s\n",
      "\titers: 200, epoch: 6 | loss: 0.0211403\n",
      "\tspeed: 0.7243s/iter; left time: 4679.6269s\n",
      "\titers: 300, epoch: 6 | loss: 0.0226115\n",
      "\tspeed: 0.7240s/iter; left time: 4605.5747s\n",
      "\titers: 400, epoch: 6 | loss: 0.0225834\n",
      "\tspeed: 0.7233s/iter; left time: 4528.5164s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:05m:21.10s\n",
      "Steps: 444 | Train Loss: 0.0215733 Vali Loss: 0.0318337 Test Loss: 0.0450657\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0199800\n",
      "\tspeed: 1.5210s/iter; left time: 9304.2040s\n",
      "\titers: 200, epoch: 7 | loss: 0.0212566\n",
      "\tspeed: 0.7234s/iter; left time: 4352.5687s\n",
      "\titers: 300, epoch: 7 | loss: 0.0221269\n",
      "\tspeed: 0.7239s/iter; left time: 4283.1790s\n",
      "\titers: 400, epoch: 7 | loss: 0.0223966\n",
      "\tspeed: 0.7233s/iter; left time: 4207.1786s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:05m:20.98s\n",
      "Steps: 444 | Train Loss: 0.0211381 Vali Loss: 0.0319989 Test Loss: 0.0451651\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0211825\n",
      "\tspeed: 1.5210s/iter; left time: 8628.6157s\n",
      "\titers: 200, epoch: 8 | loss: 0.0198315\n",
      "\tspeed: 0.7238s/iter; left time: 4033.7783s\n",
      "\titers: 300, epoch: 8 | loss: 0.0216954\n",
      "\tspeed: 0.7241s/iter; left time: 3962.9642s\n",
      "\titers: 400, epoch: 8 | loss: 0.0182057\n",
      "\tspeed: 0.7237s/iter; left time: 3888.3024s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:05m:21.14s\n",
      "Steps: 444 | Train Loss: 0.0208608 Vali Loss: 0.0317588 Test Loss: 0.0450449\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : no_patching_GB_512_168_GB_PatchTST_custom_ftM_sl512_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.04305318742990494, rmse:0.2074926197528839, mae:0.14702880382537842, rse:0.7194067239761353\n",
      "Intermediate time for GB and pred_len 168: 00h:49m:51.13s\n",
      "Intermediate time for GB: 02h:54m:59.34s\n",
      "\n",
      "=== Starting experiments for country: ES ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='no_patching_ES_336_24_ES', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='ES_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=1, stride=1, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=128, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : no_patching_ES_336_24_ES_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0195191\n",
      "\tspeed: 0.4119s/iter; left time: 1804.6806s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Capture the output in real-time\u001b[39;00m\n\u001b[1;32m     83\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 84\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Print in the .ipynb cell\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize empty list\n",
    "patchtst_results = []\n",
    "\n",
    "patch_len = 1\n",
    "stride = 1\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_no_patching.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "\n",
    "            if country == 'DE' and pred_len == 24:\n",
    "                seq_len=336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "\n",
    "            if seq_len == 512:\n",
    "                batch_size = 64\n",
    "            else:\n",
    "                batch_size = 128\n",
    "                \n",
    "            model_id = f\"no_patching_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --patch_len {patch_len} \\\n",
    "              --stride {stride} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Country': 'DE',\n",
       "  'Pred_len': 24,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.021901745349168777,\n",
       "  'RMSE': 0.14799238741397858,\n",
       "  'MAE': 0.09500051289796829},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 96,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.03542259708046913,\n",
       "  'RMSE': 0.1882089227437973,\n",
       "  'MAE': 0.13172055780887604},\n",
       " {'Country': 'DE',\n",
       "  'Pred_len': 168,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.037591367959976196,\n",
       "  'RMSE': 0.193884938955307,\n",
       "  'MAE': 0.13648755848407745},\n",
       " {'Country': 'GB',\n",
       "  'Pred_len': 24,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.02572702430188656,\n",
       "  'RMSE': 0.16039645671844482,\n",
       "  'MAE': 0.1057143285870552},\n",
       " {'Country': 'GB',\n",
       "  'Pred_len': 96,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.0402989462018013,\n",
       "  'RMSE': 0.20074597001075745,\n",
       "  'MAE': 0.14150388538837433},\n",
       " {'Country': 'GB',\n",
       "  'Pred_len': 168,\n",
       "  'Iteration': 1,\n",
       "  'MSE': 0.04305318742990494,\n",
       "  'RMSE': 0.2074926197528839,\n",
       "  'MAE': 0.14702880382537842}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchtst_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">- P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>0.1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.0632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.0894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.0822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.0877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.2061</td>\n",
       "      <td>0.1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.0587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.0815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model                - P                \n",
       "Metrics              MSE    RMSE     MAE\n",
       "Country Pred_len                        \n",
       "DE      24        0.0217  0.1472  0.0909\n",
       "        96        0.0365  0.1910  0.1267\n",
       "        168       0.0384  0.1959  0.1325\n",
       "ES      24        0.0107  0.1034  0.0632\n",
       "        96        0.0193  0.1390  0.0894\n",
       "        168       0.0217  0.1472  0.0960\n",
       "FR      24        0.0108  0.1037  0.0574\n",
       "        96        0.0201  0.1418  0.0822\n",
       "        168       0.0219  0.1478  0.0877\n",
       "GB      24        0.0263  0.1621  0.1036\n",
       "        96        0.0425  0.2061  0.1391\n",
       "        168       0.0442  0.2102  0.1445\n",
       "IT      24        0.0104  0.1021  0.0587\n",
       "        96        0.0187  0.1367  0.0815\n",
       "        168       0.0199  0.1410  0.0857"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['- P'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_no_patching.csv'))\n",
    "patchtst_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for country: FR ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_24_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0237356\n",
      "\tspeed: 0.0630s/iter; left time: 1416.7940s\n",
      "\titers: 200, epoch: 1 | loss: 0.0166661\n",
      "\tspeed: 0.0352s/iter; left time: 787.6332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:08.13s\n",
      "Steps: 226 | Train Loss: 0.0230923 Vali Loss: 0.0238321 Test Loss: 0.0298794\n",
      "Validation loss decreased (inf --> 0.023832).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0093542\n",
      "\tspeed: 0.0549s/iter; left time: 1222.3188s\n",
      "\titers: 200, epoch: 2 | loss: 0.0079462\n",
      "\tspeed: 0.0307s/iter; left time: 681.6420s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.84s\n",
      "Steps: 226 | Train Loss: 0.0099565 Vali Loss: 0.0107128 Test Loss: 0.0120591\n",
      "Validation loss decreased (0.023832 --> 0.010713).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0080557\n",
      "\tspeed: 0.0575s/iter; left time: 1267.1122s\n",
      "\titers: 200, epoch: 3 | loss: 0.0073414\n",
      "\tspeed: 0.0325s/iter; left time: 714.3936s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0072211 Vali Loss: 0.0097881 Test Loss: 0.0109410\n",
      "Validation loss decreased (0.010713 --> 0.009788).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0072077\n",
      "\tspeed: 0.0547s/iter; left time: 1193.3030s\n",
      "\titers: 200, epoch: 4 | loss: 0.0052147\n",
      "\tspeed: 0.0241s/iter; left time: 523.1088s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.09s\n",
      "Steps: 226 | Train Loss: 0.0066755 Vali Loss: 0.0095442 Test Loss: 0.0107035\n",
      "Validation loss decreased (0.009788 --> 0.009544).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0054814\n",
      "\tspeed: 0.0541s/iter; left time: 1167.6294s\n",
      "\titers: 200, epoch: 5 | loss: 0.0061579\n",
      "\tspeed: 0.0251s/iter; left time: 539.1269s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0063989 Vali Loss: 0.0092577 Test Loss: 0.0103496\n",
      "Validation loss decreased (0.009544 --> 0.009258).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0062205\n",
      "\tspeed: 0.0579s/iter; left time: 1237.9695s\n",
      "\titers: 200, epoch: 6 | loss: 0.0067256\n",
      "\tspeed: 0.0344s/iter; left time: 730.9275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.75s\n",
      "Steps: 226 | Train Loss: 0.0062187 Vali Loss: 0.0091027 Test Loss: 0.0102357\n",
      "Validation loss decreased (0.009258 --> 0.009103).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0060843\n",
      "\tspeed: 0.0654s/iter; left time: 1382.3597s\n",
      "\titers: 200, epoch: 7 | loss: 0.0056078\n",
      "\tspeed: 0.0286s/iter; left time: 602.7052s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.73s\n",
      "Steps: 226 | Train Loss: 0.0060805 Vali Loss: 0.0089617 Test Loss: 0.0101414\n",
      "Validation loss decreased (0.009103 --> 0.008962).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0050162\n",
      "\tspeed: 0.0539s/iter; left time: 1127.4219s\n",
      "\titers: 200, epoch: 8 | loss: 0.0051762\n",
      "\tspeed: 0.0289s/iter; left time: 601.8684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.76s\n",
      "Steps: 226 | Train Loss: 0.0059466 Vali Loss: 0.0089266 Test Loss: 0.0100621\n",
      "Validation loss decreased (0.008962 --> 0.008927).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0043858\n",
      "\tspeed: 0.0627s/iter; left time: 1297.6389s\n",
      "\titers: 200, epoch: 9 | loss: 0.0063614\n",
      "\tspeed: 0.0331s/iter; left time: 681.7924s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.80s\n",
      "Steps: 226 | Train Loss: 0.0058522 Vali Loss: 0.0088418 Test Loss: 0.0099862\n",
      "Validation loss decreased (0.008927 --> 0.008842).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0063100\n",
      "\tspeed: 0.0556s/iter; left time: 1137.8350s\n",
      "\titers: 200, epoch: 10 | loss: 0.0061891\n",
      "\tspeed: 0.0296s/iter; left time: 602.3603s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.86s\n",
      "Steps: 226 | Train Loss: 0.0057647 Vali Loss: 0.0088251 Test Loss: 0.0100708\n",
      "Validation loss decreased (0.008842 --> 0.008825).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0067617\n",
      "\tspeed: 0.0538s/iter; left time: 1089.2481s\n",
      "\titers: 200, epoch: 11 | loss: 0.0066305\n",
      "\tspeed: 0.0268s/iter; left time: 540.6986s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 226 | Train Loss: 0.0056819 Vali Loss: 0.0087484 Test Loss: 0.0100197\n",
      "Validation loss decreased (0.008825 --> 0.008748).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0060824\n",
      "\tspeed: 0.0518s/iter; left time: 1036.1625s\n",
      "\titers: 200, epoch: 12 | loss: 0.0058182\n",
      "\tspeed: 0.0270s/iter; left time: 538.3675s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.18s\n",
      "Steps: 226 | Train Loss: 0.0055992 Vali Loss: 0.0087655 Test Loss: 0.0100026\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0064740\n",
      "\tspeed: 0.0519s/iter; left time: 1027.6831s\n",
      "\titers: 200, epoch: 13 | loss: 0.0052136\n",
      "\tspeed: 0.0275s/iter; left time: 541.6869s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0055461 Vali Loss: 0.0087297 Test Loss: 0.0100234\n",
      "Validation loss decreased (0.008748 --> 0.008730).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0054593\n",
      "\tspeed: 0.0544s/iter; left time: 1063.8500s\n",
      "\titers: 200, epoch: 14 | loss: 0.0066011\n",
      "\tspeed: 0.0277s/iter; left time: 538.9120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.52s\n",
      "Steps: 226 | Train Loss: 0.0055118 Vali Loss: 0.0087377 Test Loss: 0.0100422\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0048896\n",
      "\tspeed: 0.0587s/iter; left time: 1134.3386s\n",
      "\titers: 200, epoch: 15 | loss: 0.0061342\n",
      "\tspeed: 0.0299s/iter; left time: 575.3251s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.48s\n",
      "Steps: 226 | Train Loss: 0.0054799 Vali Loss: 0.0087377 Test Loss: 0.0099943\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0055380\n",
      "\tspeed: 0.0584s/iter; left time: 1115.4190s\n",
      "\titers: 200, epoch: 16 | loss: 0.0058325\n",
      "\tspeed: 0.0248s/iter; left time: 471.3572s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 226 | Train Loss: 0.0054355 Vali Loss: 0.0086340 Test Loss: 0.0099511\n",
      "Validation loss decreased (0.008730 --> 0.008634).  Saving model ...\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0056160\n",
      "\tspeed: 0.0604s/iter; left time: 1141.4406s\n",
      "\titers: 200, epoch: 17 | loss: 0.0052510\n",
      "\tspeed: 0.0298s/iter; left time: 559.4422s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.36s\n",
      "Steps: 226 | Train Loss: 0.0054082 Vali Loss: 0.0086247 Test Loss: 0.0099653\n",
      "Validation loss decreased (0.008634 --> 0.008625).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0053355\n",
      "\tspeed: 0.0577s/iter; left time: 1075.9685s\n",
      "\titers: 200, epoch: 18 | loss: 0.0048924\n",
      "\tspeed: 0.0280s/iter; left time: 519.5674s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.90s\n",
      "Steps: 226 | Train Loss: 0.0053938 Vali Loss: 0.0086484 Test Loss: 0.0099769\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0055976\n",
      "\tspeed: 0.0601s/iter; left time: 1108.6693s\n",
      "\titers: 200, epoch: 19 | loss: 0.0050728\n",
      "\tspeed: 0.0374s/iter; left time: 684.8314s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:08.46s\n",
      "Steps: 226 | Train Loss: 0.0053646 Vali Loss: 0.0086138 Test Loss: 0.0099554\n",
      "Validation loss decreased (0.008625 --> 0.008614).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0056121\n",
      "\tspeed: 0.0606s/iter; left time: 1102.4992s\n",
      "\titers: 200, epoch: 20 | loss: 0.0047978\n",
      "\tspeed: 0.0271s/iter; left time: 491.5696s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:07.10s\n",
      "Steps: 226 | Train Loss: 0.0053453 Vali Loss: 0.0086795 Test Loss: 0.0099502\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0052046\n",
      "\tspeed: 0.0548s/iter; left time: 985.3313s\n",
      "\titers: 200, epoch: 21 | loss: 0.0055206\n",
      "\tspeed: 0.0274s/iter; left time: 489.2986s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.39s\n",
      "Steps: 226 | Train Loss: 0.0053154 Vali Loss: 0.0086463 Test Loss: 0.0099572\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0047986\n",
      "\tspeed: 0.0528s/iter; left time: 938.0809s\n",
      "\titers: 200, epoch: 22 | loss: 0.0047735\n",
      "\tspeed: 0.0283s/iter; left time: 500.1586s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.06s\n",
      "Steps: 226 | Train Loss: 0.0052983 Vali Loss: 0.0086482 Test Loss: 0.0099361\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0054964\n",
      "\tspeed: 0.0587s/iter; left time: 1029.2893s\n",
      "\titers: 200, epoch: 23 | loss: 0.0057613\n",
      "\tspeed: 0.0235s/iter; left time: 410.0671s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 226 | Train Loss: 0.0052910 Vali Loss: 0.0086885 Test Loss: 0.0099816\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0051943\n",
      "\tspeed: 0.0497s/iter; left time: 860.1582s\n",
      "\titers: 200, epoch: 24 | loss: 0.0046766\n",
      "\tspeed: 0.0264s/iter; left time: 453.8627s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:06.00s\n",
      "Steps: 226 | Train Loss: 0.0052728 Vali Loss: 0.0086597 Test Loss: 0.0099599\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0048268\n",
      "\tspeed: 0.0502s/iter; left time: 857.0807s\n",
      "\titers: 200, epoch: 25 | loss: 0.0055762\n",
      "\tspeed: 0.0287s/iter; left time: 487.7496s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 226 | Train Loss: 0.0052606 Vali Loss: 0.0086038 Test Loss: 0.0099352\n",
      "Validation loss decreased (0.008614 --> 0.008604).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0054199\n",
      "\tspeed: 0.0624s/iter; left time: 1051.8108s\n",
      "\titers: 200, epoch: 26 | loss: 0.0047658\n",
      "\tspeed: 0.0317s/iter; left time: 530.4635s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.51s\n",
      "Steps: 226 | Train Loss: 0.0052556 Vali Loss: 0.0086673 Test Loss: 0.0099702\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0054049\n",
      "\tspeed: 0.0532s/iter; left time: 883.9730s\n",
      "\titers: 200, epoch: 27 | loss: 0.0052203\n",
      "\tspeed: 0.0294s/iter; left time: 485.7770s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:06.80s\n",
      "Steps: 226 | Train Loss: 0.0052431 Vali Loss: 0.0086526 Test Loss: 0.0099488\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0051072\n",
      "\tspeed: 0.0580s/iter; left time: 950.9519s\n",
      "\titers: 200, epoch: 28 | loss: 0.0047350\n",
      "\tspeed: 0.0249s/iter; left time: 405.5559s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 226 | Train Loss: 0.0052365 Vali Loss: 0.0086557 Test Loss: 0.0099640\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0052623\n",
      "\tspeed: 0.0535s/iter; left time: 865.1883s\n",
      "\titers: 200, epoch: 29 | loss: 0.0066183\n",
      "\tspeed: 0.0315s/iter; left time: 506.6605s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:07.16s\n",
      "Steps: 226 | Train Loss: 0.0052327 Vali Loss: 0.0086441 Test Loss: 0.0099638\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0041976\n",
      "\tspeed: 0.0562s/iter; left time: 895.9772s\n",
      "\titers: 200, epoch: 30 | loss: 0.0064455\n",
      "\tspeed: 0.0327s/iter; left time: 517.6012s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:07.11s\n",
      "Steps: 226 | Train Loss: 0.0052169 Vali Loss: 0.0086797 Test Loss: 0.0099548\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0047889\n",
      "\tspeed: 0.0549s/iter; left time: 863.2743s\n",
      "\titers: 200, epoch: 31 | loss: 0.0054666\n",
      "\tspeed: 0.0303s/iter; left time: 472.7757s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 226 | Train Loss: 0.0052050 Vali Loss: 0.0086171 Test Loss: 0.0099502\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0052385\n",
      "\tspeed: 0.0594s/iter; left time: 920.9677s\n",
      "\titers: 200, epoch: 32 | loss: 0.0052754\n",
      "\tspeed: 0.0264s/iter; left time: 405.8018s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.77s\n",
      "Steps: 226 | Train Loss: 0.0052094 Vali Loss: 0.0086470 Test Loss: 0.0099568\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0048150\n",
      "\tspeed: 0.0542s/iter; left time: 827.8754s\n",
      "\titers: 200, epoch: 33 | loss: 0.0051399\n",
      "\tspeed: 0.0270s/iter; left time: 409.0242s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.54s\n",
      "Steps: 226 | Train Loss: 0.0052055 Vali Loss: 0.0086455 Test Loss: 0.0099734\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0053405\n",
      "\tspeed: 0.0544s/iter; left time: 818.6350s\n",
      "\titers: 200, epoch: 34 | loss: 0.0056786\n",
      "\tspeed: 0.0247s/iter; left time: 369.4467s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 226 | Train Loss: 0.0052045 Vali Loss: 0.0086125 Test Loss: 0.0099662\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0055080\n",
      "\tspeed: 0.0579s/iter; left time: 858.3640s\n",
      "\titers: 200, epoch: 35 | loss: 0.0048850\n",
      "\tspeed: 0.0247s/iter; left time: 363.5174s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:06.73s\n",
      "Steps: 226 | Train Loss: 0.0051874 Vali Loss: 0.0086392 Test Loss: 0.0099633\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_24_FR_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009935157373547554, rmse:0.0996752604842186, mae:0.056855376809835434, rse:0.38454437255859375\n",
      "Intermediate time for FR and pred_len 24: 00h:05m:09.03s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_96_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0254300\n",
      "\tspeed: 0.0466s/iter; left time: 1044.5209s\n",
      "\titers: 200, epoch: 1 | loss: 0.0226345\n",
      "\tspeed: 0.0276s/iter; left time: 616.1690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 225 | Train Loss: 0.0254588 Vali Loss: 0.0273043 Test Loss: 0.0352626\n",
      "Validation loss decreased (inf --> 0.027304).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0142459\n",
      "\tspeed: 0.0601s/iter; left time: 1333.5906s\n",
      "\titers: 200, epoch: 2 | loss: 0.0128824\n",
      "\tspeed: 0.0366s/iter; left time: 807.4442s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:07.98s\n",
      "Steps: 225 | Train Loss: 0.0145184 Vali Loss: 0.0163889 Test Loss: 0.0206476\n",
      "Validation loss decreased (0.027304 --> 0.016389).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0115962\n",
      "\tspeed: 0.0549s/iter; left time: 1205.2686s\n",
      "\titers: 200, epoch: 3 | loss: 0.0122913\n",
      "\tspeed: 0.0310s/iter; left time: 677.5091s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.54s\n",
      "Steps: 225 | Train Loss: 0.0118228 Vali Loss: 0.0154873 Test Loss: 0.0198312\n",
      "Validation loss decreased (0.016389 --> 0.015487).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120922\n",
      "\tspeed: 0.0623s/iter; left time: 1353.8541s\n",
      "\titers: 200, epoch: 4 | loss: 0.0113413\n",
      "\tspeed: 0.0300s/iter; left time: 647.8548s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0112676 Vali Loss: 0.0152080 Test Loss: 0.0196720\n",
      "Validation loss decreased (0.015487 --> 0.015208).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0102608\n",
      "\tspeed: 0.0648s/iter; left time: 1393.2321s\n",
      "\titers: 200, epoch: 5 | loss: 0.0104193\n",
      "\tspeed: 0.0377s/iter; left time: 806.3203s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:08.41s\n",
      "Steps: 225 | Train Loss: 0.0109707 Vali Loss: 0.0150341 Test Loss: 0.0193699\n",
      "Validation loss decreased (0.015208 --> 0.015034).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0108786\n",
      "\tspeed: 0.0612s/iter; left time: 1303.0145s\n",
      "\titers: 200, epoch: 6 | loss: 0.0100191\n",
      "\tspeed: 0.0305s/iter; left time: 645.2362s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.59s\n",
      "Steps: 225 | Train Loss: 0.0106828 Vali Loss: 0.0148039 Test Loss: 0.0193114\n",
      "Validation loss decreased (0.015034 --> 0.014804).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0110777\n",
      "\tspeed: 0.0646s/iter; left time: 1359.5216s\n",
      "\titers: 200, epoch: 7 | loss: 0.0127516\n",
      "\tspeed: 0.0340s/iter; left time: 712.6072s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:07.87s\n",
      "Steps: 225 | Train Loss: 0.0104120 Vali Loss: 0.0146961 Test Loss: 0.0191919\n",
      "Validation loss decreased (0.014804 --> 0.014696).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0092179\n",
      "\tspeed: 0.0595s/iter; left time: 1240.1219s\n",
      "\titers: 200, epoch: 8 | loss: 0.0089943\n",
      "\tspeed: 0.0282s/iter; left time: 584.1662s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.91s\n",
      "Steps: 225 | Train Loss: 0.0101818 Vali Loss: 0.0145038 Test Loss: 0.0191868\n",
      "Validation loss decreased (0.014696 --> 0.014504).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0098994\n",
      "\tspeed: 0.0581s/iter; left time: 1196.0263s\n",
      "\titers: 200, epoch: 9 | loss: 0.0097919\n",
      "\tspeed: 0.0307s/iter; left time: 629.9677s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.97s\n",
      "Steps: 225 | Train Loss: 0.0100057 Vali Loss: 0.0145380 Test Loss: 0.0191176\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0096034\n",
      "\tspeed: 0.0547s/iter; left time: 1114.2371s\n",
      "\titers: 200, epoch: 10 | loss: 0.0100670\n",
      "\tspeed: 0.0273s/iter; left time: 552.5516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.59s\n",
      "Steps: 225 | Train Loss: 0.0098874 Vali Loss: 0.0145757 Test Loss: 0.0192410\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0098913\n",
      "\tspeed: 0.0588s/iter; left time: 1185.8312s\n",
      "\titers: 200, epoch: 11 | loss: 0.0097004\n",
      "\tspeed: 0.0306s/iter; left time: 614.5099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.30s\n",
      "Steps: 225 | Train Loss: 0.0097905 Vali Loss: 0.0145232 Test Loss: 0.0191619\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0081249\n",
      "\tspeed: 0.0618s/iter; left time: 1231.0164s\n",
      "\titers: 200, epoch: 12 | loss: 0.0093403\n",
      "\tspeed: 0.0350s/iter; left time: 694.0710s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:08.23s\n",
      "Steps: 225 | Train Loss: 0.0096884 Vali Loss: 0.0146157 Test Loss: 0.0192267\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0094197\n",
      "\tspeed: 0.0696s/iter; left time: 1370.3090s\n",
      "\titers: 200, epoch: 13 | loss: 0.0094363\n",
      "\tspeed: 0.0310s/iter; left time: 607.9622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.82s\n",
      "Steps: 225 | Train Loss: 0.0095827 Vali Loss: 0.0145366 Test Loss: 0.0191297\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0100738\n",
      "\tspeed: 0.0626s/iter; left time: 1218.8052s\n",
      "\titers: 200, epoch: 14 | loss: 0.0103495\n",
      "\tspeed: 0.0325s/iter; left time: 630.1532s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0094880 Vali Loss: 0.0146680 Test Loss: 0.0193109\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0093224\n",
      "\tspeed: 0.0582s/iter; left time: 1119.8090s\n",
      "\titers: 200, epoch: 15 | loss: 0.0091487\n",
      "\tspeed: 0.0327s/iter; left time: 626.3071s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.72s\n",
      "Steps: 225 | Train Loss: 0.0093999 Vali Loss: 0.0145867 Test Loss: 0.0193278\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0093353\n",
      "\tspeed: 0.0560s/iter; left time: 1065.4500s\n",
      "\titers: 200, epoch: 16 | loss: 0.0092124\n",
      "\tspeed: 0.0249s/iter; left time: 471.7984s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.30s\n",
      "Steps: 225 | Train Loss: 0.0093282 Vali Loss: 0.0145779 Test Loss: 0.0191555\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0087824\n",
      "\tspeed: 0.0517s/iter; left time: 972.2200s\n",
      "\titers: 200, epoch: 17 | loss: 0.0086737\n",
      "\tspeed: 0.0283s/iter; left time: 529.2222s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 225 | Train Loss: 0.0092395 Vali Loss: 0.0146747 Test Loss: 0.0192166\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0088149\n",
      "\tspeed: 0.0655s/iter; left time: 1216.3936s\n",
      "\titers: 200, epoch: 18 | loss: 0.0099954\n",
      "\tspeed: 0.0264s/iter; left time: 487.3639s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.83s\n",
      "Steps: 225 | Train Loss: 0.0091820 Vali Loss: 0.0147064 Test Loss: 0.0192700\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_96_FR_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.019186770543456078, rmse:0.13851632177829742, mae:0.08384789526462555, rse:0.5358179211616516\n",
      "Intermediate time for FR and pred_len 96: 00h:02m:53.05s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_FR_168_168_FR', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='FR_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0269169\n",
      "\tspeed: 0.0525s/iter; left time: 1177.1305s\n",
      "\titers: 200, epoch: 1 | loss: 0.0220217\n",
      "\tspeed: 0.0244s/iter; left time: 543.9332s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.12s\n",
      "Steps: 225 | Train Loss: 0.0276227 Vali Loss: 0.0292230 Test Loss: 0.0370285\n",
      "Validation loss decreased (inf --> 0.029223).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0149196\n",
      "\tspeed: 0.0560s/iter; left time: 1241.4105s\n",
      "\titers: 200, epoch: 2 | loss: 0.0117432\n",
      "\tspeed: 0.0302s/iter; left time: 666.2404s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:06.89s\n",
      "Steps: 225 | Train Loss: 0.0159841 Vali Loss: 0.0176442 Test Loss: 0.0221663\n",
      "Validation loss decreased (0.029223 --> 0.017644).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0130947\n",
      "\tspeed: 0.0558s/iter; left time: 1225.4648s\n",
      "\titers: 200, epoch: 3 | loss: 0.0118973\n",
      "\tspeed: 0.0245s/iter; left time: 534.9855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.16s\n",
      "Steps: 225 | Train Loss: 0.0131677 Vali Loss: 0.0168402 Test Loss: 0.0214203\n",
      "Validation loss decreased (0.017644 --> 0.016840).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0120702\n",
      "\tspeed: 0.0516s/iter; left time: 1120.5532s\n",
      "\titers: 200, epoch: 4 | loss: 0.0153237\n",
      "\tspeed: 0.0275s/iter; left time: 595.6620s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:06.17s\n",
      "Steps: 225 | Train Loss: 0.0126011 Vali Loss: 0.0166417 Test Loss: 0.0214589\n",
      "Validation loss decreased (0.016840 --> 0.016642).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0122577\n",
      "\tspeed: 0.0591s/iter; left time: 1271.3524s\n",
      "\titers: 200, epoch: 5 | loss: 0.0114579\n",
      "\tspeed: 0.0310s/iter; left time: 663.5821s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:06.94s\n",
      "Steps: 225 | Train Loss: 0.0122536 Vali Loss: 0.0165372 Test Loss: 0.0212060\n",
      "Validation loss decreased (0.016642 --> 0.016537).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0122982\n",
      "\tspeed: 0.0553s/iter; left time: 1177.0684s\n",
      "\titers: 200, epoch: 6 | loss: 0.0108476\n",
      "\tspeed: 0.0239s/iter; left time: 505.1855s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:06.05s\n",
      "Steps: 225 | Train Loss: 0.0119525 Vali Loss: 0.0163623 Test Loss: 0.0210994\n",
      "Validation loss decreased (0.016537 --> 0.016362).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0103870\n",
      "\tspeed: 0.0535s/iter; left time: 1125.2335s\n",
      "\titers: 200, epoch: 7 | loss: 0.0120801\n",
      "\tspeed: 0.0255s/iter; left time: 534.3685s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.40s\n",
      "Steps: 225 | Train Loss: 0.0116693 Vali Loss: 0.0163048 Test Loss: 0.0210180\n",
      "Validation loss decreased (0.016362 --> 0.016305).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0126737\n",
      "\tspeed: 0.0701s/iter; left time: 1458.8931s\n",
      "\titers: 200, epoch: 8 | loss: 0.0116746\n",
      "\tspeed: 0.0261s/iter; left time: 540.1608s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:07.55s\n",
      "Steps: 225 | Train Loss: 0.0114354 Vali Loss: 0.0161684 Test Loss: 0.0209229\n",
      "Validation loss decreased (0.016305 --> 0.016168).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0114568\n",
      "\tspeed: 0.0590s/iter; left time: 1215.5890s\n",
      "\titers: 200, epoch: 9 | loss: 0.0103613\n",
      "\tspeed: 0.0258s/iter; left time: 528.1320s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 225 | Train Loss: 0.0112451 Vali Loss: 0.0162233 Test Loss: 0.0209913\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0106115\n",
      "\tspeed: 0.0555s/iter; left time: 1131.7468s\n",
      "\titers: 200, epoch: 10 | loss: 0.0107473\n",
      "\tspeed: 0.0241s/iter; left time: 489.1191s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.23s\n",
      "Steps: 225 | Train Loss: 0.0110886 Vali Loss: 0.0161682 Test Loss: 0.0208051\n",
      "Validation loss decreased (0.016168 --> 0.016168).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0095515\n",
      "\tspeed: 0.0610s/iter; left time: 1228.3944s\n",
      "\titers: 200, epoch: 11 | loss: 0.0108994\n",
      "\tspeed: 0.0301s/iter; left time: 604.2637s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.24s\n",
      "Steps: 225 | Train Loss: 0.0109432 Vali Loss: 0.0162758 Test Loss: 0.0210103\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0102318\n",
      "\tspeed: 0.0624s/iter; left time: 1243.9470s\n",
      "\titers: 200, epoch: 12 | loss: 0.0117106\n",
      "\tspeed: 0.0308s/iter; left time: 611.2043s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.24s\n",
      "Steps: 225 | Train Loss: 0.0108397 Vali Loss: 0.0163587 Test Loss: 0.0211157\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0103174\n",
      "\tspeed: 0.0552s/iter; left time: 1087.1800s\n",
      "\titers: 200, epoch: 13 | loss: 0.0115475\n",
      "\tspeed: 0.0374s/iter; left time: 732.2872s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.69s\n",
      "Steps: 225 | Train Loss: 0.0107320 Vali Loss: 0.0163240 Test Loss: 0.0209834\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0106057\n",
      "\tspeed: 0.0588s/iter; left time: 1145.3310s\n",
      "\titers: 200, epoch: 14 | loss: 0.0110867\n",
      "\tspeed: 0.0269s/iter; left time: 520.3227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.88s\n",
      "Steps: 225 | Train Loss: 0.0106457 Vali Loss: 0.0163372 Test Loss: 0.0208973\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0099765\n",
      "\tspeed: 0.0594s/iter; left time: 1143.2446s\n",
      "\titers: 200, epoch: 15 | loss: 0.0116516\n",
      "\tspeed: 0.0275s/iter; left time: 526.7832s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.46s\n",
      "Steps: 225 | Train Loss: 0.0105807 Vali Loss: 0.0163443 Test Loss: 0.0209021\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0096081\n",
      "\tspeed: 0.0465s/iter; left time: 885.4802s\n",
      "\titers: 200, epoch: 16 | loss: 0.0116073\n",
      "\tspeed: 0.0250s/iter; left time: 472.4955s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:05.82s\n",
      "Steps: 225 | Train Loss: 0.0105108 Vali Loss: 0.0164842 Test Loss: 0.0211343\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0115879\n",
      "\tspeed: 0.0568s/iter; left time: 1067.3560s\n",
      "\titers: 200, epoch: 17 | loss: 0.0106102\n",
      "\tspeed: 0.0367s/iter; left time: 685.6871s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:07.47s\n",
      "Steps: 225 | Train Loss: 0.0104455 Vali Loss: 0.0163789 Test Loss: 0.0211149\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0106645\n",
      "\tspeed: 0.0610s/iter; left time: 1133.4480s\n",
      "\titers: 200, epoch: 18 | loss: 0.0108051\n",
      "\tspeed: 0.0297s/iter; left time: 548.4628s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:07.31s\n",
      "Steps: 225 | Train Loss: 0.0103969 Vali Loss: 0.0164130 Test Loss: 0.0211276\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0107501\n",
      "\tspeed: 0.0599s/iter; left time: 1098.6225s\n",
      "\titers: 200, epoch: 19 | loss: 0.0097857\n",
      "\tspeed: 0.0263s/iter; left time: 479.3413s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:06.84s\n",
      "Steps: 225 | Train Loss: 0.0103496 Vali Loss: 0.0164557 Test Loss: 0.0210654\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0101434\n",
      "\tspeed: 0.0603s/iter; left time: 1093.2678s\n",
      "\titers: 200, epoch: 20 | loss: 0.0112589\n",
      "\tspeed: 0.0242s/iter; left time: 435.6044s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:06.42s\n",
      "Steps: 225 | Train Loss: 0.0103155 Vali Loss: 0.0165480 Test Loss: 0.0210437\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_FR_168_168_FR_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.020805105566978455, rmse:0.14423975348472595, mae:0.08963675051927567, rse:0.5586541891098022\n",
      "Intermediate time for FR and pred_len 168: 00h:03m:00.74s\n",
      "Intermediate time for FR: 00h:11m:02.82s\n",
      "\n",
      "=== Starting experiments for country: IT ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_24_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=24, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0430156\n",
      "\tspeed: 0.0542s/iter; left time: 1219.3584s\n",
      "\titers: 200, epoch: 1 | loss: 0.0372121\n",
      "\tspeed: 0.0293s/iter; left time: 655.9818s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:06.58s\n",
      "Steps: 226 | Train Loss: 0.0441541 Vali Loss: 0.0310717 Test Loss: 0.0345077\n",
      "Validation loss decreased (inf --> 0.031072).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0152964\n",
      "\tspeed: 0.0515s/iter; left time: 1146.2866s\n",
      "\titers: 200, epoch: 2 | loss: 0.0113698\n",
      "\tspeed: 0.0255s/iter; left time: 565.9941s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:05.99s\n",
      "Steps: 226 | Train Loss: 0.0179926 Vali Loss: 0.0115744 Test Loss: 0.0126363\n",
      "Validation loss decreased (0.031072 --> 0.011574).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0125631\n",
      "\tspeed: 0.0575s/iter; left time: 1268.2801s\n",
      "\titers: 200, epoch: 3 | loss: 0.0114366\n",
      "\tspeed: 0.0266s/iter; left time: 583.1139s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:06.77s\n",
      "Steps: 226 | Train Loss: 0.0120551 Vali Loss: 0.0104010 Test Loss: 0.0114574\n",
      "Validation loss decreased (0.011574 --> 0.010401).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0110885\n",
      "\tspeed: 0.0596s/iter; left time: 1300.6696s\n",
      "\titers: 200, epoch: 4 | loss: 0.0099184\n",
      "\tspeed: 0.0321s/iter; left time: 696.3817s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:07.85s\n",
      "Steps: 226 | Train Loss: 0.0110524 Vali Loss: 0.0100088 Test Loss: 0.0111228\n",
      "Validation loss decreased (0.010401 --> 0.010009).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0098978\n",
      "\tspeed: 0.0611s/iter; left time: 1318.8958s\n",
      "\titers: 200, epoch: 5 | loss: 0.0116828\n",
      "\tspeed: 0.0310s/iter; left time: 666.8761s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.26s\n",
      "Steps: 226 | Train Loss: 0.0105525 Vali Loss: 0.0096835 Test Loss: 0.0107585\n",
      "Validation loss decreased (0.010009 --> 0.009683).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0114763\n",
      "\tspeed: 0.0553s/iter; left time: 1182.6344s\n",
      "\titers: 200, epoch: 6 | loss: 0.0109357\n",
      "\tspeed: 0.0339s/iter; left time: 721.2530s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.39s\n",
      "Steps: 226 | Train Loss: 0.0102527 Vali Loss: 0.0095776 Test Loss: 0.0106026\n",
      "Validation loss decreased (0.009683 --> 0.009578).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0090321\n",
      "\tspeed: 0.0540s/iter; left time: 1142.2472s\n",
      "\titers: 200, epoch: 7 | loss: 0.0095975\n",
      "\tspeed: 0.0292s/iter; left time: 614.2884s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.49s\n",
      "Steps: 226 | Train Loss: 0.0099702 Vali Loss: 0.0094630 Test Loss: 0.0105136\n",
      "Validation loss decreased (0.009578 --> 0.009463).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0093714\n",
      "\tspeed: 0.0597s/iter; left time: 1247.9077s\n",
      "\titers: 200, epoch: 8 | loss: 0.0081001\n",
      "\tspeed: 0.0391s/iter; left time: 814.6955s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:08.21s\n",
      "Steps: 226 | Train Loss: 0.0097985 Vali Loss: 0.0093102 Test Loss: 0.0103237\n",
      "Validation loss decreased (0.009463 --> 0.009310).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0094263\n",
      "\tspeed: 0.0521s/iter; left time: 1077.9675s\n",
      "\titers: 200, epoch: 9 | loss: 0.0099736\n",
      "\tspeed: 0.0325s/iter; left time: 669.7630s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:06.82s\n",
      "Steps: 226 | Train Loss: 0.0096688 Vali Loss: 0.0092797 Test Loss: 0.0102676\n",
      "Validation loss decreased (0.009310 --> 0.009280).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0085524\n",
      "\tspeed: 0.0598s/iter; left time: 1223.3745s\n",
      "\titers: 200, epoch: 10 | loss: 0.0101026\n",
      "\tspeed: 0.0242s/iter; left time: 492.8865s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:06.74s\n",
      "Steps: 226 | Train Loss: 0.0095466 Vali Loss: 0.0093089 Test Loss: 0.0102406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0096203\n",
      "\tspeed: 0.0528s/iter; left time: 1068.9342s\n",
      "\titers: 200, epoch: 11 | loss: 0.0088058\n",
      "\tspeed: 0.0258s/iter; left time: 519.7897s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:06.28s\n",
      "Steps: 226 | Train Loss: 0.0094569 Vali Loss: 0.0092299 Test Loss: 0.0102465\n",
      "Validation loss decreased (0.009280 --> 0.009230).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0088285\n",
      "\tspeed: 0.0531s/iter; left time: 1062.7348s\n",
      "\titers: 200, epoch: 12 | loss: 0.0094550\n",
      "\tspeed: 0.0307s/iter; left time: 611.8542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:06.53s\n",
      "Steps: 226 | Train Loss: 0.0093679 Vali Loss: 0.0092287 Test Loss: 0.0101918\n",
      "Validation loss decreased (0.009230 --> 0.009229).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0094994\n",
      "\tspeed: 0.0581s/iter; left time: 1149.4300s\n",
      "\titers: 200, epoch: 13 | loss: 0.0095642\n",
      "\tspeed: 0.0254s/iter; left time: 499.4905s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:06.83s\n",
      "Steps: 226 | Train Loss: 0.0093120 Vali Loss: 0.0091062 Test Loss: 0.0100900\n",
      "Validation loss decreased (0.009229 --> 0.009106).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0090722\n",
      "\tspeed: 0.0507s/iter; left time: 992.2097s\n",
      "\titers: 200, epoch: 14 | loss: 0.0097731\n",
      "\tspeed: 0.0283s/iter; left time: 550.1150s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0092447 Vali Loss: 0.0091206 Test Loss: 0.0100921\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0097098\n",
      "\tspeed: 0.0505s/iter; left time: 976.0067s\n",
      "\titers: 200, epoch: 15 | loss: 0.0098209\n",
      "\tspeed: 0.0253s/iter; left time: 487.1089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:06.06s\n",
      "Steps: 226 | Train Loss: 0.0091855 Vali Loss: 0.0090958 Test Loss: 0.0100805\n",
      "Validation loss decreased (0.009106 --> 0.009096).  Saving model ...\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0091442\n",
      "\tspeed: 0.0566s/iter; left time: 1081.6922s\n",
      "\titers: 200, epoch: 16 | loss: 0.0073901\n",
      "\tspeed: 0.0308s/iter; left time: 585.8950s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:07.13s\n",
      "Steps: 226 | Train Loss: 0.0091526 Vali Loss: 0.0091489 Test Loss: 0.0100649\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0086832\n",
      "\tspeed: 0.0500s/iter; left time: 943.4212s\n",
      "\titers: 200, epoch: 17 | loss: 0.0081065\n",
      "\tspeed: 0.0297s/iter; left time: 557.3880s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:06.65s\n",
      "Steps: 226 | Train Loss: 0.0091120 Vali Loss: 0.0090676 Test Loss: 0.0100418\n",
      "Validation loss decreased (0.009096 --> 0.009068).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0089915\n",
      "\tspeed: 0.0553s/iter; left time: 1031.9393s\n",
      "\titers: 200, epoch: 18 | loss: 0.0104773\n",
      "\tspeed: 0.0270s/iter; left time: 500.9996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.55s\n",
      "Steps: 226 | Train Loss: 0.0090761 Vali Loss: 0.0090888 Test Loss: 0.0100091\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0087572\n",
      "\tspeed: 0.0504s/iter; left time: 929.6214s\n",
      "\titers: 200, epoch: 19 | loss: 0.0087940\n",
      "\tspeed: 0.0245s/iter; left time: 448.4910s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:05.71s\n",
      "Steps: 226 | Train Loss: 0.0090523 Vali Loss: 0.0090600 Test Loss: 0.0100221\n",
      "Validation loss decreased (0.009068 --> 0.009060).  Saving model ...\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0086708\n",
      "\tspeed: 0.0492s/iter; left time: 896.2677s\n",
      "\titers: 200, epoch: 20 | loss: 0.0089390\n",
      "\tspeed: 0.0255s/iter; left time: 462.4140s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:05.85s\n",
      "Steps: 226 | Train Loss: 0.0090191 Vali Loss: 0.0090712 Test Loss: 0.0099884\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0102910\n",
      "\tspeed: 0.0480s/iter; left time: 863.6423s\n",
      "\titers: 200, epoch: 21 | loss: 0.0078968\n",
      "\tspeed: 0.0260s/iter; left time: 465.2913s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:05.86s\n",
      "Steps: 226 | Train Loss: 0.0089772 Vali Loss: 0.0090484 Test Loss: 0.0099763\n",
      "Validation loss decreased (0.009060 --> 0.009048).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0083771\n",
      "\tspeed: 0.0547s/iter; left time: 971.6334s\n",
      "\titers: 200, epoch: 22 | loss: 0.0077762\n",
      "\tspeed: 0.0240s/iter; left time: 423.3853s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:06.56s\n",
      "Steps: 226 | Train Loss: 0.0089653 Vali Loss: 0.0090280 Test Loss: 0.0099728\n",
      "Validation loss decreased (0.009048 --> 0.009028).  Saving model ...\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0089312\n",
      "\tspeed: 0.0583s/iter; left time: 1022.5282s\n",
      "\titers: 200, epoch: 23 | loss: 0.0089807\n",
      "\tspeed: 0.0264s/iter; left time: 459.6983s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:06.62s\n",
      "Steps: 226 | Train Loss: 0.0089423 Vali Loss: 0.0090344 Test Loss: 0.0099611\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.0078890\n",
      "\tspeed: 0.0566s/iter; left time: 978.6115s\n",
      "\titers: 200, epoch: 24 | loss: 0.0085228\n",
      "\tspeed: 0.0320s/iter; left time: 550.6104s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 24\n",
      "Cost time: 00h:00m:07.33s\n",
      "Steps: 226 | Train Loss: 0.0089327 Vali Loss: 0.0090320 Test Loss: 0.0099370\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.0097201\n",
      "\tspeed: 0.0547s/iter; left time: 934.0444s\n",
      "\titers: 200, epoch: 25 | loss: 0.0089450\n",
      "\tspeed: 0.0244s/iter; left time: 414.1116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 25\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0089084 Vali Loss: 0.0090175 Test Loss: 0.0099251\n",
      "Validation loss decreased (0.009028 --> 0.009017).  Saving model ...\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.0097886\n",
      "\tspeed: 0.0583s/iter; left time: 982.0724s\n",
      "\titers: 200, epoch: 26 | loss: 0.0076452\n",
      "\tspeed: 0.0284s/iter; left time: 476.0494s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 26\n",
      "Cost time: 00h:00m:07.07s\n",
      "Steps: 226 | Train Loss: 0.0089052 Vali Loss: 0.0090332 Test Loss: 0.0099195\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.0091809\n",
      "\tspeed: 0.0491s/iter; left time: 817.0285s\n",
      "\titers: 200, epoch: 27 | loss: 0.0093014\n",
      "\tspeed: 0.0246s/iter; left time: 405.7418s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 27\n",
      "Cost time: 00h:00m:05.79s\n",
      "Steps: 226 | Train Loss: 0.0088916 Vali Loss: 0.0090062 Test Loss: 0.0099227\n",
      "Validation loss decreased (0.009017 --> 0.009006).  Saving model ...\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.0083350\n",
      "\tspeed: 0.0564s/iter; left time: 925.2712s\n",
      "\titers: 200, epoch: 28 | loss: 0.0081989\n",
      "\tspeed: 0.0256s/iter; left time: 418.0152s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 28\n",
      "Cost time: 00h:00m:06.50s\n",
      "Steps: 226 | Train Loss: 0.0088817 Vali Loss: 0.0090501 Test Loss: 0.0099505\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.0090654\n",
      "\tspeed: 0.0501s/iter; left time: 810.3152s\n",
      "\titers: 200, epoch: 29 | loss: 0.0101934\n",
      "\tspeed: 0.0266s/iter; left time: 427.6180s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 29\n",
      "Cost time: 00h:00m:06.33s\n",
      "Steps: 226 | Train Loss: 0.0088651 Vali Loss: 0.0090426 Test Loss: 0.0099366\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.0100033\n",
      "\tspeed: 0.0529s/iter; left time: 843.7943s\n",
      "\titers: 200, epoch: 30 | loss: 0.0091661\n",
      "\tspeed: 0.0236s/iter; left time: 374.7100s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 30\n",
      "Cost time: 00h:00m:05.92s\n",
      "Steps: 226 | Train Loss: 0.0088670 Vali Loss: 0.0090467 Test Loss: 0.0099255\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.0078916\n",
      "\tspeed: 0.0529s/iter; left time: 832.2571s\n",
      "\titers: 200, epoch: 31 | loss: 0.0098773\n",
      "\tspeed: 0.0276s/iter; left time: 430.8198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 31\n",
      "Cost time: 00h:00m:06.37s\n",
      "Steps: 226 | Train Loss: 0.0088527 Vali Loss: 0.0090145 Test Loss: 0.0099058\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.0085965\n",
      "\tspeed: 0.0513s/iter; left time: 794.7831s\n",
      "\titers: 200, epoch: 32 | loss: 0.0095884\n",
      "\tspeed: 0.0244s/iter; left time: 375.5542s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 32\n",
      "Cost time: 00h:00m:06.11s\n",
      "Steps: 226 | Train Loss: 0.0088357 Vali Loss: 0.0090503 Test Loss: 0.0099251\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.0078262\n",
      "\tspeed: 0.0540s/iter; left time: 824.3251s\n",
      "\titers: 200, epoch: 33 | loss: 0.0090609\n",
      "\tspeed: 0.0273s/iter; left time: 414.7048s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 33\n",
      "Cost time: 00h:00m:06.70s\n",
      "Steps: 226 | Train Loss: 0.0088254 Vali Loss: 0.0090212 Test Loss: 0.0099334\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.0095772\n",
      "\tspeed: 0.0596s/iter; left time: 896.4411s\n",
      "\titers: 200, epoch: 34 | loss: 0.0093180\n",
      "\tspeed: 0.0309s/iter; left time: 461.4606s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 34\n",
      "Cost time: 00h:00m:07.22s\n",
      "Steps: 226 | Train Loss: 0.0088139 Vali Loss: 0.0090119 Test Loss: 0.0099190\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.0089763\n",
      "\tspeed: 0.0459s/iter; left time: 680.6396s\n",
      "\titers: 200, epoch: 35 | loss: 0.0093619\n",
      "\tspeed: 0.0242s/iter; left time: 355.4157s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 35\n",
      "Cost time: 00h:00m:05.59s\n",
      "Steps: 226 | Train Loss: 0.0088183 Vali Loss: 0.0090063 Test Loss: 0.0099132\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.0080900\n",
      "\tspeed: 0.0515s/iter; left time: 750.9620s\n",
      "\titers: 200, epoch: 36 | loss: 0.0077684\n",
      "\tspeed: 0.0273s/iter; left time: 396.2352s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 36\n",
      "Cost time: 00h:00m:06.32s\n",
      "Steps: 226 | Train Loss: 0.0088260 Vali Loss: 0.0090389 Test Loss: 0.0099133\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.0091542\n",
      "\tspeed: 0.0482s/iter; left time: 692.9952s\n",
      "\titers: 200, epoch: 37 | loss: 0.0082184\n",
      "\tspeed: 0.0232s/iter; left time: 330.3205s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 37\n",
      "Cost time: 00h:00m:05.63s\n",
      "Steps: 226 | Train Loss: 0.0088172 Vali Loss: 0.0090156 Test Loss: 0.0099207\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_24_IT_PatchTST_custom_ftM_sl168_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.009922743774950504, rmse:0.09961297363042831, mae:0.05834198370575905, rse:0.3763883709907532\n",
      "Intermediate time for IT and pred_len 24: 00h:05m:12.70s\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_96_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=96, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0478222\n",
      "\tspeed: 0.0504s/iter; left time: 1128.4772s\n",
      "\titers: 200, epoch: 1 | loss: 0.0381344\n",
      "\tspeed: 0.0344s/iter; left time: 766.1421s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:07.08s\n",
      "Steps: 225 | Train Loss: 0.0474403 Vali Loss: 0.0352382 Test Loss: 0.0391712\n",
      "Validation loss decreased (inf --> 0.035238).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0253392\n",
      "\tspeed: 0.0644s/iter; left time: 1427.8207s\n",
      "\titers: 200, epoch: 2 | loss: 0.0213449\n",
      "\tspeed: 0.0344s/iter; left time: 759.6715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:08.28s\n",
      "Steps: 225 | Train Loss: 0.0254757 Vali Loss: 0.0179431 Test Loss: 0.0199769\n",
      "Validation loss decreased (0.035238 --> 0.017943).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0204113\n",
      "\tspeed: 0.0595s/iter; left time: 1305.1059s\n",
      "\titers: 200, epoch: 3 | loss: 0.0188128\n",
      "\tspeed: 0.0384s/iter; left time: 838.0136s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:08.12s\n",
      "Steps: 225 | Train Loss: 0.0195972 Vali Loss: 0.0166414 Test Loss: 0.0186526\n",
      "Validation loss decreased (0.017943 --> 0.016641).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0188461\n",
      "\tspeed: 0.0688s/iter; left time: 1494.3791s\n",
      "\titers: 200, epoch: 4 | loss: 0.0188044\n",
      "\tspeed: 0.0381s/iter; left time: 823.9522s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:08.80s\n",
      "Steps: 225 | Train Loss: 0.0185892 Vali Loss: 0.0163968 Test Loss: 0.0184007\n",
      "Validation loss decreased (0.016641 --> 0.016397).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0179484\n",
      "\tspeed: 0.0653s/iter; left time: 1402.9415s\n",
      "\titers: 200, epoch: 5 | loss: 0.0178978\n",
      "\tspeed: 0.0309s/iter; left time: 661.3506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:07.71s\n",
      "Steps: 225 | Train Loss: 0.0180306 Vali Loss: 0.0161476 Test Loss: 0.0181733\n",
      "Validation loss decreased (0.016397 --> 0.016148).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0198851\n",
      "\tspeed: 0.0607s/iter; left time: 1290.9299s\n",
      "\titers: 200, epoch: 6 | loss: 0.0186755\n",
      "\tspeed: 0.0389s/iter; left time: 823.1098s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:07.93s\n",
      "Steps: 225 | Train Loss: 0.0176643 Vali Loss: 0.0160395 Test Loss: 0.0180845\n",
      "Validation loss decreased (0.016148 --> 0.016040).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0174389\n",
      "\tspeed: 0.0582s/iter; left time: 1224.4866s\n",
      "\titers: 200, epoch: 7 | loss: 0.0180951\n",
      "\tspeed: 0.0243s/iter; left time: 509.7486s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:06.20s\n",
      "Steps: 225 | Train Loss: 0.0173723 Vali Loss: 0.0158992 Test Loss: 0.0178840\n",
      "Validation loss decreased (0.016040 --> 0.015899).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0174347\n",
      "\tspeed: 0.0541s/iter; left time: 1126.1975s\n",
      "\titers: 200, epoch: 8 | loss: 0.0167886\n",
      "\tspeed: 0.0322s/iter; left time: 666.6907s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:06.79s\n",
      "Steps: 225 | Train Loss: 0.0171305 Vali Loss: 0.0158986 Test Loss: 0.0178638\n",
      "Validation loss decreased (0.015899 --> 0.015899).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0182739\n",
      "\tspeed: 0.0615s/iter; left time: 1266.8376s\n",
      "\titers: 200, epoch: 9 | loss: 0.0169673\n",
      "\tspeed: 0.0300s/iter; left time: 615.0006s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:07.34s\n",
      "Steps: 225 | Train Loss: 0.0169175 Vali Loss: 0.0158338 Test Loss: 0.0178399\n",
      "Validation loss decreased (0.015899 --> 0.015834).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0168277\n",
      "\tspeed: 0.0633s/iter; left time: 1289.8898s\n",
      "\titers: 200, epoch: 10 | loss: 0.0179330\n",
      "\tspeed: 0.0273s/iter; left time: 553.6355s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:07.52s\n",
      "Steps: 225 | Train Loss: 0.0167715 Vali Loss: 0.0157845 Test Loss: 0.0177933\n",
      "Validation loss decreased (0.015834 --> 0.015784).  Saving model ...\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0163170\n",
      "\tspeed: 0.0645s/iter; left time: 1299.0787s\n",
      "\titers: 200, epoch: 11 | loss: 0.0163759\n",
      "\tspeed: 0.0258s/iter; left time: 516.3500s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0166167 Vali Loss: 0.0157842 Test Loss: 0.0178167\n",
      "Validation loss decreased (0.015784 --> 0.015784).  Saving model ...\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0151263\n",
      "\tspeed: 0.0646s/iter; left time: 1286.8668s\n",
      "\titers: 200, epoch: 12 | loss: 0.0166397\n",
      "\tspeed: 0.0265s/iter; left time: 524.7317s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:07.41s\n",
      "Steps: 225 | Train Loss: 0.0165070 Vali Loss: 0.0157736 Test Loss: 0.0178441\n",
      "Validation loss decreased (0.015784 --> 0.015774).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0164019\n",
      "\tspeed: 0.0620s/iter; left time: 1221.5179s\n",
      "\titers: 200, epoch: 13 | loss: 0.0159479\n",
      "\tspeed: 0.0319s/iter; left time: 625.8896s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0163756 Vali Loss: 0.0157088 Test Loss: 0.0178032\n",
      "Validation loss decreased (0.015774 --> 0.015709).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0154372\n",
      "\tspeed: 0.0603s/iter; left time: 1173.8247s\n",
      "\titers: 200, epoch: 14 | loss: 0.0160886\n",
      "\tspeed: 0.0308s/iter; left time: 596.5937s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:07.67s\n",
      "Steps: 225 | Train Loss: 0.0162828 Vali Loss: 0.0158301 Test Loss: 0.0179277\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0154287\n",
      "\tspeed: 0.0599s/iter; left time: 1154.0704s\n",
      "\titers: 200, epoch: 15 | loss: 0.0153177\n",
      "\tspeed: 0.0334s/iter; left time: 638.7949s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:07.15s\n",
      "Steps: 225 | Train Loss: 0.0161906 Vali Loss: 0.0157809 Test Loss: 0.0179186\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0172052\n",
      "\tspeed: 0.0558s/iter; left time: 1061.0893s\n",
      "\titers: 200, epoch: 16 | loss: 0.0152363\n",
      "\tspeed: 0.0263s/iter; left time: 497.8089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:06.96s\n",
      "Steps: 225 | Train Loss: 0.0161114 Vali Loss: 0.0158190 Test Loss: 0.0179464\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0186068\n",
      "\tspeed: 0.0673s/iter; left time: 1266.2018s\n",
      "\titers: 200, epoch: 17 | loss: 0.0161077\n",
      "\tspeed: 0.0351s/iter; left time: 656.3230s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:08.37s\n",
      "Steps: 225 | Train Loss: 0.0160482 Vali Loss: 0.0158060 Test Loss: 0.0179885\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0147823\n",
      "\tspeed: 0.0538s/iter; left time: 1000.1613s\n",
      "\titers: 200, epoch: 18 | loss: 0.0150528\n",
      "\tspeed: 0.0307s/iter; left time: 567.6914s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:06.88s\n",
      "Steps: 225 | Train Loss: 0.0159649 Vali Loss: 0.0157838 Test Loss: 0.0179825\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.0155928\n",
      "\tspeed: 0.0593s/iter; left time: 1088.2279s\n",
      "\titers: 200, epoch: 19 | loss: 0.0147181\n",
      "\tspeed: 0.0312s/iter; left time: 569.2792s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 19\n",
      "Cost time: 00h:00m:07.43s\n",
      "Steps: 225 | Train Loss: 0.0159203 Vali Loss: 0.0158155 Test Loss: 0.0180364\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.0158121\n",
      "\tspeed: 0.0637s/iter; left time: 1155.4831s\n",
      "\titers: 200, epoch: 20 | loss: 0.0156867\n",
      "\tspeed: 0.0345s/iter; left time: 622.5498s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 20\n",
      "Cost time: 00h:00m:08.30s\n",
      "Steps: 225 | Train Loss: 0.0158178 Vali Loss: 0.0158000 Test Loss: 0.0179819\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.0155895\n",
      "\tspeed: 0.0548s/iter; left time: 980.2308s\n",
      "\titers: 200, epoch: 21 | loss: 0.0158620\n",
      "\tspeed: 0.0289s/iter; left time: 515.0808s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 21\n",
      "Cost time: 00h:00m:06.98s\n",
      "Steps: 225 | Train Loss: 0.0157976 Vali Loss: 0.0157882 Test Loss: 0.0180150\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.0161253\n",
      "\tspeed: 0.0623s/iter; left time: 1100.6162s\n",
      "\titers: 200, epoch: 22 | loss: 0.0149056\n",
      "\tspeed: 0.0300s/iter; left time: 526.7779s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 22\n",
      "Cost time: 00h:00m:07.76s\n",
      "Steps: 225 | Train Loss: 0.0157508 Vali Loss: 0.0158112 Test Loss: 0.0180414\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.0168003\n",
      "\tspeed: 0.0554s/iter; left time: 966.6790s\n",
      "\titers: 200, epoch: 23 | loss: 0.0160291\n",
      "\tspeed: 0.0355s/iter; left time: 615.7245s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 23\n",
      "Cost time: 00h:00m:07.68s\n",
      "Steps: 225 | Train Loss: 0.0157218 Vali Loss: 0.0158285 Test Loss: 0.0180852\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_96_IT_PatchTST_custom_ftM_sl168_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.017803212627768517, rmse:0.1334286779165268, mae:0.08180394023656845, rse:0.5045081973075867\n",
      "Intermediate time for IT and pred_len 96: 00h:03m:42.65s\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='ts_decomp_IT_168_168_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=168, label_len=48, pred_len=168, inverse=False, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=1, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=7, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=10, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ts_decomp_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28801\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0506961\n",
      "\tspeed: 0.0676s/iter; left time: 1514.8791s\n",
      "\titers: 200, epoch: 1 | loss: 0.0434178\n",
      "\tspeed: 0.0344s/iter; left time: 768.0979s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:09.71s\n",
      "Steps: 225 | Train Loss: 0.0503334 Vali Loss: 0.0368931 Test Loss: 0.0408223\n",
      "Validation loss decreased (inf --> 0.036893).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0257164\n",
      "\tspeed: 0.0888s/iter; left time: 1970.1584s\n",
      "\titers: 200, epoch: 2 | loss: 0.0217644\n",
      "\tspeed: 0.0489s/iter; left time: 1080.6051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:11.16s\n",
      "Steps: 225 | Train Loss: 0.0272302 Vali Loss: 0.0194705 Test Loss: 0.0210699\n",
      "Validation loss decreased (0.036893 --> 0.019471).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0223614\n",
      "\tspeed: 0.0886s/iter; left time: 1945.3772s\n",
      "\titers: 200, epoch: 3 | loss: 0.0198506\n",
      "\tspeed: 0.0486s/iter; left time: 1062.0087s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:11.21s\n",
      "Steps: 225 | Train Loss: 0.0215152 Vali Loss: 0.0184542 Test Loss: 0.0200383\n",
      "Validation loss decreased (0.019471 --> 0.018454).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0190125\n",
      "\tspeed: 0.0902s/iter; left time: 1959.1852s\n",
      "\titers: 200, epoch: 4 | loss: 0.0211241\n",
      "\tspeed: 0.0493s/iter; left time: 1065.3879s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:11.31s\n",
      "Steps: 225 | Train Loss: 0.0204801 Vali Loss: 0.0180584 Test Loss: 0.0196614\n",
      "Validation loss decreased (0.018454 --> 0.018058).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0185759\n",
      "\tspeed: 0.0885s/iter; left time: 1902.8078s\n",
      "\titers: 200, epoch: 5 | loss: 0.0203622\n",
      "\tspeed: 0.0486s/iter; left time: 1040.6218s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:11.22s\n",
      "Steps: 225 | Train Loss: 0.0198778 Vali Loss: 0.0178510 Test Loss: 0.0194241\n",
      "Validation loss decreased (0.018058 --> 0.017851).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0208315\n",
      "\tspeed: 0.0883s/iter; left time: 1879.2563s\n",
      "\titers: 200, epoch: 6 | loss: 0.0172729\n",
      "\tspeed: 0.0481s/iter; left time: 1017.8850s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 225 | Train Loss: 0.0194679 Vali Loss: 0.0177488 Test Loss: 0.0192593\n",
      "Validation loss decreased (0.017851 --> 0.017749).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0172482\n",
      "\tspeed: 0.0869s/iter; left time: 1829.4538s\n",
      "\titers: 200, epoch: 7 | loss: 0.0186308\n",
      "\tspeed: 0.0483s/iter; left time: 1012.9089s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 225 | Train Loss: 0.0191360 Vali Loss: 0.0177661 Test Loss: 0.0191440\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0193071\n",
      "\tspeed: 0.0871s/iter; left time: 1814.1306s\n",
      "\titers: 200, epoch: 8 | loss: 0.0185706\n",
      "\tspeed: 0.0471s/iter; left time: 976.9506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:10.97s\n",
      "Steps: 225 | Train Loss: 0.0189352 Vali Loss: 0.0177398 Test Loss: 0.0190398\n",
      "Validation loss decreased (0.017749 --> 0.017740).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0183653\n",
      "\tspeed: 0.0858s/iter; left time: 1767.1173s\n",
      "\titers: 200, epoch: 9 | loss: 0.0181597\n",
      "\tspeed: 0.0474s/iter; left time: 972.1877s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:11.03s\n",
      "Steps: 225 | Train Loss: 0.0187318 Vali Loss: 0.0178138 Test Loss: 0.0190202\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0187022\n",
      "\tspeed: 0.0872s/iter; left time: 1776.6623s\n",
      "\titers: 200, epoch: 10 | loss: 0.0177875\n",
      "\tspeed: 0.0482s/iter; left time: 976.9009s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:11.10s\n",
      "Steps: 225 | Train Loss: 0.0185414 Vali Loss: 0.0178180 Test Loss: 0.0189794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0175327\n",
      "\tspeed: 0.0856s/iter; left time: 1725.4260s\n",
      "\titers: 200, epoch: 11 | loss: 0.0181863\n",
      "\tspeed: 0.0488s/iter; left time: 977.7620s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:11.15s\n",
      "Steps: 225 | Train Loss: 0.0183754 Vali Loss: 0.0177803 Test Loss: 0.0190018\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0174661\n",
      "\tspeed: 0.0874s/iter; left time: 1741.2727s\n",
      "\titers: 200, epoch: 12 | loss: 0.0184091\n",
      "\tspeed: 0.0480s/iter; left time: 950.8208s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:11.11s\n",
      "Steps: 225 | Train Loss: 0.0182399 Vali Loss: 0.0178374 Test Loss: 0.0189758\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0181057\n",
      "\tspeed: 0.0844s/iter; left time: 1663.4012s\n",
      "\titers: 200, epoch: 13 | loss: 0.0172303\n",
      "\tspeed: 0.0485s/iter; left time: 949.9773s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:11.08s\n",
      "Steps: 225 | Train Loss: 0.0180831 Vali Loss: 0.0178951 Test Loss: 0.0190489\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0180981\n",
      "\tspeed: 0.0875s/iter; left time: 1703.5181s\n",
      "\titers: 200, epoch: 14 | loss: 0.0179996\n",
      "\tspeed: 0.0483s/iter; left time: 935.7908s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:11.04s\n",
      "Steps: 225 | Train Loss: 0.0179621 Vali Loss: 0.0179250 Test Loss: 0.0190570\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.0168550\n",
      "\tspeed: 0.0876s/iter; left time: 1687.2333s\n",
      "\titers: 200, epoch: 15 | loss: 0.0188148\n",
      "\tspeed: 0.0477s/iter; left time: 913.8198s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:11.08s\n",
      "Steps: 225 | Train Loss: 0.0178600 Vali Loss: 0.0180246 Test Loss: 0.0190814\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.0184088\n",
      "\tspeed: 0.0878s/iter; left time: 1670.6955s\n",
      "\titers: 200, epoch: 16 | loss: 0.0172866\n",
      "\tspeed: 0.0479s/iter; left time: 907.0227s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:11.13s\n",
      "Steps: 225 | Train Loss: 0.0177449 Vali Loss: 0.0178746 Test Loss: 0.0191260\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.0191272\n",
      "\tspeed: 0.0876s/iter; left time: 1646.0430s\n",
      "\titers: 200, epoch: 17 | loss: 0.0176109\n",
      "\tspeed: 0.0475s/iter; left time: 889.1229s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:11.01s\n",
      "Steps: 225 | Train Loss: 0.0176496 Vali Loss: 0.0180301 Test Loss: 0.0191746\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.0176716\n",
      "\tspeed: 0.0869s/iter; left time: 1613.4237s\n",
      "\titers: 200, epoch: 18 | loss: 0.0185039\n",
      "\tspeed: 0.0480s/iter; left time: 886.6051s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 18\n",
      "Cost time: 00h:00m:11.06s\n",
      "Steps: 225 | Train Loss: 0.0175396 Vali Loss: 0.0180294 Test Loss: 0.0192170\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : ts_decomp_IT_168_168_IT_PatchTST_custom_ftM_sl168_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.019039802253246307, rmse:0.13798479735851288, mae:0.08742713183164597, rse:0.5222201943397522\n",
      "Intermediate time for IT and pred_len 168: 00h:04m:13.33s\n",
      "Intermediate time for IT: 00h:13m:08.68s\n",
      "Total time: 00h:24m:11.50s\n"
     ]
    }
   ],
   "source": [
    "# List to store the results\n",
    "patchtst_results = []\n",
    "\n",
    "# Log file with all the results in 1 file\n",
    "log_file_path = f\"{log_dir}/{model}_decomposition.log\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        \n",
    "        country_start = time.time()\n",
    "        statement_1 = f\"\\n=== Starting experiments for country: {country} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "\n",
    "            pred_len_start = time.time()\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            if country == \"DE\" and pred_len == 24:\n",
    "                seq_len = 336\n",
    "            else:\n",
    "                seq_len = seq_lens[i]\n",
    "                \n",
    "            model_id = f\"ts_decomp_{country}_{seq_len}_{pred_len}_{country}\"\n",
    "            dataset = f\"{country}_data.csv\"\n",
    "            \n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers {e_layers} \\\n",
    "              --factor 1 \\\n",
    "              --enc_in {num_cols[i]} \\\n",
    "              --c_out {num_cols[i]} \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 100 \\\n",
    "              --patience 10 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --overlapping_windows \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc {loss} \\\n",
    "              --decomposition 1 \\\n",
    "              --itr {itr} --batch_size {batch_size} --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Log the country and prediction length\n",
    "            log_file.write(f\"\\n--- Running model for {country}, pred_len={pred_len} ---\\n\")\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            process.wait()  # Wait for the process to finish\n",
    "            #shutil.rmtree('./checkpoints' )  # delete checkpoint files\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, scaled_metrics in enumerate(iteration_metrics, start=1):\n",
    "\n",
    "                patchtst_results.append({\n",
    "                    'Country': country,\n",
    "                    'Pred_len': pred_len,\n",
    "                    'Iteration': iteration,\n",
    "                    'MSE': scaled_metrics[0],\n",
    "                    'RMSE': scaled_metrics[1],\n",
    "                    'MAE': scaled_metrics[2],\n",
    "                    })\n",
    "                \n",
    "            pred_len_end = time.time()\n",
    "            hours_int, mins_int, secs_int = running_time(pred_len_start, pred_len_end)\n",
    "            statement_3 = \"Intermediate time for {} and pred_len {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, pred_len, hours_int, mins_int, secs_int)\n",
    "            log_file.write(statement_3)\n",
    "            print(statement_3)\n",
    "\n",
    "        country_end = time.time()\n",
    "        hours_c, mins_c, secs_c = running_time(country_start, country_end)\n",
    "        statement_4 = \"Intermediate time for {}: {:0>2}h:{:0>2}m:{:05.2f}s\".format(country, hours_c, mins_c, secs_c)\n",
    "        log_file.write(statement_4)\n",
    "        print(statement_4)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, mins, secs = running_time(start, end)\n",
    "    statement_5 = \"Total time: {:0>2}h:{:0>2}m:{:05.2f}s\".format(hours, mins, secs)\n",
    "    log_file.write(statement_5)\n",
    "    print(statement_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Decomposition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DE</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.0921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.1374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ES</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1362</td>\n",
       "      <td>0.0902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.0967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">FR</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>0.0569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.0838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.0896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GB</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.1604</td>\n",
       "      <td>0.1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>0.1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">IT</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.0818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.0874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model            Decomposition                \n",
       "Metrics                    MSE    RMSE     MAE\n",
       "Country Pred_len                              \n",
       "DE      24              0.0212  0.1455  0.0921\n",
       "        96              0.0350  0.1870  0.1285\n",
       "        168             0.0376  0.1940  0.1374\n",
       "ES      24              0.0100  0.0998  0.0622\n",
       "        96              0.0185  0.1362  0.0902\n",
       "        168             0.0209  0.1446  0.0967\n",
       "FR      24              0.0099  0.0997  0.0569\n",
       "        96              0.0192  0.1385  0.0838\n",
       "        168             0.0208  0.1442  0.0896\n",
       "GB      24              0.0257  0.1604  0.1056\n",
       "        96              0.0442  0.2102  0.1458\n",
       "        168             0.0433  0.2082  0.1481\n",
       "IT      24              0.0099  0.0996  0.0583\n",
       "        96              0.0178  0.1334  0.0818\n",
       "        168             0.0190  0.1380  0.0874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#shutil.rmtree(\"results_transformers\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "path = 'results/patchtst'\n",
    "patchtst_df = convert_results_into_df(patchtst_results, if_loss_fnc=False)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Final DF\n",
    "patchtst_df.columns = pd.MultiIndex.from_product([['Decomposition'], ['MSE','RMSE', 'MAE']], names=['Model', 'Metrics'])\n",
    "patchtst_df.to_csv(os.path.join(path, 'patchtst_decomposition.csv'))\n",
    "patchtst_df.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
