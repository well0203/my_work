{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder named \"datasets\" if it doesn't exist\n",
    "folder_name = \"datasets\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "df = pd.read_csv(\"datasets/top_5_countries.csv\", index_col=0, parse_dates=True)\n",
    "# Reset index for Data Loader\n",
    "df.reset_index(inplace=True)\n",
    "df = df.iloc[:,:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_countries = ['DE', 'GB', 'ES', 'FR', 'IT']\n",
    "\n",
    "# Split and save the datasets\n",
    "for country_prefix in top_5_countries:\n",
    "    # Filter columns with the specified prefix\n",
    "    country_columns = [col for col in df.columns if col.startswith(country_prefix)]\n",
    "    \n",
    "    # Insert the date column at the beginning of every dataset\n",
    "    country_columns.insert(0,\"date\")\n",
    "    country_df = df[country_columns]\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"datasets/{country_prefix}_data.csv\"\n",
    "    country_df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output  \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['DE_data.csv', 'GB_data.csv', 'ES_data.csv', 'FR_data.csv', 'IT_data.csv']\n",
    "num_cols = [\"5\", \"5\", \"3\", \"3\", \"3\"]\n",
    "pred_len = \"24\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    model_id = f\"_{pred_len}_{dataset[:2]}\"  # Create the model_id\n",
    "    model_arguments = [\n",
    "                \"--task_name\", \"long_term_forecast\",\n",
    "                \"--is_training\", \"1\", #True\n",
    "                \"--root_path\", \"datasets/\",\n",
    "                \"--data_path\", dataset,\n",
    "                # \"--train_epochs\", \"1\",\n",
    "                \"--model_id\", '1',\n",
    "                \"--model\", \"Informer\",\n",
    "                \"--data\", \"custom\", # Use a custom dataloader (same data preparation as in ARIMA)\n",
    "                \"--features\", \"M\", # Multivariate\n",
    "                \"--seq_len\", \"96\",\n",
    "                \"--label_len\", \"48\",\n",
    "                \"--pred_len\", pred_len,\n",
    "                \"--e_layers\", \"2\", \n",
    "                \"--d_layers\", \"5\",\n",
    "                \"--factor\", \"5\",\n",
    "                \"--enc_in\", num_cols[i], \n",
    "                \"--dec_in\", num_cols[i], \n",
    "                \"--c_out\", num_cols[i],\n",
    "                \"--des\", \"Exp\",\n",
    "                \"--itr\", \"2\",\n",
    "            ]\n",
    "\n",
    "model_output = run_output(path_to_run_file, model_arguments)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/GB_data.csv\")\n",
    "df_small = df.iloc[:240, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.to_csv(\"datasets/GB_data_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>GB_UKM_load_actual_entsoe_transparency</th>\n",
       "      <th>GB_UKM_solar_generation_actual</th>\n",
       "      <th>GB_UKM_wind_generation_actual</th>\n",
       "      <th>GB_UKM_wind_offshore_generation_actual</th>\n",
       "      <th>GB_UKM_wind_onshore_generation_actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-26 00:00:00</td>\n",
       "      <td>30680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>3463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-26 01:00:00</td>\n",
       "      <td>29218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5194.0</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>3383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-26 02:00:00</td>\n",
       "      <td>28016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4389.0</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>2633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-26 03:00:00</td>\n",
       "      <td>27402.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5104.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>3417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-26 04:00:00</td>\n",
       "      <td>27490.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5206.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>3456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2015-11-04 19:00:00</td>\n",
       "      <td>49595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2015-11-04 20:00:00</td>\n",
       "      <td>46550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2015-11-04 21:00:00</td>\n",
       "      <td>42752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2015-11-04 22:00:00</td>\n",
       "      <td>38984.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2015-11-04 23:00:00</td>\n",
       "      <td>34184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date  GB_UKM_load_actual_entsoe_transparency  \\\n",
       "0    2015-10-26 00:00:00                                 30680.0   \n",
       "1    2015-10-26 01:00:00                                 29218.0   \n",
       "2    2015-10-26 02:00:00                                 28016.0   \n",
       "3    2015-10-26 03:00:00                                 27402.0   \n",
       "4    2015-10-26 04:00:00                                 27490.0   \n",
       "..                   ...                                     ...   \n",
       "235  2015-11-04 19:00:00                                 49595.0   \n",
       "236  2015-11-04 20:00:00                                 46550.0   \n",
       "237  2015-11-04 21:00:00                                 42752.0   \n",
       "238  2015-11-04 22:00:00                                 38984.0   \n",
       "239  2015-11-04 23:00:00                                 34184.0   \n",
       "\n",
       "     GB_UKM_solar_generation_actual  GB_UKM_wind_generation_actual  \\\n",
       "0                               0.0                         5348.0   \n",
       "1                               0.0                         5194.0   \n",
       "2                               0.0                         4389.0   \n",
       "3                               0.0                         5104.0   \n",
       "4                               0.0                         5206.0   \n",
       "..                              ...                            ...   \n",
       "235                             0.0                         1012.0   \n",
       "236                             0.0                         1246.0   \n",
       "237                             0.0                         1441.0   \n",
       "238                             0.0                         1430.0   \n",
       "239                             0.0                         1400.0   \n",
       "\n",
       "     GB_UKM_wind_offshore_generation_actual  \\\n",
       "0                                    1885.0   \n",
       "1                                    1810.0   \n",
       "2                                    1756.0   \n",
       "3                                    1687.0   \n",
       "4                                    1749.0   \n",
       "..                                      ...   \n",
       "235                                   584.0   \n",
       "236                                   792.0   \n",
       "237                                   915.0   \n",
       "238                                   912.0   \n",
       "239                                   875.0   \n",
       "\n",
       "     GB_UKM_wind_onshore_generation_actual  \n",
       "0                                   3463.0  \n",
       "1                                   3383.0  \n",
       "2                                   2633.0  \n",
       "3                                   3417.0  \n",
       "4                                   3456.0  \n",
       "..                                     ...  \n",
       "235                                  429.0  \n",
       "236                                  455.0  \n",
       "237                                  526.0  \n",
       "238                                  518.0  \n",
       "239                                  525.0  \n",
       "\n",
       "[240 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1rv2rKwQqgoHDNjXtRoAEWZ2ATz0gGAKu?usp=sharing#scrollTo=yu6zzic9t_Cz\n",
    "# Popen: https://colab.research.google.com/github/aviadr1/learn-python/blob/master/content/13_multiprocessing/notebooks/os_system_subprocess.ipynb\n",
    "import subprocess\n",
    "import os\n",
    "# parent_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path_to_run_file = \"/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py\"\n",
    "\n",
    "def run_output(path_to_run_file, model_arguments):\n",
    "    try:\n",
    "        # Execute the script and capture the output\n",
    "        command = [\"python\", \"-u\", path_to_run_file] + model_arguments\n",
    "        output = subprocess.check_output(command, universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        output = e.output  \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp='GB_data_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 cost time: 31.761372804641724\n",
      "Epoch: 1, Steps: 4 | Train Loss: 0.9952646 Vali Loss: nan Test Loss: 1.1890596\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (39, 1, 10, 5) (39, 1, 10, 5)\n",
      "test shape: (39, 10, 5) (39, 10, 5)\n",
      "mse:1.1777945756912231, mae:0.8684789538383484\n",
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 cost time: 32.578853130340576\n",
      "Epoch: 1, Steps: 4 | Train Loss: 1.0547914 Vali Loss: nan Test Loss: 1.5151765\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (39, 1, 10, 5) (39, 1, 10, 5)\n",
      "test shape: (39, 10, 5) (39, 10, 5)\n",
      "mse:1.5054138898849487, mae:0.973987340927124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_arguments = [\n",
    "            \"--task_name\", \"long_term_forecast\",\n",
    "            \"--is_training\", \"1\", #True\n",
    "            \"--root_path\", \"datasets/\",\n",
    "            \"--data_path\", dp,\n",
    "            \"--train_epochs\", \"1\",\n",
    "            \"--model_id\", '1',\n",
    "            \"--model\", \"Informer\",\n",
    "            \"--data\", \"custom\", # This ensures a 70%,10%,20% train,val,test split see data_provider/data_loader.py\n",
    "            \"--features\", \"M\", # Multivariate\n",
    "            \"--seq_len\", \"10\",\n",
    "            \"--label_len\", \"5\",\n",
    "            \"--pred_len\", '10',\n",
    "            \"--e_layers\", \"3\", # Hyperparameters as in original model\n",
    "            \"--d_layers\", \"2\",\n",
    "            \"--factor\", \"3\",\n",
    "            \"--enc_in\", \"5\", #str((num_columns)-1),\n",
    "            \"--dec_in\", \"5\", #str((num_columns)-1),\n",
    "            \"--c_out\", \"5\", # str((num_columns)-1),\n",
    "            \"--des\", \"Exp\",\n",
    "            \"--itr\", \"2\",\n",
    "            \"--train_epochs\", \"1\",\n",
    "        ]\n",
    "\n",
    "model_output = run_output(path_to_run_file, model_arguments)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use CPU\\n>>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\\ntrain 149\\nval 15\\ntest 39\\nEpoch: 1 cost time: 31.332730054855347\\nEpoch: 1, Steps: 4 | Train Loss: 0.9952646 Vali Loss: nan Test Loss: 1.1890596\\nValidation loss decreased (inf --> nan).  Saving model ...\\nUpdating learning rate to 0.0001\\n>>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\ntest 39\\ntest shape: (39, 1, 10, 5) (39, 1, 10, 5)\\ntest shape: (39, 10, 5) (39, 10, 5)\\nmse:1.1777945756912231, mae:0.8684789538383484\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 149\n",
      "val 15\n",
      "test 39\n",
      "Epoch: 1 cost time: 31.332730054855347\n",
      "Epoch: 1, Steps: 4 | Train Loss: 0.9952646 Vali Loss: nan Test Loss: 1.1890596\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 39\n",
      "test shape: (39, 1, 10, 5) (39, 1, 10, 5)\n",
      "test shape: (39, 10, 5) (39, 10, 5)\n",
      "mse:1.1777945756912231, mae:0.8684789538383484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1777945756912231\n",
      "MAE: 0.8684789538383484\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression \n",
    "pattern = r\"mse:(.*?), mae:(.*?)$\"\n",
    "\n",
    "# Use re.search to find the pattern in the output string\n",
    "match = re.search(pattern, model_output)\n",
    "\n",
    "if match:\n",
    "    # Extract the MSE and MAE values from the matched groups\n",
    "    mse = float(match.group(1))\n",
    "    mae = float(match.group(2))\n",
    "    \n",
    "    \n",
    "    print(\"mse:\", mse)\n",
    "    print(\"mae:\", mae)\n",
    "else:\n",
    "    print(\"No match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.86847895,   1.1777946 ,   1.0852624 ,   3.2318454 ,\n",
       "       769.5208    ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# metrics\n",
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/metrics.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 10, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"/Users/valentyna/Documents/Master_thesis_new/results/long_term_forecast_1_Informer_custom_ftM_sl10_ll5_pl10_dm512_nh8_el3_dl2_df2048_fc3_ebtimeF_dtTrue_Exp_0/pred.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: run.py [-h] --task_name TASK_NAME --is_training IS_TRAINING --model_id\n",
      "              MODEL_ID --model MODEL --data DATA [--root_path ROOT_PATH]\n",
      "              [--data_path DATA_PATH] [--features FEATURES] [--target TARGET]\n",
      "              [--freq FREQ] [--checkpoints CHECKPOINTS] [--seq_len SEQ_LEN]\n",
      "              [--label_len LABEL_LEN] [--pred_len PRED_LEN]\n",
      "              [--seasonal_patterns SEASONAL_PATTERNS] [--inverse]\n",
      "              [--mask_rate MASK_RATE] [--anomaly_ratio ANOMALY_RATIO]\n",
      "              [--top_k TOP_K] [--num_kernels NUM_KERNELS] [--enc_in ENC_IN]\n",
      "              [--dec_in DEC_IN] [--c_out C_OUT] [--d_model D_MODEL]\n",
      "              [--n_heads N_HEADS] [--e_layers E_LAYERS] [--d_layers D_LAYERS]\n",
      "              [--d_ff D_FF] [--moving_avg MOVING_AVG] [--factor FACTOR]\n",
      "              [--distil] [--dropout DROPOUT] [--embed EMBED]\n",
      "              [--activation ACTIVATION] [--output_attention]\n",
      "              [--channel_independence CHANNEL_INDEPENDENCE]\n",
      "              [--decomp_method DECOMP_METHOD] [--use_norm USE_NORM]\n",
      "              [--down_sampling_layers DOWN_SAMPLING_LAYERS]\n",
      "              [--down_sampling_window DOWN_SAMPLING_WINDOW]\n",
      "              [--down_sampling_method DOWN_SAMPLING_METHOD]\n",
      "              [--seg_len SEG_LEN] [--num_workers NUM_WORKERS] [--itr ITR]\n",
      "              [--train_epochs TRAIN_EPOCHS] [--batch_size BATCH_SIZE]\n",
      "              [--patience PATIENCE] [--learning_rate LEARNING_RATE]\n",
      "              [--des DES] [--loss LOSS] [--lradj LRADJ] [--use_amp]\n",
      "              [--use_gpu USE_GPU] [--gpu GPU] [--use_multi_gpu]\n",
      "              [--devices DEVICES]\n",
      "              [--p_hidden_dims P_HIDDEN_DIMS [P_HIDDEN_DIMS ...]]\n",
      "              [--p_hidden_layers P_HIDDEN_LAYERS]\n",
      "run.py: error: the following arguments are required: --task_name, --is_training, --model_id, --model, --data\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', '-u', '/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m command \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-u\u001b[39m\u001b[38;5;124m\"\u001b[39m, DIR]\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniversal_newlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/subprocess.py:424\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m         empty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    422\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python', '-u', '/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "DIR = \"/Users/valentyna/Documents/Master_thesis_new/TSLibrary/run.py\"\n",
    "command = [\"python\", \"-u\", DIR]\n",
    "output = subprocess.check_output(command, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/valentyna/Documents/run.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def run_experiment(config):\n",
    "    fix_seed = 2021\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    args = config.copy()  # Copy the configuration dictionary\n",
    "    \n",
    "    args['use_gpu'] = True if torch.cuda.is_available() and args['use_gpu'] else False\n",
    "\n",
    "    if args['use_gpu'] and args['use_multi_gpu']:\n",
    "        args['devices'] = args['devices'].replace(' ', '')\n",
    "        device_ids = args['devices'].split(',')\n",
    "        args['device_ids'] = [int(id_) for id_ in device_ids]\n",
    "        args['gpu'] = args['device_ids'][0]\n",
    "\n",
    "    print('Args in experiment:')\n",
    "    print(args)\n",
    "\n",
    "    Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "    if args['is_training']:\n",
    "        for ii in range(args['itr']):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                args['task_name'],\n",
    "                args['model_id'],\n",
    "                args['model'],\n",
    "                args['data'],\n",
    "                args['features'],\n",
    "                args['seq_len'],\n",
    "                args['label_len'],\n",
    "                args['pred_len'],\n",
    "                args['d_model'],\n",
    "                args['n_heads'],\n",
    "                args['e_layers'],\n",
    "                args['d_layers'],\n",
    "                args['d_ff'],\n",
    "                args['factor'],\n",
    "                args['embed'],\n",
    "                args['distil'],\n",
    "                args['des'], ii)\n",
    "            exp = Exp(args)  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args['task_name'],\n",
    "            args['model_id'],\n",
    "            args['model'],\n",
    "            args['data'],\n",
    "            args['features'],\n",
    "            args['seq_len'],\n",
    "            args['label_len'],\n",
    "            args['pred_len'],\n",
    "            args['d_model'],\n",
    "            args['n_heads'],\n",
    "            args['e_layers'],\n",
    "            args['d_layers'],\n",
    "            args['d_ff'],\n",
    "            args['factor'],\n",
    "            args['embed'],\n",
    "            args['distil'],\n",
    "            args['des'], ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Example configuration\n",
    "config = {\n",
    "    'task_name': 'long_term_forecast',\n",
    "    'is_training': 1,\n",
    "    'model_id': 'test',\n",
    "    'model': 'Autoformer',\n",
    "    'data': 'ETTm1',\n",
    "    'root_path': './data/ETT/',\n",
    "    'data_path': 'ETTh1.csv',\n",
    "    'features': 'M',\n",
    "    'target': 'OT',\n",
    "    'freq': 'h',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'seq_len': 96,\n",
    "    'label_len': 48,\n",
    "    'pred_len': 96,\n",
    "    'seasonal_patterns': 'Monthly',\n",
    "    'inverse': False,\n",
    "    'mask_rate': 0.25,\n",
    "    'anomaly_ratio': 0.25,\n",
    "    'top_k': 5,\n",
    "    'num_kernels': 6,\n",
    "    'enc_in': 7,\n",
    "    'dec_in': 7,\n",
    "    'c_out': 7,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 1,\n",
    "    'd_ff': 2048,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'distil': True,\n",
    "    'dropout': 0.1,\n",
    "    'embed': 'timeF',\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    'num_workers': 10,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'patience': 3,\n",
    "    'learning_rate': 0.0001,\n",
    "    'des': 'test',\n",
    "    'loss': 'MSE',\n",
    "    'lradj': 'type1',\n",
    "    'use_amp': False,\n",
    "    'use_gpu': True,\n",
    "    'gpu': 0,\n",
    "    'use_multi_gpu': False,\n",
    "    'devices': '0,1,2,3',\n",
    "    'p_hidden_dims': [128, 128],\n",
    "    'p_hidden_layers': 2\n",
    "}\n",
    "\n",
    "# Run the experiment with the configuration\n",
    "run_experiment(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
