{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>Table of Contents</summary>\n",
    "\n",
    "- [1. Standard Scaler Informer ](#1-standard-scaler-informer)\n",
    "- [2. Standard Scaler PatchTST](#2-standard-scaler-patchtst)\n",
    "- [3. MinMax Scaler Informer](#3-minmax-scaler-informer)\n",
    "- [4. MinMax Scaler PatchTST](#4-minmax-scaler-patchtst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform a check on **Italy** dataset to confirm choice of scaler for our data.\n",
    "\n",
    "This script is to run the models. Final results are in the notebook \"Comparison_IT\". \n",
    "\n",
    "Please note, the cell content is almost identical. However, when duplicating code and changing some arguments, it becomes easier to store and read results (especially if you want to experiment with 1 subpart) and split long running time into subprocesses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import shutil\n",
    "from utils.helper import extract_metrics_from_output, convert_results_into_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard Scaler Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"0\"\n",
    "\n",
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "# Arguments that will be used also for file names\n",
    "model = \"Informer\"\n",
    "dataset = 'IT_data.csv'\n",
    "losses = [\"MSE\"]\n",
    "country = dataset[:2]\n",
    "\n",
    "log_dir = f\"logs/scaler_choice/standard\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file_path = f\"{log_dir}/{model}_{country}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for loss function: MSE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_24_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_24_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 1.0316310\n",
      "\tspeed: 0.0790s/iter; left time: 1423.6841s\n",
      "\titers: 200, epoch: 1 | loss: 0.8924497\n",
      "\tspeed: 0.0507s/iter; left time: 907.9624s\n",
      "\titers: 300, epoch: 1 | loss: 0.8221110\n",
      "\tspeed: 0.0508s/iter; left time: 905.0940s\n",
      "\titers: 400, epoch: 1 | loss: 0.7922397\n",
      "\tspeed: 0.0506s/iter; left time: 897.5050s\n",
      "\titers: 500, epoch: 1 | loss: 0.6204698\n",
      "\tspeed: 0.0506s/iter; left time: 891.4758s\n",
      "\titers: 600, epoch: 1 | loss: 0.6392817\n",
      "\tspeed: 0.0508s/iter; left time: 889.4901s\n",
      "\titers: 700, epoch: 1 | loss: 0.4341050\n",
      "\tspeed: 0.0507s/iter; left time: 882.4134s\n",
      "\titers: 800, epoch: 1 | loss: 0.4927379\n",
      "\tspeed: 0.0504s/iter; left time: 873.1627s\n",
      "\titers: 900, epoch: 1 | loss: 0.5263697\n",
      "\tspeed: 0.0506s/iter; left time: 870.7026s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:46.72s\n",
      "Steps: 906 | Train Loss: 0.7269765 Vali Loss: 0.4586591 Test Loss: 0.5278808\n",
      "Validation loss decreased (inf --> 0.458659).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4034863\n",
      "\tspeed: 0.1073s/iter; left time: 1836.2787s\n",
      "\titers: 200, epoch: 2 | loss: 0.3227167\n",
      "\tspeed: 0.0504s/iter; left time: 858.1356s\n",
      "\titers: 300, epoch: 2 | loss: 0.2167729\n",
      "\tspeed: 0.0506s/iter; left time: 855.5059s\n",
      "\titers: 400, epoch: 2 | loss: 0.2467622\n",
      "\tspeed: 0.0506s/iter; left time: 850.3592s\n",
      "\titers: 500, epoch: 2 | loss: 0.2275388\n",
      "\tspeed: 0.0507s/iter; left time: 846.8193s\n",
      "\titers: 600, epoch: 2 | loss: 0.3089961\n",
      "\tspeed: 0.0507s/iter; left time: 842.1465s\n",
      "\titers: 700, epoch: 2 | loss: 0.2458843\n",
      "\tspeed: 0.0506s/iter; left time: 835.2096s\n",
      "\titers: 800, epoch: 2 | loss: 0.2655441\n",
      "\tspeed: 0.0507s/iter; left time: 832.2937s\n",
      "\titers: 900, epoch: 2 | loss: 0.2456793\n",
      "\tspeed: 0.0499s/iter; left time: 814.5358s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:46.06s\n",
      "Steps: 906 | Train Loss: 0.2822418 Vali Loss: 0.2269442 Test Loss: 0.2700601\n",
      "Validation loss decreased (0.458659 --> 0.226944).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.1849614\n",
      "\tspeed: 0.1103s/iter; left time: 1787.9789s\n",
      "\titers: 200, epoch: 3 | loss: 0.1827399\n",
      "\tspeed: 0.0507s/iter; left time: 816.3584s\n",
      "\titers: 300, epoch: 3 | loss: 0.2200058\n",
      "\tspeed: 0.0504s/iter; left time: 807.5809s\n",
      "\titers: 400, epoch: 3 | loss: 0.1627856\n",
      "\tspeed: 0.0506s/iter; left time: 804.5560s\n",
      "\titers: 500, epoch: 3 | loss: 0.1899888\n",
      "\tspeed: 0.0508s/iter; left time: 802.5345s\n",
      "\titers: 600, epoch: 3 | loss: 0.2350548\n",
      "\tspeed: 0.0504s/iter; left time: 792.2311s\n",
      "\titers: 700, epoch: 3 | loss: 0.2169113\n",
      "\tspeed: 0.0504s/iter; left time: 786.2585s\n",
      "\titers: 800, epoch: 3 | loss: 0.2479716\n",
      "\tspeed: 0.0504s/iter; left time: 782.2688s\n",
      "\titers: 900, epoch: 3 | loss: 0.1756518\n",
      "\tspeed: 0.0503s/iter; left time: 774.8159s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:46.11s\n",
      "Steps: 906 | Train Loss: 0.2084415 Vali Loss: 0.2057285 Test Loss: 0.2414494\n",
      "Validation loss decreased (0.226944 --> 0.205728).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.2156612\n",
      "\tspeed: 0.1113s/iter; left time: 1703.3923s\n",
      "\titers: 200, epoch: 4 | loss: 0.1546005\n",
      "\tspeed: 0.0506s/iter; left time: 769.4344s\n",
      "\titers: 300, epoch: 4 | loss: 0.1641864\n",
      "\tspeed: 0.0506s/iter; left time: 763.8111s\n",
      "\titers: 400, epoch: 4 | loss: 0.2271351\n",
      "\tspeed: 0.0507s/iter; left time: 760.1450s\n",
      "\titers: 500, epoch: 4 | loss: 0.1795364\n",
      "\tspeed: 0.0501s/iter; left time: 746.6356s\n",
      "\titers: 600, epoch: 4 | loss: 0.1779367\n",
      "\tspeed: 0.0496s/iter; left time: 734.9436s\n",
      "\titers: 700, epoch: 4 | loss: 0.1565044\n",
      "\tspeed: 0.0501s/iter; left time: 737.2044s\n",
      "\titers: 800, epoch: 4 | loss: 0.2001050\n",
      "\tspeed: 0.0506s/iter; left time: 739.6401s\n",
      "\titers: 900, epoch: 4 | loss: 0.2238363\n",
      "\tspeed: 0.0507s/iter; left time: 735.7452s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:45.95s\n",
      "Steps: 906 | Train Loss: 0.1883810 Vali Loss: 0.2095959 Test Loss: 0.2369153\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1555237\n",
      "\tspeed: 0.1022s/iter; left time: 1471.8945s\n",
      "\titers: 200, epoch: 5 | loss: 0.2117636\n",
      "\tspeed: 0.0502s/iter; left time: 718.0646s\n",
      "\titers: 300, epoch: 5 | loss: 0.2162236\n",
      "\tspeed: 0.0501s/iter; left time: 711.8249s\n",
      "\titers: 400, epoch: 5 | loss: 0.1503055\n",
      "\tspeed: 0.0501s/iter; left time: 706.5424s\n",
      "\titers: 500, epoch: 5 | loss: 0.1649206\n",
      "\tspeed: 0.0501s/iter; left time: 701.5428s\n",
      "\titers: 600, epoch: 5 | loss: 0.1569832\n",
      "\tspeed: 0.0502s/iter; left time: 698.1861s\n",
      "\titers: 700, epoch: 5 | loss: 0.2454147\n",
      "\tspeed: 0.0501s/iter; left time: 691.5033s\n",
      "\titers: 800, epoch: 5 | loss: 0.1237849\n",
      "\tspeed: 0.0499s/iter; left time: 684.0331s\n",
      "\titers: 900, epoch: 5 | loss: 0.1612593\n",
      "\tspeed: 0.0501s/iter; left time: 681.8461s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:45.67s\n",
      "Steps: 906 | Train Loss: 0.1701086 Vali Loss: 0.1987211 Test Loss: 0.2412589\n",
      "Validation loss decreased (0.205728 --> 0.198721).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1855901\n",
      "\tspeed: 0.1077s/iter; left time: 1452.9168s\n",
      "\titers: 200, epoch: 6 | loss: 0.1696668\n",
      "\tspeed: 0.0500s/iter; left time: 670.1560s\n",
      "\titers: 300, epoch: 6 | loss: 0.2339516\n",
      "\tspeed: 0.0497s/iter; left time: 660.0007s\n",
      "\titers: 400, epoch: 6 | loss: 0.1227236\n",
      "\tspeed: 0.0498s/iter; left time: 656.4009s\n",
      "\titers: 500, epoch: 6 | loss: 0.1021472\n",
      "\tspeed: 0.0499s/iter; left time: 653.7666s\n",
      "\titers: 600, epoch: 6 | loss: 0.1541440\n",
      "\tspeed: 0.0500s/iter; left time: 650.1743s\n",
      "\titers: 700, epoch: 6 | loss: 0.2081679\n",
      "\tspeed: 0.0502s/iter; left time: 647.4345s\n",
      "\titers: 800, epoch: 6 | loss: 0.1355353\n",
      "\tspeed: 0.0502s/iter; left time: 642.6501s\n",
      "\titers: 900, epoch: 6 | loss: 0.1422778\n",
      "\tspeed: 0.0501s/iter; left time: 636.3004s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:45.64s\n",
      "Steps: 906 | Train Loss: 0.1573889 Vali Loss: 0.1970492 Test Loss: 0.2377323\n",
      "Validation loss decreased (0.198721 --> 0.197049).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1288484\n",
      "\tspeed: 0.1056s/iter; left time: 1328.9422s\n",
      "\titers: 200, epoch: 7 | loss: 0.1668594\n",
      "\tspeed: 0.0503s/iter; left time: 627.4991s\n",
      "\titers: 300, epoch: 7 | loss: 0.1261204\n",
      "\tspeed: 0.0500s/iter; left time: 619.3119s\n",
      "\titers: 400, epoch: 7 | loss: 0.1667181\n",
      "\tspeed: 0.0501s/iter; left time: 615.9967s\n",
      "\titers: 500, epoch: 7 | loss: 0.1452182\n",
      "\tspeed: 0.0498s/iter; left time: 606.7520s\n",
      "\titers: 600, epoch: 7 | loss: 0.1088573\n",
      "\tspeed: 0.0473s/iter; left time: 572.1031s\n",
      "\titers: 700, epoch: 7 | loss: 0.1446692\n",
      "\tspeed: 0.0471s/iter; left time: 564.2122s\n",
      "\titers: 800, epoch: 7 | loss: 0.1485261\n",
      "\tspeed: 0.0462s/iter; left time: 549.0099s\n",
      "\titers: 900, epoch: 7 | loss: 0.1337977\n",
      "\tspeed: 0.0469s/iter; left time: 552.4355s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:44.41s\n",
      "Steps: 906 | Train Loss: 0.1462945 Vali Loss: 0.2067386 Test Loss: 0.2396837\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0927421\n",
      "\tspeed: 0.0997s/iter; left time: 1164.5615s\n",
      "\titers: 200, epoch: 8 | loss: 0.1006707\n",
      "\tspeed: 0.0469s/iter; left time: 542.7078s\n",
      "\titers: 300, epoch: 8 | loss: 0.1167498\n",
      "\tspeed: 0.0455s/iter; left time: 522.0240s\n",
      "\titers: 400, epoch: 8 | loss: 0.1339692\n",
      "\tspeed: 0.0456s/iter; left time: 518.4756s\n",
      "\titers: 500, epoch: 8 | loss: 0.1319740\n",
      "\tspeed: 0.0452s/iter; left time: 510.3111s\n",
      "\titers: 600, epoch: 8 | loss: 0.1420134\n",
      "\tspeed: 0.0455s/iter; left time: 508.4812s\n",
      "\titers: 700, epoch: 8 | loss: 0.1047732\n",
      "\tspeed: 0.0456s/iter; left time: 504.8171s\n",
      "\titers: 800, epoch: 8 | loss: 0.1361858\n",
      "\tspeed: 0.0456s/iter; left time: 500.1178s\n",
      "\titers: 900, epoch: 8 | loss: 0.1444074\n",
      "\tspeed: 0.0455s/iter; left time: 494.5154s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:42.14s\n",
      "Steps: 906 | Train Loss: 0.1338023 Vali Loss: 0.1966827 Test Loss: 0.2461240\n",
      "Validation loss decreased (0.197049 --> 0.196683).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0846957\n",
      "\tspeed: 0.1043s/iter; left time: 1123.8272s\n",
      "\titers: 200, epoch: 9 | loss: 0.1550560\n",
      "\tspeed: 0.0489s/iter; left time: 521.5001s\n",
      "\titers: 300, epoch: 9 | loss: 0.1646176\n",
      "\tspeed: 0.0492s/iter; left time: 520.1492s\n",
      "\titers: 400, epoch: 9 | loss: 0.1244546\n",
      "\tspeed: 0.0492s/iter; left time: 515.7401s\n",
      "\titers: 500, epoch: 9 | loss: 0.1033260\n",
      "\tspeed: 0.0491s/iter; left time: 509.2604s\n",
      "\titers: 600, epoch: 9 | loss: 0.1585059\n",
      "\tspeed: 0.0496s/iter; left time: 509.1888s\n",
      "\titers: 700, epoch: 9 | loss: 0.1169235\n",
      "\tspeed: 0.0493s/iter; left time: 501.6143s\n",
      "\titers: 800, epoch: 9 | loss: 0.0956298\n",
      "\tspeed: 0.0493s/iter; left time: 496.3773s\n",
      "\titers: 900, epoch: 9 | loss: 0.0997296\n",
      "\tspeed: 0.0492s/iter; left time: 491.0250s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:44.77s\n",
      "Steps: 906 | Train Loss: 0.1235738 Vali Loss: 0.2272157 Test Loss: 0.2594654\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.1164403\n",
      "\tspeed: 0.1034s/iter; left time: 1019.7873s\n",
      "\titers: 200, epoch: 10 | loss: 0.1317146\n",
      "\tspeed: 0.0491s/iter; left time: 479.4843s\n",
      "\titers: 300, epoch: 10 | loss: 0.1148992\n",
      "\tspeed: 0.0492s/iter; left time: 475.9576s\n",
      "\titers: 400, epoch: 10 | loss: 0.1210067\n",
      "\tspeed: 0.0491s/iter; left time: 470.2079s\n",
      "\titers: 500, epoch: 10 | loss: 0.0858889\n",
      "\tspeed: 0.0490s/iter; left time: 464.2520s\n",
      "\titers: 600, epoch: 10 | loss: 0.1298886\n",
      "\tspeed: 0.0491s/iter; left time: 459.8328s\n",
      "\titers: 700, epoch: 10 | loss: 0.1060434\n",
      "\tspeed: 0.0490s/iter; left time: 454.2363s\n",
      "\titers: 800, epoch: 10 | loss: 0.1048332\n",
      "\tspeed: 0.0491s/iter; left time: 449.9757s\n",
      "\titers: 900, epoch: 10 | loss: 0.1041241\n",
      "\tspeed: 0.0491s/iter; left time: 444.8733s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:44.78s\n",
      "Steps: 906 | Train Loss: 0.1143394 Vali Loss: 0.2093962 Test Loss: 0.2623040\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.1073215\n",
      "\tspeed: 0.1035s/iter; left time: 927.8348s\n",
      "\titers: 200, epoch: 11 | loss: 0.0938183\n",
      "\tspeed: 0.0492s/iter; left time: 436.1408s\n",
      "\titers: 300, epoch: 11 | loss: 0.1069875\n",
      "\tspeed: 0.0494s/iter; left time: 433.0739s\n",
      "\titers: 400, epoch: 11 | loss: 0.1008828\n",
      "\tspeed: 0.0491s/iter; left time: 425.5331s\n",
      "\titers: 500, epoch: 11 | loss: 0.1198396\n",
      "\tspeed: 0.0496s/iter; left time: 424.2260s\n",
      "\titers: 600, epoch: 11 | loss: 0.1205902\n",
      "\tspeed: 0.0493s/iter; left time: 416.9104s\n",
      "\titers: 700, epoch: 11 | loss: 0.1044135\n",
      "\tspeed: 0.0491s/iter; left time: 410.7397s\n",
      "\titers: 800, epoch: 11 | loss: 0.1197494\n",
      "\tspeed: 0.0493s/iter; left time: 407.2388s\n",
      "\titers: 900, epoch: 11 | loss: 0.0974338\n",
      "\tspeed: 0.0486s/iter; left time: 396.7938s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:44.87s\n",
      "Steps: 906 | Train Loss: 0.1054826 Vali Loss: 0.2153554 Test Loss: 0.2711528\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0859880\n",
      "\tspeed: 0.1052s/iter; left time: 847.7020s\n",
      "\titers: 200, epoch: 12 | loss: 0.1081913\n",
      "\tspeed: 0.0490s/iter; left time: 389.9691s\n",
      "\titers: 300, epoch: 12 | loss: 0.0846110\n",
      "\tspeed: 0.0491s/iter; left time: 386.0069s\n",
      "\titers: 400, epoch: 12 | loss: 0.1025017\n",
      "\tspeed: 0.0492s/iter; left time: 381.6555s\n",
      "\titers: 500, epoch: 12 | loss: 0.1515075\n",
      "\tspeed: 0.0497s/iter; left time: 380.2929s\n",
      "\titers: 600, epoch: 12 | loss: 0.0897325\n",
      "\tspeed: 0.0497s/iter; left time: 375.7711s\n",
      "\titers: 700, epoch: 12 | loss: 0.0808695\n",
      "\tspeed: 0.0496s/iter; left time: 369.5729s\n",
      "\titers: 800, epoch: 12 | loss: 0.1155443\n",
      "\tspeed: 0.0491s/iter; left time: 360.9095s\n",
      "\titers: 900, epoch: 12 | loss: 0.0916021\n",
      "\tspeed: 0.0493s/iter; left time: 357.5122s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:45.05s\n",
      "Steps: 906 | Train Loss: 0.0981688 Vali Loss: 0.2335731 Test Loss: 0.2948381\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0830040\n",
      "\tspeed: 0.1051s/iter; left time: 751.3685s\n",
      "\titers: 200, epoch: 13 | loss: 0.1011155\n",
      "\tspeed: 0.0494s/iter; left time: 348.4240s\n",
      "\titers: 300, epoch: 13 | loss: 0.0702539\n",
      "\tspeed: 0.0492s/iter; left time: 342.1681s\n",
      "\titers: 400, epoch: 13 | loss: 0.0862625\n",
      "\tspeed: 0.0492s/iter; left time: 337.1834s\n",
      "\titers: 500, epoch: 13 | loss: 0.0823857\n",
      "\tspeed: 0.0497s/iter; left time: 335.5179s\n",
      "\titers: 600, epoch: 13 | loss: 0.0750791\n",
      "\tspeed: 0.0494s/iter; left time: 328.4409s\n",
      "\titers: 700, epoch: 13 | loss: 0.0991566\n",
      "\tspeed: 0.0494s/iter; left time: 323.3899s\n",
      "\titers: 800, epoch: 13 | loss: 0.0885527\n",
      "\tspeed: 0.0495s/iter; left time: 318.9356s\n",
      "\titers: 900, epoch: 13 | loss: 0.1014303\n",
      "\tspeed: 0.0493s/iter; left time: 312.6968s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:45.05s\n",
      "Steps: 906 | Train Loss: 0.0910340 Vali Loss: 0.2294364 Test Loss: 0.2778772\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_24_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.2462237924337387, rmse:0.49620941281318665, mae:0.30451908707618713, rse:0.4544472098350525\n",
      "Original data scale mse:2325943.5, rmse:1525.1043701171875, mae:940.2656860351562, rse:0.10717269033193588\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_96_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_96_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 1.0737864\n",
      "\tspeed: 0.0831s/iter; left time: 1494.7004s\n",
      "\titers: 200, epoch: 1 | loss: 1.0098399\n",
      "\tspeed: 0.0518s/iter; left time: 926.7674s\n",
      "\titers: 300, epoch: 1 | loss: 0.8668569\n",
      "\tspeed: 0.0518s/iter; left time: 920.5025s\n",
      "\titers: 400, epoch: 1 | loss: 0.8826305\n",
      "\tspeed: 0.0516s/iter; left time: 912.5955s\n",
      "\titers: 500, epoch: 1 | loss: 0.8849045\n",
      "\tspeed: 0.0517s/iter; left time: 908.2073s\n",
      "\titers: 600, epoch: 1 | loss: 0.7967799\n",
      "\tspeed: 0.0517s/iter; left time: 904.6254s\n",
      "\titers: 700, epoch: 1 | loss: 0.7579708\n",
      "\tspeed: 0.0517s/iter; left time: 898.0212s\n",
      "\titers: 800, epoch: 1 | loss: 0.7350165\n",
      "\tspeed: 0.0515s/iter; left time: 890.7799s\n",
      "\titers: 900, epoch: 1 | loss: 0.7763956\n",
      "\tspeed: 0.0516s/iter; left time: 887.0178s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:47.61s\n",
      "Steps: 904 | Train Loss: 0.8732454 Vali Loss: 0.6784076 Test Loss: 0.7782529\n",
      "Validation loss decreased (inf --> 0.678408).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.6628851\n",
      "\tspeed: 0.1179s/iter; left time: 2013.3727s\n",
      "\titers: 200, epoch: 2 | loss: 0.5570996\n",
      "\tspeed: 0.0516s/iter; left time: 876.6171s\n",
      "\titers: 300, epoch: 2 | loss: 0.5401216\n",
      "\tspeed: 0.0517s/iter; left time: 872.5737s\n",
      "\titers: 400, epoch: 2 | loss: 0.4877618\n",
      "\tspeed: 0.0517s/iter; left time: 867.4627s\n",
      "\titers: 500, epoch: 2 | loss: 0.4567522\n",
      "\tspeed: 0.0517s/iter; left time: 862.9420s\n",
      "\titers: 600, epoch: 2 | loss: 0.3768754\n",
      "\tspeed: 0.0517s/iter; left time: 857.3316s\n",
      "\titers: 700, epoch: 2 | loss: 0.4163083\n",
      "\tspeed: 0.0516s/iter; left time: 849.5398s\n",
      "\titers: 800, epoch: 2 | loss: 0.3482113\n",
      "\tspeed: 0.0516s/iter; left time: 845.7774s\n",
      "\titers: 900, epoch: 2 | loss: 0.3422059\n",
      "\tspeed: 0.0516s/iter; left time: 840.5550s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:47.07s\n",
      "Steps: 904 | Train Loss: 0.4753363 Vali Loss: 0.3555111 Test Loss: 0.4102994\n",
      "Validation loss decreased (0.678408 --> 0.355511).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.3416770\n",
      "\tspeed: 0.1168s/iter; left time: 1888.6911s\n",
      "\titers: 200, epoch: 3 | loss: 0.3394969\n",
      "\tspeed: 0.0516s/iter; left time: 829.8194s\n",
      "\titers: 300, epoch: 3 | loss: 0.3537698\n",
      "\tspeed: 0.0518s/iter; left time: 828.0375s\n",
      "\titers: 400, epoch: 3 | loss: 0.3339500\n",
      "\tspeed: 0.0518s/iter; left time: 821.5301s\n",
      "\titers: 500, epoch: 3 | loss: 0.3026560\n",
      "\tspeed: 0.0516s/iter; left time: 813.5109s\n",
      "\titers: 600, epoch: 3 | loss: 0.3234853\n",
      "\tspeed: 0.0516s/iter; left time: 808.7656s\n",
      "\titers: 700, epoch: 3 | loss: 0.2705842\n",
      "\tspeed: 0.0516s/iter; left time: 804.0664s\n",
      "\titers: 800, epoch: 3 | loss: 0.3298855\n",
      "\tspeed: 0.0514s/iter; left time: 795.6295s\n",
      "\titers: 900, epoch: 3 | loss: 0.2724562\n",
      "\tspeed: 0.0513s/iter; left time: 789.2105s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:46.99s\n",
      "Steps: 904 | Train Loss: 0.3275198 Vali Loss: 0.3335762 Test Loss: 0.3880027\n",
      "Validation loss decreased (0.355511 --> 0.333576).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.2906321\n",
      "\tspeed: 0.1173s/iter; left time: 1791.2677s\n",
      "\titers: 200, epoch: 4 | loss: 0.3347478\n",
      "\tspeed: 0.0517s/iter; left time: 784.3127s\n",
      "\titers: 300, epoch: 4 | loss: 0.3311287\n",
      "\tspeed: 0.0517s/iter; left time: 778.9295s\n",
      "\titers: 400, epoch: 4 | loss: 0.2728124\n",
      "\tspeed: 0.0516s/iter; left time: 772.2581s\n",
      "\titers: 500, epoch: 4 | loss: 0.2741998\n",
      "\tspeed: 0.0516s/iter; left time: 767.0768s\n",
      "\titers: 600, epoch: 4 | loss: 0.2542885\n",
      "\tspeed: 0.0510s/iter; left time: 752.7006s\n",
      "\titers: 700, epoch: 4 | loss: 0.3259801\n",
      "\tspeed: 0.0517s/iter; left time: 758.5140s\n",
      "\titers: 800, epoch: 4 | loss: 0.2504831\n",
      "\tspeed: 0.0516s/iter; left time: 752.0335s\n",
      "\titers: 900, epoch: 4 | loss: 0.2904728\n",
      "\tspeed: 0.0518s/iter; left time: 749.0984s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:46.97s\n",
      "Steps: 904 | Train Loss: 0.2964880 Vali Loss: 0.3177445 Test Loss: 0.3852187\n",
      "Validation loss decreased (0.333576 --> 0.317744).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2898504\n",
      "\tspeed: 0.1158s/iter; left time: 1663.9319s\n",
      "\titers: 200, epoch: 5 | loss: 0.2491028\n",
      "\tspeed: 0.0517s/iter; left time: 737.3204s\n",
      "\titers: 300, epoch: 5 | loss: 0.2826107\n",
      "\tspeed: 0.0518s/iter; left time: 733.4277s\n",
      "\titers: 400, epoch: 5 | loss: 0.2733886\n",
      "\tspeed: 0.0517s/iter; left time: 726.5299s\n",
      "\titers: 500, epoch: 5 | loss: 0.2549545\n",
      "\tspeed: 0.0512s/iter; left time: 714.7083s\n",
      "\titers: 600, epoch: 5 | loss: 0.3018548\n",
      "\tspeed: 0.0521s/iter; left time: 721.7491s\n",
      "\titers: 700, epoch: 5 | loss: 0.2434069\n",
      "\tspeed: 0.0521s/iter; left time: 716.7342s\n",
      "\titers: 800, epoch: 5 | loss: 0.3028922\n",
      "\tspeed: 0.0517s/iter; left time: 706.7857s\n",
      "\titers: 900, epoch: 5 | loss: 0.2814904\n",
      "\tspeed: 0.0518s/iter; left time: 702.9755s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:47.10s\n",
      "Steps: 904 | Train Loss: 0.2688890 Vali Loss: 0.3529716 Test Loss: 0.4241291\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3295359\n",
      "\tspeed: 0.1135s/iter; left time: 1527.5366s\n",
      "\titers: 200, epoch: 6 | loss: 0.2403131\n",
      "\tspeed: 0.0516s/iter; left time: 688.9549s\n",
      "\titers: 300, epoch: 6 | loss: 0.2798400\n",
      "\tspeed: 0.0518s/iter; left time: 686.2594s\n",
      "\titers: 400, epoch: 6 | loss: 0.2702154\n",
      "\tspeed: 0.0515s/iter; left time: 677.7571s\n",
      "\titers: 500, epoch: 6 | loss: 0.2289588\n",
      "\tspeed: 0.0517s/iter; left time: 674.9058s\n",
      "\titers: 600, epoch: 6 | loss: 0.2161679\n",
      "\tspeed: 0.0516s/iter; left time: 669.0769s\n",
      "\titers: 700, epoch: 6 | loss: 0.2424191\n",
      "\tspeed: 0.0515s/iter; left time: 662.6651s\n",
      "\titers: 800, epoch: 6 | loss: 0.2258888\n",
      "\tspeed: 0.0518s/iter; left time: 661.3814s\n",
      "\titers: 900, epoch: 6 | loss: 0.2196469\n",
      "\tspeed: 0.0517s/iter; left time: 655.1008s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:47.02s\n",
      "Steps: 904 | Train Loss: 0.2440395 Vali Loss: 0.3611445 Test Loss: 0.4353454\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2200799\n",
      "\tspeed: 0.1129s/iter; left time: 1417.6238s\n",
      "\titers: 200, epoch: 7 | loss: 0.2501575\n",
      "\tspeed: 0.0518s/iter; left time: 645.2019s\n",
      "\titers: 300, epoch: 7 | loss: 0.2066524\n",
      "\tspeed: 0.0517s/iter; left time: 639.3166s\n",
      "\titers: 400, epoch: 7 | loss: 0.1980086\n",
      "\tspeed: 0.0517s/iter; left time: 634.2420s\n",
      "\titers: 500, epoch: 7 | loss: 0.1923607\n",
      "\tspeed: 0.0519s/iter; left time: 630.4622s\n",
      "\titers: 600, epoch: 7 | loss: 0.1719964\n",
      "\tspeed: 0.0517s/iter; left time: 623.6747s\n",
      "\titers: 700, epoch: 7 | loss: 0.2438216\n",
      "\tspeed: 0.0517s/iter; left time: 618.0147s\n",
      "\titers: 800, epoch: 7 | loss: 0.1751473\n",
      "\tspeed: 0.0519s/iter; left time: 615.3588s\n",
      "\titers: 900, epoch: 7 | loss: 0.2444810\n",
      "\tspeed: 0.0518s/iter; left time: 608.9990s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:47.15s\n",
      "Steps: 904 | Train Loss: 0.2175637 Vali Loss: 0.3802153 Test Loss: 0.4351691\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2132299\n",
      "\tspeed: 0.1130s/iter; left time: 1316.3507s\n",
      "\titers: 200, epoch: 8 | loss: 0.1990463\n",
      "\tspeed: 0.0516s/iter; left time: 596.1389s\n",
      "\titers: 300, epoch: 8 | loss: 0.2054024\n",
      "\tspeed: 0.0515s/iter; left time: 590.0050s\n",
      "\titers: 400, epoch: 8 | loss: 0.1858490\n",
      "\tspeed: 0.0516s/iter; left time: 585.9421s\n",
      "\titers: 500, epoch: 8 | loss: 0.1914058\n",
      "\tspeed: 0.0515s/iter; left time: 579.5616s\n",
      "\titers: 600, epoch: 8 | loss: 0.1679373\n",
      "\tspeed: 0.0515s/iter; left time: 574.9077s\n",
      "\titers: 700, epoch: 8 | loss: 0.1738705\n",
      "\tspeed: 0.0515s/iter; left time: 568.8606s\n",
      "\titers: 800, epoch: 8 | loss: 0.1722076\n",
      "\tspeed: 0.0517s/iter; left time: 566.3206s\n",
      "\titers: 900, epoch: 8 | loss: 0.1838802\n",
      "\tspeed: 0.0514s/iter; left time: 558.2551s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:46.88s\n",
      "Steps: 904 | Train Loss: 0.1922198 Vali Loss: 0.3816887 Test Loss: 0.4377359\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.1975159\n",
      "\tspeed: 0.1131s/iter; left time: 1215.4275s\n",
      "\titers: 200, epoch: 9 | loss: 0.1883935\n",
      "\tspeed: 0.0516s/iter; left time: 549.3837s\n",
      "\titers: 300, epoch: 9 | loss: 0.1685865\n",
      "\tspeed: 0.0516s/iter; left time: 544.1175s\n",
      "\titers: 400, epoch: 9 | loss: 0.1531708\n",
      "\tspeed: 0.0517s/iter; left time: 539.7590s\n",
      "\titers: 500, epoch: 9 | loss: 0.1819919\n",
      "\tspeed: 0.0516s/iter; left time: 534.2410s\n",
      "\titers: 600, epoch: 9 | loss: 0.1596032\n",
      "\tspeed: 0.0517s/iter; left time: 530.3409s\n",
      "\titers: 700, epoch: 9 | loss: 0.1527569\n",
      "\tspeed: 0.0517s/iter; left time: 525.0576s\n",
      "\titers: 800, epoch: 9 | loss: 0.1634300\n",
      "\tspeed: 0.0517s/iter; left time: 519.2757s\n",
      "\titers: 900, epoch: 9 | loss: 0.1541090\n",
      "\tspeed: 0.0516s/iter; left time: 513.8176s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:47.01s\n",
      "Steps: 904 | Train Loss: 0.1704146 Vali Loss: 0.4038353 Test Loss: 0.4542762\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_96_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.3854016661643982, rmse:0.6208072900772095, mae:0.4134919345378876, rse:0.568414032459259\n",
      "Original data scale mse:3884761.25, rmse:1970.979736328125, mae:1295.9857177734375, rse:0.13870589435100555\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_168_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_168_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 1.0599571\n",
      "\tspeed: 0.0823s/iter; left time: 1476.1908s\n",
      "\titers: 200, epoch: 1 | loss: 0.9146472\n",
      "\tspeed: 0.0516s/iter; left time: 919.7708s\n",
      "\titers: 300, epoch: 1 | loss: 0.9862018\n",
      "\tspeed: 0.0517s/iter; left time: 916.3574s\n",
      "\titers: 400, epoch: 1 | loss: 0.9430930\n",
      "\tspeed: 0.0513s/iter; left time: 905.7961s\n",
      "\titers: 500, epoch: 1 | loss: 0.9028535\n",
      "\tspeed: 0.0515s/iter; left time: 904.1855s\n",
      "\titers: 600, epoch: 1 | loss: 0.8925955\n",
      "\tspeed: 0.0493s/iter; left time: 860.4688s\n",
      "\titers: 700, epoch: 1 | loss: 0.8302385\n",
      "\tspeed: 0.0518s/iter; left time: 898.2658s\n",
      "\titers: 800, epoch: 1 | loss: 0.9435801\n",
      "\tspeed: 0.0516s/iter; left time: 890.0281s\n",
      "\titers: 900, epoch: 1 | loss: 0.8077694\n",
      "\tspeed: 0.0516s/iter; left time: 884.3360s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:47.12s\n",
      "Steps: 902 | Train Loss: 0.9146074 Vali Loss: 0.7948765 Test Loss: 0.8959451\n",
      "Validation loss decreased (inf --> 0.794877).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.7416722\n",
      "\tspeed: 0.1228s/iter; left time: 2093.1566s\n",
      "\titers: 200, epoch: 2 | loss: 0.7197086\n",
      "\tspeed: 0.0516s/iter; left time: 873.9553s\n",
      "\titers: 300, epoch: 2 | loss: 0.6376743\n",
      "\tspeed: 0.0516s/iter; left time: 869.4985s\n",
      "\titers: 400, epoch: 2 | loss: 0.6235409\n",
      "\tspeed: 0.0516s/iter; left time: 864.2817s\n",
      "\titers: 500, epoch: 2 | loss: 0.4932248\n",
      "\tspeed: 0.0516s/iter; left time: 858.9910s\n",
      "\titers: 600, epoch: 2 | loss: 0.4522580\n",
      "\tspeed: 0.0511s/iter; left time: 845.2259s\n",
      "\titers: 700, epoch: 2 | loss: 0.4234111\n",
      "\tspeed: 0.0501s/iter; left time: 823.2990s\n",
      "\titers: 800, epoch: 2 | loss: 0.4185427\n",
      "\tspeed: 0.0511s/iter; left time: 835.2608s\n",
      "\titers: 900, epoch: 2 | loss: 0.4197146\n",
      "\tspeed: 0.0516s/iter; left time: 838.7220s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:46.61s\n",
      "Steps: 902 | Train Loss: 0.5736095 Vali Loss: 0.3972848 Test Loss: 0.4653198\n",
      "Validation loss decreased (0.794877 --> 0.397285).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.4323506\n",
      "\tspeed: 0.1291s/iter; left time: 2083.9764s\n",
      "\titers: 200, epoch: 3 | loss: 0.3972625\n",
      "\tspeed: 0.0517s/iter; left time: 829.1890s\n",
      "\titers: 300, epoch: 3 | loss: 0.3877845\n",
      "\tspeed: 0.0517s/iter; left time: 823.6793s\n",
      "\titers: 400, epoch: 3 | loss: 0.3104851\n",
      "\tspeed: 0.0517s/iter; left time: 818.6824s\n",
      "\titers: 500, epoch: 3 | loss: 0.3706648\n",
      "\tspeed: 0.0517s/iter; left time: 814.1062s\n",
      "\titers: 600, epoch: 3 | loss: 0.3845878\n",
      "\tspeed: 0.0516s/iter; left time: 806.5120s\n",
      "\titers: 700, epoch: 3 | loss: 0.3442249\n",
      "\tspeed: 0.0516s/iter; left time: 802.2507s\n",
      "\titers: 800, epoch: 3 | loss: 0.3448684\n",
      "\tspeed: 0.0515s/iter; left time: 795.0149s\n",
      "\titers: 900, epoch: 3 | loss: 0.3636410\n",
      "\tspeed: 0.0516s/iter; left time: 790.9849s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:46.92s\n",
      "Steps: 902 | Train Loss: 0.3729933 Vali Loss: 0.3829978 Test Loss: 0.4045010\n",
      "Validation loss decreased (0.397285 --> 0.382998).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.3502441\n",
      "\tspeed: 0.1273s/iter; left time: 1938.6921s\n",
      "\titers: 200, epoch: 4 | loss: 0.3205854\n",
      "\tspeed: 0.0517s/iter; left time: 782.7661s\n",
      "\titers: 300, epoch: 4 | loss: 0.3474055\n",
      "\tspeed: 0.0517s/iter; left time: 776.6885s\n",
      "\titers: 400, epoch: 4 | loss: 0.3215352\n",
      "\tspeed: 0.0517s/iter; left time: 772.7155s\n",
      "\titers: 500, epoch: 4 | loss: 0.3431256\n",
      "\tspeed: 0.0517s/iter; left time: 767.3724s\n",
      "\titers: 600, epoch: 4 | loss: 0.2975606\n",
      "\tspeed: 0.0517s/iter; left time: 761.1627s\n",
      "\titers: 700, epoch: 4 | loss: 0.3334012\n",
      "\tspeed: 0.0517s/iter; left time: 756.9393s\n",
      "\titers: 800, epoch: 4 | loss: 0.3398586\n",
      "\tspeed: 0.0519s/iter; left time: 754.6424s\n",
      "\titers: 900, epoch: 4 | loss: 0.3267857\n",
      "\tspeed: 0.0518s/iter; left time: 747.9781s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:46.99s\n",
      "Steps: 902 | Train Loss: 0.3316460 Vali Loss: 0.3764287 Test Loss: 0.4217106\n",
      "Validation loss decreased (0.382998 --> 0.376429).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3080970\n",
      "\tspeed: 0.1256s/iter; left time: 1800.5005s\n",
      "\titers: 200, epoch: 5 | loss: 0.2868743\n",
      "\tspeed: 0.0519s/iter; left time: 739.0251s\n",
      "\titers: 300, epoch: 5 | loss: 0.3161268\n",
      "\tspeed: 0.0520s/iter; left time: 734.3758s\n",
      "\titers: 400, epoch: 5 | loss: 0.3128489\n",
      "\tspeed: 0.0518s/iter; left time: 726.7395s\n",
      "\titers: 500, epoch: 5 | loss: 0.2708879\n",
      "\tspeed: 0.0520s/iter; left time: 723.8270s\n",
      "\titers: 600, epoch: 5 | loss: 0.3207425\n",
      "\tspeed: 0.0521s/iter; left time: 720.5214s\n",
      "\titers: 700, epoch: 5 | loss: 0.3223326\n",
      "\tspeed: 0.0518s/iter; left time: 711.1901s\n",
      "\titers: 800, epoch: 5 | loss: 0.2741796\n",
      "\tspeed: 0.0518s/iter; left time: 706.7656s\n",
      "\titers: 900, epoch: 5 | loss: 0.3119611\n",
      "\tspeed: 0.0519s/iter; left time: 701.9715s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:47.11s\n",
      "Steps: 902 | Train Loss: 0.2968804 Vali Loss: 0.3735010 Test Loss: 0.4260625\n",
      "Validation loss decreased (0.376429 --> 0.373501).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2857382\n",
      "\tspeed: 0.1241s/iter; left time: 1666.4930s\n",
      "\titers: 200, epoch: 6 | loss: 0.3208684\n",
      "\tspeed: 0.0517s/iter; left time: 689.1320s\n",
      "\titers: 300, epoch: 6 | loss: 0.3019917\n",
      "\tspeed: 0.0517s/iter; left time: 683.6566s\n",
      "\titers: 400, epoch: 6 | loss: 0.2456153\n",
      "\tspeed: 0.0512s/iter; left time: 672.3353s\n",
      "\titers: 500, epoch: 6 | loss: 0.2614456\n",
      "\tspeed: 0.0518s/iter; left time: 674.4629s\n",
      "\titers: 600, epoch: 6 | loss: 0.2548637\n",
      "\tspeed: 0.0522s/iter; left time: 674.9946s\n",
      "\titers: 700, epoch: 6 | loss: 0.2542764\n",
      "\tspeed: 0.0519s/iter; left time: 665.6016s\n",
      "\titers: 800, epoch: 6 | loss: 0.2772080\n",
      "\tspeed: 0.0518s/iter; left time: 659.0721s\n",
      "\titers: 900, epoch: 6 | loss: 0.2557196\n",
      "\tspeed: 0.0519s/iter; left time: 656.0120s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:47.07s\n",
      "Steps: 902 | Train Loss: 0.2628448 Vali Loss: 0.3993384 Test Loss: 0.4352078\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2949790\n",
      "\tspeed: 0.1216s/iter; left time: 1523.9224s\n",
      "\titers: 200, epoch: 7 | loss: 0.2448027\n",
      "\tspeed: 0.0517s/iter; left time: 642.3195s\n",
      "\titers: 300, epoch: 7 | loss: 0.2382711\n",
      "\tspeed: 0.0518s/iter; left time: 638.2055s\n",
      "\titers: 400, epoch: 7 | loss: 0.2376073\n",
      "\tspeed: 0.0519s/iter; left time: 635.1735s\n",
      "\titers: 500, epoch: 7 | loss: 0.2278139\n",
      "\tspeed: 0.0514s/iter; left time: 623.5473s\n",
      "\titers: 600, epoch: 7 | loss: 0.2414389\n",
      "\tspeed: 0.0519s/iter; left time: 624.0002s\n",
      "\titers: 700, epoch: 7 | loss: 0.2097620\n",
      "\tspeed: 0.0513s/iter; left time: 611.3947s\n",
      "\titers: 800, epoch: 7 | loss: 0.2199466\n",
      "\tspeed: 0.0515s/iter; left time: 609.3758s\n",
      "\titers: 900, epoch: 7 | loss: 0.2336553\n",
      "\tspeed: 0.0518s/iter; left time: 607.4826s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:46.93s\n",
      "Steps: 902 | Train Loss: 0.2334994 Vali Loss: 0.4261711 Test Loss: 0.4602007\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2245642\n",
      "\tspeed: 0.1218s/iter; left time: 1415.9003s\n",
      "\titers: 200, epoch: 8 | loss: 0.2025407\n",
      "\tspeed: 0.0519s/iter; left time: 597.9041s\n",
      "\titers: 300, epoch: 8 | loss: 0.2375422\n",
      "\tspeed: 0.0512s/iter; left time: 584.9635s\n",
      "\titers: 400, epoch: 8 | loss: 0.2024304\n",
      "\tspeed: 0.0519s/iter; left time: 588.3409s\n",
      "\titers: 500, epoch: 8 | loss: 0.2074630\n",
      "\tspeed: 0.0512s/iter; left time: 575.3170s\n",
      "\titers: 600, epoch: 8 | loss: 0.2052093\n",
      "\tspeed: 0.0518s/iter; left time: 576.9294s\n",
      "\titers: 700, epoch: 8 | loss: 0.2008850\n",
      "\tspeed: 0.0519s/iter; left time: 571.7887s\n",
      "\titers: 800, epoch: 8 | loss: 0.2065239\n",
      "\tspeed: 0.0518s/iter; left time: 566.5132s\n",
      "\titers: 900, epoch: 8 | loss: 0.1829730\n",
      "\tspeed: 0.0508s/iter; left time: 550.4341s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:46.80s\n",
      "Steps: 902 | Train Loss: 0.2074005 Vali Loss: 0.4316106 Test Loss: 0.4637832\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.1811360\n",
      "\tspeed: 0.1206s/iter; left time: 1293.7010s\n",
      "\titers: 200, epoch: 9 | loss: 0.1887605\n",
      "\tspeed: 0.0511s/iter; left time: 543.3320s\n",
      "\titers: 300, epoch: 9 | loss: 0.1895094\n",
      "\tspeed: 0.0505s/iter; left time: 532.0015s\n",
      "\titers: 400, epoch: 9 | loss: 0.1703750\n",
      "\tspeed: 0.0512s/iter; left time: 533.4924s\n",
      "\titers: 500, epoch: 9 | loss: 0.1833475\n",
      "\tspeed: 0.0513s/iter; left time: 529.4879s\n",
      "\titers: 600, epoch: 9 | loss: 0.1778692\n",
      "\tspeed: 0.0518s/iter; left time: 529.1875s\n",
      "\titers: 700, epoch: 9 | loss: 0.1967353\n",
      "\tspeed: 0.0516s/iter; left time: 522.8551s\n",
      "\titers: 800, epoch: 9 | loss: 0.1660226\n",
      "\tspeed: 0.0517s/iter; left time: 518.6163s\n",
      "\titers: 900, epoch: 9 | loss: 0.1866916\n",
      "\tspeed: 0.0518s/iter; left time: 514.0871s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:46.69s\n",
      "Steps: 902 | Train Loss: 0.1851904 Vali Loss: 0.4627993 Test Loss: 0.4957201\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.1800543\n",
      "\tspeed: 0.1197s/iter; left time: 1175.5895s\n",
      "\titers: 200, epoch: 10 | loss: 0.1974288\n",
      "\tspeed: 0.0502s/iter; left time: 488.5650s\n",
      "\titers: 300, epoch: 10 | loss: 0.1635192\n",
      "\tspeed: 0.0517s/iter; left time: 497.9180s\n",
      "\titers: 400, epoch: 10 | loss: 0.1809885\n",
      "\tspeed: 0.0516s/iter; left time: 491.3303s\n",
      "\titers: 500, epoch: 10 | loss: 0.1682457\n",
      "\tspeed: 0.0503s/iter; left time: 473.6145s\n",
      "\titers: 600, epoch: 10 | loss: 0.1658060\n",
      "\tspeed: 0.0516s/iter; left time: 481.3395s\n",
      "\titers: 700, epoch: 10 | loss: 0.1527503\n",
      "\tspeed: 0.0516s/iter; left time: 476.1106s\n",
      "\titers: 800, epoch: 10 | loss: 0.1724045\n",
      "\tspeed: 0.0511s/iter; left time: 465.7599s\n",
      "\titers: 900, epoch: 10 | loss: 0.1535063\n",
      "\tspeed: 0.0517s/iter; left time: 466.9235s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:46.47s\n",
      "Steps: 902 | Train Loss: 0.1671392 Vali Loss: 0.4554501 Test Loss: 0.4908405\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_168_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.42608726024627686, rmse:0.6527535915374756, mae:0.42866116762161255, rse:0.5978423357009888\n",
      "Original data scale mse:4238251.5, rmse:2058.701416015625, mae:1349.240234375, rse:0.14501526951789856\n"
     ]
    }
   ],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "seq_len = \"96\"\n",
    "lr = \"0.0001\"\n",
    "itr = 1  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Lists to store the results\n",
    "informer_results_scaled, informer_results_unscaled = [], []\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    for loss in losses:\n",
    "        statement_1 = f\"\\n=== Starting experiments for loss function: {loss} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)  # Print to .ipynb output cell\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_scaler_choice_for_{country}\"\n",
    "\n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --random_seed 2021 \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --label_len 48 \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers 2 \\\n",
    "              --d_layers 1 \\\n",
    "              --factor 5 \\\n",
    "              --enc_in 3 \\\n",
    "              --dec_in 3 \\\n",
    "              --c_out 3 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --dropout 0.1 \\\n",
    "              --overlapping_windows \\\n",
    "              --inverse \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --scaler_type standard \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            # Wait for the process to complete\n",
    "            process.wait()\n",
    "            \n",
    "            # Delete the checkpoints folder and all its contents\n",
    "            shutil.rmtree('./checkpoints' )\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "            iteration_metrics_unscaled = extract_metrics_from_output(output, itr, if_scaled=False)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, (scaled_metrics, unscaled_metrics) in enumerate(zip(iteration_metrics, iteration_metrics_unscaled), start=1):\n",
    "                log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}, iteration={iteration}:\\n\")\n",
    "                log_file.write(f\"Scaled Metrics - MSE: {scaled_metrics[0]}, RMSE: {scaled_metrics[1]}, MAE: {scaled_metrics[2]}, RSE: {scaled_metrics[3]}\\n\")\n",
    "                log_file.write(f\"Unscaled Metrics - MSE: {unscaled_metrics[0]}, RMSE: {unscaled_metrics[1]}, MAE: {unscaled_metrics[2]}, RSE: {unscaled_metrics[3]}\\n\")\n",
    "\n",
    "                # Append the results to the informer_results lists\n",
    "                metrics_data = [(informer_results_scaled, scaled_metrics), (informer_results_unscaled, unscaled_metrics)]\n",
    "\n",
    "                for result_list, metrics in metrics_data:\n",
    "                    result_list.append({\n",
    "                        'Loss_function': loss,\n",
    "                        'Pred_len': pred_len,\n",
    "                        'Iteration': iteration,\n",
    "                        'MSE': metrics[0],\n",
    "                        'RMSE': metrics[1],\n",
    "                        'MAE': metrics[2],\n",
    "                        'RSE': metrics[3]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>0.4544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.3854</td>\n",
       "      <td>0.6208</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.5684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.4261</td>\n",
       "      <td>0.6528</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>0.5978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MSE    RMSE     MAE     RSE\n",
       "Loss_function Iteration Pred_len                                \n",
       "MSE           1         24        0.2462  0.4962  0.3045  0.4544\n",
       "                        96        0.3854  0.6208  0.4135  0.5684\n",
       "                        168       0.4261  0.6528  0.4287  0.5978"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir = './results/scaler_choice'\n",
    "csv_name_scaled = 'informer_scaler_results_scaled_IT_default.csv'\n",
    "csv_name_unscaled = 'informer_scaler_results_unscaled_IT_default.csv'\n",
    "\n",
    "# Convert the results into a DataFrame and save as CSV\n",
    "informer_df_scaled = convert_results_into_df(informer_results_scaled, path_dir, csv_name_scaled)\n",
    "informer_df_unscaled = convert_results_into_df(informer_results_unscaled, path_dir, csv_name_unscaled)\n",
    "informer_df_scaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>2325943.50</td>\n",
       "      <td>1525.1044</td>\n",
       "      <td>940.2657</td>\n",
       "      <td>0.1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3884761.25</td>\n",
       "      <td>1970.9797</td>\n",
       "      <td>1295.9857</td>\n",
       "      <td>0.1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>4238251.50</td>\n",
       "      <td>2058.7014</td>\n",
       "      <td>1349.2402</td>\n",
       "      <td>0.1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MSE       RMSE        MAE     RSE\n",
       "Loss_function Iteration Pred_len                                          \n",
       "MSE           1         24        2325943.50  1525.1044   940.2657  0.1072\n",
       "                        96        3884761.25  1970.9797  1295.9857  0.1387\n",
       "                        168       4238251.50  2058.7014  1349.2402  0.1450"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informer_df_unscaled.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standard Scaler PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for loss function: MSE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_24_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_24_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.8173407\n",
      "\tspeed: 0.0545s/iter; left time: 974.0828s\n",
      "\titers: 200, epoch: 1 | loss: 0.8205349\n",
      "\tspeed: 0.0260s/iter; left time: 461.5904s\n",
      "\titers: 300, epoch: 1 | loss: 0.5769502\n",
      "\tspeed: 0.0234s/iter; left time: 413.8217s\n",
      "\titers: 400, epoch: 1 | loss: 0.5308835\n",
      "\tspeed: 0.0243s/iter; left time: 427.0660s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 500, epoch: 1 | loss: 0.4479343\n",
      "\tspeed: 0.0262s/iter; left time: 457.2129s\n",
      "\titers: 600, epoch: 1 | loss: 0.4395982\n",
      "\tspeed: 0.0264s/iter; left time: 459.0301s\n",
      "\titers: 700, epoch: 1 | loss: 0.3167984\n",
      "\tspeed: 0.0262s/iter; left time: 453.1086s\n",
      "\titers: 800, epoch: 1 | loss: 0.3509845\n",
      "\tspeed: 0.0258s/iter; left time: 442.4732s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:23.45s\n",
      "Steps: 899 | Train Loss: 0.5659182 Vali Loss: 0.3030057 Test Loss: 0.3194748\n",
      "Validation loss decreased (inf --> 0.303006).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2820484\n",
      "\tspeed: 0.0808s/iter; left time: 1371.6311s\n",
      "\titers: 200, epoch: 2 | loss: 0.2486698\n",
      "\tspeed: 0.0269s/iter; left time: 454.7992s\n",
      "\titers: 300, epoch: 2 | loss: 0.2356297\n",
      "\tspeed: 0.0273s/iter; left time: 458.4444s\n",
      "\titers: 400, epoch: 2 | loss: 0.2551486\n",
      "\tspeed: 0.0273s/iter; left time: 455.0637s\n",
      "\titers: 500, epoch: 2 | loss: 0.2183162\n",
      "\tspeed: 0.0277s/iter; left time: 459.4737s\n",
      "\titers: 600, epoch: 2 | loss: 0.2147112\n",
      "\tspeed: 0.0266s/iter; left time: 438.4539s\n",
      "\titers: 700, epoch: 2 | loss: 0.2786916\n",
      "\tspeed: 0.0268s/iter; left time: 438.7921s\n",
      "\titers: 800, epoch: 2 | loss: 0.2435412\n",
      "\tspeed: 0.0269s/iter; left time: 438.7816s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:24.72s\n",
      "Steps: 899 | Train Loss: 0.2417682 Vali Loss: 0.1958337 Test Loss: 0.2185773\n",
      "Validation loss decreased (0.303006 --> 0.195834).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.1898149\n",
      "\tspeed: 0.0791s/iter; left time: 1272.9658s\n",
      "\titers: 200, epoch: 3 | loss: 0.1478680\n",
      "\tspeed: 0.0260s/iter; left time: 415.5330s\n",
      "\titers: 300, epoch: 3 | loss: 0.2326348\n",
      "\tspeed: 0.0260s/iter; left time: 412.6034s\n",
      "\titers: 400, epoch: 3 | loss: 0.1961844\n",
      "\tspeed: 0.0256s/iter; left time: 403.5883s\n",
      "\titers: 500, epoch: 3 | loss: 0.2435740\n",
      "\tspeed: 0.0259s/iter; left time: 405.7107s\n",
      "\titers: 600, epoch: 3 | loss: 0.2442115\n",
      "\tspeed: 0.0260s/iter; left time: 405.1488s\n",
      "\titers: 700, epoch: 3 | loss: 0.1891328\n",
      "\tspeed: 0.0262s/iter; left time: 404.9245s\n",
      "\titers: 800, epoch: 3 | loss: 0.2464072\n",
      "\tspeed: 0.0267s/iter; left time: 411.0560s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:23.45s\n",
      "Steps: 899 | Train Loss: 0.2039831 Vali Loss: 0.1867643 Test Loss: 0.2078768\n",
      "Validation loss decreased (0.195834 --> 0.186764).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.1523269\n",
      "\tspeed: 0.0791s/iter; left time: 1201.2370s\n",
      "\titers: 200, epoch: 4 | loss: 0.1430327\n",
      "\tspeed: 0.0263s/iter; left time: 397.2359s\n",
      "\titers: 300, epoch: 4 | loss: 0.1972496\n",
      "\tspeed: 0.0260s/iter; left time: 389.3362s\n",
      "\titers: 400, epoch: 4 | loss: 0.1803472\n",
      "\tspeed: 0.0259s/iter; left time: 385.1820s\n",
      "\titers: 500, epoch: 4 | loss: 0.2055981\n",
      "\tspeed: 0.0260s/iter; left time: 384.7030s\n",
      "\titers: 600, epoch: 4 | loss: 0.1683220\n",
      "\tspeed: 0.0259s/iter; left time: 381.0463s\n",
      "\titers: 700, epoch: 4 | loss: 0.2165362\n",
      "\tspeed: 0.0266s/iter; left time: 388.4014s\n",
      "\titers: 800, epoch: 4 | loss: 0.1463665\n",
      "\tspeed: 0.0273s/iter; left time: 395.0116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:23.61s\n",
      "Steps: 899 | Train Loss: 0.1934826 Vali Loss: 0.1840229 Test Loss: 0.2052956\n",
      "Validation loss decreased (0.186764 --> 0.184023).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2043330\n",
      "\tspeed: 0.0793s/iter; left time: 1133.0762s\n",
      "\titers: 200, epoch: 5 | loss: 0.1575059\n",
      "\tspeed: 0.0274s/iter; left time: 389.1746s\n",
      "\titers: 300, epoch: 5 | loss: 0.1582072\n",
      "\tspeed: 0.0277s/iter; left time: 389.6209s\n",
      "\titers: 400, epoch: 5 | loss: 0.2323864\n",
      "\tspeed: 0.0280s/iter; left time: 391.4387s\n",
      "\titers: 500, epoch: 5 | loss: 0.1773194\n",
      "\tspeed: 0.0272s/iter; left time: 378.3517s\n",
      "\titers: 600, epoch: 5 | loss: 0.2574989\n",
      "\tspeed: 0.0277s/iter; left time: 382.0144s\n",
      "\titers: 700, epoch: 5 | loss: 0.1830094\n",
      "\tspeed: 0.0275s/iter; left time: 376.0233s\n",
      "\titers: 800, epoch: 5 | loss: 0.2089909\n",
      "\tspeed: 0.0274s/iter; left time: 372.4690s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:24.93s\n",
      "Steps: 899 | Train Loss: 0.1855423 Vali Loss: 0.1839958 Test Loss: 0.2085562\n",
      "Validation loss decreased (0.184023 --> 0.183996).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.1464173\n",
      "\tspeed: 0.0822s/iter; left time: 1100.3252s\n",
      "\titers: 200, epoch: 6 | loss: 0.1833275\n",
      "\tspeed: 0.0260s/iter; left time: 346.0270s\n",
      "\titers: 300, epoch: 6 | loss: 0.1565607\n",
      "\tspeed: 0.0277s/iter; left time: 365.6047s\n",
      "\titers: 400, epoch: 6 | loss: 0.1426036\n",
      "\tspeed: 0.0278s/iter; left time: 364.3110s\n",
      "\titers: 500, epoch: 6 | loss: 0.1469463\n",
      "\tspeed: 0.0278s/iter; left time: 361.0011s\n",
      "\titers: 600, epoch: 6 | loss: 0.2014067\n",
      "\tspeed: 0.0279s/iter; left time: 359.7564s\n",
      "\titers: 700, epoch: 6 | loss: 0.1651355\n",
      "\tspeed: 0.0278s/iter; left time: 354.9407s\n",
      "\titers: 800, epoch: 6 | loss: 0.1587914\n",
      "\tspeed: 0.0274s/iter; left time: 347.3506s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:24.65s\n",
      "Steps: 899 | Train Loss: 0.1807978 Vali Loss: 0.1777520 Test Loss: 0.2007811\n",
      "Validation loss decreased (0.183996 --> 0.177752).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.1751475\n",
      "\tspeed: 0.0797s/iter; left time: 994.8816s\n",
      "\titers: 200, epoch: 7 | loss: 0.1367436\n",
      "\tspeed: 0.0265s/iter; left time: 327.7501s\n",
      "\titers: 300, epoch: 7 | loss: 0.1929462\n",
      "\tspeed: 0.0268s/iter; left time: 329.7753s\n",
      "\titers: 400, epoch: 7 | loss: 0.1402995\n",
      "\tspeed: 0.0258s/iter; left time: 314.2479s\n",
      "\titers: 500, epoch: 7 | loss: 0.2135105\n",
      "\tspeed: 0.0266s/iter; left time: 321.0307s\n",
      "\titers: 600, epoch: 7 | loss: 0.1834536\n",
      "\tspeed: 0.0262s/iter; left time: 313.9696s\n",
      "\titers: 700, epoch: 7 | loss: 0.1517590\n",
      "\tspeed: 0.0263s/iter; left time: 312.9932s\n",
      "\titers: 800, epoch: 7 | loss: 0.1591292\n",
      "\tspeed: 0.0264s/iter; left time: 311.6459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:23.95s\n",
      "Steps: 899 | Train Loss: 0.1761310 Vali Loss: 0.1778424 Test Loss: 0.2001090\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.1718522\n",
      "\tspeed: 0.0784s/iter; left time: 908.9043s\n",
      "\titers: 200, epoch: 8 | loss: 0.1981576\n",
      "\tspeed: 0.0264s/iter; left time: 302.8192s\n",
      "\titers: 300, epoch: 8 | loss: 0.1597967\n",
      "\tspeed: 0.0271s/iter; left time: 308.9109s\n",
      "\titers: 400, epoch: 8 | loss: 0.1710954\n",
      "\tspeed: 0.0268s/iter; left time: 302.1501s\n",
      "\titers: 500, epoch: 8 | loss: 0.1998722\n",
      "\tspeed: 0.0275s/iter; left time: 307.2381s\n",
      "\titers: 600, epoch: 8 | loss: 0.1848805\n",
      "\tspeed: 0.0268s/iter; left time: 297.1757s\n",
      "\titers: 700, epoch: 8 | loss: 0.1758774\n",
      "\tspeed: 0.0274s/iter; left time: 301.4617s\n",
      "\titers: 800, epoch: 8 | loss: 0.1726665\n",
      "\tspeed: 0.0266s/iter; left time: 289.5105s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:24.42s\n",
      "Steps: 899 | Train Loss: 0.1728860 Vali Loss: 0.1750151 Test Loss: 0.1991610\n",
      "Validation loss decreased (0.177752 --> 0.175015).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.1626787\n",
      "\tspeed: 0.0813s/iter; left time: 869.0714s\n",
      "\titers: 200, epoch: 9 | loss: 0.2048347\n",
      "\tspeed: 0.0268s/iter; left time: 283.7772s\n",
      "\titers: 300, epoch: 9 | loss: 0.1736195\n",
      "\tspeed: 0.0268s/iter; left time: 280.7883s\n",
      "\titers: 400, epoch: 9 | loss: 0.1658203\n",
      "\tspeed: 0.0270s/iter; left time: 279.9996s\n",
      "\titers: 500, epoch: 9 | loss: 0.1626475\n",
      "\tspeed: 0.0269s/iter; left time: 276.2622s\n",
      "\titers: 600, epoch: 9 | loss: 0.1833851\n",
      "\tspeed: 0.0266s/iter; left time: 271.5163s\n",
      "\titers: 700, epoch: 9 | loss: 0.1671328\n",
      "\tspeed: 0.0276s/iter; left time: 277.9631s\n",
      "\titers: 800, epoch: 9 | loss: 0.2495812\n",
      "\tspeed: 0.0248s/iter; left time: 247.5591s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:24.15s\n",
      "Steps: 899 | Train Loss: 0.1694475 Vali Loss: 0.1763463 Test Loss: 0.2008619\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.1204130\n",
      "\tspeed: 0.0799s/iter; left time: 782.4802s\n",
      "\titers: 200, epoch: 10 | loss: 0.1968858\n",
      "\tspeed: 0.0284s/iter; left time: 275.0506s\n",
      "\titers: 300, epoch: 10 | loss: 0.1686702\n",
      "\tspeed: 0.0276s/iter; left time: 264.2460s\n",
      "\titers: 400, epoch: 10 | loss: 0.1917014\n",
      "\tspeed: 0.0270s/iter; left time: 256.1670s\n",
      "\titers: 500, epoch: 10 | loss: 0.1951108\n",
      "\tspeed: 0.0272s/iter; left time: 255.3364s\n",
      "\titers: 600, epoch: 10 | loss: 0.1220782\n",
      "\tspeed: 0.0263s/iter; left time: 244.7150s\n",
      "\titers: 700, epoch: 10 | loss: 0.1804576\n",
      "\tspeed: 0.0265s/iter; left time: 243.1696s\n",
      "\titers: 800, epoch: 10 | loss: 0.1660571\n",
      "\tspeed: 0.0271s/iter; left time: 245.9160s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:24.72s\n",
      "Steps: 899 | Train Loss: 0.1663082 Vali Loss: 0.1763272 Test Loss: 0.2025419\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.1632800\n",
      "\tspeed: 0.0774s/iter; left time: 687.7770s\n",
      "\titers: 200, epoch: 11 | loss: 0.1723397\n",
      "\tspeed: 0.0275s/iter; left time: 241.7415s\n",
      "\titers: 300, epoch: 11 | loss: 0.1466605\n",
      "\tspeed: 0.0274s/iter; left time: 237.8172s\n",
      "\titers: 400, epoch: 11 | loss: 0.1904792\n",
      "\tspeed: 0.0273s/iter; left time: 234.4320s\n",
      "\titers: 500, epoch: 11 | loss: 0.1586496\n",
      "\tspeed: 0.0273s/iter; left time: 231.7028s\n",
      "\titers: 600, epoch: 11 | loss: 0.1352534\n",
      "\tspeed: 0.0273s/iter; left time: 228.7107s\n",
      "\titers: 700, epoch: 11 | loss: 0.1522498\n",
      "\tspeed: 0.0271s/iter; left time: 224.7959s\n",
      "\titers: 800, epoch: 11 | loss: 0.1605842\n",
      "\tspeed: 0.0262s/iter; left time: 214.7035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:24.40s\n",
      "Steps: 899 | Train Loss: 0.1637438 Vali Loss: 0.1755170 Test Loss: 0.2018310\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.1769325\n",
      "\tspeed: 0.0806s/iter; left time: 644.2919s\n",
      "\titers: 200, epoch: 12 | loss: 0.1769821\n",
      "\tspeed: 0.0281s/iter; left time: 222.0327s\n",
      "\titers: 300, epoch: 12 | loss: 0.1857043\n",
      "\tspeed: 0.0270s/iter; left time: 210.7270s\n",
      "\titers: 400, epoch: 12 | loss: 0.1533699\n",
      "\tspeed: 0.0274s/iter; left time: 210.6242s\n",
      "\titers: 500, epoch: 12 | loss: 0.1861766\n",
      "\tspeed: 0.0262s/iter; left time: 198.7943s\n",
      "\titers: 600, epoch: 12 | loss: 0.2058143\n",
      "\tspeed: 0.0271s/iter; left time: 202.8411s\n",
      "\titers: 700, epoch: 12 | loss: 0.1782941\n",
      "\tspeed: 0.0276s/iter; left time: 204.0764s\n",
      "\titers: 800, epoch: 12 | loss: 0.1737522\n",
      "\tspeed: 0.0278s/iter; left time: 202.8179s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:24.96s\n",
      "Steps: 899 | Train Loss: 0.1614738 Vali Loss: 0.1736168 Test Loss: 0.2015185\n",
      "Validation loss decreased (0.175015 --> 0.173617).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.1744665\n",
      "\tspeed: 0.0811s/iter; left time: 575.5499s\n",
      "\titers: 200, epoch: 13 | loss: 0.1784617\n",
      "\tspeed: 0.0277s/iter; left time: 193.5568s\n",
      "\titers: 300, epoch: 13 | loss: 0.1298072\n",
      "\tspeed: 0.0276s/iter; left time: 189.9669s\n",
      "\titers: 400, epoch: 13 | loss: 0.1713600\n",
      "\tspeed: 0.0276s/iter; left time: 187.3085s\n",
      "\titers: 500, epoch: 13 | loss: 0.2452477\n",
      "\tspeed: 0.0274s/iter; left time: 183.5868s\n",
      "\titers: 600, epoch: 13 | loss: 0.1479025\n",
      "\tspeed: 0.0266s/iter; left time: 175.3407s\n",
      "\titers: 700, epoch: 13 | loss: 0.1719865\n",
      "\tspeed: 0.0271s/iter; left time: 175.9276s\n",
      "\titers: 800, epoch: 13 | loss: 0.2252502\n",
      "\tspeed: 0.0282s/iter; left time: 179.9726s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:24.83s\n",
      "Steps: 899 | Train Loss: 0.1597973 Vali Loss: 0.1738537 Test Loss: 0.2023633\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.2519771\n",
      "\tspeed: 0.0795s/iter; left time: 492.2416s\n",
      "\titers: 200, epoch: 14 | loss: 0.1420342\n",
      "\tspeed: 0.0258s/iter; left time: 157.2663s\n",
      "\titers: 300, epoch: 14 | loss: 0.1578795\n",
      "\tspeed: 0.0259s/iter; left time: 155.1419s\n",
      "\titers: 400, epoch: 14 | loss: 0.1623245\n",
      "\tspeed: 0.0258s/iter; left time: 152.1970s\n",
      "\titers: 500, epoch: 14 | loss: 0.1903390\n",
      "\tspeed: 0.0259s/iter; left time: 150.0422s\n",
      "\titers: 600, epoch: 14 | loss: 0.2421660\n",
      "\tspeed: 0.0259s/iter; left time: 147.2428s\n",
      "\titers: 700, epoch: 14 | loss: 0.1616682\n",
      "\tspeed: 0.0267s/iter; left time: 149.2620s\n",
      "\titers: 800, epoch: 14 | loss: 0.1815393\n",
      "\tspeed: 0.0272s/iter; left time: 149.3032s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:23.82s\n",
      "Steps: 899 | Train Loss: 0.1580051 Vali Loss: 0.1744019 Test Loss: 0.2024658\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.1843944\n",
      "\tspeed: 0.0809s/iter; left time: 428.2256s\n",
      "\titers: 200, epoch: 15 | loss: 0.1800953\n",
      "\tspeed: 0.0274s/iter; left time: 142.6002s\n",
      "\titers: 300, epoch: 15 | loss: 0.1088820\n",
      "\tspeed: 0.0283s/iter; left time: 144.2170s\n",
      "\titers: 400, epoch: 15 | loss: 0.1398392\n",
      "\tspeed: 0.0280s/iter; left time: 139.8668s\n",
      "\titers: 500, epoch: 15 | loss: 0.1207169\n",
      "\tspeed: 0.0279s/iter; left time: 136.7835s\n",
      "\titers: 600, epoch: 15 | loss: 0.1362888\n",
      "\tspeed: 0.0275s/iter; left time: 131.7772s\n",
      "\titers: 700, epoch: 15 | loss: 0.1511611\n",
      "\tspeed: 0.0275s/iter; left time: 129.1093s\n",
      "\titers: 800, epoch: 15 | loss: 0.1398814\n",
      "\tspeed: 0.0262s/iter; left time: 120.1992s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 15\n",
      "Cost time: 00h:00m:25.10s\n",
      "Steps: 899 | Train Loss: 0.1562872 Vali Loss: 0.1765140 Test Loss: 0.2042661\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.2216947\n",
      "\tspeed: 0.0816s/iter; left time: 358.7258s\n",
      "\titers: 200, epoch: 16 | loss: 0.1482905\n",
      "\tspeed: 0.0264s/iter; left time: 113.6222s\n",
      "\titers: 300, epoch: 16 | loss: 0.1429549\n",
      "\tspeed: 0.0263s/iter; left time: 110.1450s\n",
      "\titers: 400, epoch: 16 | loss: 0.1534701\n",
      "\tspeed: 0.0262s/iter; left time: 107.4754s\n",
      "\titers: 500, epoch: 16 | loss: 0.1501694\n",
      "\tspeed: 0.0263s/iter; left time: 104.9079s\n",
      "\titers: 600, epoch: 16 | loss: 0.1283535\n",
      "\tspeed: 0.0275s/iter; left time: 107.1685s\n",
      "\titers: 700, epoch: 16 | loss: 0.1540165\n",
      "\tspeed: 0.0279s/iter; left time: 106.0242s\n",
      "\titers: 800, epoch: 16 | loss: 0.1564166\n",
      "\tspeed: 0.0275s/iter; left time: 101.7383s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 16\n",
      "Cost time: 00h:00m:24.37s\n",
      "Steps: 899 | Train Loss: 0.1547956 Vali Loss: 0.1769402 Test Loss: 0.2031865\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.1830991\n",
      "\tspeed: 0.0785s/iter; left time: 274.4093s\n",
      "\titers: 200, epoch: 17 | loss: 0.1517261\n",
      "\tspeed: 0.0279s/iter; left time: 94.7970s\n",
      "\titers: 300, epoch: 17 | loss: 0.1420031\n",
      "\tspeed: 0.0275s/iter; left time: 90.5345s\n",
      "\titers: 400, epoch: 17 | loss: 0.1804932\n",
      "\tspeed: 0.0282s/iter; left time: 90.2262s\n",
      "\titers: 500, epoch: 17 | loss: 0.1730111\n",
      "\tspeed: 0.0272s/iter; left time: 84.1765s\n",
      "\titers: 600, epoch: 17 | loss: 0.2372685\n",
      "\tspeed: 0.0276s/iter; left time: 82.6145s\n",
      "\titers: 700, epoch: 17 | loss: 0.0937090\n",
      "\tspeed: 0.0242s/iter; left time: 70.1082s\n",
      "\titers: 800, epoch: 17 | loss: 0.1922628\n",
      "\tspeed: 0.0266s/iter; left time: 74.4852s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 17\n",
      "Cost time: 00h:00m:24.53s\n",
      "Steps: 899 | Train Loss: 0.1534246 Vali Loss: 0.1759535 Test Loss: 0.2028490\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_24_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.20151850581169128, rmse:0.44890812039375305, mae:0.26450738310813904, rse:0.41112691164016724\n",
      "Original data scale mse:1206662.25, rmse:1098.4818115234375, mae:719.5242919921875, rse:0.07719290256500244\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_96_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_96_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.8982205\n",
      "\tspeed: 0.0517s/iter; left time: 921.6720s\n",
      "\titers: 200, epoch: 1 | loss: 0.7215098\n",
      "\tspeed: 0.0262s/iter; left time: 464.8553s\n",
      "\titers: 300, epoch: 1 | loss: 0.6306594\n",
      "\tspeed: 0.0262s/iter; left time: 462.7834s\n",
      "\titers: 400, epoch: 1 | loss: 0.5872046\n",
      "\tspeed: 0.0261s/iter; left time: 457.0955s\n",
      "\titers: 500, epoch: 1 | loss: 0.5854586\n",
      "\tspeed: 0.0265s/iter; left time: 462.2632s\n",
      "\titers: 600, epoch: 1 | loss: 0.5550293\n",
      "\tspeed: 0.0262s/iter; left time: 455.0917s\n",
      "\titers: 700, epoch: 1 | loss: 0.5238206\n",
      "\tspeed: 0.0259s/iter; left time: 445.8279s\n",
      "\titers: 800, epoch: 1 | loss: 0.6284278\n",
      "\tspeed: 0.0260s/iter; left time: 446.0323s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:23.78s\n",
      "Steps: 897 | Train Loss: 0.6540915 Vali Loss: 0.4186523 Test Loss: 0.4428711\n",
      "Validation loss decreased (inf --> 0.418652).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4428511\n",
      "\tspeed: 0.0816s/iter; left time: 1382.8166s\n",
      "\titers: 200, epoch: 2 | loss: 0.3632823\n",
      "\tspeed: 0.0267s/iter; left time: 449.0796s\n",
      "\titers: 300, epoch: 2 | loss: 0.3481222\n",
      "\tspeed: 0.0269s/iter; left time: 450.1669s\n",
      "\titers: 400, epoch: 2 | loss: 0.3841143\n",
      "\tspeed: 0.0273s/iter; left time: 454.5195s\n",
      "\titers: 500, epoch: 2 | loss: 0.4033758\n",
      "\tspeed: 0.0272s/iter; left time: 450.7985s\n",
      "\titers: 600, epoch: 2 | loss: 0.3377796\n",
      "\tspeed: 0.0285s/iter; left time: 469.1660s\n",
      "\titers: 700, epoch: 2 | loss: 0.2977356\n",
      "\tspeed: 0.0276s/iter; left time: 450.4869s\n",
      "\titers: 800, epoch: 2 | loss: 0.3502355\n",
      "\tspeed: 0.0272s/iter; left time: 441.1469s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:24.83s\n",
      "Steps: 897 | Train Loss: 0.3849741 Vali Loss: 0.3324248 Test Loss: 0.3662775\n",
      "Validation loss decreased (0.418652 --> 0.332425).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.3381602\n",
      "\tspeed: 0.0849s/iter; left time: 1361.7772s\n",
      "\titers: 200, epoch: 3 | loss: 0.3395536\n",
      "\tspeed: 0.0278s/iter; left time: 442.9155s\n",
      "\titers: 300, epoch: 3 | loss: 0.2745466\n",
      "\tspeed: 0.0283s/iter; left time: 448.3270s\n",
      "\titers: 400, epoch: 3 | loss: 0.3474124\n",
      "\tspeed: 0.0270s/iter; left time: 425.6978s\n",
      "\titers: 500, epoch: 3 | loss: 0.2548940\n",
      "\tspeed: 0.0264s/iter; left time: 413.0801s\n",
      "\titers: 600, epoch: 3 | loss: 0.4413446\n",
      "\tspeed: 0.0282s/iter; left time: 439.2027s\n",
      "\titers: 700, epoch: 3 | loss: 0.3207553\n",
      "\tspeed: 0.0293s/iter; left time: 453.1005s\n",
      "\titers: 800, epoch: 3 | loss: 0.3273182\n",
      "\tspeed: 0.0274s/iter; left time: 420.7272s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:25.14s\n",
      "Steps: 897 | Train Loss: 0.3450622 Vali Loss: 0.3289139 Test Loss: 0.3600413\n",
      "Validation loss decreased (0.332425 --> 0.328914).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.3568062\n",
      "\tspeed: 0.0838s/iter; left time: 1269.1493s\n",
      "\titers: 200, epoch: 4 | loss: 0.3142166\n",
      "\tspeed: 0.0273s/iter; left time: 411.1455s\n",
      "\titers: 300, epoch: 4 | loss: 0.3390699\n",
      "\tspeed: 0.0272s/iter; left time: 406.5531s\n",
      "\titers: 400, epoch: 4 | loss: 0.3522867\n",
      "\tspeed: 0.0275s/iter; left time: 408.6137s\n",
      "\titers: 500, epoch: 4 | loss: 0.2854418\n",
      "\tspeed: 0.0271s/iter; left time: 399.1297s\n",
      "\titers: 600, epoch: 4 | loss: 0.2733245\n",
      "\tspeed: 0.0277s/iter; left time: 405.0926s\n",
      "\titers: 700, epoch: 4 | loss: 0.4004251\n",
      "\tspeed: 0.0276s/iter; left time: 401.8583s\n",
      "\titers: 800, epoch: 4 | loss: 0.3210239\n",
      "\tspeed: 0.0270s/iter; left time: 389.7442s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:24.74s\n",
      "Steps: 897 | Train Loss: 0.3297988 Vali Loss: 0.3257979 Test Loss: 0.3557080\n",
      "Validation loss decreased (0.328914 --> 0.325798).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.2870131\n",
      "\tspeed: 0.0824s/iter; left time: 1174.3444s\n",
      "\titers: 200, epoch: 5 | loss: 0.2657115\n",
      "\tspeed: 0.0272s/iter; left time: 385.1687s\n",
      "\titers: 300, epoch: 5 | loss: 0.3206735\n",
      "\tspeed: 0.0270s/iter; left time: 379.7514s\n",
      "\titers: 400, epoch: 5 | loss: 0.3673348\n",
      "\tspeed: 0.0273s/iter; left time: 380.9626s\n",
      "\titers: 500, epoch: 5 | loss: 0.3834136\n",
      "\tspeed: 0.0269s/iter; left time: 373.1639s\n",
      "\titers: 600, epoch: 5 | loss: 0.2638610\n",
      "\tspeed: 0.0297s/iter; left time: 408.1026s\n",
      "\titers: 700, epoch: 5 | loss: 0.3175229\n",
      "\tspeed: 0.0295s/iter; left time: 403.4177s\n",
      "\titers: 800, epoch: 5 | loss: 0.4385733\n",
      "\tspeed: 0.0294s/iter; left time: 398.9973s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.47s\n",
      "Steps: 897 | Train Loss: 0.3155551 Vali Loss: 0.3291469 Test Loss: 0.3500108\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2776343\n",
      "\tspeed: 0.0835s/iter; left time: 1115.6263s\n",
      "\titers: 200, epoch: 6 | loss: 0.2304663\n",
      "\tspeed: 0.0274s/iter; left time: 363.2885s\n",
      "\titers: 300, epoch: 6 | loss: 0.2945766\n",
      "\tspeed: 0.0272s/iter; left time: 357.4298s\n",
      "\titers: 400, epoch: 6 | loss: 0.2874967\n",
      "\tspeed: 0.0284s/iter; left time: 371.0329s\n",
      "\titers: 500, epoch: 6 | loss: 0.2905499\n",
      "\tspeed: 0.0294s/iter; left time: 380.3194s\n",
      "\titers: 600, epoch: 6 | loss: 0.2480074\n",
      "\tspeed: 0.0295s/iter; left time: 379.3393s\n",
      "\titers: 700, epoch: 6 | loss: 0.3303727\n",
      "\tspeed: 0.0296s/iter; left time: 377.4946s\n",
      "\titers: 800, epoch: 6 | loss: 0.2673053\n",
      "\tspeed: 0.0293s/iter; left time: 370.5795s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:25.85s\n",
      "Steps: 897 | Train Loss: 0.3024820 Vali Loss: 0.3258515 Test Loss: 0.3552745\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3160546\n",
      "\tspeed: 0.0852s/iter; left time: 1061.2212s\n",
      "\titers: 200, epoch: 7 | loss: 0.2504831\n",
      "\tspeed: 0.0291s/iter; left time: 359.2734s\n",
      "\titers: 300, epoch: 7 | loss: 0.2431525\n",
      "\tspeed: 0.0273s/iter; left time: 334.9753s\n",
      "\titers: 400, epoch: 7 | loss: 0.2496891\n",
      "\tspeed: 0.0272s/iter; left time: 331.1373s\n",
      "\titers: 500, epoch: 7 | loss: 0.2359595\n",
      "\tspeed: 0.0274s/iter; left time: 330.2352s\n",
      "\titers: 600, epoch: 7 | loss: 0.3389344\n",
      "\tspeed: 0.0266s/iter; left time: 318.0007s\n",
      "\titers: 700, epoch: 7 | loss: 0.3281342\n",
      "\tspeed: 0.0287s/iter; left time: 340.6221s\n",
      "\titers: 800, epoch: 7 | loss: 0.2874728\n",
      "\tspeed: 0.0282s/iter; left time: 331.5222s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:25.44s\n",
      "Steps: 897 | Train Loss: 0.2892897 Vali Loss: 0.3291989 Test Loss: 0.3592260\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2462728\n",
      "\tspeed: 0.0824s/iter; left time: 953.1349s\n",
      "\titers: 200, epoch: 8 | loss: 0.3187109\n",
      "\tspeed: 0.0262s/iter; left time: 300.3121s\n",
      "\titers: 300, epoch: 8 | loss: 0.3189376\n",
      "\tspeed: 0.0274s/iter; left time: 311.7847s\n",
      "\titers: 400, epoch: 8 | loss: 0.2753440\n",
      "\tspeed: 0.0262s/iter; left time: 295.1942s\n",
      "\titers: 500, epoch: 8 | loss: 0.2332773\n",
      "\tspeed: 0.0262s/iter; left time: 292.8877s\n",
      "\titers: 600, epoch: 8 | loss: 0.2544248\n",
      "\tspeed: 0.0262s/iter; left time: 289.3123s\n",
      "\titers: 700, epoch: 8 | loss: 0.2583940\n",
      "\tspeed: 0.0269s/iter; left time: 295.2305s\n",
      "\titers: 800, epoch: 8 | loss: 0.2880840\n",
      "\tspeed: 0.0280s/iter; left time: 303.9988s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:24.34s\n",
      "Steps: 897 | Train Loss: 0.2782341 Vali Loss: 0.3329138 Test Loss: 0.3625235\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3086308\n",
      "\tspeed: 0.0824s/iter; left time: 878.8962s\n",
      "\titers: 200, epoch: 9 | loss: 0.2864911\n",
      "\tspeed: 0.0291s/iter; left time: 307.2663s\n",
      "\titers: 300, epoch: 9 | loss: 0.3052218\n",
      "\tspeed: 0.0299s/iter; left time: 312.7335s\n",
      "\titers: 400, epoch: 9 | loss: 0.2707603\n",
      "\tspeed: 0.0296s/iter; left time: 307.3138s\n",
      "\titers: 500, epoch: 9 | loss: 0.2511828\n",
      "\tspeed: 0.0288s/iter; left time: 295.1801s\n",
      "\titers: 600, epoch: 9 | loss: 0.2551185\n",
      "\tspeed: 0.0294s/iter; left time: 298.7348s\n",
      "\titers: 700, epoch: 9 | loss: 0.3016778\n",
      "\tspeed: 0.0294s/iter; left time: 296.2382s\n",
      "\titers: 800, epoch: 9 | loss: 0.2272486\n",
      "\tspeed: 0.0282s/iter; left time: 280.6459s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:26.07s\n",
      "Steps: 897 | Train Loss: 0.2694112 Vali Loss: 0.3355160 Test Loss: 0.3589134\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_96_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.35570818185806274, rmse:0.5964127779006958, mae:0.3791942000389099, rse:0.5460783243179321\n",
      "Original data scale mse:2601725.75, rmse:1612.986572265625, mae:1082.8504638671875, rse:0.11351245641708374\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_168_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='standard', if_relu=False, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_168_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.8728520\n",
      "\tspeed: 0.0526s/iter; left time: 935.2891s\n",
      "\titers: 200, epoch: 1 | loss: 0.7187911\n",
      "\tspeed: 0.0285s/iter; left time: 503.1668s\n",
      "\titers: 300, epoch: 1 | loss: 0.6924965\n",
      "\tspeed: 0.0299s/iter; left time: 525.8757s\n",
      "\titers: 400, epoch: 1 | loss: 0.6007586\n",
      "\tspeed: 0.0295s/iter; left time: 516.4295s\n",
      "\titers: 500, epoch: 1 | loss: 0.6248522\n",
      "\tspeed: 0.0299s/iter; left time: 519.1737s\n",
      "\titers: 600, epoch: 1 | loss: 0.5164961\n",
      "\tspeed: 0.0300s/iter; left time: 518.6598s\n",
      "\titers: 700, epoch: 1 | loss: 0.5162773\n",
      "\tspeed: 0.0296s/iter; left time: 508.6952s\n",
      "\titers: 800, epoch: 1 | loss: 0.4912508\n",
      "\tspeed: 0.0284s/iter; left time: 484.3811s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:26.24s\n",
      "Steps: 894 | Train Loss: 0.6730894 Vali Loss: 0.4427028 Test Loss: 0.4612014\n",
      "Validation loss decreased (inf --> 0.442703).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4378038\n",
      "\tspeed: 0.0823s/iter; left time: 1390.5955s\n",
      "\titers: 200, epoch: 2 | loss: 0.4630416\n",
      "\tspeed: 0.0280s/iter; left time: 470.3383s\n",
      "\titers: 300, epoch: 2 | loss: 0.4398068\n",
      "\tspeed: 0.0292s/iter; left time: 486.4926s\n",
      "\titers: 400, epoch: 2 | loss: 0.4898156\n",
      "\tspeed: 0.0278s/iter; left time: 461.9044s\n",
      "\titers: 500, epoch: 2 | loss: 0.4362145\n",
      "\tspeed: 0.0275s/iter; left time: 454.0559s\n",
      "\titers: 600, epoch: 2 | loss: 0.4669081\n",
      "\tspeed: 0.0282s/iter; left time: 462.1400s\n",
      "\titers: 700, epoch: 2 | loss: 0.4289129\n",
      "\tspeed: 0.0294s/iter; left time: 479.5605s\n",
      "\titers: 800, epoch: 2 | loss: 0.4223727\n",
      "\tspeed: 0.0290s/iter; left time: 468.8211s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:25.68s\n",
      "Steps: 894 | Train Loss: 0.4141342 Vali Loss: 0.3606954 Test Loss: 0.3860567\n",
      "Validation loss decreased (0.442703 --> 0.360695).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.4347907\n",
      "\tspeed: 0.0836s/iter; left time: 1337.7170s\n",
      "\titers: 200, epoch: 3 | loss: 0.4064242\n",
      "\tspeed: 0.0277s/iter; left time: 440.9957s\n",
      "\titers: 300, epoch: 3 | loss: 0.3620875\n",
      "\tspeed: 0.0297s/iter; left time: 468.9233s\n",
      "\titers: 400, epoch: 3 | loss: 0.4197786\n",
      "\tspeed: 0.0301s/iter; left time: 472.9845s\n",
      "\titers: 500, epoch: 3 | loss: 0.3200060\n",
      "\tspeed: 0.0294s/iter; left time: 458.5041s\n",
      "\titers: 600, epoch: 3 | loss: 0.4206914\n",
      "\tspeed: 0.0262s/iter; left time: 405.1889s\n",
      "\titers: 700, epoch: 3 | loss: 0.4146284\n",
      "\tspeed: 0.0265s/iter; left time: 408.0398s\n",
      "\titers: 800, epoch: 3 | loss: 0.4074716\n",
      "\tspeed: 0.0269s/iter; left time: 412.0723s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:25.29s\n",
      "Steps: 894 | Train Loss: 0.3726581 Vali Loss: 0.3617584 Test Loss: 0.3911827\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.4119256\n",
      "\tspeed: 0.0840s/iter; left time: 1268.6755s\n",
      "\titers: 200, epoch: 4 | loss: 0.3394215\n",
      "\tspeed: 0.0279s/iter; left time: 419.0550s\n",
      "\titers: 300, epoch: 4 | loss: 0.3565576\n",
      "\tspeed: 0.0279s/iter; left time: 416.3833s\n",
      "\titers: 400, epoch: 4 | loss: 0.3463457\n",
      "\tspeed: 0.0275s/iter; left time: 407.1237s\n",
      "\titers: 500, epoch: 4 | loss: 0.3495067\n",
      "\tspeed: 0.0293s/iter; left time: 430.9459s\n",
      "\titers: 600, epoch: 4 | loss: 0.3116083\n",
      "\tspeed: 0.0297s/iter; left time: 434.0501s\n",
      "\titers: 700, epoch: 4 | loss: 0.3545830\n",
      "\tspeed: 0.0291s/iter; left time: 421.5716s\n",
      "\titers: 800, epoch: 4 | loss: 0.3039629\n",
      "\tspeed: 0.0292s/iter; left time: 420.8275s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:25.98s\n",
      "Steps: 894 | Train Loss: 0.3577604 Vali Loss: 0.3623317 Test Loss: 0.3928007\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3157778\n",
      "\tspeed: 0.0830s/iter; left time: 1178.8921s\n",
      "\titers: 200, epoch: 5 | loss: 0.3462746\n",
      "\tspeed: 0.0275s/iter; left time: 388.5213s\n",
      "\titers: 300, epoch: 5 | loss: 0.3840885\n",
      "\tspeed: 0.0277s/iter; left time: 388.4300s\n",
      "\titers: 400, epoch: 5 | loss: 0.3827516\n",
      "\tspeed: 0.0281s/iter; left time: 390.2369s\n",
      "\titers: 500, epoch: 5 | loss: 0.3510361\n",
      "\tspeed: 0.0279s/iter; left time: 385.3639s\n",
      "\titers: 600, epoch: 5 | loss: 0.3494957\n",
      "\tspeed: 0.0278s/iter; left time: 380.6212s\n",
      "\titers: 700, epoch: 5 | loss: 0.3269484\n",
      "\tspeed: 0.0276s/iter; left time: 375.2932s\n",
      "\titers: 800, epoch: 5 | loss: 0.3627323\n",
      "\tspeed: 0.0279s/iter; left time: 376.5928s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.11s\n",
      "Steps: 894 | Train Loss: 0.3412680 Vali Loss: 0.3581115 Test Loss: 0.3935860\n",
      "Validation loss decreased (0.360695 --> 0.358112).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3635050\n",
      "\tspeed: 0.0820s/iter; left time: 1092.0698s\n",
      "\titers: 200, epoch: 6 | loss: 0.2798483\n",
      "\tspeed: 0.0281s/iter; left time: 371.2022s\n",
      "\titers: 300, epoch: 6 | loss: 0.3586640\n",
      "\tspeed: 0.0278s/iter; left time: 364.8181s\n",
      "\titers: 400, epoch: 6 | loss: 0.3425811\n",
      "\tspeed: 0.0277s/iter; left time: 359.8157s\n",
      "\titers: 500, epoch: 6 | loss: 0.3351663\n",
      "\tspeed: 0.0276s/iter; left time: 356.2726s\n",
      "\titers: 600, epoch: 6 | loss: 0.3437107\n",
      "\tspeed: 0.0277s/iter; left time: 354.9713s\n",
      "\titers: 700, epoch: 6 | loss: 0.3832268\n",
      "\tspeed: 0.0278s/iter; left time: 353.3972s\n",
      "\titers: 800, epoch: 6 | loss: 0.2959476\n",
      "\tspeed: 0.0272s/iter; left time: 342.5964s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:25.27s\n",
      "Steps: 894 | Train Loss: 0.3243792 Vali Loss: 0.3656214 Test Loss: 0.3891410\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3329647\n",
      "\tspeed: 0.0863s/iter; left time: 1072.0116s\n",
      "\titers: 200, epoch: 7 | loss: 0.2545021\n",
      "\tspeed: 0.0294s/iter; left time: 361.9677s\n",
      "\titers: 300, epoch: 7 | loss: 0.3085818\n",
      "\tspeed: 0.0295s/iter; left time: 360.5677s\n",
      "\titers: 400, epoch: 7 | loss: 0.3307681\n",
      "\tspeed: 0.0295s/iter; left time: 357.4600s\n",
      "\titers: 500, epoch: 7 | loss: 0.3522631\n",
      "\tspeed: 0.0293s/iter; left time: 352.5746s\n",
      "\titers: 600, epoch: 7 | loss: 0.3183246\n",
      "\tspeed: 0.0278s/iter; left time: 331.3201s\n",
      "\titers: 700, epoch: 7 | loss: 0.2923101\n",
      "\tspeed: 0.0277s/iter; left time: 327.5592s\n",
      "\titers: 800, epoch: 7 | loss: 0.3129258\n",
      "\tspeed: 0.0278s/iter; left time: 326.0368s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:25.97s\n",
      "Steps: 894 | Train Loss: 0.3098359 Vali Loss: 0.3665877 Test Loss: 0.3990648\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2857350\n",
      "\tspeed: 0.0821s/iter; left time: 946.4473s\n",
      "\titers: 200, epoch: 8 | loss: 0.2498169\n",
      "\tspeed: 0.0279s/iter; left time: 318.9898s\n",
      "\titers: 300, epoch: 8 | loss: 0.3107233\n",
      "\tspeed: 0.0281s/iter; left time: 317.6542s\n",
      "\titers: 400, epoch: 8 | loss: 0.2612979\n",
      "\tspeed: 0.0279s/iter; left time: 313.3668s\n",
      "\titers: 500, epoch: 8 | loss: 0.2816587\n",
      "\tspeed: 0.0278s/iter; left time: 309.6661s\n",
      "\titers: 600, epoch: 8 | loss: 0.2565982\n",
      "\tspeed: 0.0278s/iter; left time: 306.6596s\n",
      "\titers: 700, epoch: 8 | loss: 0.3396057\n",
      "\tspeed: 0.0277s/iter; left time: 302.9633s\n",
      "\titers: 800, epoch: 8 | loss: 0.3042001\n",
      "\tspeed: 0.0278s/iter; left time: 300.9777s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:25.15s\n",
      "Steps: 894 | Train Loss: 0.2978945 Vali Loss: 0.3629001 Test Loss: 0.3944166\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.2816455\n",
      "\tspeed: 0.0824s/iter; left time: 875.3916s\n",
      "\titers: 200, epoch: 9 | loss: 0.3172236\n",
      "\tspeed: 0.0278s/iter; left time: 293.0521s\n",
      "\titers: 300, epoch: 9 | loss: 0.3236761\n",
      "\tspeed: 0.0280s/iter; left time: 291.6463s\n",
      "\titers: 400, epoch: 9 | loss: 0.2958028\n",
      "\tspeed: 0.0285s/iter; left time: 294.2189s\n",
      "\titers: 500, epoch: 9 | loss: 0.2918476\n",
      "\tspeed: 0.0282s/iter; left time: 288.7696s\n",
      "\titers: 600, epoch: 9 | loss: 0.2721641\n",
      "\tspeed: 0.0296s/iter; left time: 299.5550s\n",
      "\titers: 700, epoch: 9 | loss: 0.2667270\n",
      "\tspeed: 0.0295s/iter; left time: 295.6637s\n",
      "\titers: 800, epoch: 9 | loss: 0.3177175\n",
      "\tspeed: 0.0294s/iter; left time: 291.4699s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:25.59s\n",
      "Steps: 894 | Train Loss: 0.2870459 Vali Loss: 0.3591893 Test Loss: 0.3954564\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.2636019\n",
      "\tspeed: 0.0812s/iter; left time: 790.3450s\n",
      "\titers: 200, epoch: 10 | loss: 0.2680298\n",
      "\tspeed: 0.0300s/iter; left time: 289.1698s\n",
      "\titers: 300, epoch: 10 | loss: 0.2444529\n",
      "\tspeed: 0.0286s/iter; left time: 272.5101s\n",
      "\titers: 400, epoch: 10 | loss: 0.2860496\n",
      "\tspeed: 0.0278s/iter; left time: 262.3085s\n",
      "\titers: 500, epoch: 10 | loss: 0.3106808\n",
      "\tspeed: 0.0279s/iter; left time: 260.8840s\n",
      "\titers: 600, epoch: 10 | loss: 0.2942710\n",
      "\tspeed: 0.0278s/iter; left time: 256.9464s\n",
      "\titers: 700, epoch: 10 | loss: 0.2600013\n",
      "\tspeed: 0.0281s/iter; left time: 256.9198s\n",
      "\titers: 800, epoch: 10 | loss: 0.2603240\n",
      "\tspeed: 0.0280s/iter; left time: 253.0442s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:25.51s\n",
      "Steps: 894 | Train Loss: 0.2778562 Vali Loss: 0.3633821 Test Loss: 0.4017079\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_168_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.39358630776405334, rmse:0.6273645758628845, mae:0.40511223673820496, rse:0.5745890736579895\n",
      "Original data scale mse:3049026.75, rmse:1746.146240234375, mae:1175.301025390625, rse:0.12299883365631104\n"
     ]
    }
   ],
   "source": [
    "# Dynamic + default variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "seq_len = \"336\"\n",
    "lr = \"0.0001\"\n",
    "model = \"PatchTST\"\n",
    "itr = 1  \n",
    "n_heads = \"16\"\n",
    "d_model = \"128\"\n",
    "d_ff = \"256\"\n",
    "dropout = \"0.2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# New log file path\n",
    "log_file_path = f\"{log_dir}/{model}_{country}.log\"\n",
    "\n",
    "patchtst_results_scaled, patchtst_results_unscaled = [], []\n",
    "\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    for loss in losses:\n",
    "        statement_1 = f\"\\n=== Starting experiments for loss function: {loss} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)  # Print to notebook\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_scaler_choice_for_{country}\"\n",
    "\n",
    "            # Command arguments\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --random_seed 2021 \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers 3 \\\n",
    "              --factor 1 \\\n",
    "              --enc_in 3 \\\n",
    "              --dec_in 3 \\\n",
    "              --c_out 3 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --patch_len 32 \\\n",
    "              --stride 16 \\\n",
    "              --overlapping_windows \\\n",
    "              --inverse \\\n",
    "              --scaler_type standard \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            # Wait for the process to complete\n",
    "            process.wait()\n",
    "\n",
    "            # Delete the checkpoints folder and all its contents\n",
    "            shutil.rmtree('./checkpoints' )\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics_scaled = extract_metrics_from_output(output, itr)\n",
    "            iteration_metrics_unscaled = extract_metrics_from_output(output, itr, if_scaled=False)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, (scaled_metrics, unscaled_metrics) in enumerate(zip(iteration_metrics_scaled, iteration_metrics_unscaled), start=1):\n",
    "                log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}, iteration={iteration}:\\n\")\n",
    "                log_file.write(f\"Scaled Metrics - MSE: {scaled_metrics[0]}, RMSE: {scaled_metrics[1]}, MAE: {scaled_metrics[2]}, RSE: {scaled_metrics[3]}\\n\")\n",
    "                log_file.write(f\"Unscaled Metrics - MSE: {unscaled_metrics[0]}, RMSE: {unscaled_metrics[1]}, MAE: {unscaled_metrics[2]}, RSE: {unscaled_metrics[3]}\\n\")\n",
    "\n",
    "                # Append the results to the informer_results lists\n",
    "                metrics_data = [(patchtst_results_scaled, scaled_metrics), (patchtst_results_unscaled, unscaled_metrics)]\n",
    "\n",
    "                for result_list, metrics in metrics_data:\n",
    "                    result_list.append({\n",
    "                        'Loss_function': loss,\n",
    "                        'Pred_len': pred_len,\n",
    "                        'Iteration': iteration,\n",
    "                        'MSE': metrics[0],\n",
    "                        'RMSE': metrics[1],\n",
    "                        'MAE': metrics[2],\n",
    "                        'RSE': metrics[3]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>0.2015</td>\n",
       "      <td>0.4489</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.4111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.5461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.3936</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>0.4051</td>\n",
       "      <td>0.5746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MSE    RMSE     MAE     RSE\n",
       "Loss_function Iteration Pred_len                                \n",
       "MSE           1         24        0.2015  0.4489  0.2645  0.4111\n",
       "                        96        0.3557  0.5964  0.3792  0.5461\n",
       "                        168       0.3936  0.6274  0.4051  0.5746"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir = './results/scaler_choice'\n",
    "csv_name_scaled = 'patchtst_scaler_results_scaled_IT_default.csv'\n",
    "csv_name_unscaled = 'patchtst_scaler_results_unscaled_IT_default.csv'\n",
    "\n",
    "\n",
    "# Convert the results into a DataFrame and save as CSV\n",
    "patchtst_df_scaled = convert_results_into_df(patchtst_results_scaled, path_dir, csv_name_scaled)\n",
    "patchtst_df_unscaled = convert_results_into_df(patchtst_results_unscaled, path_dir, csv_name_unscaled)\n",
    "patchtst_df_scaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>1206662.25</td>\n",
       "      <td>1098.4818</td>\n",
       "      <td>719.5243</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2601725.75</td>\n",
       "      <td>1612.9866</td>\n",
       "      <td>1082.8505</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3049026.75</td>\n",
       "      <td>1746.1462</td>\n",
       "      <td>1175.3010</td>\n",
       "      <td>0.1230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MSE       RMSE        MAE     RSE\n",
       "Loss_function Iteration Pred_len                                          \n",
       "MSE           1         24        1206662.25  1098.4818   719.5243  0.0772\n",
       "                        96        2601725.75  1612.9866  1082.8505  0.1135\n",
       "                        168       3049026.75  1746.1462  1175.3010  0.1230"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchtst_df_unscaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"results_loss_scaled\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "\n",
    "# Rename folder\n",
    "os.rename(\"results_loss_unscaled\", 'standard_unscaled_IT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MinMax Scaler Informer\n",
    "\n",
    "We can use now \"ReLU\" activation function due to MinMax Scaler.\n",
    "\n",
    "With BS 1036, ReLU - results are bad. (as twice as bad as with 32!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to files and data\n",
    "data_path = os.getcwd() + \"/datasets/\"\n",
    "\n",
    "script_path = \"./PatchTST-main/PatchTST_supervised/run_longExp.py\"\n",
    "\n",
    "# Arguments that will be used also for file names\n",
    "model = \"Informer\"\n",
    "dataset = 'IT_data.csv'\n",
    "losses = [\"MSE\"]\n",
    "country = dataset[:2]\n",
    "\n",
    "log_dir = f\"logs/scaler_choice/min_max\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file_path = f\"{log_dir}/{model}_{country}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for loss function: MSE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_24_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=24, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_24_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 29017\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0957632\n",
      "\tspeed: 0.0809s/iter; left time: 1457.0052s\n",
      "\titers: 200, epoch: 1 | loss: 0.0817575\n",
      "\tspeed: 0.0487s/iter; left time: 873.0459s\n",
      "\titers: 300, epoch: 1 | loss: 0.0646085\n",
      "\tspeed: 0.0488s/iter; left time: 869.1343s\n",
      "\titers: 400, epoch: 1 | loss: 0.0612782\n",
      "\tspeed: 0.0486s/iter; left time: 860.4880s\n",
      "\titers: 500, epoch: 1 | loss: 0.0484514\n",
      "\tspeed: 0.0486s/iter; left time: 857.0282s\n",
      "\titers: 600, epoch: 1 | loss: 0.0481590\n",
      "\tspeed: 0.0486s/iter; left time: 850.8175s\n",
      "\titers: 700, epoch: 1 | loss: 0.0343953\n",
      "\tspeed: 0.0486s/iter; left time: 846.9034s\n",
      "\titers: 800, epoch: 1 | loss: 0.0335397\n",
      "\tspeed: 0.0484s/iter; left time: 838.4272s\n",
      "\titers: 900, epoch: 1 | loss: 0.0383372\n",
      "\tspeed: 0.0487s/iter; left time: 838.5035s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:45.02s\n",
      "Steps: 906 | Train Loss: 0.0605991 Vali Loss: 0.0236095 Test Loss: 0.0250904\n",
      "Validation loss decreased (inf --> 0.023610).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0261842\n",
      "\tspeed: 0.1050s/iter; left time: 1797.5936s\n",
      "\titers: 200, epoch: 2 | loss: 0.0226637\n",
      "\tspeed: 0.0484s/iter; left time: 823.9906s\n",
      "\titers: 300, epoch: 2 | loss: 0.0147367\n",
      "\tspeed: 0.0485s/iter; left time: 820.7596s\n",
      "\titers: 400, epoch: 2 | loss: 0.0144873\n",
      "\tspeed: 0.0485s/iter; left time: 815.7621s\n",
      "\titers: 500, epoch: 2 | loss: 0.0160467\n",
      "\tspeed: 0.0485s/iter; left time: 810.6326s\n",
      "\titers: 600, epoch: 2 | loss: 0.0193212\n",
      "\tspeed: 0.0485s/iter; left time: 805.6156s\n",
      "\titers: 700, epoch: 2 | loss: 0.0140467\n",
      "\tspeed: 0.0485s/iter; left time: 800.2748s\n",
      "\titers: 800, epoch: 2 | loss: 0.0140468\n",
      "\tspeed: 0.0485s/iter; left time: 796.5617s\n",
      "\titers: 900, epoch: 2 | loss: 0.0126574\n",
      "\tspeed: 0.0484s/iter; left time: 790.0546s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:44.24s\n",
      "Steps: 906 | Train Loss: 0.0177892 Vali Loss: 0.0124438 Test Loss: 0.0138910\n",
      "Validation loss decreased (0.023610 --> 0.012444).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0111033\n",
      "\tspeed: 0.1077s/iter; left time: 1745.3513s\n",
      "\titers: 200, epoch: 3 | loss: 0.0115581\n",
      "\tspeed: 0.0483s/iter; left time: 777.6412s\n",
      "\titers: 300, epoch: 3 | loss: 0.0118196\n",
      "\tspeed: 0.0485s/iter; left time: 776.5214s\n",
      "\titers: 400, epoch: 3 | loss: 0.0087795\n",
      "\tspeed: 0.0485s/iter; left time: 771.5096s\n",
      "\titers: 500, epoch: 3 | loss: 0.0111346\n",
      "\tspeed: 0.0486s/iter; left time: 767.8246s\n",
      "\titers: 600, epoch: 3 | loss: 0.0131450\n",
      "\tspeed: 0.0485s/iter; left time: 761.3012s\n",
      "\titers: 700, epoch: 3 | loss: 0.0124399\n",
      "\tspeed: 0.0486s/iter; left time: 757.8940s\n",
      "\titers: 800, epoch: 3 | loss: 0.0134451\n",
      "\tspeed: 0.0485s/iter; left time: 752.4053s\n",
      "\titers: 900, epoch: 3 | loss: 0.0096626\n",
      "\tspeed: 0.0485s/iter; left time: 746.7146s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:44.18s\n",
      "Steps: 906 | Train Loss: 0.0118055 Vali Loss: 0.0124286 Test Loss: 0.0138577\n",
      "Validation loss decreased (0.012444 --> 0.012429).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0127130\n",
      "\tspeed: 0.1055s/iter; left time: 1614.2232s\n",
      "\titers: 200, epoch: 4 | loss: 0.0096245\n",
      "\tspeed: 0.0489s/iter; left time: 744.0887s\n",
      "\titers: 300, epoch: 4 | loss: 0.0105408\n",
      "\tspeed: 0.0489s/iter; left time: 739.2215s\n",
      "\titers: 400, epoch: 4 | loss: 0.0131704\n",
      "\tspeed: 0.0488s/iter; left time: 732.1610s\n",
      "\titers: 500, epoch: 4 | loss: 0.0106625\n",
      "\tspeed: 0.0490s/iter; left time: 729.8353s\n",
      "\titers: 600, epoch: 4 | loss: 0.0090194\n",
      "\tspeed: 0.0491s/iter; left time: 726.0906s\n",
      "\titers: 700, epoch: 4 | loss: 0.0096538\n",
      "\tspeed: 0.0481s/iter; left time: 707.3440s\n",
      "\titers: 800, epoch: 4 | loss: 0.0114234\n",
      "\tspeed: 0.0484s/iter; left time: 706.4750s\n",
      "\titers: 900, epoch: 4 | loss: 0.0113872\n",
      "\tspeed: 0.0484s/iter; left time: 701.8622s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:44.42s\n",
      "Steps: 906 | Train Loss: 0.0106975 Vali Loss: 0.0112019 Test Loss: 0.0123353\n",
      "Validation loss decreased (0.012429 --> 0.011202).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0091417\n",
      "\tspeed: 0.1053s/iter; left time: 1516.3313s\n",
      "\titers: 200, epoch: 5 | loss: 0.0132211\n",
      "\tspeed: 0.0487s/iter; left time: 695.8080s\n",
      "\titers: 300, epoch: 5 | loss: 0.0108337\n",
      "\tspeed: 0.0487s/iter; left time: 691.6340s\n",
      "\titers: 400, epoch: 5 | loss: 0.0088053\n",
      "\tspeed: 0.0488s/iter; left time: 687.3494s\n",
      "\titers: 500, epoch: 5 | loss: 0.0094521\n",
      "\tspeed: 0.0488s/iter; left time: 683.1385s\n",
      "\titers: 600, epoch: 5 | loss: 0.0083584\n",
      "\tspeed: 0.0488s/iter; left time: 678.6959s\n",
      "\titers: 700, epoch: 5 | loss: 0.0135315\n",
      "\tspeed: 0.0488s/iter; left time: 672.9443s\n",
      "\titers: 800, epoch: 5 | loss: 0.0068324\n",
      "\tspeed: 0.0487s/iter; left time: 666.5997s\n",
      "\titers: 900, epoch: 5 | loss: 0.0104479\n",
      "\tspeed: 0.0486s/iter; left time: 660.3837s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:44.42s\n",
      "Steps: 906 | Train Loss: 0.0097477 Vali Loss: 0.0103169 Test Loss: 0.0118225\n",
      "Validation loss decreased (0.011202 --> 0.010317).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0084765\n",
      "\tspeed: 0.1049s/iter; left time: 1415.5304s\n",
      "\titers: 200, epoch: 6 | loss: 0.0092567\n",
      "\tspeed: 0.0486s/iter; left time: 650.3530s\n",
      "\titers: 300, epoch: 6 | loss: 0.0127090\n",
      "\tspeed: 0.0485s/iter; left time: 644.8357s\n",
      "\titers: 400, epoch: 6 | loss: 0.0078653\n",
      "\tspeed: 0.0486s/iter; left time: 641.3219s\n",
      "\titers: 500, epoch: 6 | loss: 0.0061968\n",
      "\tspeed: 0.0486s/iter; left time: 635.5992s\n",
      "\titers: 600, epoch: 6 | loss: 0.0094398\n",
      "\tspeed: 0.0485s/iter; left time: 630.6005s\n",
      "\titers: 700, epoch: 6 | loss: 0.0095741\n",
      "\tspeed: 0.0486s/iter; left time: 626.1683s\n",
      "\titers: 800, epoch: 6 | loss: 0.0084911\n",
      "\tspeed: 0.0485s/iter; left time: 620.7695s\n",
      "\titers: 900, epoch: 6 | loss: 0.0074697\n",
      "\tspeed: 0.0484s/iter; left time: 613.9940s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:44.26s\n",
      "Steps: 906 | Train Loss: 0.0089266 Vali Loss: 0.0098866 Test Loss: 0.0116170\n",
      "Validation loss decreased (0.010317 --> 0.009887).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0078370\n",
      "\tspeed: 0.1046s/iter; left time: 1316.9476s\n",
      "\titers: 200, epoch: 7 | loss: 0.0097201\n",
      "\tspeed: 0.0482s/iter; left time: 602.0353s\n",
      "\titers: 300, epoch: 7 | loss: 0.0068884\n",
      "\tspeed: 0.0483s/iter; left time: 598.2411s\n",
      "\titers: 400, epoch: 7 | loss: 0.0094338\n",
      "\tspeed: 0.0483s/iter; left time: 592.7989s\n",
      "\titers: 500, epoch: 7 | loss: 0.0086338\n",
      "\tspeed: 0.0483s/iter; left time: 589.0812s\n",
      "\titers: 600, epoch: 7 | loss: 0.0053967\n",
      "\tspeed: 0.0482s/iter; left time: 582.7840s\n",
      "\titers: 700, epoch: 7 | loss: 0.0085320\n",
      "\tspeed: 0.0483s/iter; left time: 578.3621s\n",
      "\titers: 800, epoch: 7 | loss: 0.0095296\n",
      "\tspeed: 0.0483s/iter; left time: 573.9164s\n",
      "\titers: 900, epoch: 7 | loss: 0.0069363\n",
      "\tspeed: 0.0482s/iter; left time: 567.8871s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:44.01s\n",
      "Steps: 906 | Train Loss: 0.0083970 Vali Loss: 0.0095849 Test Loss: 0.0113854\n",
      "Validation loss decreased (0.009887 --> 0.009585).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0068547\n",
      "\tspeed: 0.1046s/iter; left time: 1221.3498s\n",
      "\titers: 200, epoch: 8 | loss: 0.0062880\n",
      "\tspeed: 0.0488s/iter; left time: 565.2733s\n",
      "\titers: 300, epoch: 8 | loss: 0.0063780\n",
      "\tspeed: 0.0488s/iter; left time: 559.9106s\n",
      "\titers: 400, epoch: 8 | loss: 0.0076308\n",
      "\tspeed: 0.0489s/iter; left time: 556.0048s\n",
      "\titers: 500, epoch: 8 | loss: 0.0084253\n",
      "\tspeed: 0.0487s/iter; left time: 549.4192s\n",
      "\titers: 600, epoch: 8 | loss: 0.0079748\n",
      "\tspeed: 0.0488s/iter; left time: 545.3736s\n",
      "\titers: 700, epoch: 8 | loss: 0.0071999\n",
      "\tspeed: 0.0487s/iter; left time: 539.6797s\n",
      "\titers: 800, epoch: 8 | loss: 0.0082351\n",
      "\tspeed: 0.0488s/iter; left time: 535.7184s\n",
      "\titers: 900, epoch: 8 | loss: 0.0076345\n",
      "\tspeed: 0.0487s/iter; left time: 529.8313s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:44.48s\n",
      "Steps: 906 | Train Loss: 0.0078698 Vali Loss: 0.0095649 Test Loss: 0.0113864\n",
      "Validation loss decreased (0.009585 --> 0.009565).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0054177\n",
      "\tspeed: 0.1048s/iter; left time: 1128.5025s\n",
      "\titers: 200, epoch: 9 | loss: 0.0087093\n",
      "\tspeed: 0.0481s/iter; left time: 513.4635s\n",
      "\titers: 300, epoch: 9 | loss: 0.0088883\n",
      "\tspeed: 0.0486s/iter; left time: 513.5006s\n",
      "\titers: 400, epoch: 9 | loss: 0.0067530\n",
      "\tspeed: 0.0487s/iter; left time: 509.9381s\n",
      "\titers: 500, epoch: 9 | loss: 0.0068062\n",
      "\tspeed: 0.0487s/iter; left time: 505.3949s\n",
      "\titers: 600, epoch: 9 | loss: 0.0070813\n",
      "\tspeed: 0.0487s/iter; left time: 500.0157s\n",
      "\titers: 700, epoch: 9 | loss: 0.0077914\n",
      "\tspeed: 0.0487s/iter; left time: 495.3033s\n",
      "\titers: 800, epoch: 9 | loss: 0.0067969\n",
      "\tspeed: 0.0487s/iter; left time: 490.3120s\n",
      "\titers: 900, epoch: 9 | loss: 0.0065149\n",
      "\tspeed: 0.0486s/iter; left time: 484.8901s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:44.31s\n",
      "Steps: 906 | Train Loss: 0.0073633 Vali Loss: 0.0106109 Test Loss: 0.0119681\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0078787\n",
      "\tspeed: 0.1008s/iter; left time: 994.6874s\n",
      "\titers: 200, epoch: 10 | loss: 0.0073015\n",
      "\tspeed: 0.0484s/iter; left time: 472.4272s\n",
      "\titers: 300, epoch: 10 | loss: 0.0071232\n",
      "\tspeed: 0.0482s/iter; left time: 466.2905s\n",
      "\titers: 400, epoch: 10 | loss: 0.0069564\n",
      "\tspeed: 0.0483s/iter; left time: 462.3624s\n",
      "\titers: 500, epoch: 10 | loss: 0.0055014\n",
      "\tspeed: 0.0484s/iter; left time: 458.2573s\n",
      "\titers: 600, epoch: 10 | loss: 0.0067581\n",
      "\tspeed: 0.0483s/iter; left time: 452.2664s\n",
      "\titers: 700, epoch: 10 | loss: 0.0056662\n",
      "\tspeed: 0.0483s/iter; left time: 447.3826s\n",
      "\titers: 800, epoch: 10 | loss: 0.0064611\n",
      "\tspeed: 0.0482s/iter; left time: 441.9085s\n",
      "\titers: 900, epoch: 10 | loss: 0.0061233\n",
      "\tspeed: 0.0481s/iter; left time: 436.2460s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:43.98s\n",
      "Steps: 906 | Train Loss: 0.0069422 Vali Loss: 0.0097545 Test Loss: 0.0121330\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0067805\n",
      "\tspeed: 0.1016s/iter; left time: 910.8243s\n",
      "\titers: 200, epoch: 11 | loss: 0.0054183\n",
      "\tspeed: 0.0486s/iter; left time: 430.7755s\n",
      "\titers: 300, epoch: 11 | loss: 0.0069844\n",
      "\tspeed: 0.0484s/iter; left time: 423.6183s\n",
      "\titers: 400, epoch: 11 | loss: 0.0054563\n",
      "\tspeed: 0.0486s/iter; left time: 421.2761s\n",
      "\titers: 500, epoch: 11 | loss: 0.0078667\n",
      "\tspeed: 0.0486s/iter; left time: 415.9784s\n",
      "\titers: 600, epoch: 11 | loss: 0.0084661\n",
      "\tspeed: 0.0484s/iter; left time: 409.6942s\n",
      "\titers: 700, epoch: 11 | loss: 0.0064551\n",
      "\tspeed: 0.0484s/iter; left time: 404.4055s\n",
      "\titers: 800, epoch: 11 | loss: 0.0078192\n",
      "\tspeed: 0.0486s/iter; left time: 401.8850s\n",
      "\titers: 900, epoch: 11 | loss: 0.0064657\n",
      "\tspeed: 0.0485s/iter; left time: 396.2133s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:44.26s\n",
      "Steps: 906 | Train Loss: 0.0065644 Vali Loss: 0.0102390 Test Loss: 0.0123357\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0049833\n",
      "\tspeed: 0.1017s/iter; left time: 819.2750s\n",
      "\titers: 200, epoch: 12 | loss: 0.0075870\n",
      "\tspeed: 0.0486s/iter; left time: 386.3999s\n",
      "\titers: 300, epoch: 12 | loss: 0.0057872\n",
      "\tspeed: 0.0485s/iter; left time: 380.6715s\n",
      "\titers: 400, epoch: 12 | loss: 0.0063423\n",
      "\tspeed: 0.0483s/iter; left time: 374.7971s\n",
      "\titers: 500, epoch: 12 | loss: 0.0083102\n",
      "\tspeed: 0.0484s/iter; left time: 370.2066s\n",
      "\titers: 600, epoch: 12 | loss: 0.0066547\n",
      "\tspeed: 0.0484s/iter; left time: 365.8611s\n",
      "\titers: 700, epoch: 12 | loss: 0.0055785\n",
      "\tspeed: 0.0484s/iter; left time: 361.1328s\n",
      "\titers: 800, epoch: 12 | loss: 0.0077896\n",
      "\tspeed: 0.0484s/iter; left time: 356.2063s\n",
      "\titers: 900, epoch: 12 | loss: 0.0061444\n",
      "\tspeed: 0.0484s/iter; left time: 350.8553s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:44.18s\n",
      "Steps: 906 | Train Loss: 0.0062224 Vali Loss: 0.0102077 Test Loss: 0.0127671\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0060006\n",
      "\tspeed: 0.1011s/iter; left time: 722.4409s\n",
      "\titers: 200, epoch: 13 | loss: 0.0065340\n",
      "\tspeed: 0.0484s/iter; left time: 340.9649s\n",
      "\titers: 300, epoch: 13 | loss: 0.0052825\n",
      "\tspeed: 0.0485s/iter; left time: 336.8735s\n",
      "\titers: 400, epoch: 13 | loss: 0.0075953\n",
      "\tspeed: 0.0484s/iter; left time: 331.1754s\n",
      "\titers: 500, epoch: 13 | loss: 0.0046650\n",
      "\tspeed: 0.0482s/iter; left time: 325.0557s\n",
      "\titers: 600, epoch: 13 | loss: 0.0054308\n",
      "\tspeed: 0.0476s/iter; left time: 316.2525s\n",
      "\titers: 700, epoch: 13 | loss: 0.0058771\n",
      "\tspeed: 0.0484s/iter; left time: 316.7926s\n",
      "\titers: 800, epoch: 13 | loss: 0.0050994\n",
      "\tspeed: 0.0482s/iter; left time: 311.1338s\n",
      "\titers: 900, epoch: 13 | loss: 0.0066943\n",
      "\tspeed: 0.0482s/iter; left time: 306.3360s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:43.87s\n",
      "Steps: 906 | Train Loss: 0.0058265 Vali Loss: 0.0103207 Test Loss: 0.0131232\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_24_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.01137869618833065, rmse:0.10667096823453903, mae:0.06598816812038422, rse:0.40311703085899353\n",
      "Original data scale mse:1897220.5, rmse:1377.396240234375, mae:889.3832397460938, rse:0.09679288417100906\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_96_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=96, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_96_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28945\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.1052735\n",
      "\tspeed: 0.0805s/iter; left time: 1446.8473s\n",
      "\titers: 200, epoch: 1 | loss: 0.0845075\n",
      "\tspeed: 0.0497s/iter; left time: 888.2199s\n",
      "\titers: 300, epoch: 1 | loss: 0.0778775\n",
      "\tspeed: 0.0501s/iter; left time: 891.4439s\n",
      "\titers: 400, epoch: 1 | loss: 0.0722261\n",
      "\tspeed: 0.0509s/iter; left time: 899.1364s\n",
      "\titers: 500, epoch: 1 | loss: 0.0733849\n",
      "\tspeed: 0.0504s/iter; left time: 885.9382s\n",
      "\titers: 600, epoch: 1 | loss: 0.0649374\n",
      "\tspeed: 0.0507s/iter; left time: 885.8577s\n",
      "\titers: 700, epoch: 1 | loss: 0.0627303\n",
      "\tspeed: 0.0504s/iter; left time: 875.8841s\n",
      "\titers: 800, epoch: 1 | loss: 0.0568795\n",
      "\tspeed: 0.0502s/iter; left time: 867.3952s\n",
      "\titers: 900, epoch: 1 | loss: 0.0588607\n",
      "\tspeed: 0.0503s/iter; left time: 865.0524s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:46.36s\n",
      "Steps: 904 | Train Loss: 0.0770370 Vali Loss: 0.0422612 Test Loss: 0.0492609\n",
      "Validation loss decreased (inf --> 0.042261).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0423253\n",
      "\tspeed: 0.1143s/iter; left time: 1951.2053s\n",
      "\titers: 200, epoch: 2 | loss: 0.0328577\n",
      "\tspeed: 0.0512s/iter; left time: 869.3117s\n",
      "\titers: 300, epoch: 2 | loss: 0.0310498\n",
      "\tspeed: 0.0510s/iter; left time: 861.0180s\n",
      "\titers: 400, epoch: 2 | loss: 0.0267299\n",
      "\tspeed: 0.0509s/iter; left time: 854.4213s\n",
      "\titers: 500, epoch: 2 | loss: 0.0250892\n",
      "\tspeed: 0.0509s/iter; left time: 848.5097s\n",
      "\titers: 600, epoch: 2 | loss: 0.0215443\n",
      "\tspeed: 0.0507s/iter; left time: 840.5637s\n",
      "\titers: 700, epoch: 2 | loss: 0.0228624\n",
      "\tspeed: 0.0508s/iter; left time: 837.3714s\n",
      "\titers: 800, epoch: 2 | loss: 0.0199976\n",
      "\tspeed: 0.0507s/iter; left time: 830.6593s\n",
      "\titers: 900, epoch: 2 | loss: 0.0192713\n",
      "\tspeed: 0.0510s/iter; left time: 829.5684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:46.25s\n",
      "Steps: 904 | Train Loss: 0.0281857 Vali Loss: 0.0192857 Test Loss: 0.0213712\n",
      "Validation loss decreased (0.042261 --> 0.019286).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0187188\n",
      "\tspeed: 0.1150s/iter; left time: 1860.2736s\n",
      "\titers: 200, epoch: 3 | loss: 0.0193838\n",
      "\tspeed: 0.0506s/iter; left time: 812.5755s\n",
      "\titers: 300, epoch: 3 | loss: 0.0201494\n",
      "\tspeed: 0.0498s/iter; left time: 795.1681s\n",
      "\titers: 400, epoch: 3 | loss: 0.0181761\n",
      "\tspeed: 0.0499s/iter; left time: 792.0418s\n",
      "\titers: 500, epoch: 3 | loss: 0.0159714\n",
      "\tspeed: 0.0505s/iter; left time: 796.4509s\n",
      "\titers: 600, epoch: 3 | loss: 0.0182038\n",
      "\tspeed: 0.0505s/iter; left time: 791.1482s\n",
      "\titers: 700, epoch: 3 | loss: 0.0159824\n",
      "\tspeed: 0.0504s/iter; left time: 784.4332s\n",
      "\titers: 800, epoch: 3 | loss: 0.0199805\n",
      "\tspeed: 0.0503s/iter; left time: 779.0482s\n",
      "\titers: 900, epoch: 3 | loss: 0.0154733\n",
      "\tspeed: 0.0503s/iter; left time: 773.3523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:45.77s\n",
      "Steps: 904 | Train Loss: 0.0183871 Vali Loss: 0.0164777 Test Loss: 0.0187503\n",
      "Validation loss decreased (0.019286 --> 0.016478).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0155618\n",
      "\tspeed: 0.1137s/iter; left time: 1735.7656s\n",
      "\titers: 200, epoch: 4 | loss: 0.0181367\n",
      "\tspeed: 0.0505s/iter; left time: 766.3386s\n",
      "\titers: 300, epoch: 4 | loss: 0.0170885\n",
      "\tspeed: 0.0504s/iter; left time: 759.9688s\n",
      "\titers: 400, epoch: 4 | loss: 0.0159648\n",
      "\tspeed: 0.0501s/iter; left time: 749.4519s\n",
      "\titers: 500, epoch: 4 | loss: 0.0150804\n",
      "\tspeed: 0.0506s/iter; left time: 753.1031s\n",
      "\titers: 600, epoch: 4 | loss: 0.0142211\n",
      "\tspeed: 0.0508s/iter; left time: 750.3437s\n",
      "\titers: 700, epoch: 4 | loss: 0.0189483\n",
      "\tspeed: 0.0508s/iter; left time: 745.5344s\n",
      "\titers: 800, epoch: 4 | loss: 0.0140306\n",
      "\tspeed: 0.0502s/iter; left time: 731.9301s\n",
      "\titers: 900, epoch: 4 | loss: 0.0153686\n",
      "\tspeed: 0.0503s/iter; left time: 727.9476s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:45.90s\n",
      "Steps: 904 | Train Loss: 0.0166229 Vali Loss: 0.0166674 Test Loss: 0.0183428\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0166662\n",
      "\tspeed: 0.1093s/iter; left time: 1569.5393s\n",
      "\titers: 200, epoch: 5 | loss: 0.0135682\n",
      "\tspeed: 0.0497s/iter; left time: 709.4186s\n",
      "\titers: 300, epoch: 5 | loss: 0.0158774\n",
      "\tspeed: 0.0496s/iter; left time: 702.5685s\n",
      "\titers: 400, epoch: 5 | loss: 0.0165986\n",
      "\tspeed: 0.0497s/iter; left time: 699.7198s\n",
      "\titers: 500, epoch: 5 | loss: 0.0149370\n",
      "\tspeed: 0.0496s/iter; left time: 692.7150s\n",
      "\titers: 600, epoch: 5 | loss: 0.0161001\n",
      "\tspeed: 0.0499s/iter; left time: 691.3440s\n",
      "\titers: 700, epoch: 5 | loss: 0.0142459\n",
      "\tspeed: 0.0497s/iter; left time: 684.0442s\n",
      "\titers: 800, epoch: 5 | loss: 0.0150630\n",
      "\tspeed: 0.0498s/iter; left time: 680.3678s\n",
      "\titers: 900, epoch: 5 | loss: 0.0157289\n",
      "\tspeed: 0.0498s/iter; left time: 675.3105s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:45.23s\n",
      "Steps: 904 | Train Loss: 0.0152686 Vali Loss: 0.0166363 Test Loss: 0.0196131\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0172512\n",
      "\tspeed: 0.1103s/iter; left time: 1484.7449s\n",
      "\titers: 200, epoch: 6 | loss: 0.0152996\n",
      "\tspeed: 0.0503s/iter; left time: 672.2461s\n",
      "\titers: 300, epoch: 6 | loss: 0.0147258\n",
      "\tspeed: 0.0502s/iter; left time: 666.0468s\n",
      "\titers: 400, epoch: 6 | loss: 0.0128632\n",
      "\tspeed: 0.0497s/iter; left time: 653.9338s\n",
      "\titers: 500, epoch: 6 | loss: 0.0138966\n",
      "\tspeed: 0.0499s/iter; left time: 651.7620s\n",
      "\titers: 600, epoch: 6 | loss: 0.0140565\n",
      "\tspeed: 0.0501s/iter; left time: 648.7582s\n",
      "\titers: 700, epoch: 6 | loss: 0.0149950\n",
      "\tspeed: 0.0499s/iter; left time: 641.5402s\n",
      "\titers: 800, epoch: 6 | loss: 0.0133245\n",
      "\tspeed: 0.0496s/iter; left time: 633.5508s\n",
      "\titers: 900, epoch: 6 | loss: 0.0130404\n",
      "\tspeed: 0.0497s/iter; left time: 629.4319s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:45.45s\n",
      "Steps: 904 | Train Loss: 0.0141668 Vali Loss: 0.0181570 Test Loss: 0.0206778\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0123304\n",
      "\tspeed: 0.1111s/iter; left time: 1395.6960s\n",
      "\titers: 200, epoch: 7 | loss: 0.0136654\n",
      "\tspeed: 0.0504s/iter; left time: 628.2651s\n",
      "\titers: 300, epoch: 7 | loss: 0.0142162\n",
      "\tspeed: 0.0506s/iter; left time: 624.9387s\n",
      "\titers: 400, epoch: 7 | loss: 0.0128690\n",
      "\tspeed: 0.0501s/iter; left time: 614.0827s\n",
      "\titers: 500, epoch: 7 | loss: 0.0128486\n",
      "\tspeed: 0.0506s/iter; left time: 614.7757s\n",
      "\titers: 600, epoch: 7 | loss: 0.0104035\n",
      "\tspeed: 0.0504s/iter; left time: 607.4868s\n",
      "\titers: 700, epoch: 7 | loss: 0.0125170\n",
      "\tspeed: 0.0506s/iter; left time: 604.9773s\n",
      "\titers: 800, epoch: 7 | loss: 0.0103865\n",
      "\tspeed: 0.0507s/iter; left time: 600.6022s\n",
      "\titers: 900, epoch: 7 | loss: 0.0143755\n",
      "\tspeed: 0.0504s/iter; left time: 592.3181s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:45.90s\n",
      "Steps: 904 | Train Loss: 0.0130236 Vali Loss: 0.0173678 Test Loss: 0.0208934\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0150618\n",
      "\tspeed: 0.1105s/iter; left time: 1287.8489s\n",
      "\titers: 200, epoch: 8 | loss: 0.0120275\n",
      "\tspeed: 0.0501s/iter; left time: 579.0655s\n",
      "\titers: 300, epoch: 8 | loss: 0.0121574\n",
      "\tspeed: 0.0497s/iter; left time: 569.2083s\n",
      "\titers: 400, epoch: 8 | loss: 0.0106792\n",
      "\tspeed: 0.0497s/iter; left time: 564.0515s\n",
      "\titers: 500, epoch: 8 | loss: 0.0111393\n",
      "\tspeed: 0.0499s/iter; left time: 561.1490s\n",
      "\titers: 600, epoch: 8 | loss: 0.0108992\n",
      "\tspeed: 0.0499s/iter; left time: 556.0449s\n",
      "\titers: 700, epoch: 8 | loss: 0.0118494\n",
      "\tspeed: 0.0498s/iter; left time: 550.6994s\n",
      "\titers: 800, epoch: 8 | loss: 0.0115658\n",
      "\tspeed: 0.0498s/iter; left time: 545.6069s\n",
      "\titers: 900, epoch: 8 | loss: 0.0098825\n",
      "\tspeed: 0.0498s/iter; left time: 540.7533s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:45.35s\n",
      "Steps: 904 | Train Loss: 0.0118994 Vali Loss: 0.0178741 Test Loss: 0.0224808\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_96_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018757950514554977, rmse:0.13695967197418213, mae:0.09164635092020035, rse:0.5178592801094055\n",
      "Original data scale mse:4011030.25, rmse:2002.755615234375, mae:1317.1807861328125, rse:0.14094209671020508\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_96_168_scaler_choice_for_IT', model='Informer', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=96, label_len=48, pred_len=168, inverse=True, loss_fnc='MSE', fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=5, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_96_168_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28873\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.1080960\n",
      "\tspeed: 0.0807s/iter; left time: 1448.6037s\n",
      "\titers: 200, epoch: 1 | loss: 0.0824163\n",
      "\tspeed: 0.0517s/iter; left time: 922.5309s\n",
      "\titers: 300, epoch: 1 | loss: 0.0842993\n",
      "\tspeed: 0.0512s/iter; left time: 908.9383s\n",
      "\titers: 400, epoch: 1 | loss: 0.0736092\n",
      "\tspeed: 0.0511s/iter; left time: 900.6010s\n",
      "\titers: 500, epoch: 1 | loss: 0.0733240\n",
      "\tspeed: 0.0499s/iter; left time: 874.6721s\n",
      "\titers: 600, epoch: 1 | loss: 0.0706275\n",
      "\tspeed: 0.0502s/iter; left time: 875.4694s\n",
      "\titers: 700, epoch: 1 | loss: 0.0656435\n",
      "\tspeed: 0.0499s/iter; left time: 865.6878s\n",
      "\titers: 800, epoch: 1 | loss: 0.0658726\n",
      "\tspeed: 0.0495s/iter; left time: 852.7293s\n",
      "\titers: 900, epoch: 1 | loss: 0.0630696\n",
      "\tspeed: 0.0502s/iter; left time: 860.6820s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:46.26s\n",
      "Steps: 902 | Train Loss: 0.0786550 Vali Loss: 0.0498637 Test Loss: 0.0583538\n",
      "Validation loss decreased (inf --> 0.049864).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0482759\n",
      "\tspeed: 0.1247s/iter; left time: 2125.1491s\n",
      "\titers: 200, epoch: 2 | loss: 0.0421801\n",
      "\tspeed: 0.0517s/iter; left time: 876.3713s\n",
      "\titers: 300, epoch: 2 | loss: 0.0385429\n",
      "\tspeed: 0.0517s/iter; left time: 870.9840s\n",
      "\titers: 400, epoch: 2 | loss: 0.0367902\n",
      "\tspeed: 0.0509s/iter; left time: 851.8933s\n",
      "\titers: 500, epoch: 2 | loss: 0.0308838\n",
      "\tspeed: 0.0505s/iter; left time: 841.0018s\n",
      "\titers: 600, epoch: 2 | loss: 0.0255439\n",
      "\tspeed: 0.0504s/iter; left time: 833.8026s\n",
      "\titers: 700, epoch: 2 | loss: 0.0225206\n",
      "\tspeed: 0.0517s/iter; left time: 850.5914s\n",
      "\titers: 800, epoch: 2 | loss: 0.0219971\n",
      "\tspeed: 0.0516s/iter; left time: 843.5026s\n",
      "\titers: 900, epoch: 2 | loss: 0.0218719\n",
      "\tspeed: 0.0503s/iter; left time: 816.3116s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:46.54s\n",
      "Steps: 902 | Train Loss: 0.0340902 Vali Loss: 0.0191990 Test Loss: 0.0217300\n",
      "Validation loss decreased (0.049864 --> 0.019199).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0235283\n",
      "\tspeed: 0.1256s/iter; left time: 2026.2922s\n",
      "\titers: 200, epoch: 3 | loss: 0.0206867\n",
      "\tspeed: 0.0517s/iter; left time: 829.4887s\n",
      "\titers: 300, epoch: 3 | loss: 0.0206621\n",
      "\tspeed: 0.0517s/iter; left time: 823.4673s\n",
      "\titers: 400, epoch: 3 | loss: 0.0177734\n",
      "\tspeed: 0.0517s/iter; left time: 818.3076s\n",
      "\titers: 500, epoch: 3 | loss: 0.0205937\n",
      "\tspeed: 0.0517s/iter; left time: 813.3265s\n",
      "\titers: 600, epoch: 3 | loss: 0.0205176\n",
      "\tspeed: 0.0500s/iter; left time: 782.0579s\n",
      "\titers: 700, epoch: 3 | loss: 0.0188191\n",
      "\tspeed: 0.0503s/iter; left time: 781.3555s\n",
      "\titers: 800, epoch: 3 | loss: 0.0180016\n",
      "\tspeed: 0.0517s/iter; left time: 798.8572s\n",
      "\titers: 900, epoch: 3 | loss: 0.0204296\n",
      "\tspeed: 0.0515s/iter; left time: 790.5516s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:46.67s\n",
      "Steps: 902 | Train Loss: 0.0200820 Vali Loss: 0.0191245 Test Loss: 0.0210825\n",
      "Validation loss decreased (0.019199 --> 0.019124).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0196903\n",
      "\tspeed: 0.1252s/iter; left time: 1906.8443s\n",
      "\titers: 200, epoch: 4 | loss: 0.0173575\n",
      "\tspeed: 0.0517s/iter; left time: 782.0716s\n",
      "\titers: 300, epoch: 4 | loss: 0.0186932\n",
      "\tspeed: 0.0513s/iter; left time: 770.8456s\n",
      "\titers: 400, epoch: 4 | loss: 0.0181120\n",
      "\tspeed: 0.0517s/iter; left time: 772.2122s\n",
      "\titers: 500, epoch: 4 | loss: 0.0186082\n",
      "\tspeed: 0.0517s/iter; left time: 767.3235s\n",
      "\titers: 600, epoch: 4 | loss: 0.0175178\n",
      "\tspeed: 0.0500s/iter; left time: 737.3200s\n",
      "\titers: 700, epoch: 4 | loss: 0.0184030\n",
      "\tspeed: 0.0510s/iter; left time: 746.1875s\n",
      "\titers: 800, epoch: 4 | loss: 0.0189094\n",
      "\tspeed: 0.0515s/iter; left time: 749.1585s\n",
      "\titers: 900, epoch: 4 | loss: 0.0182288\n",
      "\tspeed: 0.0513s/iter; left time: 740.4346s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:46.69s\n",
      "Steps: 902 | Train Loss: 0.0183361 Vali Loss: 0.0185760 Test Loss: 0.0211089\n",
      "Validation loss decreased (0.019124 --> 0.018576).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0169285\n",
      "\tspeed: 0.1255s/iter; left time: 1798.9083s\n",
      "\titers: 200, epoch: 5 | loss: 0.0162984\n",
      "\tspeed: 0.0507s/iter; left time: 721.2711s\n",
      "\titers: 300, epoch: 5 | loss: 0.0175885\n",
      "\tspeed: 0.0505s/iter; left time: 713.0344s\n",
      "\titers: 400, epoch: 5 | loss: 0.0168064\n",
      "\tspeed: 0.0504s/iter; left time: 707.0160s\n",
      "\titers: 500, epoch: 5 | loss: 0.0158014\n",
      "\tspeed: 0.0510s/iter; left time: 710.8311s\n",
      "\titers: 600, epoch: 5 | loss: 0.0175041\n",
      "\tspeed: 0.0516s/iter; left time: 713.1741s\n",
      "\titers: 700, epoch: 5 | loss: 0.0175251\n",
      "\tspeed: 0.0509s/iter; left time: 699.4839s\n",
      "\titers: 800, epoch: 5 | loss: 0.0160738\n",
      "\tspeed: 0.0505s/iter; left time: 688.6262s\n",
      "\titers: 900, epoch: 5 | loss: 0.0180332\n",
      "\tspeed: 0.0504s/iter; left time: 682.3207s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:46.09s\n",
      "Steps: 902 | Train Loss: 0.0168270 Vali Loss: 0.0191929 Test Loss: 0.0207321\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0170055\n",
      "\tspeed: 0.1237s/iter; left time: 1661.1817s\n",
      "\titers: 200, epoch: 6 | loss: 0.0187532\n",
      "\tspeed: 0.0511s/iter; left time: 681.8065s\n",
      "\titers: 300, epoch: 6 | loss: 0.0166333\n",
      "\tspeed: 0.0517s/iter; left time: 684.0687s\n",
      "\titers: 400, epoch: 6 | loss: 0.0146327\n",
      "\tspeed: 0.0510s/iter; left time: 670.2077s\n",
      "\titers: 500, epoch: 6 | loss: 0.0151519\n",
      "\tspeed: 0.0487s/iter; left time: 634.2786s\n",
      "\titers: 600, epoch: 6 | loss: 0.0148442\n",
      "\tspeed: 0.0512s/iter; left time: 662.2534s\n",
      "\titers: 700, epoch: 6 | loss: 0.0146263\n",
      "\tspeed: 0.0517s/iter; left time: 662.8162s\n",
      "\titers: 800, epoch: 6 | loss: 0.0176119\n",
      "\tspeed: 0.0517s/iter; left time: 658.2691s\n",
      "\titers: 900, epoch: 6 | loss: 0.0159310\n",
      "\tspeed: 0.0514s/iter; left time: 649.3356s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:46.56s\n",
      "Steps: 902 | Train Loss: 0.0153580 Vali Loss: 0.0195957 Test Loss: 0.0217750\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0179181\n",
      "\tspeed: 0.1220s/iter; left time: 1528.3797s\n",
      "\titers: 200, epoch: 7 | loss: 0.0150460\n",
      "\tspeed: 0.0517s/iter; left time: 642.5987s\n",
      "\titers: 300, epoch: 7 | loss: 0.0146198\n",
      "\tspeed: 0.0517s/iter; left time: 637.9321s\n",
      "\titers: 400, epoch: 7 | loss: 0.0149516\n",
      "\tspeed: 0.0465s/iter; left time: 569.0682s\n",
      "\titers: 500, epoch: 7 | loss: 0.0124086\n",
      "\tspeed: 0.0498s/iter; left time: 603.9657s\n",
      "\titers: 600, epoch: 7 | loss: 0.0153495\n",
      "\tspeed: 0.0518s/iter; left time: 622.5164s\n",
      "\titers: 700, epoch: 7 | loss: 0.0124983\n",
      "\tspeed: 0.0517s/iter; left time: 616.1809s\n",
      "\titers: 800, epoch: 7 | loss: 0.0126885\n",
      "\tspeed: 0.0517s/iter; left time: 611.8553s\n",
      "\titers: 900, epoch: 7 | loss: 0.0129139\n",
      "\tspeed: 0.0512s/iter; left time: 600.8593s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:46.23s\n",
      "Steps: 902 | Train Loss: 0.0140290 Vali Loss: 0.0201149 Test Loss: 0.0227018\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0139799\n",
      "\tspeed: 0.1207s/iter; left time: 1402.8173s\n",
      "\titers: 200, epoch: 8 | loss: 0.0130178\n",
      "\tspeed: 0.0518s/iter; left time: 597.0199s\n",
      "\titers: 300, epoch: 8 | loss: 0.0146793\n",
      "\tspeed: 0.0515s/iter; left time: 588.8790s\n",
      "\titers: 400, epoch: 8 | loss: 0.0124940\n",
      "\tspeed: 0.0517s/iter; left time: 585.5494s\n",
      "\titers: 500, epoch: 8 | loss: 0.0133813\n",
      "\tspeed: 0.0518s/iter; left time: 581.8778s\n",
      "\titers: 600, epoch: 8 | loss: 0.0127013\n",
      "\tspeed: 0.0516s/iter; left time: 574.3011s\n",
      "\titers: 700, epoch: 8 | loss: 0.0136042\n",
      "\tspeed: 0.0517s/iter; left time: 569.8301s\n",
      "\titers: 800, epoch: 8 | loss: 0.0127588\n",
      "\tspeed: 0.0513s/iter; left time: 560.3385s\n",
      "\titers: 900, epoch: 8 | loss: 0.0109258\n",
      "\tspeed: 0.0486s/iter; left time: 525.7427s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:46.59s\n",
      "Steps: 902 | Train Loss: 0.0127591 Vali Loss: 0.0202601 Test Loss: 0.0218758\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0109588\n",
      "\tspeed: 0.1176s/iter; left time: 1260.7772s\n",
      "\titers: 200, epoch: 9 | loss: 0.0122424\n",
      "\tspeed: 0.0492s/iter; left time: 522.4758s\n",
      "\titers: 300, epoch: 9 | loss: 0.0136600\n",
      "\tspeed: 0.0517s/iter; left time: 544.5617s\n",
      "\titers: 400, epoch: 9 | loss: 0.0114705\n",
      "\tspeed: 0.0519s/iter; left time: 540.5563s\n",
      "\titers: 500, epoch: 9 | loss: 0.0127231\n",
      "\tspeed: 0.0517s/iter; left time: 534.0541s\n",
      "\titers: 600, epoch: 9 | loss: 0.0126706\n",
      "\tspeed: 0.0517s/iter; left time: 529.0946s\n",
      "\titers: 700, epoch: 9 | loss: 0.0122179\n",
      "\tspeed: 0.0517s/iter; left time: 523.4351s\n",
      "\titers: 800, epoch: 9 | loss: 0.0115100\n",
      "\tspeed: 0.0518s/iter; left time: 519.4031s\n",
      "\titers: 900, epoch: 9 | loss: 0.0118221\n",
      "\tspeed: 0.0516s/iter; left time: 511.8267s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:46.24s\n",
      "Steps: 902 | Train Loss: 0.0116011 Vali Loss: 0.0210053 Test Loss: 0.0222173\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_96_168_scaler_choice_for_IT_Informer_custom_ftM_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc5_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.021097851917147636, rmse:0.14525099098682404, mae:0.09748273342847824, rse:0.5495890974998474\n",
      "Original data scale mse:4433544.5, rmse:2105.598388671875, mae:1389.800537109375, rse:0.14831870794296265\n"
     ]
    }
   ],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "seq_len = \"96\"\n",
    "lr = \"0.0001\"\n",
    "itr = 1  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# Lists to store the results\n",
    "informer_results_scaled, informer_results_unscaled = [], []\n",
    "\n",
    "# Log file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    for loss in losses:\n",
    "        statement_1 = f\"\\n=== Starting experiments for loss function: {loss} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)  # Print to .ipynb output cell\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_scaler_choice_for_{country}\"\n",
    "\n",
    "            # Arguments for the command\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --random_seed 2021 \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --label_len 48 \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers 2 \\\n",
    "              --d_layers 1 \\\n",
    "              --factor 5 \\\n",
    "              --enc_in 3 \\\n",
    "              --dec_in 3 \\\n",
    "              --c_out 3 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --dropout 0.1 \\\n",
    "              --overlapping_windows \\\n",
    "              --inverse \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            # Wait for the process to complete\n",
    "            process.wait()\n",
    "            \n",
    "            # Delete the checkpoints folder and all its contents\n",
    "            shutil.rmtree('./checkpoints' )\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics = extract_metrics_from_output(output, itr)\n",
    "            iteration_metrics_unscaled = extract_metrics_from_output(output, itr, if_scaled=False)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, (scaled_metrics, unscaled_metrics) in enumerate(zip(iteration_metrics, iteration_metrics_unscaled), start=1):\n",
    "                log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}, iteration={iteration}:\\n\")\n",
    "                log_file.write(f\"Scaled Metrics - MSE: {scaled_metrics[0]}, RMSE: {scaled_metrics[1]}, MAE: {scaled_metrics[2]}, RSE: {scaled_metrics[3]}\\n\")\n",
    "                log_file.write(f\"Unscaled Metrics - MSE: {unscaled_metrics[0]}, RMSE: {unscaled_metrics[1]}, MAE: {unscaled_metrics[2]}, RSE: {unscaled_metrics[3]}\\n\")\n",
    "\n",
    "                # Append the results to the informer_results lists\n",
    "                metrics_data = [(informer_results_scaled, scaled_metrics), (informer_results_unscaled, unscaled_metrics)]\n",
    "\n",
    "                for result_list, metrics in metrics_data:\n",
    "                    result_list.append({\n",
    "                        'Loss_function': loss,\n",
    "                        'Pred_len': pred_len,\n",
    "                        'Iteration': iteration,\n",
    "                        'MSE': metrics[0],\n",
    "                        'RMSE': metrics[1],\n",
    "                        'MAE': metrics[2],\n",
    "                        'RSE': metrics[3]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.4031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.5179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.5496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MSE    RMSE     MAE     RSE\n",
       "Loss_function Iteration Pred_len                                \n",
       "MSE           1         24        0.0114  0.1067  0.0660  0.4031\n",
       "                        96        0.0188  0.1370  0.0916  0.5179\n",
       "                        168       0.0211  0.1453  0.0975  0.5496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir = './results/scaler_choice'\n",
    "csv_name_scaled = 'informer_scaler_results_scaled_minmax_IT_default.csv'\n",
    "csv_name_unscaled = 'informer_scaler_results_unscaled_minmax_IT_default.csv'\n",
    "\n",
    "# Convert the results into a DataFrame and save as CSV\n",
    "informer_df_scaled = convert_results_into_df(informer_results_scaled, path_dir, csv_name_scaled)\n",
    "informer_df_unscaled = convert_results_into_df(informer_results_unscaled, path_dir, csv_name_unscaled)\n",
    "informer_df_scaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>1897220.50</td>\n",
       "      <td>1377.3962</td>\n",
       "      <td>889.3832</td>\n",
       "      <td>0.0968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4011030.25</td>\n",
       "      <td>2002.7556</td>\n",
       "      <td>1317.1808</td>\n",
       "      <td>0.1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>4433544.50</td>\n",
       "      <td>2105.5984</td>\n",
       "      <td>1389.8005</td>\n",
       "      <td>0.1483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MSE       RMSE        MAE     RSE\n",
       "Loss_function Iteration Pred_len                                          \n",
       "MSE           1         24        1897220.50  1377.3962   889.3832  0.0968\n",
       "                        96        4011030.25  2002.7556  1317.1808  0.1409\n",
       "                        168       4433544.50  2105.5984  1389.8005  0.1483"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informer_df_unscaled.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MinMax Scaler PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/scaler_choice/min_max\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting experiments for loss function: MSE ===\n",
      "\n",
      "\n",
      "=== Starting experiments for pred_len: 24 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_24_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=24, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_24_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28777\n",
      "val 6217\n",
      "test 6217\n",
      "\titers: 100, epoch: 1 | loss: 0.0433534\n",
      "\tspeed: 0.0537s/iter; left time: 959.5066s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 1 | loss: 0.0413011\n",
      "\tspeed: 0.0265s/iter; left time: 471.8433s\n",
      "\titers: 300, epoch: 1 | loss: 0.0289694\n",
      "\tspeed: 0.0267s/iter; left time: 472.9291s\n",
      "\titers: 400, epoch: 1 | loss: 0.0264602\n",
      "\tspeed: 0.0274s/iter; left time: 481.8262s\n",
      "\titers: 500, epoch: 1 | loss: 0.0223774\n",
      "\tspeed: 0.0274s/iter; left time: 478.6850s\n",
      "\titers: 600, epoch: 1 | loss: 0.0224326\n",
      "\tspeed: 0.0278s/iter; left time: 482.8776s\n",
      "\titers: 700, epoch: 1 | loss: 0.0153831\n",
      "\tspeed: 0.0276s/iter; left time: 477.0692s\n",
      "\titers: 800, epoch: 1 | loss: 0.0177559\n",
      "\tspeed: 0.0271s/iter; left time: 466.0280s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:24.67s\n",
      "Steps: 899 | Train Loss: 0.0290216 Vali Loss: 0.0152750 Test Loss: 0.0163096\n",
      "Validation loss decreased (inf --> 0.015275).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0137294\n",
      "\tspeed: 0.0829s/iter; left time: 1408.5552s\n",
      "\titers: 200, epoch: 2 | loss: 0.0125926\n",
      "\tspeed: 0.0275s/iter; left time: 463.6556s\n",
      "\titers: 300, epoch: 2 | loss: 0.0113571\n",
      "\tspeed: 0.0275s/iter; left time: 461.1356s\n",
      "\titers: 400, epoch: 2 | loss: 0.0121392\n",
      "\tspeed: 0.0267s/iter; left time: 445.0356s\n",
      "\titers: 500, epoch: 2 | loss: 0.0106669\n",
      "\tspeed: 0.0265s/iter; left time: 439.6547s\n",
      "\titers: 600, epoch: 2 | loss: 0.0113629\n",
      "\tspeed: 0.0265s/iter; left time: 436.4282s\n",
      "\titers: 700, epoch: 2 | loss: 0.0137541\n",
      "\tspeed: 0.0272s/iter; left time: 445.3279s\n",
      "\titers: 800, epoch: 2 | loss: 0.0119306\n",
      "\tspeed: 0.0271s/iter; left time: 441.3187s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:24.58s\n",
      "Steps: 899 | Train Loss: 0.0120210 Vali Loss: 0.0097616 Test Loss: 0.0109822\n",
      "Validation loss decreased (0.015275 --> 0.009762).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0097358\n",
      "\tspeed: 0.0828s/iter; left time: 1331.4069s\n",
      "\titers: 200, epoch: 3 | loss: 0.0073494\n",
      "\tspeed: 0.0270s/iter; left time: 431.8587s\n",
      "\titers: 300, epoch: 3 | loss: 0.0116293\n",
      "\tspeed: 0.0270s/iter; left time: 429.2034s\n",
      "\titers: 400, epoch: 3 | loss: 0.0099664\n",
      "\tspeed: 0.0274s/iter; left time: 431.9573s\n",
      "\titers: 500, epoch: 3 | loss: 0.0119654\n",
      "\tspeed: 0.0274s/iter; left time: 430.0721s\n",
      "\titers: 600, epoch: 3 | loss: 0.0121947\n",
      "\tspeed: 0.0274s/iter; left time: 427.7186s\n",
      "\titers: 700, epoch: 3 | loss: 0.0094518\n",
      "\tspeed: 0.0273s/iter; left time: 423.2019s\n",
      "\titers: 800, epoch: 3 | loss: 0.0128985\n",
      "\tspeed: 0.0272s/iter; left time: 418.7339s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:24.73s\n",
      "Steps: 899 | Train Loss: 0.0101906 Vali Loss: 0.0093568 Test Loss: 0.0104973\n",
      "Validation loss decreased (0.009762 --> 0.009357).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0076987\n",
      "\tspeed: 0.0809s/iter; left time: 1227.8745s\n",
      "\titers: 200, epoch: 4 | loss: 0.0071735\n",
      "\tspeed: 0.0274s/iter; left time: 412.6497s\n",
      "\titers: 300, epoch: 4 | loss: 0.0097333\n",
      "\tspeed: 0.0274s/iter; left time: 409.9009s\n",
      "\titers: 400, epoch: 4 | loss: 0.0089947\n",
      "\tspeed: 0.0272s/iter; left time: 405.4009s\n",
      "\titers: 500, epoch: 4 | loss: 0.0102414\n",
      "\tspeed: 0.0274s/iter; left time: 404.5763s\n",
      "\titers: 600, epoch: 4 | loss: 0.0088427\n",
      "\tspeed: 0.0273s/iter; left time: 400.2993s\n",
      "\titers: 700, epoch: 4 | loss: 0.0105690\n",
      "\tspeed: 0.0274s/iter; left time: 399.9542s\n",
      "\titers: 800, epoch: 4 | loss: 0.0072219\n",
      "\tspeed: 0.0278s/iter; left time: 402.4611s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:24.99s\n",
      "Steps: 899 | Train Loss: 0.0096689 Vali Loss: 0.0091700 Test Loss: 0.0102992\n",
      "Validation loss decreased (0.009357 --> 0.009170).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0099527\n",
      "\tspeed: 0.0830s/iter; left time: 1185.6683s\n",
      "\titers: 200, epoch: 5 | loss: 0.0079270\n",
      "\tspeed: 0.0293s/iter; left time: 416.1728s\n",
      "\titers: 300, epoch: 5 | loss: 0.0081204\n",
      "\tspeed: 0.0275s/iter; left time: 387.5770s\n",
      "\titers: 400, epoch: 5 | loss: 0.0121630\n",
      "\tspeed: 0.0272s/iter; left time: 380.7060s\n",
      "\titers: 500, epoch: 5 | loss: 0.0087160\n",
      "\tspeed: 0.0275s/iter; left time: 381.2484s\n",
      "\titers: 600, epoch: 5 | loss: 0.0126874\n",
      "\tspeed: 0.0275s/iter; left time: 379.0256s\n",
      "\titers: 700, epoch: 5 | loss: 0.0090514\n",
      "\tspeed: 0.0274s/iter; left time: 375.1251s\n",
      "\titers: 800, epoch: 5 | loss: 0.0105917\n",
      "\tspeed: 0.0275s/iter; left time: 373.5684s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:25.10s\n",
      "Steps: 899 | Train Loss: 0.0092919 Vali Loss: 0.0092398 Test Loss: 0.0104589\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0074347\n",
      "\tspeed: 0.0810s/iter; left time: 1084.4507s\n",
      "\titers: 200, epoch: 6 | loss: 0.0091002\n",
      "\tspeed: 0.0270s/iter; left time: 358.7405s\n",
      "\titers: 300, epoch: 6 | loss: 0.0077439\n",
      "\tspeed: 0.0272s/iter; left time: 359.1489s\n",
      "\titers: 400, epoch: 6 | loss: 0.0074061\n",
      "\tspeed: 0.0272s/iter; left time: 355.8635s\n",
      "\titers: 500, epoch: 6 | loss: 0.0074313\n",
      "\tspeed: 0.0273s/iter; left time: 354.7575s\n",
      "\titers: 600, epoch: 6 | loss: 0.0098568\n",
      "\tspeed: 0.0273s/iter; left time: 352.3720s\n",
      "\titers: 700, epoch: 6 | loss: 0.0083915\n",
      "\tspeed: 0.0269s/iter; left time: 344.3279s\n",
      "\titers: 800, epoch: 6 | loss: 0.0081046\n",
      "\tspeed: 0.0275s/iter; left time: 349.2946s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:24.77s\n",
      "Steps: 899 | Train Loss: 0.0090776 Vali Loss: 0.0089008 Test Loss: 0.0101379\n",
      "Validation loss decreased (0.009170 --> 0.008901).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0086015\n",
      "\tspeed: 0.0807s/iter; left time: 1007.6699s\n",
      "\titers: 200, epoch: 7 | loss: 0.0072695\n",
      "\tspeed: 0.0297s/iter; left time: 367.6595s\n",
      "\titers: 300, epoch: 7 | loss: 0.0097130\n",
      "\tspeed: 0.0294s/iter; left time: 361.0172s\n",
      "\titers: 400, epoch: 7 | loss: 0.0067226\n",
      "\tspeed: 0.0292s/iter; left time: 356.2084s\n",
      "\titers: 500, epoch: 7 | loss: 0.0100733\n",
      "\tspeed: 0.0277s/iter; left time: 334.4096s\n",
      "\titers: 600, epoch: 7 | loss: 0.0091730\n",
      "\tspeed: 0.0270s/iter; left time: 323.5619s\n",
      "\titers: 700, epoch: 7 | loss: 0.0074848\n",
      "\tspeed: 0.0269s/iter; left time: 320.2075s\n",
      "\titers: 800, epoch: 7 | loss: 0.0081078\n",
      "\tspeed: 0.0271s/iter; left time: 319.9099s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:25.28s\n",
      "Steps: 899 | Train Loss: 0.0088556 Vali Loss: 0.0088826 Test Loss: 0.0100817\n",
      "Validation loss decreased (0.008901 --> 0.008883).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0087008\n",
      "\tspeed: 0.0806s/iter; left time: 934.0934s\n",
      "\titers: 200, epoch: 8 | loss: 0.0094060\n",
      "\tspeed: 0.0285s/iter; left time: 327.2959s\n",
      "\titers: 300, epoch: 8 | loss: 0.0082022\n",
      "\tspeed: 0.0281s/iter; left time: 319.5182s\n",
      "\titers: 400, epoch: 8 | loss: 0.0088738\n",
      "\tspeed: 0.0279s/iter; left time: 314.8216s\n",
      "\titers: 500, epoch: 8 | loss: 0.0098694\n",
      "\tspeed: 0.0241s/iter; left time: 269.8515s\n",
      "\titers: 600, epoch: 8 | loss: 0.0091947\n",
      "\tspeed: 0.0239s/iter; left time: 265.0705s\n",
      "\titers: 700, epoch: 8 | loss: 0.0089150\n",
      "\tspeed: 0.0236s/iter; left time: 259.7003s\n",
      "\titers: 800, epoch: 8 | loss: 0.0086942\n",
      "\tspeed: 0.0257s/iter; left time: 279.9996s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:23.71s\n",
      "Steps: 899 | Train Loss: 0.0087033 Vali Loss: 0.0087204 Test Loss: 0.0100128\n",
      "Validation loss decreased (0.008883 --> 0.008720).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0084817\n",
      "\tspeed: 0.0793s/iter; left time: 848.0704s\n",
      "\titers: 200, epoch: 9 | loss: 0.0100727\n",
      "\tspeed: 0.0271s/iter; left time: 287.0226s\n",
      "\titers: 300, epoch: 9 | loss: 0.0091586\n",
      "\tspeed: 0.0264s/iter; left time: 277.3887s\n",
      "\titers: 400, epoch: 9 | loss: 0.0089532\n",
      "\tspeed: 0.0267s/iter; left time: 277.3023s\n",
      "\titers: 500, epoch: 9 | loss: 0.0078344\n",
      "\tspeed: 0.0287s/iter; left time: 295.1860s\n",
      "\titers: 600, epoch: 9 | loss: 0.0096123\n",
      "\tspeed: 0.0264s/iter; left time: 269.3999s\n",
      "\titers: 700, epoch: 9 | loss: 0.0080819\n",
      "\tspeed: 0.0275s/iter; left time: 277.1504s\n",
      "\titers: 800, epoch: 9 | loss: 0.0123093\n",
      "\tspeed: 0.0276s/iter; left time: 275.2010s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:24.37s\n",
      "Steps: 899 | Train Loss: 0.0085649 Vali Loss: 0.0088404 Test Loss: 0.0101281\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0062878\n",
      "\tspeed: 0.0765s/iter; left time: 748.6026s\n",
      "\titers: 200, epoch: 10 | loss: 0.0103948\n",
      "\tspeed: 0.0262s/iter; left time: 253.5448s\n",
      "\titers: 300, epoch: 10 | loss: 0.0081633\n",
      "\tspeed: 0.0263s/iter; left time: 252.3613s\n",
      "\titers: 400, epoch: 10 | loss: 0.0097685\n",
      "\tspeed: 0.0268s/iter; left time: 254.5586s\n",
      "\titers: 500, epoch: 10 | loss: 0.0094890\n",
      "\tspeed: 0.0260s/iter; left time: 244.1510s\n",
      "\titers: 600, epoch: 10 | loss: 0.0062090\n",
      "\tspeed: 0.0258s/iter; left time: 239.2771s\n",
      "\titers: 700, epoch: 10 | loss: 0.0084335\n",
      "\tspeed: 0.0258s/iter; left time: 237.4306s\n",
      "\titers: 800, epoch: 10 | loss: 0.0090479\n",
      "\tspeed: 0.0266s/iter; left time: 242.1498s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:23.87s\n",
      "Steps: 899 | Train Loss: 0.0084241 Vali Loss: 0.0087949 Test Loss: 0.0101705\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0078626\n",
      "\tspeed: 0.0787s/iter; left time: 699.3937s\n",
      "\titers: 200, epoch: 11 | loss: 0.0080864\n",
      "\tspeed: 0.0272s/iter; left time: 239.5024s\n",
      "\titers: 300, epoch: 11 | loss: 0.0073451\n",
      "\tspeed: 0.0274s/iter; left time: 237.7807s\n",
      "\titers: 400, epoch: 11 | loss: 0.0095469\n",
      "\tspeed: 0.0264s/iter; left time: 226.5066s\n",
      "\titers: 500, epoch: 11 | loss: 0.0082168\n",
      "\tspeed: 0.0272s/iter; left time: 230.5924s\n",
      "\titers: 600, epoch: 11 | loss: 0.0066404\n",
      "\tspeed: 0.0270s/iter; left time: 226.9096s\n",
      "\titers: 700, epoch: 11 | loss: 0.0078623\n",
      "\tspeed: 0.0269s/iter; left time: 222.7232s\n",
      "\titers: 800, epoch: 11 | loss: 0.0085156\n",
      "\tspeed: 0.0264s/iter; left time: 215.9761s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:24.30s\n",
      "Steps: 899 | Train Loss: 0.0083089 Vali Loss: 0.0087825 Test Loss: 0.0101685\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0092288\n",
      "\tspeed: 0.0786s/iter; left time: 628.4602s\n",
      "\titers: 200, epoch: 12 | loss: 0.0081863\n",
      "\tspeed: 0.0279s/iter; left time: 220.4953s\n",
      "\titers: 300, epoch: 12 | loss: 0.0098355\n",
      "\tspeed: 0.0277s/iter; left time: 216.0412s\n",
      "\titers: 400, epoch: 12 | loss: 0.0081439\n",
      "\tspeed: 0.0241s/iter; left time: 185.0714s\n",
      "\titers: 500, epoch: 12 | loss: 0.0094105\n",
      "\tspeed: 0.0240s/iter; left time: 182.5743s\n",
      "\titers: 600, epoch: 12 | loss: 0.0101332\n",
      "\tspeed: 0.0248s/iter; left time: 185.9657s\n",
      "\titers: 700, epoch: 12 | loss: 0.0090127\n",
      "\tspeed: 0.0243s/iter; left time: 179.6054s\n",
      "\titers: 800, epoch: 12 | loss: 0.0085759\n",
      "\tspeed: 0.0255s/iter; left time: 185.7898s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:23.26s\n",
      "Steps: 899 | Train Loss: 0.0082065 Vali Loss: 0.0087255 Test Loss: 0.0101429\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0083329\n",
      "\tspeed: 0.0773s/iter; left time: 548.3487s\n",
      "\titers: 200, epoch: 13 | loss: 0.0089455\n",
      "\tspeed: 0.0259s/iter; left time: 180.9460s\n",
      "\titers: 300, epoch: 13 | loss: 0.0060856\n",
      "\tspeed: 0.0259s/iter; left time: 178.5735s\n",
      "\titers: 400, epoch: 13 | loss: 0.0080279\n",
      "\tspeed: 0.0260s/iter; left time: 176.6821s\n",
      "\titers: 500, epoch: 13 | loss: 0.0120840\n",
      "\tspeed: 0.0276s/iter; left time: 184.5479s\n",
      "\titers: 600, epoch: 13 | loss: 0.0072903\n",
      "\tspeed: 0.0274s/iter; left time: 180.9037s\n",
      "\titers: 700, epoch: 13 | loss: 0.0098777\n",
      "\tspeed: 0.0276s/iter; left time: 179.2623s\n",
      "\titers: 800, epoch: 13 | loss: 0.0115916\n",
      "\tspeed: 0.0268s/iter; left time: 171.1806s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:24.15s\n",
      "Steps: 899 | Train Loss: 0.0081482 Vali Loss: 0.0087364 Test Loss: 0.0101883\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_24_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl24_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6217\n",
      "Scaled mse:0.010012807324528694, rmse:0.10006401687860489, mae:0.05882488191127777, rse:0.3781489133834839\n",
      "Original data scale mse:1266680.625, rmse:1125.4691162109375, mae:719.821533203125, rse:0.07908936589956284\n",
      "\n",
      "=== Starting experiments for pred_len: 96 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_96_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=96, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_96_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 6145\n",
      "test 6145\n",
      "\titers: 100, epoch: 1 | loss: 0.0479194\n",
      "\tspeed: 0.0521s/iter; left time: 928.8328s\n",
      "\titers: 200, epoch: 1 | loss: 0.0367523\n",
      "\tspeed: 0.0245s/iter; left time: 434.2328s\n",
      "\titers: 300, epoch: 1 | loss: 0.0315834\n",
      "\tspeed: 0.0258s/iter; left time: 454.5002s\n",
      "\titers: 400, epoch: 1 | loss: 0.0292022\n",
      "\tspeed: 0.0258s/iter; left time: 451.9067s\n",
      "\titers: 500, epoch: 1 | loss: 0.0289386\n",
      "\tspeed: 0.0257s/iter; left time: 448.8522s\n",
      "\titers: 600, epoch: 1 | loss: 0.0270958\n",
      "\tspeed: 0.0278s/iter; left time: 482.4853s\n",
      "\titers: 700, epoch: 1 | loss: 0.0261125\n",
      "\tspeed: 0.0263s/iter; left time: 453.5399s\n",
      "\titers: 800, epoch: 1 | loss: 0.0309384\n",
      "\tspeed: 0.0269s/iter; left time: 461.9395s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:23.77s\n",
      "Steps: 897 | Train Loss: 0.0331451 Vali Loss: 0.0207033 Test Loss: 0.0222558\n",
      "Validation loss decreased (inf --> 0.020703).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0220400\n",
      "\tspeed: 0.0798s/iter; left time: 1351.3494s\n",
      "\titers: 200, epoch: 2 | loss: 0.0180074\n",
      "\tspeed: 0.0272s/iter; left time: 457.6116s\n",
      "\titers: 300, epoch: 2 | loss: 0.0171522\n",
      "\tspeed: 0.0273s/iter; left time: 456.9618s\n",
      "\titers: 400, epoch: 2 | loss: 0.0191673\n",
      "\tspeed: 0.0273s/iter; left time: 454.4224s\n",
      "\titers: 500, epoch: 2 | loss: 0.0193093\n",
      "\tspeed: 0.0270s/iter; left time: 446.6057s\n",
      "\titers: 600, epoch: 2 | loss: 0.0167046\n",
      "\tspeed: 0.0269s/iter; left time: 443.1512s\n",
      "\titers: 700, epoch: 2 | loss: 0.0149533\n",
      "\tspeed: 0.0271s/iter; left time: 443.6221s\n",
      "\titers: 800, epoch: 2 | loss: 0.0173981\n",
      "\tspeed: 0.0261s/iter; left time: 423.3334s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:24.37s\n",
      "Steps: 897 | Train Loss: 0.0190539 Vali Loss: 0.0163429 Test Loss: 0.0182461\n",
      "Validation loss decreased (0.020703 --> 0.016343).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0168507\n",
      "\tspeed: 0.0790s/iter; left time: 1268.3581s\n",
      "\titers: 200, epoch: 3 | loss: 0.0169879\n",
      "\tspeed: 0.0274s/iter; left time: 437.6735s\n",
      "\titers: 300, epoch: 3 | loss: 0.0137229\n",
      "\tspeed: 0.0268s/iter; left time: 425.2595s\n",
      "\titers: 400, epoch: 3 | loss: 0.0170866\n",
      "\tspeed: 0.0277s/iter; left time: 435.4158s\n",
      "\titers: 500, epoch: 3 | loss: 0.0126257\n",
      "\tspeed: 0.0276s/iter; left time: 431.5691s\n",
      "\titers: 600, epoch: 3 | loss: 0.0217702\n",
      "\tspeed: 0.0252s/iter; left time: 392.2228s\n",
      "\titers: 700, epoch: 3 | loss: 0.0160225\n",
      "\tspeed: 0.0271s/iter; left time: 418.5771s\n",
      "\titers: 800, epoch: 3 | loss: 0.0165418\n",
      "\tspeed: 0.0262s/iter; left time: 402.0523s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:24.23s\n",
      "Steps: 897 | Train Loss: 0.0170694 Vali Loss: 0.0160884 Test Loss: 0.0180791\n",
      "Validation loss decreased (0.016343 --> 0.016088).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0174640\n",
      "\tspeed: 0.0795s/iter; left time: 1204.9647s\n",
      "\titers: 200, epoch: 4 | loss: 0.0155674\n",
      "\tspeed: 0.0262s/iter; left time: 394.0727s\n",
      "\titers: 300, epoch: 4 | loss: 0.0165646\n",
      "\tspeed: 0.0268s/iter; left time: 401.1645s\n",
      "\titers: 400, epoch: 4 | loss: 0.0175954\n",
      "\tspeed: 0.0282s/iter; left time: 418.6717s\n",
      "\titers: 500, epoch: 4 | loss: 0.0144394\n",
      "\tspeed: 0.0272s/iter; left time: 401.4029s\n",
      "\titers: 600, epoch: 4 | loss: 0.0136460\n",
      "\tspeed: 0.0276s/iter; left time: 404.8990s\n",
      "\titers: 700, epoch: 4 | loss: 0.0196132\n",
      "\tspeed: 0.0273s/iter; left time: 396.5670s\n",
      "\titers: 800, epoch: 4 | loss: 0.0154899\n",
      "\tspeed: 0.0274s/iter; left time: 395.5167s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:24.46s\n",
      "Steps: 897 | Train Loss: 0.0163677 Vali Loss: 0.0158555 Test Loss: 0.0179338\n",
      "Validation loss decreased (0.016088 --> 0.015856).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0143957\n",
      "\tspeed: 0.0781s/iter; left time: 1113.1223s\n",
      "\titers: 200, epoch: 5 | loss: 0.0134037\n",
      "\tspeed: 0.0257s/iter; left time: 363.5968s\n",
      "\titers: 300, epoch: 5 | loss: 0.0155811\n",
      "\tspeed: 0.0235s/iter; left time: 330.7805s\n",
      "\titers: 400, epoch: 5 | loss: 0.0180193\n",
      "\tspeed: 0.0270s/iter; left time: 376.7012s\n",
      "\titers: 500, epoch: 5 | loss: 0.0190787\n",
      "\tspeed: 0.0273s/iter; left time: 377.9492s\n",
      "\titers: 600, epoch: 5 | loss: 0.0129474\n",
      "\tspeed: 0.0275s/iter; left time: 377.6848s\n",
      "\titers: 700, epoch: 5 | loss: 0.0153296\n",
      "\tspeed: 0.0266s/iter; left time: 363.8239s\n",
      "\titers: 800, epoch: 5 | loss: 0.0222451\n",
      "\tspeed: 0.0268s/iter; left time: 363.3991s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:23.79s\n",
      "Steps: 897 | Train Loss: 0.0157634 Vali Loss: 0.0159345 Test Loss: 0.0176752\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0137834\n",
      "\tspeed: 0.0770s/iter; left time: 1029.0589s\n",
      "\titers: 200, epoch: 6 | loss: 0.0120786\n",
      "\tspeed: 0.0244s/iter; left time: 323.5489s\n",
      "\titers: 300, epoch: 6 | loss: 0.0149280\n",
      "\tspeed: 0.0259s/iter; left time: 341.1043s\n",
      "\titers: 400, epoch: 6 | loss: 0.0145359\n",
      "\tspeed: 0.0262s/iter; left time: 342.2593s\n",
      "\titers: 500, epoch: 6 | loss: 0.0147159\n",
      "\tspeed: 0.0264s/iter; left time: 341.9166s\n",
      "\titers: 600, epoch: 6 | loss: 0.0124444\n",
      "\tspeed: 0.0266s/iter; left time: 342.3313s\n",
      "\titers: 700, epoch: 6 | loss: 0.0171844\n",
      "\tspeed: 0.0261s/iter; left time: 332.5607s\n",
      "\titers: 800, epoch: 6 | loss: 0.0128529\n",
      "\tspeed: 0.0261s/iter; left time: 330.4788s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:23.60s\n",
      "Steps: 897 | Train Loss: 0.0152163 Vali Loss: 0.0157266 Test Loss: 0.0180607\n",
      "Validation loss decreased (0.015856 --> 0.015727).  Saving model ...\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0157620\n",
      "\tspeed: 0.0786s/iter; left time: 979.8825s\n",
      "\titers: 200, epoch: 7 | loss: 0.0124391\n",
      "\tspeed: 0.0263s/iter; left time: 324.5383s\n",
      "\titers: 300, epoch: 7 | loss: 0.0129646\n",
      "\tspeed: 0.0262s/iter; left time: 320.5831s\n",
      "\titers: 400, epoch: 7 | loss: 0.0122032\n",
      "\tspeed: 0.0269s/iter; left time: 327.2103s\n",
      "\titers: 500, epoch: 7 | loss: 0.0118982\n",
      "\tspeed: 0.0268s/iter; left time: 322.8110s\n",
      "\titers: 600, epoch: 7 | loss: 0.0169142\n",
      "\tspeed: 0.0277s/iter; left time: 331.7951s\n",
      "\titers: 700, epoch: 7 | loss: 0.0159416\n",
      "\tspeed: 0.0264s/iter; left time: 313.3656s\n",
      "\titers: 800, epoch: 7 | loss: 0.0145466\n",
      "\tspeed: 0.0266s/iter; left time: 312.7782s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:24.11s\n",
      "Steps: 897 | Train Loss: 0.0146579 Vali Loss: 0.0157575 Test Loss: 0.0180740\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0125403\n",
      "\tspeed: 0.0779s/iter; left time: 901.1327s\n",
      "\titers: 200, epoch: 8 | loss: 0.0158415\n",
      "\tspeed: 0.0269s/iter; left time: 308.2096s\n",
      "\titers: 300, epoch: 8 | loss: 0.0162096\n",
      "\tspeed: 0.0255s/iter; left time: 290.1101s\n",
      "\titers: 400, epoch: 8 | loss: 0.0133717\n",
      "\tspeed: 0.0255s/iter; left time: 286.8277s\n",
      "\titers: 500, epoch: 8 | loss: 0.0128165\n",
      "\tspeed: 0.0276s/iter; left time: 308.5003s\n",
      "\titers: 600, epoch: 8 | loss: 0.0134770\n",
      "\tspeed: 0.0255s/iter; left time: 281.8643s\n",
      "\titers: 700, epoch: 8 | loss: 0.0129966\n",
      "\tspeed: 0.0255s/iter; left time: 279.4954s\n",
      "\titers: 800, epoch: 8 | loss: 0.0144852\n",
      "\tspeed: 0.0265s/iter; left time: 287.3708s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:23.75s\n",
      "Steps: 897 | Train Loss: 0.0141237 Vali Loss: 0.0158571 Test Loss: 0.0182666\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0161261\n",
      "\tspeed: 0.0770s/iter; left time: 821.6423s\n",
      "\titers: 200, epoch: 9 | loss: 0.0152626\n",
      "\tspeed: 0.0264s/iter; left time: 278.8189s\n",
      "\titers: 300, epoch: 9 | loss: 0.0163682\n",
      "\tspeed: 0.0273s/iter; left time: 286.1802s\n",
      "\titers: 400, epoch: 9 | loss: 0.0143066\n",
      "\tspeed: 0.0272s/iter; left time: 281.6211s\n",
      "\titers: 500, epoch: 9 | loss: 0.0120810\n",
      "\tspeed: 0.0272s/iter; left time: 279.5045s\n",
      "\titers: 600, epoch: 9 | loss: 0.0136788\n",
      "\tspeed: 0.0276s/iter; left time: 280.2793s\n",
      "\titers: 700, epoch: 9 | loss: 0.0149706\n",
      "\tspeed: 0.0282s/iter; left time: 283.8151s\n",
      "\titers: 800, epoch: 9 | loss: 0.0124064\n",
      "\tspeed: 0.0276s/iter; left time: 274.9800s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:24.18s\n",
      "Steps: 897 | Train Loss: 0.0136832 Vali Loss: 0.0160514 Test Loss: 0.0180704\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0101732\n",
      "\tspeed: 0.0780s/iter; left time: 761.5128s\n",
      "\titers: 200, epoch: 10 | loss: 0.0131334\n",
      "\tspeed: 0.0271s/iter; left time: 262.2873s\n",
      "\titers: 300, epoch: 10 | loss: 0.0120628\n",
      "\tspeed: 0.0238s/iter; left time: 227.5049s\n",
      "\titers: 400, epoch: 10 | loss: 0.0174733\n",
      "\tspeed: 0.0235s/iter; left time: 222.0664s\n",
      "\titers: 500, epoch: 10 | loss: 0.0166012\n",
      "\tspeed: 0.0251s/iter; left time: 235.0213s\n",
      "\titers: 600, epoch: 10 | loss: 0.0125723\n",
      "\tspeed: 0.0271s/iter; left time: 250.7946s\n",
      "\titers: 700, epoch: 10 | loss: 0.0120434\n",
      "\tspeed: 0.0265s/iter; left time: 242.8351s\n",
      "\titers: 800, epoch: 10 | loss: 0.0126542\n",
      "\tspeed: 0.0268s/iter; left time: 243.3744s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:23.47s\n",
      "Steps: 897 | Train Loss: 0.0133161 Vali Loss: 0.0158865 Test Loss: 0.0183205\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0134034\n",
      "\tspeed: 0.0802s/iter; left time: 711.2438s\n",
      "\titers: 200, epoch: 11 | loss: 0.0140413\n",
      "\tspeed: 0.0263s/iter; left time: 230.5854s\n",
      "\titers: 300, epoch: 11 | loss: 0.0140108\n",
      "\tspeed: 0.0267s/iter; left time: 231.4727s\n",
      "\titers: 400, epoch: 11 | loss: 0.0136469\n",
      "\tspeed: 0.0280s/iter; left time: 239.7596s\n",
      "\titers: 500, epoch: 11 | loss: 0.0149010\n",
      "\tspeed: 0.0263s/iter; left time: 223.0835s\n",
      "\titers: 600, epoch: 11 | loss: 0.0115766\n",
      "\tspeed: 0.0258s/iter; left time: 216.1678s\n",
      "\titers: 700, epoch: 11 | loss: 0.0151809\n",
      "\tspeed: 0.0260s/iter; left time: 215.2598s\n",
      "\titers: 800, epoch: 11 | loss: 0.0109011\n",
      "\tspeed: 0.0268s/iter; left time: 218.9477s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:24.10s\n",
      "Steps: 897 | Train Loss: 0.0129667 Vali Loss: 0.0158089 Test Loss: 0.0185169\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_96_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6145\n",
      "Scaled mse:0.018060674890875816, rmse:0.13439001142978668, mae:0.08285937458276749, rse:0.5081431269645691\n",
      "Original data scale mse:2552228.75, rmse:1597.569580078125, mae:1041.3875732421875, rse:0.11242750287055969\n",
      "\n",
      "=== Starting experiments for pred_len: 168 ===\n",
      "\n",
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='IT_336_168_scaler_choice_for_IT', model='PatchTST', data='custom', root_path='/vol/fob-vol3/nebenf24/riabchuv/my_work-1/datasets/', data_path='IT_data.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', overlapping_windows=True, scaler_type='minmax', if_relu=True, channel_mixing=0, seq_len=336, label_len=48, pred_len=168, inverse=True, loss_fnc='MSE', fc_dropout=0.2, head_dropout=0.0, patch_len=32, stride=16, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=3, dec_in=3, c_out=3, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=5, learning_rate=0.0001, des='Exp', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : IT_336_168_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28633\n",
      "val 6073\n",
      "test 6073\n",
      "\titers: 100, epoch: 1 | loss: 0.0468358\n",
      "\tspeed: 0.0550s/iter; left time: 978.7197s\n",
      "\titers: 200, epoch: 1 | loss: 0.0364227\n",
      "\tspeed: 0.0256s/iter; left time: 453.1201s\n",
      "\titers: 300, epoch: 1 | loss: 0.0345268\n",
      "\tspeed: 0.0269s/iter; left time: 473.4585s\n",
      "\titers: 400, epoch: 1 | loss: 0.0299338\n",
      "\tspeed: 0.0264s/iter; left time: 462.2310s\n",
      "\titers: 500, epoch: 1 | loss: 0.0306479\n",
      "\tspeed: 0.0263s/iter; left time: 456.5369s\n",
      "\titers: 600, epoch: 1 | loss: 0.0256786\n",
      "\tspeed: 0.0262s/iter; left time: 452.3779s\n",
      "\titers: 700, epoch: 1 | loss: 0.0255443\n",
      "\tspeed: 0.0260s/iter; left time: 446.4390s\n",
      "\titers: 800, epoch: 1 | loss: 0.0244500\n",
      "\tspeed: 0.0261s/iter; left time: 445.8956s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 1\n",
      "Cost time: 00h:00m:23.95s\n",
      "Steps: 894 | Train Loss: 0.0340934 Vali Loss: 0.0218622 Test Loss: 0.0231717\n",
      "Validation loss decreased (inf --> 0.021862).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0214387\n",
      "\tspeed: 0.0811s/iter; left time: 1369.4786s\n",
      "\titers: 200, epoch: 2 | loss: 0.0230934\n",
      "\tspeed: 0.0272s/iter; left time: 455.8969s\n",
      "\titers: 300, epoch: 2 | loss: 0.0217513\n",
      "\tspeed: 0.0266s/iter; left time: 444.2983s\n",
      "\titers: 400, epoch: 2 | loss: 0.0241619\n",
      "\tspeed: 0.0274s/iter; left time: 455.1919s\n",
      "\titers: 500, epoch: 2 | loss: 0.0215770\n",
      "\tspeed: 0.0268s/iter; left time: 441.5619s\n",
      "\titers: 600, epoch: 2 | loss: 0.0230498\n",
      "\tspeed: 0.0262s/iter; left time: 430.1162s\n",
      "\titers: 700, epoch: 2 | loss: 0.0212361\n",
      "\tspeed: 0.0264s/iter; left time: 430.1826s\n",
      "\titers: 800, epoch: 2 | loss: 0.0207776\n",
      "\tspeed: 0.0266s/iter; left time: 429.7993s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 2\n",
      "Cost time: 00h:00m:24.17s\n",
      "Steps: 894 | Train Loss: 0.0204774 Vali Loss: 0.0177245 Test Loss: 0.0193067\n",
      "Validation loss decreased (0.021862 --> 0.017724).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.0216748\n",
      "\tspeed: 0.0816s/iter; left time: 1305.3322s\n",
      "\titers: 200, epoch: 3 | loss: 0.0201155\n",
      "\tspeed: 0.0263s/iter; left time: 417.7009s\n",
      "\titers: 300, epoch: 3 | loss: 0.0179192\n",
      "\tspeed: 0.0274s/iter; left time: 432.5706s\n",
      "\titers: 400, epoch: 3 | loss: 0.0203477\n",
      "\tspeed: 0.0266s/iter; left time: 416.6961s\n",
      "\titers: 500, epoch: 3 | loss: 0.0155373\n",
      "\tspeed: 0.0250s/iter; left time: 390.2216s\n",
      "\titers: 600, epoch: 3 | loss: 0.0205163\n",
      "\tspeed: 0.0301s/iter; left time: 466.5283s\n",
      "\titers: 700, epoch: 3 | loss: 0.0201422\n",
      "\tspeed: 0.0303s/iter; left time: 465.9987s\n",
      "\titers: 800, epoch: 3 | loss: 0.0201780\n",
      "\tspeed: 0.0295s/iter; left time: 450.4383s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 3\n",
      "Cost time: 00h:00m:25.15s\n",
      "Steps: 894 | Train Loss: 0.0184123 Vali Loss: 0.0177597 Test Loss: 0.0195924\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.0203963\n",
      "\tspeed: 0.0815s/iter; left time: 1230.6844s\n",
      "\titers: 200, epoch: 4 | loss: 0.0166834\n",
      "\tspeed: 0.0283s/iter; left time: 423.9060s\n",
      "\titers: 300, epoch: 4 | loss: 0.0174340\n",
      "\tspeed: 0.0278s/iter; left time: 414.4029s\n",
      "\titers: 400, epoch: 4 | loss: 0.0168462\n",
      "\tspeed: 0.0276s/iter; left time: 408.1653s\n",
      "\titers: 500, epoch: 4 | loss: 0.0172863\n",
      "\tspeed: 0.0274s/iter; left time: 402.9628s\n",
      "\titers: 600, epoch: 4 | loss: 0.0157902\n",
      "\tspeed: 0.0285s/iter; left time: 415.9337s\n",
      "\titers: 700, epoch: 4 | loss: 0.0181585\n",
      "\tspeed: 0.0268s/iter; left time: 388.4719s\n",
      "\titers: 800, epoch: 4 | loss: 0.0146427\n",
      "\tspeed: 0.0283s/iter; left time: 407.4876s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 4\n",
      "Cost time: 00h:00m:24.97s\n",
      "Steps: 894 | Train Loss: 0.0177280 Vali Loss: 0.0178394 Test Loss: 0.0196512\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0157198\n",
      "\tspeed: 0.0790s/iter; left time: 1121.8630s\n",
      "\titers: 200, epoch: 5 | loss: 0.0171373\n",
      "\tspeed: 0.0263s/iter; left time: 370.4569s\n",
      "\titers: 300, epoch: 5 | loss: 0.0186989\n",
      "\tspeed: 0.0270s/iter; left time: 377.8141s\n",
      "\titers: 400, epoch: 5 | loss: 0.0188229\n",
      "\tspeed: 0.0266s/iter; left time: 370.1294s\n",
      "\titers: 500, epoch: 5 | loss: 0.0169121\n",
      "\tspeed: 0.0284s/iter; left time: 392.5060s\n",
      "\titers: 600, epoch: 5 | loss: 0.0173411\n",
      "\tspeed: 0.0290s/iter; left time: 397.7944s\n",
      "\titers: 700, epoch: 5 | loss: 0.0159637\n",
      "\tspeed: 0.0269s/iter; left time: 365.4068s\n",
      "\titers: 800, epoch: 5 | loss: 0.0177589\n",
      "\tspeed: 0.0268s/iter; left time: 362.1660s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 5\n",
      "Cost time: 00h:00m:24.47s\n",
      "Steps: 894 | Train Loss: 0.0169687 Vali Loss: 0.0175322 Test Loss: 0.0196702\n",
      "Validation loss decreased (0.017724 --> 0.017532).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.0180251\n",
      "\tspeed: 0.0803s/iter; left time: 1068.6322s\n",
      "\titers: 200, epoch: 6 | loss: 0.0142061\n",
      "\tspeed: 0.0278s/iter; left time: 366.9647s\n",
      "\titers: 300, epoch: 6 | loss: 0.0177228\n",
      "\tspeed: 0.0270s/iter; left time: 353.8137s\n",
      "\titers: 400, epoch: 6 | loss: 0.0173732\n",
      "\tspeed: 0.0264s/iter; left time: 343.7737s\n",
      "\titers: 500, epoch: 6 | loss: 0.0161611\n",
      "\tspeed: 0.0277s/iter; left time: 357.5347s\n",
      "\titers: 600, epoch: 6 | loss: 0.0165672\n",
      "\tspeed: 0.0286s/iter; left time: 366.4560s\n",
      "\titers: 700, epoch: 6 | loss: 0.0187562\n",
      "\tspeed: 0.0288s/iter; left time: 366.3882s\n",
      "\titers: 800, epoch: 6 | loss: 0.0143878\n",
      "\tspeed: 0.0278s/iter; left time: 351.1848s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 6\n",
      "Cost time: 00h:00m:24.93s\n",
      "Steps: 894 | Train Loss: 0.0161495 Vali Loss: 0.0177436 Test Loss: 0.0194848\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.0167251\n",
      "\tspeed: 0.0831s/iter; left time: 1031.8146s\n",
      "\titers: 200, epoch: 7 | loss: 0.0134050\n",
      "\tspeed: 0.0290s/iter; left time: 357.6225s\n",
      "\titers: 300, epoch: 7 | loss: 0.0150428\n",
      "\tspeed: 0.0284s/iter; left time: 346.9968s\n",
      "\titers: 400, epoch: 7 | loss: 0.0162963\n",
      "\tspeed: 0.0271s/iter; left time: 328.8772s\n",
      "\titers: 500, epoch: 7 | loss: 0.0162206\n",
      "\tspeed: 0.0275s/iter; left time: 330.5743s\n",
      "\titers: 600, epoch: 7 | loss: 0.0149635\n",
      "\tspeed: 0.0278s/iter; left time: 331.5470s\n",
      "\titers: 700, epoch: 7 | loss: 0.0152940\n",
      "\tspeed: 0.0270s/iter; left time: 318.6371s\n",
      "\titers: 800, epoch: 7 | loss: 0.0150323\n",
      "\tspeed: 0.0262s/iter; left time: 306.8167s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 7\n",
      "Cost time: 00h:00m:24.97s\n",
      "Steps: 894 | Train Loss: 0.0154356 Vali Loss: 0.0181230 Test Loss: 0.0200175\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0143989\n",
      "\tspeed: 0.0805s/iter; left time: 927.8395s\n",
      "\titers: 200, epoch: 8 | loss: 0.0128264\n",
      "\tspeed: 0.0281s/iter; left time: 321.4257s\n",
      "\titers: 300, epoch: 8 | loss: 0.0145268\n",
      "\tspeed: 0.0284s/iter; left time: 321.2405s\n",
      "\titers: 400, epoch: 8 | loss: 0.0141174\n",
      "\tspeed: 0.0285s/iter; left time: 319.4590s\n",
      "\titers: 500, epoch: 8 | loss: 0.0135030\n",
      "\tspeed: 0.0284s/iter; left time: 316.1795s\n",
      "\titers: 600, epoch: 8 | loss: 0.0128851\n",
      "\tspeed: 0.0283s/iter; left time: 312.3402s\n",
      "\titers: 700, epoch: 8 | loss: 0.0173855\n",
      "\tspeed: 0.0276s/iter; left time: 301.9787s\n",
      "\titers: 800, epoch: 8 | loss: 0.0146162\n",
      "\tspeed: 0.0279s/iter; left time: 301.5894s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 8\n",
      "Cost time: 00h:00m:25.28s\n",
      "Steps: 894 | Train Loss: 0.0148375 Vali Loss: 0.0177649 Test Loss: 0.0200448\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0142486\n",
      "\tspeed: 0.0818s/iter; left time: 869.4589s\n",
      "\titers: 200, epoch: 9 | loss: 0.0151161\n",
      "\tspeed: 0.0275s/iter; left time: 289.9995s\n",
      "\titers: 300, epoch: 9 | loss: 0.0145301\n",
      "\tspeed: 0.0273s/iter; left time: 284.6398s\n",
      "\titers: 400, epoch: 9 | loss: 0.0142033\n",
      "\tspeed: 0.0266s/iter; left time: 274.7079s\n",
      "\titers: 500, epoch: 9 | loss: 0.0132278\n",
      "\tspeed: 0.0267s/iter; left time: 273.0939s\n",
      "\titers: 600, epoch: 9 | loss: 0.0138034\n",
      "\tspeed: 0.0267s/iter; left time: 269.9725s\n",
      "\titers: 700, epoch: 9 | loss: 0.0137077\n",
      "\tspeed: 0.0259s/iter; left time: 260.0256s\n",
      "\titers: 800, epoch: 9 | loss: 0.0153154\n",
      "\tspeed: 0.0260s/iter; left time: 257.8629s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 9\n",
      "Cost time: 00h:00m:24.13s\n",
      "Steps: 894 | Train Loss: 0.0143147 Vali Loss: 0.0174908 Test Loss: 0.0201449\n",
      "Validation loss decreased (0.017532 --> 0.017491).  Saving model ...\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0131972\n",
      "\tspeed: 0.0817s/iter; left time: 794.9332s\n",
      "\titers: 200, epoch: 10 | loss: 0.0129444\n",
      "\tspeed: 0.0281s/iter; left time: 270.3001s\n",
      "\titers: 300, epoch: 10 | loss: 0.0133396\n",
      "\tspeed: 0.0269s/iter; left time: 256.5858s\n",
      "\titers: 400, epoch: 10 | loss: 0.0133162\n",
      "\tspeed: 0.0273s/iter; left time: 257.1303s\n",
      "\titers: 500, epoch: 10 | loss: 0.0157871\n",
      "\tspeed: 0.0288s/iter; left time: 268.5654s\n",
      "\titers: 600, epoch: 10 | loss: 0.0145895\n",
      "\tspeed: 0.0285s/iter; left time: 263.4306s\n",
      "\titers: 700, epoch: 10 | loss: 0.0129983\n",
      "\tspeed: 0.0271s/iter; left time: 247.6666s\n",
      "\titers: 800, epoch: 10 | loss: 0.0133235\n",
      "\tspeed: 0.0264s/iter; left time: 238.5728s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 10\n",
      "Cost time: 00h:00m:24.83s\n",
      "Steps: 894 | Train Loss: 0.0138881 Vali Loss: 0.0178413 Test Loss: 0.0206092\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.0126372\n",
      "\tspeed: 0.0800s/iter; left time: 707.2913s\n",
      "\titers: 200, epoch: 11 | loss: 0.0157023\n",
      "\tspeed: 0.0269s/iter; left time: 234.8094s\n",
      "\titers: 300, epoch: 11 | loss: 0.0133373\n",
      "\tspeed: 0.0266s/iter; left time: 229.7069s\n",
      "\titers: 400, epoch: 11 | loss: 0.0126846\n",
      "\tspeed: 0.0254s/iter; left time: 216.6548s\n",
      "\titers: 500, epoch: 11 | loss: 0.0133657\n",
      "\tspeed: 0.0265s/iter; left time: 223.8852s\n",
      "\titers: 600, epoch: 11 | loss: 0.0136134\n",
      "\tspeed: 0.0289s/iter; left time: 240.8114s\n",
      "\titers: 700, epoch: 11 | loss: 0.0147949\n",
      "\tspeed: 0.0272s/iter; left time: 224.4693s\n",
      "\titers: 800, epoch: 11 | loss: 0.0124131\n",
      "\tspeed: 0.0278s/iter; left time: 226.1129s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 11\n",
      "Cost time: 00h:00m:24.40s\n",
      "Steps: 894 | Train Loss: 0.0135262 Vali Loss: 0.0176643 Test Loss: 0.0205082\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.0156275\n",
      "\tspeed: 0.0801s/iter; left time: 636.1584s\n",
      "\titers: 200, epoch: 12 | loss: 0.0131262\n",
      "\tspeed: 0.0268s/iter; left time: 210.5074s\n",
      "\titers: 300, epoch: 12 | loss: 0.0152973\n",
      "\tspeed: 0.0264s/iter; left time: 204.4881s\n",
      "\titers: 400, epoch: 12 | loss: 0.0148951\n",
      "\tspeed: 0.0266s/iter; left time: 203.5462s\n",
      "\titers: 500, epoch: 12 | loss: 0.0135750\n",
      "\tspeed: 0.0266s/iter; left time: 200.4590s\n",
      "\titers: 600, epoch: 12 | loss: 0.0122548\n",
      "\tspeed: 0.0271s/iter; left time: 201.5196s\n",
      "\titers: 700, epoch: 12 | loss: 0.0118699\n",
      "\tspeed: 0.0264s/iter; left time: 194.0689s\n",
      "\titers: 800, epoch: 12 | loss: 0.0155911\n",
      "\tspeed: 0.0265s/iter; left time: 191.8970s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 12\n",
      "Cost time: 00h:00m:24.10s\n",
      "Steps: 894 | Train Loss: 0.0132243 Vali Loss: 0.0177575 Test Loss: 0.0204768\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.0141740\n",
      "\tspeed: 0.0801s/iter; left time: 564.5991s\n",
      "\titers: 200, epoch: 13 | loss: 0.0122019\n",
      "\tspeed: 0.0275s/iter; left time: 191.2387s\n",
      "\titers: 300, epoch: 13 | loss: 0.0125042\n",
      "\tspeed: 0.0276s/iter; left time: 188.8835s\n",
      "\titers: 400, epoch: 13 | loss: 0.0144470\n",
      "\tspeed: 0.0276s/iter; left time: 186.4665s\n",
      "\titers: 500, epoch: 13 | loss: 0.0140528\n",
      "\tspeed: 0.0268s/iter; left time: 178.3611s\n",
      "\titers: 600, epoch: 13 | loss: 0.0145486\n",
      "\tspeed: 0.0267s/iter; left time: 174.6980s\n",
      "\titers: 700, epoch: 13 | loss: 0.0114644\n",
      "\tspeed: 0.0268s/iter; left time: 172.8622s\n",
      "\titers: 800, epoch: 13 | loss: 0.0102247\n",
      "\tspeed: 0.0270s/iter; left time: 171.2238s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 13\n",
      "Cost time: 00h:00m:24.44s\n",
      "Steps: 894 | Train Loss: 0.0129531 Vali Loss: 0.0179610 Test Loss: 0.0206518\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.0134476\n",
      "\tspeed: 0.0786s/iter; left time: 483.9784s\n",
      "\titers: 200, epoch: 14 | loss: 0.0135818\n",
      "\tspeed: 0.0280s/iter; left time: 169.6643s\n",
      "\titers: 300, epoch: 14 | loss: 0.0145175\n",
      "\tspeed: 0.0275s/iter; left time: 164.0849s\n",
      "\titers: 400, epoch: 14 | loss: 0.0135296\n",
      "\tspeed: 0.0286s/iter; left time: 167.3777s\n",
      "\titers: 500, epoch: 14 | loss: 0.0133688\n",
      "\tspeed: 0.0285s/iter; left time: 164.1828s\n",
      "\titers: 600, epoch: 14 | loss: 0.0123135\n",
      "\tspeed: 0.0286s/iter; left time: 161.6953s\n",
      "\titers: 700, epoch: 14 | loss: 0.0132166\n",
      "\tspeed: 0.0285s/iter; left time: 158.2549s\n",
      "\titers: 800, epoch: 14 | loss: 0.0112075\n",
      "\tspeed: 0.0272s/iter; left time: 148.2660s\n",
      "-------------------------------------------------------------------------------------\n",
      "Epoch: 14\n",
      "Cost time: 00h:00m:25.05s\n",
      "Steps: 894 | Train Loss: 0.0127357 Vali Loss: 0.0180731 Test Loss: 0.0209275\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "-------------------------------------------------------------------------------------\n",
      ">>>>>>>testing : IT_336_168_scaler_choice_for_IT_PatchTST_custom_ftM_sl336_ll48_pl168_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_lossMSE_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 6073\n",
      "Scaled mse:0.020144881680607796, rmse:0.1419326663017273, mae:0.08858419209718704, rse:0.5370334982872009\n",
      "Original data scale mse:2968210.25, rmse:1722.8494873046875, mae:1123.265625, rse:0.12135780602693558\n"
     ]
    }
   ],
   "source": [
    "# Dynamic variables\n",
    "pred_lens = [\"24\", \"96\", \"168\"]\n",
    "seq_len = \"336\"\n",
    "lr = \"0.0001\"\n",
    "model = \"PatchTST\"\n",
    "itr = 1 \n",
    "n_heads = \"16\"\n",
    "d_model = \"128\"\n",
    "d_ff = \"256\"\n",
    "dropout = \"0.2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "# New log file path\n",
    "log_file_path = f\"{log_dir}/{model}_{country}.log\"\n",
    "\n",
    "patchtst_results_scaled, patchtst_results_unscaled = [], []\n",
    "\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    for loss in losses:\n",
    "        statement_1 = f\"\\n=== Starting experiments for loss function: {loss} ===\\n\"\n",
    "        log_file.write(statement_1)\n",
    "        print(statement_1)  # Print to notebook\n",
    "\n",
    "        for pred_len in pred_lens:\n",
    "            statement_2 = f\"\\n=== Starting experiments for pred_len: {pred_len} ===\\n\"\n",
    "            log_file.write(statement_2)\n",
    "            print(statement_2) \n",
    "            model_id = f\"{country}_{seq_len}_{pred_len}_scaler_choice_for_{country}\"\n",
    "\n",
    "            # Command arguments\n",
    "            command = f\"\"\"\n",
    "            python {script_path} \\\n",
    "              --random_seed 2021 \\\n",
    "              --is_training 1 \\\n",
    "              --root_path \"{data_path}\" \\\n",
    "              --data_path \"{dataset}\" \\\n",
    "              --model_id {model_id} \\\n",
    "              --model \"{model}\" \\\n",
    "              --data \"custom\" \\\n",
    "              --features M \\\n",
    "              --seq_len {seq_len} \\\n",
    "              --pred_len {pred_len} \\\n",
    "              --e_layers 3 \\\n",
    "              --factor 1 \\\n",
    "              --enc_in 3 \\\n",
    "              --dec_in 3 \\\n",
    "              --c_out 3 \\\n",
    "              --des 'Exp' \\\n",
    "              --train_epochs 20 \\\n",
    "              --patience 5 \\\n",
    "              --n_heads {n_heads} \\\n",
    "              --d_model {d_model} \\\n",
    "              --d_ff {d_ff} \\\n",
    "              --dropout {dropout} \\\n",
    "              --fc_dropout {dropout} \\\n",
    "              --patch_len 32 \\\n",
    "              --stride 16 \\\n",
    "              --overlapping_windows \\\n",
    "              --inverse \\\n",
    "              --scaler_type minmax \\\n",
    "              --if_relu \\\n",
    "              --loss_fnc \"{loss}\" \\\n",
    "              --itr {itr} --batch_size 32 --learning_rate \"{lr}\"\n",
    "            \"\"\"\n",
    "\n",
    "            # Run the command and capture the output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            # Capture the output in real-time\n",
    "            output = []\n",
    "            for line in process.stdout:\n",
    "                output.append(line)\n",
    "                print(line, end='')  # Print in the .ipynb cell\n",
    "                log_file.write(line)  # Write to the log file\n",
    "\n",
    "            # Wait for the process to complete\n",
    "            process.wait()\n",
    "\n",
    "            # Delete the checkpoints folder and all its contents\n",
    "            shutil.rmtree('./checkpoints' )\n",
    "\n",
    "            # Extract metrics for each iteration\n",
    "            iteration_metrics_scaled = extract_metrics_from_output(output, itr)\n",
    "            iteration_metrics_unscaled = extract_metrics_from_output(output, itr, if_scaled=False)\n",
    "\n",
    "            # Log the extracted metrics and save them\n",
    "            for iteration, (scaled_metrics, unscaled_metrics) in enumerate(zip(iteration_metrics_scaled, iteration_metrics_unscaled), start=1):\n",
    "                log_file.write(f\"\\nExtracted Metrics for {country}, pred_len={pred_len}, iteration={iteration}:\\n\")\n",
    "                log_file.write(f\"Scaled Metrics - MSE: {scaled_metrics[0]}, RMSE: {scaled_metrics[1]}, MAE: {scaled_metrics[2]}, RSE: {scaled_metrics[3]}\\n\")\n",
    "                log_file.write(f\"Unscaled Metrics - MSE: {unscaled_metrics[0]}, RMSE: {unscaled_metrics[1]}, MAE: {unscaled_metrics[2]}, RSE: {unscaled_metrics[3]}\\n\")\n",
    "\n",
    "                # Append the results to the informer_results lists\n",
    "                metrics_data = [(patchtst_results_scaled, scaled_metrics), (patchtst_results_unscaled, unscaled_metrics)]\n",
    "\n",
    "                for result_list, metrics in metrics_data:\n",
    "                    result_list.append({\n",
    "                        'Loss_function': loss,\n",
    "                        'Pred_len': pred_len,\n",
    "                        'Iteration': iteration,\n",
    "                        'MSE': metrics[0],\n",
    "                        'RMSE': metrics[1],\n",
    "                        'MAE': metrics[2],\n",
    "                        'RSE': metrics[3]\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.3781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.1344</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.5370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MSE    RMSE     MAE     RSE\n",
       "Loss_function Iteration Pred_len                                \n",
       "MSE           1         24        0.0100  0.1001  0.0588  0.3781\n",
       "                        96        0.0181  0.1344  0.0829  0.5081\n",
       "                        168       0.0201  0.1419  0.0886  0.5370"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir = './results/scaler_choice'\n",
    "csv_name_scaled = 'patchtst_scaler_results_scaled_minmax_IT_default.csv'\n",
    "csv_name_unscaled = 'patchtst_scaler_results_unscaled_minmax_IT_default.csv'\n",
    "\n",
    "\n",
    "# Convert the results into a DataFrame and save as CSV\n",
    "patchtst_df_scaled = convert_results_into_df(patchtst_results_scaled, path_dir, csv_name_scaled)\n",
    "patchtst_df_unscaled = convert_results_into_df(patchtst_results_unscaled, path_dir, csv_name_unscaled)\n",
    "#patchtst_df_scaled = pd.read_csv(os.path.join(path_dir, csv_name_scaled))\n",
    "patchtst_df_scaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss_function</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Pred_len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">MSE</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>24</th>\n",
       "      <td>1266680.625</td>\n",
       "      <td>1125.4691</td>\n",
       "      <td>719.8215</td>\n",
       "      <td>0.0791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2552228.750</td>\n",
       "      <td>1597.5696</td>\n",
       "      <td>1041.3876</td>\n",
       "      <td>0.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2968210.250</td>\n",
       "      <td>1722.8495</td>\n",
       "      <td>1123.2656</td>\n",
       "      <td>0.1214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          MSE       RMSE        MAE     RSE\n",
       "Loss_function Iteration Pred_len                                           \n",
       "MSE           1         24        1266680.625  1125.4691   719.8215  0.0791\n",
       "                        96        2552228.750  1597.5696  1041.3876  0.1124\n",
       "                        168       2968210.250  1722.8495  1123.2656  0.1214"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchtst_results_scaled = pd.read_csv(os.path.join(path_dir, csv_name_scaled))\n",
    "patchtst_df_unscaled.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename folders\n",
    "new_path_name = 'minmax_IT'\n",
    "shutil.rmtree(\"results_loss_scaled\") # we do not need this directory and results anymore. If you need - comment this line\n",
    "os.rename(\"results_loss_unscaled\", new_path_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "val",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
