train_loss,valid_loss
0.973522,0.921482
0.920801,0.907851
0.906433,0.902502
0.898376,0.897510
0.891290,0.894471
0.887578,0.891222
0.884118,0.892764
0.881693,0.888436
0.881114,0.890150
0.879543,0.888721
0.878216,0.888462
0.877548,0.889949
0.876295,0.887886
0.876163,0.887568
0.734161,0.467601
0.508366,0.456450
0.495795,0.448361
0.486413,0.442441
0.481900,0.441055
0.477852,0.441403
0.476755,0.438993
0.473857,0.439539
0.472454,0.439049
0.471319,0.437631
0.470485,0.437510
0.469087,0.437644
0.467785,0.436688
0.468074,0.437769
0.462423,0.412443
0.430772,0.380746
0.405054,0.339776
0.371161,0.267742
0.318170,0.230100
0.286547,0.214349
0.266435,0.204241
0.254113,0.194981
0.244088,0.188132
0.236335,0.182725
0.230077,0.181212
0.225290,0.175527
0.218781,0.164185
0.208234,0.151276
0.196188,0.141474
0.187398,0.136978
0.181318,0.131690
0.176529,0.130103
0.172478,0.127742
0.169640,0.126386
0.167084,0.125278
0.164625,0.122028
0.162430,0.122076
0.160397,0.120799
0.157992,0.117797
0.156159,0.118970
0.154474,0.118813
0.153640,0.115178
0.152074,0.114859
0.150669,0.115538
0.149099,0.114100
0.147959,0.112603
0.146825,0.112068
0.145909,0.113246
0.145216,0.111762
0.144471,0.110678
0.143606,0.111776
0.142600,0.109782
0.142218,0.110467
0.142123,0.109543
0.140568,0.109822
0.140310,0.109917
0.139650,0.109063
0.139542,0.110224
0.138369,0.109040
0.138770,0.108346
0.137912,0.107585
0.137333,0.108983
0.137707,0.108208
0.137344,0.108019
0.137014,0.107742
0.136704,0.108392
0.136713,0.108110
